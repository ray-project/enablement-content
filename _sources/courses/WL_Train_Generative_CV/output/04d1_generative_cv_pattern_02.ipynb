{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and setup  \n",
    "Pull in standard Python utilities, Ray (core, Data, Train, Lightning), and PyTorch Lightning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var (safe to set here)\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies (same pinned versions as build.sh)\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"torchvision==0.23.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "    \"datasets==2.19.2\",\n",
    "    \"lightning==2.5.5\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# Standard libraries\n",
    "import os, io, json, shutil, tempfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Ray\n",
    "import ray, ray.data\n",
    "from ray.train import ScalingConfig, get_context, RunConfig, FailureConfig, CheckpointConfig, Checkpoint, get_checkpoint\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train.lightning import RayLightningEnvironment\n",
    "\n",
    "# PyTorch / Lightning\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm  \n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load 10 % of Food-101  \n",
    "Next, grab roughly 7 500 images, exactly 10 % of Food-101\u2014using a single call to `load_dataset`. This trimmed subset trains quickly while still being large enough to demonstrate Ray\u2019s scaling behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load 10% of food101 (~7,500 images)\n",
    "hf_ds = load_dataset(\"food101\", split=\"train[:10%]\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Resize and encode images  \n",
    "Use **Ray Data** to preprocess images in parallel across the cluster.  \n",
    "Convert each image to raw JPEG bytes (a serializable format) and then decoded, resized to 256 pixels, center-cropped to 224 pixels, and re-encoded.  \n",
    "Processing with Ray Data makes the pipeline distributed, fault-tolerant, and Parquet-friendly\u2014keeping the dataset compact while scaling efficiently across workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Resize + encode as JPEG bytes (Ray Data; BYTES-BASED)\n",
    "\n",
    "# Build Ray items with RAW BYTES (serializable) + label\n",
    "rows = []\n",
    "buf = io.BytesIO()\n",
    "for ex in hf_ds:\n",
    "    img = ex[\"image\"].convert(\"RGB\")\n",
    "    buf.seek(0); buf.truncate(0)\n",
    "    img.save(buf, format=\"JPEG\")\n",
    "    rows.append({\"image_bytes_raw\": buf.getvalue(), \"label\": ex[\"label\"]})\n",
    "\n",
    "# Create a Ray Dataset from serializable dicts\n",
    "ds = ray.data.from_items(rows)\n",
    "\n",
    "# Define preprocessing (runs on Ray workers)\n",
    "transform = Compose([Resize(256), CenterCrop(224)])\n",
    "\n",
    "def preprocess_images(batch_df):\n",
    "    out_img_bytes, out_labels = [], []\n",
    "    for b, lbl in zip(batch_df[\"image_bytes_raw\"], batch_df[\"label\"]):\n",
    "        try:\n",
    "            img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "            img = transform(img)\n",
    "            out = io.BytesIO()\n",
    "            img.save(out, format=\"JPEG\")\n",
    "            out_img_bytes.append(out.getvalue())\n",
    "            out_labels.append(lbl)\n",
    "        except Exception:\n",
    "            # Skip unreadable/corrupt rows but don't kill the batch\n",
    "            continue\n",
    "    return {\"image_bytes\": out_img_bytes, \"label\": out_labels}\n",
    "\n",
    "# Parallel preprocessing\n",
    "processed_ds = ds.map_batches(\n",
    "    preprocess_images,\n",
    "    batch_format=\"pandas\",\n",
    "    num_cpus=1,\n",
    ")\n",
    "\n",
    "print(\"\u2705 Processed records:\", processed_ds.count())\n",
    "processed_ds.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visual sanity check  \n",
    "Before committing to hours of training, take nine random samples and plot them with their class names. This quick inspection lets you confirm that images are correctly resized and preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Visualize the dataset (Ray Data version)\n",
    "label_names = hf_ds.features[\"label\"].names  # int -> class name\n",
    "\n",
    "samples = processed_ds.random_shuffle().take(9)\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n",
    "fig.suptitle(\"Sample Resized Images from food101-lite\", fontsize=16)\n",
    "\n",
    "for ax, rec in zip(axs.flatten(), samples):\n",
    "    img = Image.open(io.BytesIO(rec[\"image_bytes\"]))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(label_names[rec[\"label\"]])\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Persist to Parquet  \n",
    "Now, write the images and labels to a Parquet file. Because Parquet is columnar, you can read just the columns you need during training, which speeds up IO---especially when multiple workers are reading in parallel under Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Persist Ray Dataset to Parquet\n",
    "import os\n",
    "\n",
    "output_dir = \"/mnt/cluster_storage/food101_lite/parquet_256\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Write each block as its own Parquet shard\n",
    "processed_ds.write_parquet(output_dir)\n",
    "\n",
    "print(f\"\u2705 Wrote {processed_ds.count()} records to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load and decode with Ray Data  \n",
    "Read the Parquet shard into a **Ray Dataset**, decode the JPEG bytes to **Channel-Height-Width (CHW) float32 tensors**, scale to \\[-1, 1\\], and drop the original byte column.  \n",
    "Because `decode_and_normalize` is stateless, the default **task-based** execution is perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Load & Decode Food-101-Lite\n",
    "\n",
    "# Path to Parquet shards written earlier\n",
    "PARQUET_PATH = \"/mnt/cluster_storage/food101_lite/parquet_256\"\n",
    "\n",
    "# Read the Parquet files (\u22487\u202f500 rows with JPEG bytes + label)\n",
    "ds = ray.data.read_parquet(PARQUET_PATH)\n",
    "print(\"Raw rows:\", ds.count())\n",
    "\n",
    "# Decode JPEG \u2192 CHW float32 in [\u20111,\u202f1]\n",
    "\n",
    "def decode_and_normalize(batch_df):\n",
    "    \"\"\"Decode JPEG bytes and scale to [-1, 1].\"\"\"\n",
    "    images = []\n",
    "    for b in batch_df[\"image_bytes\"]:\n",
    "        img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "        arr = np.asarray(img, dtype=np.float32) / 255.0       # H\u202f\u00d7\u202fW\u202f\u00d7\u202f3, 0\u20111\n",
    "        arr = (arr - 0.5) / 0.5                               # \u20111\u202f\u2026\u202f1\n",
    "        arr = arr.transpose(2, 0, 1)                          # 3\u202f\u00d7\u202fH\u202f\u00d7\u202fW (CHW)\n",
    "        images.append(arr)\n",
    "    return {\"image\": images}\n",
    "\n",
    "# Apply in parallel\n",
    "#   batch_format=\"pandas\" \u2192 batch_df is a DataFrame, return dict of lists.\n",
    "#   default task\u2011based compute is sufficient for a stateless function.\n",
    "\n",
    "ds = ds.map_batches(\n",
    "    decode_and_normalize,\n",
    "    batch_format=\"pandas\",\n",
    "    # Use the default (task\u2011based) compute strategy since `decode_and_normalize` is a plain function.\n",
    "    num_cpus=1,\n",
    ")\n",
    "\n",
    "# Drop the original JPEG column to save memory\n",
    "if \"image_bytes\" in ds.schema().names:\n",
    "    ds = ds.drop_columns([\"image_bytes\", \"label\"])\n",
    "\n",
    "print(\"Decoded rows:\", ds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Shuffle and Train/Val split  \n",
    "Perform a reproducible shuffle, then split 80 % / 20 % into `train_ds` and `val_ds`.  \n",
    "Each split remains a first-class Ray Dataset, enabling distributed, sharded DataLoaders later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Shuffle & Train/Val Split\n",
    "\n",
    "# Typical 80\u202f/\u202f20 split\n",
    "TOTAL = ds.count()\n",
    "train_count = int(TOTAL * 0.8)\n",
    "ds = ds.random_shuffle()  # expensive operation -- for large datasets, consider file shuffling or local shuffling. Ray offers both options\n",
    "train_ds, val_ds = ds.split_at_indices([train_count])\n",
    "print(\"Train rows:\", train_ds.count())\n",
    "print(\"Val rows:\",   val_ds.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}