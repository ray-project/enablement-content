{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced LLM Features with Ray Serve LLM\n",
        "\n",
        "¬© 2025, Anyscale. All Rights Reserved\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üíª **Launch Locally**: You can run this notebook locally, but you'll need access to GPUs.\n",
        "\n",
        "üöÄ **Launch on Cloud**: A Ray Cluster with GPUs (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook explores advanced features and capabilities of Ray Serve LLM beyond basic model deployment. We'll dive into practical examples that showcase the power and flexibility of production LLM serving.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b> Here is the roadmap for this notebook:</b>\n",
        "<ul>\n",
        "    <li>Overview: Advanced Features Preview</li>\n",
        "    <li>Example: Deploying LoRA Adapters</li>\n",
        "    <li>Example: Getting Structured JSON Output</li>\n",
        "    <li>Example: Setting up Tool Calling</li>\n",
        "    <li>How to Choose an LLM?</li>\n",
        "    <li>Conclusion: Next Steps</li>\n",
        "</ul>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview: Advanced Features Preview\n",
        "\n",
        "Now that you've mastered the basics of LLM deployment with Ray Serve LLM, let's explore some advanced features that make production LLM serving more powerful and flexible.\n",
        "\n",
        "### What We'll Cover\n",
        "\n",
        "In this module, we'll focus on **3 practical examples** that demonstrate advanced capabilities:\n",
        "\n",
        "1. **LoRA Adapters**: Deploy multiple fine-tuned adapters on a single base model\n",
        "2. **Structured Output**: Generate consistent JSON and other structured formats\n",
        "3. **Tool Calling**: Enable models to call external functions and APIs\n",
        "\n",
        "### Why These Features Matter\n",
        "\n",
        "**LoRA Adapters** allow you to:\n",
        "- Serve multiple specialized models from one base model\n",
        "- Reduce memory usage and deployment complexity\n",
        "- Switch between different fine-tuned behaviors at runtime\n",
        "\n",
        "**Structured Output** enables:\n",
        "- Consistent, parseable responses for applications\n",
        "- Integration with downstream systems\n",
        "- Better reliability for production use cases\n",
        "\n",
        "**Tool Calling** provides:\n",
        "- Integration with external APIs and databases\n",
        "- Enhanced model capabilities through function execution\n",
        "- Building more sophisticated AI applications\n",
        "\n",
        "### Learning Approach\n",
        "\n",
        "We'll take a **hands-on approach** - each example will show you:\n",
        "- Why the feature is useful\n",
        "- How to configure it\n",
        "- Working code you can run\n",
        "- Links to comprehensive guides for deeper learning\n",
        "\n",
        "Let's dive in!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Deploying LoRA Adapters\n",
        "\n",
        "LoRA (Low-Rank Adaptation) adapters are small, efficient fine-tuned models that can be loaded on top of a base model. This allows you to serve multiple specialized behaviors from a single deployment.\n",
        "\n",
        "### Why Use LoRA Adapters?\n",
        "\n",
        "- **Parameter Efficiency**: LoRA adapters are typically less than 1% of the base model's size\n",
        "- **Runtime Adaptation**: Switch between different adapters without reloading the base model\n",
        "- **Simpler MLOps**: Centralize inference around one model while supporting multiple use cases\n",
        "- **Cost Effective**: Share expensive base model across multiple specialized tasks\n",
        "\n",
        "### Example: Code Assistant LoRA\n",
        "\n",
        "Let's deploy a base model with multiple LoRA adapters. This will allow the model to switch between general and specialized generation.\n",
        "\n",
        "For this example, we'll use publicly available adapters from Hugging Face.\n",
        "\n",
        "First, we need to prepare our LoRA adapters and save them in our cloud storage. \n",
        "\n",
        "For example, here is an example script for downloading adapters from Huggingface and saving them in an AWS bucket:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# Mapping of custom names to Hugging Face LoRA adapter repo IDs\n",
        "adapters = {\n",
        "    \"nemoguard\": \"nvidia/llama-3.1-nemoguard-8b-topic-control\",\n",
        "    \"cv_job_matching\": \"LlamaFactoryAI/Llama-3.1-8B-Instruct-cv-job-description-matching\",\n",
        "    \"yara\": \"vtriple/Llama-3.1-8B-yara\"\n",
        "}\n",
        "\n",
        "# S3 target\n",
        "bucket_name = \"llm-docs-aydin\"\n",
        "base_s3_path = \"1-5-multi-lora/lora_checkpoints\"\n",
        "\n",
        "# Initialize S3 client\n",
        "s3 = boto3.client(\"s3\")\n",
        "\n",
        "for custom_name, repo_id in adapters.items():\n",
        "    print(f\"\\nüì• Downloading adapter '{custom_name}' from {repo_id}...\")\n",
        "    local_path = snapshot_download(repo_id)\n",
        "\n",
        "    print(f\"‚¨ÜÔ∏è Uploading files to s3://{bucket_name}/{base_s3_path}/{custom_name}/\")\n",
        "\n",
        "    for root, _, files in os.walk(local_path):\n",
        "        for file_name in files:\n",
        "            local_file_path = os.path.join(root, file_name)\n",
        "            rel_path = os.path.relpath(local_file_path, local_path)\n",
        "            s3_key = f\"{base_s3_path}/{custom_name}/{rel_path}\".replace(\"\\\\\", \"/\")\n",
        "\n",
        "            print(f\"  ‚Üí {s3_key}\")\n",
        "            s3.upload_file(local_file_path, bucket_name, s3_key)\n",
        "\n",
        "print(\"\\n‚úÖ All adapters uploaded successfully.\")\n",
        "\n",
        "# List all objects in the bucket to confirm\n",
        "response = s3.list_objects_v2(Bucket=bucket_name)\n",
        "\n",
        "print(f\"Files in s3://{bucket_name}/:\")\n",
        "for obj in response[\"Contents\"]:\n",
        "    print(obj[\"Key\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should end up with this folder structure for each adapter.\n",
        "```\n",
        "s3://your-bucket/lora-adapters/\n",
        "‚îú‚îÄ‚îÄ nemoguard/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ adapter_model.safetensors\n",
        "‚îú‚îÄ‚îÄ cv_job_matching/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ adapter_model.safetensors\n",
        "‚îú‚îÄ‚îÄ yara/\n",
        "    ‚îú‚îÄ‚îÄ adapter_config.json\n",
        "    ‚îî‚îÄ‚îÄ adapter_model.safetensors\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Ray Serve LLM with LoRA\n",
        "\n",
        "Now let's configure our LLM with LoRA support. The key additions are the `lora_config` and enabling LoRA in the engine arguments:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from ray.serve.llm import LLMConfig, build_openai_app\n",
        "\n",
        "# Configure LLM with LoRA support\n",
        "llm_config = LLMConfig(\n",
        "    model_loading_config=dict(\n",
        "        model_id=\"my-llama\",\n",
        "        # Make sure your huggingface token has access/authorization\n",
        "        # Go to https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct and request access otherwise\n",
        "        # Or switch to the unsloth/ version for an ungated LLama \n",
        "        model_source=\"meta-llama/Llama-3.1-8B-Instruct\" # Base model\n",
        "    ),\n",
        "    accelerator_type=\"L4\",\n",
        "    # LoRA configuration\n",
        "    lora_config=dict(\n",
        "        dynamic_lora_loading_path=\"s3://llm-docs-aydin/1-5-multi-lora/lora_checkpoints/\",  # Your S3/GCS path\n",
        "        max_num_adapters_per_replica=3  # (optional) Limit adapters per replica\n",
        "    ),\n",
        "    runtime_env=dict(\n",
        "        env_vars={\n",
        "            \"HF_TOKEN\": os.environ.get(\"HF_TOKEN\"), # Set your token beforehand: export HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>\n",
        "            \"AWS_REGION\": \"us-west-2\"  # Your AWS region\n",
        "        }\n",
        "    ),\n",
        "    engine_kwargs=dict(\n",
        "        max_model_len=8192,\n",
        "        # Enable LoRA support\n",
        "        enable_lora=True,\n",
        "        max_lora_rank=32,  # Maximum LoRA rank. Set to the largest rank you plan to use.\n",
        "        max_loras=3,  # Must match max_num_adapters_per_replica\n",
        "    ),\n",
        ")\n",
        "\n",
        "app = build_openai_app({\"llm_configs\": [llm_config]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve run serve_my_lora_app:app --non-blocking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using LoRA Adapters\n",
        "\n",
        "Once deployed, you can query different adapters by specifying them in the model name using the format `<base_model_id>:<adapter_name>`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#client.py\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"FAKE_KEY\")\n",
        "\n",
        "############################ Base model request (no adapter) #####################\n",
        "print(\"=== Base model ===\")\n",
        "response = client.chat.completions.create(\n",
        "    model=\"my-llama\",  # no adapter\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
        "    stream=True,\n",
        ")\n",
        "for chunk in response:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "############################ nemoguard adapter (moderation) #####################\n",
        "print(\"=== LoRA: nemoguard ===\")\n",
        "# As per Nemoguard's usage instruction, add this to your system prompt\n",
        "# https://huggingface.co/nvidia/llama-3.1-nemoguard-8b-topic-control#system-instruction\n",
        "TOPIC_SAFETY_OUTPUT_RESTRICTION = 'If any of the above conditions are violated, please respond with \"off-topic\". Otherwise, respond with \"on-topic\". You must respond with \"on-topic\" or \"off-topic\".'\n",
        "messages_nemoguard = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": f'In the next conversation always use a polite tone and do not engage in any talk about travelling and touristic destinations.{TOPIC_SAFETY_OUTPUT_RESTRICTION}',\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"Do you know which is the most popular beach in Barcelona?\"},\n",
        "]\n",
        "#response = client.chat.completions.create(\n",
        "##    model=\"my-llama:nemoguard\", ### with nemoguard adapter\n",
        " #   messages=messages_nemoguard,\n",
        " #   stream=True\n",
        "#)\n",
        "for chunk in response:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "############################ cv_job_matching adapter (structured JSON output) ############################\n",
        "print(\"=== LoRA: cv_job_matching ===\")\n",
        "messages_cv = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are an advanced AI model designed to analyze the compatibility between a CV and a job description. You will receive a CV and a job description. Your task is to output a structured JSON format that includes the following:\n",
        "\n",
        "1. matching_analysis: Analyze the CV against the job description to identify key strengths and gaps.\n",
        "2. description: Summarize the relevance of the CV to the job description in a few concise sentences.\n",
        "3. score: Provide a numerical compatibility score (0-100) based on qualifications, skills, and experience.\n",
        "4. recommendation: Suggest actions for the candidate to improve their match or readiness for the role.\n",
        "\n",
        "Your output must be in JSON format as follows:\n",
        "{\n",
        "  \"matching_analysis\": \"Your detailed analysis here.\",\n",
        "  \"description\": \"A brief summary here.\",\n",
        "  \"score\": 85,\n",
        "  \"recommendation\": \"Your suggestions here.\"\n",
        "}\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"<CV> Software engineer with 5 years of experience in Python and cloud infrastructure. </CV>\\n<job_description> Looking for a backend engineer with Python and AWS experience. </job_description>\",\n",
        "    },\n",
        "]\n",
        "response = client.chat.completions.create(\n",
        "    model=\"my-llama:cv_job_matching\", ### with cv_job_matching adapter\n",
        "    messages=messages_cv,\n",
        "    stream=True,\n",
        ")\n",
        "for chunk in response:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "############################ yara adapter (cybersecurity task) ############################\n",
        "print(\"=== LoRA: yara ===\")\n",
        "messages_yara = [{\"role\": \"user\", \"content\": \"Generate a YARA rule to detect a PowerShell-based keylogger. Generate ONLY the YARA rule, do not add explanations.\"}]\n",
        "response = client.chat.completions.create(\n",
        "    model=\"my-llama:yara\", ### with yara adapter\n",
        "    messages=messages_yara,\n",
        "    stream=True,\n",
        ")\n",
        "for chunk in response:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Shutdown the deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve shutdown -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Benefits\n",
        "\n",
        "- **Single Deployment**: One base model serves multiple specialized behaviors\n",
        "- **Dynamic Switching**: Change adapters at runtime without restarting\n",
        "- **Memory Efficient**: Adapters are much smaller than full fine-tuned models\n",
        "- **Cost Effective**: Share expensive base model across multiple use cases\n",
        "\n",
        "### Learn More\n",
        "\n",
        "For comprehensive multi-LoRA deployment guides, see:\n",
        "- [Multi-LoRA deployment guide on Anyscale](https://docs.anyscale.com/llm/serving/multi-lora) - Complete guide with best practices\n",
        "- [Multi-LoRA with Ray Serve LLM (Ray docs)](https://docs.ray.io/en/latest/serve/llm/user-guides/multi-lora.html) - Quick-start configuration details\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Getting Structured JSON Output\n",
        "\n",
        "Many applications need consistent, parseable output from LLMs. Ray Serve LLM supports structured output generation, ensuring your model returns data in the exact format you need.\n",
        "\n",
        "### Why Structured Output Matters\n",
        "\n",
        "- **Consistent Format**: Guaranteed JSON structure for downstream processing\n",
        "- **Integration Ready**: Easy to parse and use in applications\n",
        "- **Reliability**: Reduces parsing errors and improves system robustness\n",
        "- **Type Safety**: Enforces data types and required fields\n",
        "\n",
        "### Example: Car type description\n",
        "\n",
        "Let's deploy a model. It is recommended to research the performance of your model in structured output benchmarks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```yaml\n",
        "# serve_my_qwen.yaml\n",
        "applications:\n",
        "- name: json-mode-app\n",
        "  route_prefix: \"/\"\n",
        "  import_path: ray.serve.llm:build_openai_app\n",
        "  args:\n",
        "    llm_configs:\n",
        "      - model_loading_config:\n",
        "          model_id: my-qwen\n",
        "          model_source: Qwen/Qwen2.5-3B-Instruct\n",
        "        accelerator_type: L4\n",
        "        ### Uncomment if your model is gated and need your Huggingface Token to access it\n",
        "        #runtime_env:\n",
        "        #  env_vars:\n",
        "        #    HF_TOKEN: <YOUR-TOKEN-HERE>\n",
        "        engine_kwargs:\n",
        "          max_model_len: 8192\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve run serve_my_qwen.yaml --non-blocking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Structured Output\n",
        "\n",
        "Now let's test our structured output model with some product descriptions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#json_method1.py\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel\n",
        "from enum import Enum\n",
        "\n",
        "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"FAKE_KEY\")\n",
        "\n",
        "# (Optional) We use Pydantic model to handle schema definition/validation\n",
        "class CarType(str, Enum):\n",
        "    sedan = \"sedan\"\n",
        "    suv = \"SUV\"\n",
        "    truck = \"Truck\"\n",
        "    coupe = \"Coupe\"\n",
        "\n",
        "class CarDescription(BaseModel):\n",
        "    brand: str\n",
        "    model: str\n",
        "    car_type: CarType\n",
        "\n",
        "# 1. Define your schema\n",
        "json_schema = CarDescription.model_json_schema()\n",
        "\n",
        "# 2. Send a request\n",
        "response = client.chat.completions.create(\n",
        "    model=\"my-qwen\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate a JSON with the brand, model and car_type of the most iconic car from the 90's\",\n",
        "        }\n",
        "    ],\n",
        "    # 3. Set `response_format` of type `json_schema`\n",
        "    response_format= {\n",
        "        \"type\": \"json_schema\",\n",
        "        # 4. Provide `name`and `schema` (both required)\n",
        "        \"json_schema\": {\n",
        "            \"name\": \"car-description\", # arbitrary\n",
        "            \"schema\": json_schema # your JSON schema\n",
        "        },\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expected Output\n",
        "\n",
        "The model will return a consistent JSON structure like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"brand\": \"Lexus\",\n",
        "  \"model\": \"IS F\",\n",
        "  \"car_type\": \"SUV\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Shutdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve shutdown -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Benefits\n",
        "\n",
        "- **Guaranteed Structure**: Always returns valid JSON matching your schema\n",
        "- **Type Safety**: Enforces data types (strings, numbers, arrays)\n",
        "- **Required Fields**: Ensures all specified fields are present\n",
        "- **Easy Integration**: Directly usable in applications without parsing\n",
        "\n",
        "### Learn More\n",
        "\n",
        "For comprehensive structured output guides, see:\n",
        "- [LLM deployment with structured output on Anyscale](https://docs.anyscale.com/llm/serving/structured-output) - Complete guide with all output formats\n",
        "- [Request structured output (vLLM documentation)](https://docs.vllm.ai/en/stable/features/structured_outputs.html) - Complete guide on vLLM API for structured outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Setting up Tool Calling\n",
        "\n",
        "Tool calling enables LLMs to interact with external functions, APIs, and databases. This opens up powerful possibilities for building sophisticated AI applications that can perform actions beyond just text generation.\n",
        "\n",
        "### Why Tool Calling Matters\n",
        "\n",
        "- **Enhanced Capabilities**: Models can perform actions, not just generate text\n",
        "- **Real-time Data**: Access current information from APIs and databases\n",
        "- **Workflow Automation**: Integrate AI into existing business processes\n",
        "- **Interactive Applications**: Build chatbots that can actually do things\n",
        "\n",
        "### Example: Weather Assistant with Tool Calling\n",
        "\n",
        "Let's create a model that can check weather information by calling a weather API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# serve_my_qwen3.py\n",
        "from ray.serve.llm import LLMConfig, build_openai_app\n",
        "\n",
        "llm_config = LLMConfig(\n",
        "    model_loading_config=dict(\n",
        "        model_id=\"my-qwen3\",\n",
        "        model_source=\"Qwen/Qwen3-32B\",\n",
        "    ),\n",
        "    accelerator_type=\"L40S\",\n",
        "    deployment_config=dict(\n",
        "        autoscaling_config=dict(\n",
        "            min_replicas=1,\n",
        "            max_replicas=2,\n",
        "        )\n",
        "    ),\n",
        "    ### Uncomment if your model is gated and needs your Hugging Face token to access it.\n",
        "    # runtime_env=dict(env_vars={\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}),\n",
        "    engine_kwargs=dict(\n",
        "        tensor_parallel_size=4, \n",
        "        max_model_len=32768, \n",
        "        reasoning_parser=\"qwen3\",\n",
        "        enable_auto_tool_choice= True,\n",
        "        tool_call_parser= \"hermes\"\n",
        "    ),\n",
        ")\n",
        "app = build_openai_app({\"llm_configs\": [llm_config]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve run serve_my_qwen3:app --non-blocking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Tool Calling\n",
        "\n",
        "Now let's test our tool-calling model. The model will decide when to call tools and provide the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tool_call_client.py\n",
        "import random\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "# Dummy APIs\n",
        "def get_current_temperature(location: str, unit: str = \"celsius\"):\n",
        "    temperature = random.randint(15, 30) if unit == \"celsius\" else random.randint(59, 86)\n",
        "    return {\n",
        "        \"temperature\": temperature,\n",
        "        \"location\": location,\n",
        "        \"unit\": unit\n",
        "    }\n",
        "\n",
        "def get_temperature_date(location: str, date: str, unit: str = \"celsius\"):\n",
        "    temperature = random.randint(15, 30) if unit == \"celsius\" else random.randint(59, 86)\n",
        "    return {\n",
        "        \"temperature\": temperature,\n",
        "        \"location\": location,\n",
        "        \"date\": date,\n",
        "        \"unit\": unit\n",
        "    }\n",
        "\n",
        "# Tools schema definitions\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_temperature\",\n",
        "            \"description\": \"Get current temperature at a location.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"\n",
        "                    },\n",
        "                    \"unit\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_temperature_date\",\n",
        "            \"description\": \"Get temperature at a location and date.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"\n",
        "                    },\n",
        "                    \"date\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The date to get the temperature for, in the format \\\"Year-Month-Day\\\".\"\n",
        "                    },\n",
        "                    \"unit\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\", \"date\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "######################### Sending request for tool calls #########################\n",
        "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"FAKE_KEY\")\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a weather assistant. Use the given functions to get weather data and provide the results.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the temperature in San Francisco now? How about tomorrow? Current Date: 2025-07-29.\"\n",
        "    }\n",
        "]\n",
        "response = client.chat.completions.create(\n",
        "    model=\"my-qwen3\",\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice= \"auto\" # let the model decide to use tools or not\n",
        ")\n",
        "\n",
        "######################### Process tool calls #########################\n",
        "for tc in response.choices[0].message.tool_calls:\n",
        "    print(f\"Tool call id: {tc.id}\")\n",
        "    print(f\"Tool call function name: {tc.function.name}\")\n",
        "    print(f\"Tool call arguments: {tc.function.arguments}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Helper tool map (str -> python callable to your APIs)\n",
        "helper_tool_map = {\n",
        "    \"get_current_temperature\": get_current_temperature,\n",
        "    \"get_temperature_date\": get_temperature_date\n",
        "}\n",
        "\n",
        "######################### Add your model's tool calls request to the chat history #########################\n",
        "# `response` is your model's last response containing the tool calls it requests.\n",
        "# Add the previous response containing the tool calls\n",
        "messages.append(response.choices[0].message.model_dump())\n",
        "\n",
        "######################### Add `tool` messages in your chat history #########################\n",
        "# Loop through the tool calls and create `tool` messages\n",
        "for tool_call in response.choices[0].message.tool_calls:\n",
        "    call_id, fn_call = tool_call.id, tool_call.function\n",
        "    \n",
        "    fn_callable = helper_tool_map[fn_call.name]\n",
        "    fn_args = json.loads(fn_call.arguments)\n",
        "\n",
        "    output = json.dumps(fn_callable(**fn_args))\n",
        "\n",
        "    # Create a new message of role `\"tool\"` containing the output of your tool\n",
        "    messages.append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": output,\n",
        "        \"tool_call_id\": call_id\n",
        "    })\n",
        "\n",
        "######################### Sending final request #########################\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"my-qwen3\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve shutdown -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Benefits\n",
        "\n",
        "- **Intelligent Tool Selection**: Model decides when and which tools to use\n",
        "- **Structured Parameters**: Tools receive properly formatted arguments\n",
        "- **Seamless Integration**: Natural conversation flow with tool execution\n",
        "- **Extensible**: Easy to add new tools and capabilities\n",
        "\n",
        "### Learn More\n",
        "\n",
        "For comprehensive tool calling guides, see:\n",
        "- [LLM deployment with tool and function calling on Anyscale](https://docs.anyscale.com/llm/serving/tool-function-calling) - Complete tool calling setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Choose an LLM?\n",
        "\n",
        "With so many models available, choosing the right one for your use case is crucial. Here's a practical framework for model selection based on the [Anyscale documentation](https://docs.anyscale.com/llm/serving/intro#selecting-model).\n",
        "\n",
        "### Model Selection Framework\n",
        "\n",
        "#### 1. **Model Quality Benchmarks**\n",
        "\n",
        "Use established benchmarks to evaluate model capabilities:\n",
        "\n",
        "- **Chatbot Arena**: For conversational capabilities and user preference\n",
        "- **MMLU-Pro**: For domain-specific performance across academic subjects\n",
        "- **Code Benchmarks**: For programming and code generation tasks\n",
        "- **Reasoning Tests**: For logical reasoning and problem-solving\n",
        "\n",
        "#### 2. **Task and Domain Alignment**\n",
        "\n",
        "Match your model to your specific use case:\n",
        "\n",
        "| Model Type | Best For | Example Use Cases |\n",
        "|------------|----------|-------------------|\n",
        "| **Base Models** | Next-token prediction, open-ended continuation | Sentence completion, code autocomplete |\n",
        "| **Instruction-tuned** | Following explicit directions | Chatbots, coding assistants, Q&A |\n",
        "| **Reasoning-optimized** | Complex problem-solving | Mathematical reasoning, scientific analysis |\n",
        "\n",
        "\n",
        "#### 3. **Context Window Requirements**\n",
        "\n",
        "Match context length to your use case:\n",
        "\n",
        "| Context Length | Use Cases | Memory Impact |\n",
        "|----------------|-----------|---------------|\n",
        "| **4K-8K tokens** | Q&A, simple chat | Low memory requirements |\n",
        "| **32K-128K tokens** | Document analysis, summarization | Moderate memory usage |\n",
        "| **128K+ tokens** | Multi-step agents, complex reasoning | High memory requirements |\n",
        "\n",
        "#### 4. **Hardware and Cost Considerations**\n",
        "\n",
        "Balance performance with resource constraints:\n",
        "\n",
        "- **Small Models (7B-13B)**: 1-2 GPUs, fast deployment, lower cost\n",
        "- **Medium Models (70B-80B)**: 4-8 GPUs, balanced performance/cost\n",
        "- **Large Models (400B+)**: Multiple nodes, maximum capability, higher cost\n",
        "\n",
        "### Practical Selection Process\n",
        "\n",
        "1. **Define Requirements**: Latency, accuracy, context length, budget\n",
        "2. **Benchmark Models**: Test on your specific tasks and data\n",
        "3. **Consider Trade-offs**: Speed vs. accuracy, cost vs. capability\n",
        "4. **Start Simple**: Begin with smaller models, scale up as needed\n",
        "5. **Iterate and Optimize**: Monitor performance and adjust accordingly\n",
        "\n",
        "### Model Recommendations by Use Case\n",
        "\n",
        "**For Production Chatbots:**\n",
        "- Llama 3.1 8B/70B (balanced performance)\n",
        "- Mistral 7B (fast inference)\n",
        "\n",
        "**For Code Generation:**\n",
        "- Code Llama 7B/13B (specialized for code)\n",
        "- DeepSeek-Coder (reasoning + code)\n",
        "\n",
        "**For Complex Reasoning:**\n",
        "- Qwen 3 32B (hybrid thinking)\n",
        "- DeepSeek-R1 (dedicated reasoning)\n",
        "\n",
        "**For Document Processing:**\n",
        "- Llama 3.1 70B (large context)\n",
        "- Claude 3.5 Sonnet (excellent long context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion: Next Steps\n",
        "\n",
        "Congratulations! You've now explored advanced features of Ray Serve LLM and learned how to deploy sophisticated LLM applications. Let's summarize what we've covered and look ahead to even more possibilities.\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "**Module 3 Summary:**\n",
        "1. **LoRA Adapters**: Deployed multiple specialized models from a single base model\n",
        "2. **Structured Output**: Generated consistent JSON and structured data formats\n",
        "3. **Tool Calling**: Enabled models to interact with external functions and APIs\n",
        "4. **Model Selection**: Learned a framework for choosing the right LLM for your use case\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Advanced Features**: Ray Serve LLM supports sophisticated production capabilities\n",
        "- **Practical Examples**: Each feature has real-world applications and benefits\n",
        "- **Easy Integration**: Advanced features build on the same foundation as basic deployment\n",
        "- **Production Ready**: All features are designed for scalable, reliable deployments\n",
        "\n",
        "### More Advanced Topics\n",
        "\n",
        "Ready to dive deeper? Here are additional areas to explore:\n",
        "\n",
        "**Performance & Optimization:**\n",
        "- [Choose a GPU for LLM serving](https://docs.anyscale.com/llm/serving/gpu-guidance) - Hardware selection and optimization\n",
        "- [Tune parameters for LLMs](https://docs.anyscale.com/llm/serving/parameter-tuning) - Advanced configuration tuning\n",
        "- [Troubleshoot LLM serving](https://docs.anyscale.com/llm/serving/troubleshooting) - Common issues and solutions\n",
        "- [Optimize performance for Ray Serve LLM](https://docs.anyscale.com/llm/serving/performance-optimization) - Performance optimization guide\n",
        "\n",
        "**Enterprise Features:**\n",
        "- **Monitoring & Observability**: Advanced metrics and debugging tools\n",
        "- **Security & Compliance**: Enterprise-grade security features\n",
        "- **CI/CD Integration**: Automated deployment and testing pipelines\n",
        "- **Multi-tenant Deployments**: Serve multiple customers from shared infrastructure\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Practice**: Try deploying your own models with these advanced features\n",
        "2. **Explore**: Dive into the comprehensive guides we've linked\n",
        "3. **Build**: Create real applications using what you've learned\n",
        "4. **Share**: Join the Ray community and share your experiences\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Ray Serve LLM Documentation](https://docs.ray.io/en/latest/serve/llm/index.html)\n",
        "- [Anyscale LLM Serving Guide](https://docs.anyscale.com/llm/serving)\n",
        "- [Ray Community Forum](https://discuss.ray.io/)\n",
        "- [Anyscale Console](https://console.anyscale.com/) - Deploy your models\n",
        "\n",
        "**Course Complete** üéâ\n",
        "\n",
        "Thank you for learning with us! You're now ready to build amazing LLM applications with Ray Serve LLM.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
