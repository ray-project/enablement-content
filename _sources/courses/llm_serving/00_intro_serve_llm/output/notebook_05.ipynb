{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve LLM + Anyscale Architecture\n",
    "\n",
    "Here is a diagram of how Ray Serve LLM + Anyscale provides a production-grade solution to your LLM deployment:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/public-images/ray-serve-llm/diagrams/anyscale-serve-vllm.png\" width=\"800\">\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- The above shows only one replica per model, but Ray Serve can easily scale to deploying multiple replicas.\n",
    "\n",
    "Ray Serve LLM + Anyscale provides a production-grade solution through three integrated components:\n",
    "\n",
    "### 1. Ray Serve for Orchestration\n",
    "\n",
    "Ray Serve handles the **orchestration and scaling** of your LLM deployment:\n",
    "\n",
    "- **Automatic scaling**: Adds/removes model replicas based on traffic\n",
    "- **Load balancing**: Distributes requests across available replicas\n",
    "- **Unified multi-model deployment**: Deploy and manage multiple models\n",
    "- **OpenAI-compatible API**: Drop-in replacement for OpenAI clients\n",
    "\n",
    "Here is a diagram of how Ray Serve LLM interact with a client's request\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/Ray+Serve+LLM.png\" width=\"800\">\n",
    "\n",
    "### 2. vLLM as the inference engine\n",
    "\n",
    "LLM inference is a non-trivial problem that requires tuning low-level hardware use and high-level algorithms. An **inference engine** abstracts this complexity and optimizes model execution. Ray Serve LLM natively integrates **vLLM** as its inference engine for several reasons:\n",
    "\n",
    "- **Fast GPU computation** with CUDA kernels specifically optimized for LLM inference.\n",
    "- **Continuous batching**: Continuously schedule tokens to be processed to maximize GPU utilization.\n",
    "- **Smart memory use**: Optimize memory usage with state-of-the-art algorithms like PagedAttention\n",
    "\n",
    "Ray Serve LLM gives you high flexibility on how to configure your vLLM engine (more on that later).\n",
    "\n",
    "### 3. Anyscale for Infrastructure\n",
    "\n",
    "Anyscale provides **managed infrastructure** and enterprise features:\n",
    "\n",
    "- **Managed infrastructure**: Optimized Ray clusters in your cloud\n",
    "- **Cost optimization**: Pay-as-you-go, scale-to-zero\n",
    "- **Enterprise security**: VPC, SSO, audit logs\n",
    "- **Seamless scaling**: Handle traffic spikes automatically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}