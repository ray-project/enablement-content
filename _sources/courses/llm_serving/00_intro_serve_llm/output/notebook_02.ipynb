{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LLM Serving?\n",
    "\n",
    "Large Language Model (LLM) serving refers to the process of deploying trained language models to production environments where they can handle user requests and generate responses in real-time. This is fundamentally different from training models - serving focuses on making models available, scalable, and performant for end users.\n",
    "\n",
    "### The LLM Text Generation Process\n",
    "\n",
    "LLMs operate as **next-token predictors**. Here's how they work:\n",
    "\n",
    "1. **Tokenization**: Input text is converted into tokens (words, subwords, or characters)\n",
    "2. **Processing**: The model processes these tokens to understand context\n",
    "3. **Generation**: The model generates output one token at a time\n",
    "4. **Completion**: Generation stops when reaching stopping criteria or maximum length\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/public-images/ray-serve-llm/diagrams/LLM-text-generation.png\" width=\"800\">\n",
    "\n",
    "### Two Phases of LLM Inference\n",
    "\n",
    "LLM inference operates through two distinct phases that determine performance characteristics:\n",
    "\n",
    "#### Prefill Phase\n",
    "- The model encodes **all input tokens simultaneously**\n",
    "- High efficiency through parallelized computations\n",
    "- Maximizes GPU utilization\n",
    "- Precomputes and caches key-value (KV) vectors as intermediate token representations\n",
    "\n",
    "#### Decode Phase\n",
    "- The model generates tokens **sequentially** using the key-value cache (KV cache)\n",
    "- Each token depends on all previous tokens\n",
    "- Limited by memory bandwidth rather than compute capacity\n",
    "- Underutilizes GPU resources compared to prefill phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://cdn-uploads.huggingface.co/production/uploads/65263bfb3177c2a794997821/BGKtYLqM1X9o72oc9NW8Y.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|prefill: parallel processing of prompt tokens, decode: sequential processing of single output tokens.|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}