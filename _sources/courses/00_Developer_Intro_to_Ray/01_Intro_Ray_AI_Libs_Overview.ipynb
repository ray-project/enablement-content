{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model\n",
    "\n",
    "Â© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’» **Launch Locally**: You can run this notebook locally, but performance will be reduced.\n",
    "\n",
    "ðŸš€ **Launch on Cloud**: A Ray Cluster with 4 GPUs (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a quick end-to-end example to get a sense of what the Ray AI Libraries can do.\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li>Overview of the Ray AI Libraries</li>\n",
    "    <li>Quick end-to-end example</li>\n",
    "    <ul>\n",
    "      <li>Vanilla XGBoost code</li>\n",
    "      <li>Hyperparameter tuning with Ray Tune</li>\n",
    "      <li>Distributed training with Ray Train</li>\n",
    "      <li>Serving an ensemble model with Ray Serve</li>\n",
    "      <li>Batch inference with Ray Data</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional): If you get an XGBoostError at import, you might have to `brew install libomp` before importing xgboost again\n",
    "!brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import fastapi\n",
    "import pandas as pd\n",
    "import requests\n",
    "# macos: If you get an XGBoostError at import, you might have to `brew install libomp` before importing xgboost again\n",
    "import xgboost\n",
    "from pydantic import BaseModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ray\n",
    "import ray.tune\n",
    "import ray.train\n",
    "from ray.train.xgboost import XGBoostTrainer as RayTrainXGBoostTrainer\n",
    "from ray.train import RunConfig\n",
    "import ray.data\n",
    "import ray.serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of the Ray AI Libraries\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_AI_Libraries/Ray+AI+Libraries.png\" width=\"700px\" loading=\"lazy\">\n",
    "\n",
    "Built on top of Ray Core, the Ray AI Libraries inherit all the performance and scalability benefits offered by Core while providing a convenient abstraction layer for machine learning. These Python-first native libraries allow ML practitioners to distribute individual workloads, end-to-end applications, and build custom use cases in a unified framework.\n",
    "\n",
    "The Ray AI Libraries bring together an ever-growing ecosystem of integrations with popular machine learning frameworks to create a common interface for development.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=\"100%\" loading=\"lazy\">|\n",
    "|:-:|\n",
    "|Ray AI Libraries enable end-to-end ML development and provides multiple options for integrating with other tools and libraries from the MLOps ecosystem.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick end-to-end example\n",
    "\n",
    "For this classification task, you will apply a simple [XGBoost](https://xgboost.readthedocs.io/en/stable/) (a gradient boosted trees framework) model to the June 2021 [New York City Taxi & Limousine Commission's Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). \n",
    "\n",
    "The full dataset contains millions of samples of yellow cab rides, and the goal is to predict the tip amount.\n",
    "\n",
    "**Dataset features**\n",
    "* **`passenger_count`**\n",
    "    * Float (whole number) representing number of passengers.\n",
    "* **`trip_distance`** \n",
    "    * Float representing trip distance in miles.\n",
    "* **`fare_amount`**\n",
    "    * Float representing total price including tax, tip, fees, etc.\n",
    "* **`tolls_amount`**\n",
    "    * Float representing the total paid on tolls if any.\n",
    "\n",
    "**Target**\n",
    "* **`trip_amount`**\n",
    "    * Float representing the total paid as tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vanilla XGboost code\n",
    "\n",
    "Let's start with the vanilla XGBoost code to predict the tip amount for a NYC taxi cab data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"passenger_count\", \n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"tolls_amount\",\n",
    "]\n",
    "\n",
    "label_column = \"tip_amount\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to load the data and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    path = \"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_2021-03.parquet\"\n",
    "    df = pd.read_parquet(path, columns=features + [label_column])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features], df[label_column], test_size=0.2, random_state=42\n",
    "    )\n",
    "    dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to run `xgboost.train` given some hyperparameter dictionary `params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_folder = \"/mnt/cluster_storage/\" # Modify this path to your local folder if it runs on your local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_path = Path(storage_folder) / \"model.ubj\"\n",
    "\n",
    "def my_xgboost_func(params):    \n",
    "    evals_result = {}\n",
    "    dtrain, dtest = load_data()\n",
    "    bst = xgboost.train(\n",
    "        params, \n",
    "        dtrain, \n",
    "        num_boost_round=10, \n",
    "        evals=[(dtest, \"eval\")], \n",
    "        evals_result=evals_result,\n",
    "    )\n",
    "    # Use Path\n",
    "    bst.save_model(model_path)\n",
    "    print(f\"{evals_result['eval']}\")\n",
    "    return {\"eval-rmse\": evals_result[\"eval\"][\"rmse\"][-1]}\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "}\n",
    "my_xgboost_func(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyperparameter tuning with Ray Tune\n",
    "\n",
    "Let's use Ray Tune to run distributed hyperparameter tuning for the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = ray.tune.Tuner(  # Create a tuner\n",
    "    my_xgboost_func,  # Pass it the training function which Ray Tune calls Trainable.\n",
    "    param_space={  # Pass it the parameter space to search over\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 6,\n",
    "        \"eta\": ray.tune.uniform(0.01, 0.3),\n",
    "    },\n",
    "    run_config=RunConfig(storage_path=storage_folder),\n",
    "    tune_config=ray.tune.TuneConfig(  # Tell it which metric to tune\n",
    "        metric=\"eval-rmse\",\n",
    "        mode=\"min\",\n",
    "        num_samples=10,\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()  # Run the tuning job\n",
    "print(results.get_best_result().config)  # Get back the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that shows what Tune does:\n",
    "\n",
    "It is effectively scheduling many trials and returning the best performing one.\n",
    "\n",
    "<img src=\"https://bair.berkeley.edu/static/blog/tune/tune-arch-simple.png\" width=\"700px\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Distributed training with Ray Train\n",
    "\n",
    "In case your training data is too large, your training might take a long time to complete.\n",
    "\n",
    "To speed it up, shard the dataset across training workers and perform distributed XGBoost training.\n",
    "\n",
    "Let's redefine `load_data` to now load a different slice of the data given the worker index/rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # find out which training worker is running this code\n",
    "    train_ctx = ray.train.get_context()\n",
    "    worker_rank = train_ctx.get_world_rank()\n",
    "    print(f\"Loading data for worker {worker_rank}...\")\n",
    "\n",
    "    # build path based on training worker rank\n",
    "    month = (worker_rank + 1) % 12\n",
    "    year = 2021 + (worker_rank + 1) // 12\n",
    "    path = f\"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_{year}-{month:02}.parquet\"\n",
    "\n",
    "    # same as before\n",
    "    df = pd.read_parquet(path, columns=features + [label_column])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features], df[label_column], test_size=0.2, random_state=42\n",
    "    )\n",
    "    dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run distributed XGBoost training using Ray Train's XGBoostTrainer - similar trainers exist for other popular ML frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RayTrainXGBoostTrainer(  # Create a trainer\n",
    "    my_xgboost_func,  # Pass it the training function\n",
    "    scaling_config=ray.train.ScalingConfig(\n",
    "        num_workers=2, use_gpu=False\n",
    "    ),  # Define how many training workers\n",
    "    train_loop_config=params,  # Pass it the hyperparameters\n",
    ")\n",
    "\n",
    "trainer.fit()  # Run the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that shows what Train does:\n",
    "\n",
    "A train controller will create training workers and execute the training function on each worker.\n",
    "\n",
    "Ray Train delegates the distributed training to the underlying XGBoost framework.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"700px\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Serving an ensemble model with Ray Serve\n",
    "\n",
    "Ray Serve allows for distributed serving of models and complex inference pipelines.\n",
    "\n",
    "Here is a diagram showing how to deploy an ensemble model with Ray Serve:\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/xjan103pcp94/3DJ7vVRxYIvcFO7JmIUMCx/77a45caa275ffa46f5135f4d6726dd4f/Figure_2_-_Fanout_and_ensemble.png\" width=\"700px\" loading=\"lazy\">\n",
    "\n",
    "Here is how the resulting code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = fastapi.FastAPI()\n",
    "\n",
    "class Payload(BaseModel):\n",
    "    passenger_count: int\n",
    "    trip_distance: float\n",
    "    fare_amount: float\n",
    "    tolls_amount: float\n",
    "\n",
    "\n",
    "@ray.serve.deployment\n",
    "@ray.serve.ingress(app)\n",
    "class Ensemble:\n",
    "    def __init__(self, model1, model2):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    @app.post(\"/predict\")\n",
    "    async def predict(self, data: Payload) -> dict:\n",
    "        model1_prediction, model2_prediction = await asyncio.gather(\n",
    "            self.model1.predict.remote([data.model_dump()]),\n",
    "            self.model2.predict.remote([data.model_dump()]),\n",
    "        )\n",
    "        out = {\"prediction\": float(model1_prediction + model2_prediction) / 2}\n",
    "        return out\n",
    "\n",
    "\n",
    "@ray.serve.deployment\n",
    "class Model:\n",
    "    def __init__(self, path: str):\n",
    "        self._model = xgboost.Booster()\n",
    "        self._model.load_model(path)\n",
    "\n",
    "    def predict(self, data: list[dict]) -> list[float]:\n",
    "        # Make prediction\n",
    "        dmatrix = xgboost.DMatrix(pd.DataFrame(data))\n",
    "        model_prediction = self._model.predict(dmatrix)\n",
    "        return model_prediction\n",
    "\n",
    "\n",
    "# Run the deployment\n",
    "handle = ray.serve.run(\n",
    "    Ensemble.bind(\n",
    "        model1=Model.bind(model_path),\n",
    "        model2=Model.bind(model_path),\n",
    "    ),\n",
    "    route_prefix=\"/ensemble\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an HTTP request to the Ray Serve instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"http://localhost:8000/ensemble/predict\",\n",
    "    json={  # Use json parameter instead of params\n",
    "        \"passenger_count\": 1,\n",
    "        \"trip_distance\": 2.5,\n",
    "        \"fare_amount\": 10.0,\n",
    "        \"tolls_amount\": 0.5,\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Batch inference with Ray Data\n",
    "\n",
    "Ray Data allows for distributed data processing through streaming execution across a heterogeneous cluster of CPUs and GPUs.\n",
    "\n",
    "This makes Ray Data ideal for workloads like compute-intensive data processing, data ingestion, and batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflinePredictor:\n",
    "    def __init__(self):\n",
    "        # Load expensive state\n",
    "        self._model = xgboost.Booster()\n",
    "        self._model.load_model(model_path)\n",
    "\n",
    "    def predict(self, data: list[dict]) -> list[float]:\n",
    "        # Make prediction in batch\n",
    "        dmatrix = xgboost.DMatrix(pd.DataFrame(data))\n",
    "        model_prediction = self._model.predict(dmatrix)\n",
    "        return model_prediction\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        batch[\"predictions\"] = self.predict(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Apply the predictor to the validation dataset\n",
    "prediction_pipeline = (\n",
    "    ray.data.read_parquet(\n",
    "        \"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_2021-03.parquet\"\n",
    "    )\n",
    "    .select_columns(features)\n",
    "    .map_batches(OfflinePredictor, concurrency=(2, 10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the pipeline, we can execute it in a distributed manner by writing the output to a sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pipeline.write_parquet(\"./xgboost_predictions\") #update this to your local path if runs on your local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the produced predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {storage_folder}/xgboost_predictions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm -rf {storage_folder}/xgboost_predictions/\n",
    "!rm {model_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
