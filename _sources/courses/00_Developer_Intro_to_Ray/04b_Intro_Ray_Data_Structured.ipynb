{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray Data: Ray Data + Structured Data\n",
    "Â© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’» **Launch Locally**: You can run this notebook locally, but performance will be reduced.\n",
    "\n",
    "ðŸš€ **Launch on Cloud**: A Ray Cluster (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will provide an overview of Ray Data and how to use it to load, and transform data in a distributed manner.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li><b>Part 0:</b> What is Ray Data?</a></li>\n",
    "    <li><b>Part 1:</b> How to use Ray Data?</a></li>\n",
    "    <li><b>Part 2:</b> Loading Data</a></li>\n",
    "    <li><b>Part 3:</b> Transforming Data</a></li>\n",
    "    <li><b>Part 4:</b> Writing Data</a></li>\n",
    "    <li><b>Part 5:</b> Data Operations: Shuffling, Grouping and Aggregation</a></li>\n",
    "    <li><b>Part 6:</b> When to use Ray Data</a></li>\n",
    "    <li><b>Part 7:</b> Ray Data in Production</a></li>\n",
    "    <li><b>Part 8:</b> Upcoming Features in Ray Data</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. What is Ray Data?\n",
    "\n",
    "Ray Data is a distributed data processing library that provides a Python API for parallel data processing. \n",
    "\n",
    "It is built on top of Ray, a fast and simple framework for building and running distributed applications. Ray Data is designed to be easy to use, scalable, and fault-tolerant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How to Use Ray Data?\n",
    "\n",
    "You typically should use the Ray Data API in this way:\n",
    "\n",
    "1. **Create a Ray Dataset** from external storage or in-memory data.\n",
    "2. **Apply transformations** to the data.\n",
    "3. **Write the outputs** to external storage or **feed the outputs** to training workers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "\n",
    "Our Dataset is the New York City Taxi & Limousine Commission's Trip Record Data\n",
    "\n",
    "**Dataset features**\n",
    "\n",
    "| Column | Description | \n",
    "| ------ | ----------- |\n",
    "| `trip_distance` | Float representing trip distance in miles. |\n",
    "| `passenger_count` | The number of passengers |\n",
    "| `PULocationID` | TLC Taxi Zone in which the taximeter was engaged | \n",
    "| `DOLocationID` | TLC Taxi Zone in which the taximeter was disengaged | \n",
    "| `payment_type` | A numeric code signifying how the passenger paid for the trip. |\n",
    "| `tolls_amount` | Total amount of all tolls paid in trip. | \n",
    "| `tip_amount` | Tip amount â€“ This field is automatically populated for credit card tips. Cash tips are not included. | \n",
    "| `total_amount` | The total amount charged to passengers. Does not include cash tips. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\n",
    "    \"trip_distance\",\n",
    "    \"passenger_count\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"payment_type\",\n",
    "    \"tolls_amount\",\n",
    "    \"tip_amount\",\n",
    "    \"total_amount\",\n",
    "]\n",
    "\n",
    "DATA_PATH = \"s3://anyscale-public-materials/nyc-taxi-cab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data for a single month. It takes up to 2 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    f\"{DATA_PATH}/yellow_tripdata_2011-05.parquet\",\n",
    "    columns=COLUMNS,\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how much memory the dataset is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(deep=True).sum().sum() / 1024**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many files there are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://anyscale-public-materials/nyc-taxi-cab/ --human-readable | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not making use of all the columns and are already consuming ~1GB of data per file -> will quickly become a problem if you want to scale to entire dataset (~155 files) if we are running on a small node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instead make use of a distributed data preprocessing library like Ray Data to load the full dataset in a distributed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_parquet(\n",
    "    DATA_PATH,\n",
    "    columns=COLUMNS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Ray data equivalents for common pandas functions like `read_csv`, `read_parquet`, `read_json`, etc.\n",
    "\n",
    "Refer to the [Input/Output docs](https://docs.ray.io/en/latest/data/api/input_output.html) for a comprehensive list of read functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Let's view our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Data by default adopts **lazy execution** this means that the data is not loaded into memory until it is needed. Instead only a small part of the dataset is loaded into memory to infer the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dataset specifies a sequence of transformations that will be applied to the data. \n",
    "\n",
    "The data itself will be organized into blocks, where each block is a collection of rows.\n",
    "\n",
    "The following figure visualizes a tabular dataset with three blocks, each block holding 1000 rows each:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-arch.svg' width=50%/>\n",
    "\n",
    "Since a Dataset is just a list of Ray object references, it can be freely passed between Ray tasks, actors, and libraries like any other object reference. This flexibility is a unique characteristic of Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transforming Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple function to generate features from the data. Here is how we would do so using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_total_amount(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"adjusted_total_amount\"] = df[\"total_amount\"] - df[\"tip_amount\"]\n",
    "    return df\n",
    "\n",
    "df = adjust_total_amount(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the same function and apply it to the Ray dataset using `map_batches`. \n",
    "\n",
    "`map_batches` will batch each block of the dataset and apply the function to each batch in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_adjusted = ds.map_batches(adjust_total_amount, batch_format=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b> \n",
    "\n",
    "The default `batch_format` in Ray Data is `numpy`, which means that the data is returned as a numpy array. For optimal performance, it is recommended to **avoid converting the data to pandas dataframes unless necessary**.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another transformation, for the sake of this example, we will add a simple transformation to calculate the tip percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tip_percentage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"tip_percentage\"] = df[\"tip_amount\"] / df[\"total_amount\"]\n",
    "    return df\n",
    "\n",
    "df = compute_tip_percentage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would apply it again using `map_batches`. Note that we can control certain additional parameters such as the batch size to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tip = ds_adjusted.map_batches(compute_tip_percentage, batch_format=\"pandas\", batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution mode\n",
    "\n",
    "Most transformations are **lazy** in Ray Data - i.e. they don't execute until you either:\n",
    "- **write a dataset to storage**\n",
    "- explicitly **materialize** the data\n",
    "- **iterate over the dataset** (usually when feeding data to model training).\n",
    "\n",
    "To explicitly *materialize* a very small subset of the data, you can use the `take_batch` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.take_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view a batch of the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tip.take_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the adjusted data. Here is how we would do it with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_folder = '/mnt/cluster_storage' # Modify this path to your local folder if it runs on your local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f\"{storage_folder}/adjusted_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file we just wrote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {storage_folder}/adjusted_data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we would do so with Ray Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /mnt/cluster_storage/adjusted_data_ray/ # let's remove the directory if it exists\n",
    "ds_limited = ds_adjusted.limit(df.shape[0]) # we limit to avoid writing too much data\n",
    "ds_limited.write_parquet(f\"{storage_folder}/adjusted_data_ray/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Ray data equivalents for common pandas functions like `write_parquet` for `to_parquet`, `write_csv` for `to_csv`, etc.\n",
    "\n",
    "See the [Input/Output docs](https://docs.ray.io/en/latest/data/api/input_output.html) for a comprehensive list of write functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the files in the directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {storage_folder}/adjusted_data_ray/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have **multiple files** in the directory. This is because Ray Data writes data in a **distributed manner**. \n",
    "\n",
    "**Each task writes its own file**, and the number of files is proportional to the number of CPUs in the cluster.\n",
    "\n",
    "**Ray Data uses Ray tasks** to process data.\n",
    "\n",
    "When reading from a file-based datasource (e.g., S3, GCS). Each read task reads its assigned files and produces an output block which in turn is consumed by the next task in the pipeline.\n",
    "    \n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/dataset-read-cropped-v2.svg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b> \n",
    "\n",
    "We passed `/mnt/cluster_storage/` as the path to write the data. This is a path on the Ray cluster's shared storage. If instead you use a path that is only local to one of the nodes in a multi-node cluster, you will see errors like `FileNotFoundError: [Errno 2] No such file or directory: '/path/to/file'`.\n",
    "\n",
    "This is because Ray Data is designed to work with distributed storage systems like S3, HDFS, etc. If you want to write to local storage, you can add a special prefix `local://` to the path. For example, `local:///path/to/file`. However to do so you will need to ensure that Ray is enabled to schedule and run tasks on the head node of the cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Operations: Shuffling, Grouping and Aggregation\n",
    "\n",
    "Let's look at some more involved transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling data \n",
    "\n",
    "There are different options to shuffle data in Ray Data of varying degrees of randomness and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File based shuffle on read\n",
    "\n",
    "To randomly shuffle the ordering of input files before reading, use the shuffle=\"files\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file_shuffled = ray.data.read_parquet(DATA_PATH, columns=COLUMNS, shuffle=\"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_file_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling block order\n",
    "This option randomizes the order of blocks in a dataset.\n",
    "\n",
    "Applying this operation alone doesnâ€™t involve heavy computation and communication. However, it requires Ray Data to materialize all blocks before applying the operation.\n",
    "\n",
    "Let's read the data and shuffle the block order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    ray.data.read_parquet(\n",
    "        \"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_2011-05.parquet\",\n",
    "        columns=COLUMNS,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform block order shuffling, use `randomize_block_order`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_block_based_shuffle = ds.randomize_block_order()\n",
    "ds_block_based_shuffle.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle all rows globally\n",
    "To randomly shuffle all rows globally, call `random_shuffle()`. This is the slowest option for shuffle, and requires transferring data across network between workers. This option achieves the best randomness among all options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_row_based_shuffle = ds.random_shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_row_based_shuffle.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom batching using `groupby` and aggregations\n",
    "\n",
    "In case you want to generate batches according to a specific key, you can use `groupby` to group the data by the key and then use `map_groups` to apply the transformation.\n",
    "\n",
    "For instance, let's compute the average trip distance per passenger count. Here is how we would do it with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"payment_type\")[\"trip_distance\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we would do the same operation with Ray Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = 8\n",
    "ds.repartition(num_cpus).groupby(\"payment_type\").mean(\"trip_distance\").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the main aggregation functions available in Ray Data:\n",
    "- count\n",
    "- max\n",
    "- mean\n",
    "- min\n",
    "- sum\n",
    "- std\n",
    "\n",
    "See [relevant docs page here](https://docs.ray.io/en/latest/data/api/grouped_data.html#computations-or-descriptive-stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Note:</b> This is an area of active development in Ray Data. The current implementation of groupby is not as optimized as it could be. We are working on improving the performance of `groupby` and `map_groups` operations.\n",
    "\n",
    "For more details, the current implementation makes use of a sort operation which instead can be done using a hash-based implementation. Additionally, we had to repartition the data to maximize parallelism - in the future Ray Data should be able to dynamically repartition the data to maximize parallelism.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. When to use Ray Data\n",
    "\n",
    "Ray Data is especially performant when needing to:\n",
    "- run data processing in a **streaming fashion** \n",
    "- run across a **large dataset**\n",
    "- run inside a **heterogeneous cluster of CPUs and GPUs**.\n",
    "\n",
    "Here is one use case for Batch Inference with Ray Data over a large dataset:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/stream-example.png' width=60%/>\n",
    "\n",
    "\n",
    "Ray Data also integrates seamlessly with Ray Train, making it an optimal choice for **data preprocessing in machine learning training pipelines**. Especially when you need to:\n",
    "- **Independently scale out data loading and transformation** from model training.\n",
    "- **Enable fault tolerance** for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ray Data in Production\n",
    "\n",
    "1. Runway AI is using Ray Data to scale its ML workloads. See [this interview with Runway AI](https://siliconangle.com/2024/10/02/runway-transforming-ai-driven-filmmaking-innovative-tools-techniques-raysummit/) to learn more.\n",
    "2. Netflix is using Ray Data for multi-modal batch inference pipelines. See [this talk at the Ray Summit 2024](https://raysummit.anyscale.com/flow/anyscale/raysummit2024/landing/page/sessioncatalog/session/1722028596844001bCg0) to learn more.\n",
    "3. Spotify uses Ray Data for large-scale data processing. See [this talk at the Ray Summit 2023](https://www.anyscale.com/blog/how-spotify-built-a-robust-ray-platform-with-a-frictionless-developer) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Upcoming Features in Ray Data\n",
    "\n",
    "Here are some relevant upcoming features in Ray Data:\n",
    "\n",
    "For structured data:\n",
    "- improved `groupby` and `map_groups` performance\n",
    "- using parquet metadata for computing statistics like `count`\n",
    "- enabling predicate pushdown for parquet files when calling `filter`\n",
    "- supporting `join` and `merge` operations\n",
    "- optimizing performance of the `Preprocessor` API for distributed feature engineering\n",
    "- running spark on Ray more seamlessly\n",
    "\n",
    "\n",
    "For all data types:\n",
    "- data checkpointing for fault tolerance\n",
    "- optimizing data connectors\n",
    "- concurrent execution of multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm {storage_folder}/adjusted_data.parquet\n",
    "!rm -rf {storage_folder}/adjusted_data_ray/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
