{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2c2063-8988-4687-b1d4-b24e0a5a3d66",
   "metadata": {},
   "source": [
    "## 3. Distributed Training with Ray Train and PyTorch Lightning\n",
    "\n",
    "Let's consider the case where we have a very large dataset of images that would take a long time to train on a single GPU. We would now like to scale this training job to run on multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49711c5",
   "metadata": {},
   "source": [
    "### 3.1 Distributed Data Parallel Training\n",
    "Here is a diagram showing the standard distributed data parallel training loop.\n",
    "\n",
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_v4.png\" width=\"900px\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Schematic overview of DistributedDataParallel (DDP) training: (1) the model is replicated from the <code>GPU rank 0</code> to all other workers; (2) each worker receives a shard of the dataset and processes a mini-batch; (3) during the backward pass, gradients are averaged across GPUs; (4) checkpoint and metrics from rank 0 GPU are saved to the persistent storage.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f8204",
   "metadata": {},
   "source": [
    "### 3.2 Ray Train Migration\n",
    "\n",
    "Here are the changes we need to make to the training loop to migrate it to Ray Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42be9f-fad5-4f3f-8a1c-5812c5573eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop_per_worker(\n",
    "    config: dict,  # Update the function signature to comply with Ray Train\n",
    "):\n",
    "    # Note lightning prepares the dataloader (adding a distributed sampler) for us\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config[\"batch_size_per_worker\"],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    # Same model initialization as vanilla lightning\n",
    "    model = StableDiffusion(\n",
    "        lr=config[\"lr\"],\n",
    "        resolution=config[\"resolution\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        num_warmup_steps=config[\"num_warmup_steps\"],\n",
    "        model_name=config[\"model_name\"],\n",
    "    )\n",
    "\n",
    "    # Same trainer setup as vanilla lightning except we add Ray Train specific arguments\n",
    "    trainer = pl.Trainer(\n",
    "        max_steps=config[\"max_steps\"],\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        accelerator=\"gpu\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        devices=\"auto\",  # Set devices to \"auto\" to use all available GPUs\n",
    "        strategy=RayDDPStrategy(),  # Use RayDDPStrategy for distributed data parallel training\n",
    "        plugins=[\n",
    "            RayLightningEnvironment()\n",
    "        ],  # Use RayLightningEnvironment to run the Lightning Trainer\n",
    "        callbacks=[\n",
    "            RayTrainReportCallback()\n",
    "        ],  # Use RayTrainReportCallback to report metrics and checkpoints\n",
    "        enable_checkpointing=False,  # Disable lightning checkpointing\n",
    "    )\n",
    "\n",
    "    # 4. Same as vanilla lightning\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5328b5",
   "metadata": {},
   "source": [
    "Here is the same diagram as before but with the Ray Train specific components highlighted.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_lightning_annotated_no_data_v2.png\" width=900 loading=\"lazy\">\n",
    "\n",
    "We made use of:\n",
    "- `ray.train.get_dataset_shard(\"train\")` to get the training dataset shard.\n",
    "- `RayDDPStrategy` to perform distributed data parallel training.\n",
    "- `RayLightningEnvironment` to run the Lightning Trainer.\n",
    "- `RayTrainReportCallback` to report metrics and checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af7dd4",
   "metadata": {},
   "source": [
    "### 3.3. Configure scale and GPUs\n",
    "Outside of our training function, we create a `ScalingConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a327630",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ray.train.ScalingConfig(num_workers=2, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14744be2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html#ray-train-scalingconfig\" target=\"_blank\">ScalingConfig</a> configures:\n",
    "\n",
    "<ul>\n",
    "  <li><code>num_workers</code>: The number of distributed training worker processes.</li>\n",
    "  <li><code>use_gpu</code>: Whether each worker should use a GPU (or CPU).</li>\n",
    "</ul>\n",
    "\n",
    "See docs on configuring <a href=\"https://docs.ray.io/en/latest/train/user-guides/using-gpus.html\" target=\"_blank\">scale and GPUs</a> for more details.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74098a02",
   "metadata": {},
   "source": [
    "#### 3.3.1. Note on Ray Train key concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77815f",
   "metadata": {},
   "source": [
    "Ray Train is built around [four key concepts](https://docs.ray.io/en/latest/train/overview.html):\n",
    "1. **Training function**: (implemented above `train_loop_ray_train`): A Python function that contains your model training logic.\n",
    "1. **Worker**: A process that runs the training function.\n",
    "1. **Scaling config**: specifies number of workers and compute resources (CPUs or GPUs, TPUs).\n",
    "1. **Trainer**: A Python class (Ray Actor) that ties together the training function, workers, and scaling configuration to execute a distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288fe598",
   "metadata": {},
   "source": [
    "|<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"700px\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|High-level architecture of how Ray Train|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d8ccb-30c2-4491-a381-63ac3330bc2e",
   "metadata": {},
   "source": [
    "### 3.4 Create and fit a Ray Train TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b32a89",
   "metadata": {},
   "source": [
    "We first specify the run configuration to tell Ray Train where to store the checkpoints and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = \"/mnt/cluster_storage/\"\n",
    "experiment_name = \"stable-diffusion-pretraining\"\n",
    "\n",
    "run_config = ray.train.RunConfig(name=experiment_name, storage_path=storage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d2f69",
   "metadata": {},
   "source": [
    "Now we can create our Ray Train `TorchTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop_config = {\n",
    "    \"batch_size_per_worker\": 8,\n",
    "    \"prefetch_batches\": 1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"num_warmup_steps\": 10_000,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_steps\": 550_000,\n",
    "    \"max_epochs\": 1,\n",
    "    \"resolution\": 256,\n",
    "    \"model_name\": \"stabilityai/stable-diffusion-2-base\",\n",
    "}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81045b35",
   "metadata": {},
   "source": [
    "We call `.fit()` to start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e338b7",
   "metadata": {},
   "source": [
    "### 3.5. Access the training results\n",
    "\n",
    "We can check the metrics produced by the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.metrics_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916616d",
   "metadata": {},
   "source": [
    "### 3.6. Load the checkpointed model to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89719e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = result.checkpoint\n",
    "with ckpt.as_directory() as ckpt_dir:\n",
    "    ckpt_path = os.path.join(ckpt_dir, \"checkpoint.ckpt\")\n",
    "    loaded_model_ray_train = StableDiffusion.load_from_checkpoint(\n",
    "        checkpoint_path=ckpt_path,\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "        weights_only=True,\n",
    "    )\n",
    "    loaded_model_ray_train.eval()\n",
    "\n",
    "loaded_model_ray_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ee2ff",
   "metadata": {},
   "source": [
    "### 3.7. Activity: Run the distributed training with more workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797a47f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "1. Update the scaling configuration to make use of 4 GPU workers\n",
    "2. Run the trainer using the same hyperparameters\n",
    "\n",
    "Use the following code snippets to guide you:\n",
    "\n",
    "```python\n",
    "# Hint: Update the scaling configuration\n",
    "scaling_config = ...\n",
    "\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    train_loop_config=train_loop_config,\n",
    ")\n",
    "result = trainer.fit()\n",
    "result.metrics_dataframe\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80e92c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Click here to see the solution </summary>\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(num_workers=4, use_gpu=True)\n",
    "\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    train_loop_config=train_loop_config,\n",
    ")\n",
    "result = trainer.fit()\n",
    "result.metrics_dataframe\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}