{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Deploying LoRA Adapters\n",
    "\n",
    "LoRA (Low-Rank Adaptation) adapters are small, efficient fine-tuned models that can be loaded on top of a base model. This allows you to serve multiple specialized behaviors from a single deployment.\n",
    "\n",
    "### Why Use LoRA Adapters?\n",
    "\n",
    "- **Parameter Efficiency**: LoRA adapters are typically less than 1% of the base model's size\n",
    "- **Runtime Adaptation**: Switch between different adapters without reloading the base model\n",
    "- **Simpler MLOps**: Centralize inference around one model while supporting multiple use cases\n",
    "- **Cost Effective**: Share expensive base model across multiple specialized tasks\n",
    "\n",
    "### Example: Code Assistant LoRA\n",
    "\n",
    "Let's deploy a base model with multiple LoRA adapters. This will allow the model to switch between general and specialized generation.\n",
    "\n",
    "For this example, we'll use publicly available adapters from Hugging Face.\n",
    "\n",
    "First, we need to prepare our LoRA adapters and save them in our cloud storage. \n",
    "\n",
    "For example, here is an example script for downloading adapters from Huggingface and saving them in an AWS bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Mapping of custom names to Hugging Face LoRA adapter repo IDs\n",
    "adapters = {\n",
    "    \"nemoguard\": \"nvidia/llama-3.1-nemoguard-8b-topic-control\",\n",
    "    \"cv_job_matching\": \"LlamaFactoryAI/Llama-3.1-8B-Instruct-cv-job-description-matching\",\n",
    "    \"yara\": \"vtriple/Llama-3.1-8B-yara\"\n",
    "}\n",
    "\n",
    "# S3 target\n",
    "bucket_name = \"llm-docs-aydin\"\n",
    "base_s3_path = \"1-5-multi-lora/lora_checkpoints\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "for custom_name, repo_id in adapters.items():\n",
    "    print(f\"\\n\ud83d\udce5 Downloading adapter '{custom_name}' from {repo_id}...\")\n",
    "    local_path = snapshot_download(repo_id)\n",
    "\n",
    "    print(f\"\u2b06\ufe0f Uploading files to s3://{bucket_name}/{base_s3_path}/{custom_name}/\")\n",
    "\n",
    "    for root, _, files in os.walk(local_path):\n",
    "        for file_name in files:\n",
    "            local_file_path = os.path.join(root, file_name)\n",
    "            rel_path = os.path.relpath(local_file_path, local_path)\n",
    "            s3_key = f\"{base_s3_path}/{custom_name}/{rel_path}\".replace(\"\\\\\", \"/\")\n",
    "\n",
    "            print(f\"  \u2192 {s3_key}\")\n",
    "            s3.upload_file(local_file_path, bucket_name, s3_key)\n",
    "\n",
    "print(\"\\n\u2705 All adapters uploaded successfully.\")\n",
    "\n",
    "# List all objects in the bucket to confirm\n",
    "response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "print(f\"Files in s3://{bucket_name}/:\")\n",
    "for obj in response[\"Contents\"]:\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should end up with this folder structure for each adapter.\n",
    "```\n",
    "s3://your-bucket/lora-adapters/\n",
    "\u251c\u2500\u2500 nemoguard/\n",
    "\u2502   \u251c\u2500\u2500 adapter_config.json\n",
    "\u2502   \u2514\u2500\u2500 adapter_model.safetensors\n",
    "\u251c\u2500\u2500 cv_job_matching/\n",
    "\u2502   \u251c\u2500\u2500 adapter_config.json\n",
    "\u2502   \u2514\u2500\u2500 adapter_model.safetensors\n",
    "\u251c\u2500\u2500 yara/\n",
    "    \u251c\u2500\u2500 adapter_config.json\n",
    "    \u2514\u2500\u2500 adapter_model.safetensors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Ray Serve LLM with LoRA\n",
    "\n",
    "Now let's configure our LLM with LoRA support. The key additions are the `lora_config` and enabling LoRA in the engine arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "\n",
    "# Configure LLM with LoRA support\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-llama\",\n",
    "        # Make sure your huggingface token has access/authorization\n",
    "        # Go to https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct and request access otherwise\n",
    "        # Or switch to the unsloth/ version for an ungated LLama \n",
    "        model_source=\"meta-llama/Llama-3.1-8B-Instruct\" # Base model\n",
    "    ),\n",
    "    accelerator_type=\"L4\",\n",
    "    # LoRA configuration\n",
    "    lora_config=dict(\n",
    "        dynamic_lora_loading_path=\"s3://llm-docs-aydin/1-5-multi-lora/lora_checkpoints/\",  # Your S3/GCS path\n",
    "        max_num_adapters_per_replica=3  # (optional) Limit adapters per replica\n",
    "    ),\n",
    "    runtime_env=dict(\n",
    "        env_vars={\n",
    "            \"HF_TOKEN\": os.environ.get(\"HF_TOKEN\"), # Set your token beforehand: export HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>\n",
    "            \"AWS_REGION\": \"us-west-2\"  # Your AWS region\n",
    "        }\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len=8192,\n",
    "        # Enable LoRA support\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=32,  # Maximum LoRA rank. Set to the largest rank you plan to use.\n",
    "        max_loras=3,  # Must match max_num_adapters_per_replica\n",
    "    ),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run serve_my_lora_app:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LoRA Adapters\n",
    "\n",
    "Once deployed, you can query different adapters by specifying them in the model name using the format `<base_model_id>:<adapter_name>`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.py\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"FAKE_KEY\")\n",
    "\n",
    "############################ Base model request (no adapter) #####################\n",
    "print(\"=== Base model ===\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-llama\",  # no adapter\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "############################ nemoguard adapter (moderation) #####################\n",
    "print(\"=== LoRA: nemoguard ===\")\n",
    "# As per Nemoguard's usage instruction, add this to your system prompt\n",
    "# https://huggingface.co/nvidia/llama-3.1-nemoguard-8b-topic-control#system-instruction\n",
    "TOPIC_SAFETY_OUTPUT_RESTRICTION = 'If any of the above conditions are violated, please respond with \"off-topic\". Otherwise, respond with \"on-topic\". You must respond with \"on-topic\" or \"off-topic\".'\n",
    "messages_nemoguard = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f'In the next conversation always use a polite tone and do not engage in any talk about travelling and touristic destinations.{TOPIC_SAFETY_OUTPUT_RESTRICTION}',\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Do you know which is the most popular beach in Barcelona?\"},\n",
    "]\n",
    "#response = client.chat.completions.create(\n",
    "##    model=\"my-llama:nemoguard\", ### with nemoguard adapter\n",
    " #   messages=messages_nemoguard,\n",
    " #   stream=True\n",
    "#)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print(\"\\n\")\n",
    "\n",
    "############################ cv_job_matching adapter (structured JSON output) ############################\n",
    "print(\"=== LoRA: cv_job_matching ===\")\n",
    "messages_cv = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are an advanced AI model designed to analyze the compatibility between a CV and a job description. You will receive a CV and a job description. Your task is to output a structured JSON format that includes the following:\n",
    "\n",
    "1. matching_analysis: Analyze the CV against the job description to identify key strengths and gaps.\n",
    "2. description: Summarize the relevance of the CV to the job description in a few concise sentences.\n",
    "3. score: Provide a numerical compatibility score (0-100) based on qualifications, skills, and experience.\n",
    "4. recommendation: Suggest actions for the candidate to improve their match or readiness for the role.\n",
    "\n",
    "Your output must be in JSON format as follows:\n",
    "{\n",
    "  \"matching_analysis\": \"Your detailed analysis here.\",\n",
    "  \"description\": \"A brief summary here.\",\n",
    "  \"score\": 85,\n",
    "  \"recommendation\": \"Your suggestions here.\"\n",
    "}\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<CV> Software engineer with 5 years of experience in Python and cloud infrastructure. </CV>\\n<job_description> Looking for a backend engineer with Python and AWS experience. </job_description>\",\n",
    "    },\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-llama:cv_job_matching\", ### with cv_job_matching adapter\n",
    "    messages=messages_cv,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print(\"\\n\")\n",
    "\n",
    "############################ yara adapter (cybersecurity task) ############################\n",
    "print(\"=== LoRA: yara ===\")\n",
    "messages_yara = [{\"role\": \"user\", \"content\": \"Generate a YARA rule to detect a PowerShell-based keylogger. Generate ONLY the YARA rule, do not add explanations.\"}]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-llama:yara\", ### with yara adapter\n",
    "    messages=messages_yara,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shutdown the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Benefits\n",
    "\n",
    "- **Single Deployment**: One base model serves multiple specialized behaviors\n",
    "- **Dynamic Switching**: Change adapters at runtime without restarting\n",
    "- **Memory Efficient**: Adapters are much smaller than full fine-tuned models\n",
    "- **Cost Effective**: Share expensive base model across multiple use cases\n",
    "\n",
    "### Learn More\n",
    "\n",
    "For comprehensive multi-LoRA deployment guides, see:\n",
    "- [Multi-LoRA deployment guide on Anyscale](https://docs.anyscale.com/llm/serving/multi-lora) - Complete guide with best practices\n",
    "- [Multi-LoRA with Ray Serve LLM (Ray docs)](https://docs.ray.io/en/latest/serve/llm/user-guides/multi-lora.html) - Quick-start configuration details\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}