{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Setting up Tool Calling\n",
    "\n",
    "Tool calling enables LLMs to interact with external functions, APIs, and databases. This opens up powerful possibilities for building sophisticated AI applications that can perform actions beyond just text generation.\n",
    "\n",
    "### Why Tool Calling Matters\n",
    "\n",
    "- **Enhanced Capabilities**: Models can perform actions, not just generate text\n",
    "- **Real-time Data**: Access current information from APIs and databases\n",
    "- **Workflow Automation**: Integrate AI into existing business processes\n",
    "- **Interactive Applications**: Build chatbots that can actually do things\n",
    "\n",
    "### Example: Weather Assistant with Tool Calling\n",
    "\n",
    "Let's create a model that can check weather information by calling a weather API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serve_my_qwen3.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-qwen3\",\n",
    "        model_source=\"Qwen/Qwen3-32B\",\n",
    "    ),\n",
    "    accelerator_type=\"L40S\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1,\n",
    "            max_replicas=2,\n",
    "        )\n",
    "    ),\n",
    "    ### Uncomment if your model is gated and needs your Hugging Face token to access it.\n",
    "    # runtime_env=dict(env_vars={\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}),\n",
    "    engine_kwargs=dict(\n",
    "        tensor_parallel_size=4, \n",
    "        max_model_len=32768, \n",
    "        reasoning_parser=\"qwen3\",\n",
    "        enable_auto_tool_choice= True,\n",
    "        tool_call_parser= \"hermes\"\n",
    "    ),\n",
    ")\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run serve_my_qwen3:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tool Calling\n",
    "\n",
    "Now let's test our tool-calling model. The model will decide when to call tools and provide the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tool_call_client.py\n",
    "import random\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Dummy APIs\n",
    "def get_current_temperature(location: str, unit: str = \"celsius\"):\n",
    "    temperature = random.randint(15, 30) if unit == \"celsius\" else random.randint(59, 86)\n",
    "    return {\n",
    "        \"temperature\": temperature,\n",
    "        \"location\": location,\n",
    "        \"unit\": unit\n",
    "    }\n",
    "\n",
    "def get_temperature_date(location: str, date: str, unit: str = \"celsius\"):\n",
    "    temperature = random.randint(15, 30) if unit == \"celsius\" else random.randint(59, 86)\n",
    "    return {\n",
    "        \"temperature\": temperature,\n",
    "        \"location\": location,\n",
    "        \"date\": date,\n",
    "        \"unit\": unit\n",
    "    }\n",
    "\n",
    "# Tools schema definitions\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_temperature\",\n",
    "            \"description\": \"Get current temperature at a location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_temperature_date\",\n",
    "            \"description\": \"Get temperature at a location and date.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The date to get the temperature for, in the format \\\"Year-Month-Day\\\".\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\", \"date\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "######################### Sending request for tool calls #########################\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"FAKE_KEY\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a weather assistant. Use the given functions to get weather data and provide the results.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the temperature in San Francisco now? How about tomorrow? Current Date: 2025-07-29.\"\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen3\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice= \"auto\" # let the model decide to use tools or not\n",
    ")\n",
    "\n",
    "######################### Process tool calls #########################\n",
    "for tc in response.choices[0].message.tool_calls:\n",
    "    print(f\"Tool call id: {tc.id}\")\n",
    "    print(f\"Tool call function name: {tc.function.name}\")\n",
    "    print(f\"Tool call arguments: {tc.function.arguments}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Helper tool map (str -> python callable to your APIs)\n",
    "helper_tool_map = {\n",
    "    \"get_current_temperature\": get_current_temperature,\n",
    "    \"get_temperature_date\": get_temperature_date\n",
    "}\n",
    "\n",
    "######################### Add your model's tool calls request to the chat history #########################\n",
    "# `response` is your model's last response containing the tool calls it requests.\n",
    "# Add the previous response containing the tool calls\n",
    "messages.append(response.choices[0].message.model_dump())\n",
    "\n",
    "######################### Add `tool` messages in your chat history #########################\n",
    "# Loop through the tool calls and create `tool` messages\n",
    "for tool_call in response.choices[0].message.tool_calls:\n",
    "    call_id, fn_call = tool_call.id, tool_call.function\n",
    "    \n",
    "    fn_callable = helper_tool_map[fn_call.name]\n",
    "    fn_args = json.loads(fn_call.arguments)\n",
    "\n",
    "    output = json.dumps(fn_callable(**fn_args))\n",
    "\n",
    "    # Create a new message of role `\"tool\"` containing the output of your tool\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": output,\n",
    "        \"tool_call_id\": call_id\n",
    "    })\n",
    "\n",
    "######################### Sending final request #########################\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen3\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Benefits\n",
    "\n",
    "- **Intelligent Tool Selection**: Model decides when and which tools to use\n",
    "- **Structured Parameters**: Tools receive properly formatted arguments\n",
    "- **Seamless Integration**: Natural conversation flow with tool execution\n",
    "- **Extensible**: Easy to add new tools and capabilities\n",
    "\n",
    "### Learn More\n",
    "\n",
    "For comprehensive tool calling guides, see:\n",
    "- [LLM deployment with tool and function calling on Anyscale](https://docs.anyscale.com/llm/serving/tool-function-calling) - Complete tool calling setup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}