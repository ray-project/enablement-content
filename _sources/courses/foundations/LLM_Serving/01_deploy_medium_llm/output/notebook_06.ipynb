{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topics: Monitoring & Optimization\n",
    "\n",
    "Now let's explore advanced features for production deployments.\n",
    "\n",
    "### Enabling LLM Monitoring\n",
    "\n",
    "The Serve LLM Dashboard offers deep visibility into model performance. Let's enable comprehensive monitoring:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serve_llama_3_1_70b.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-llama-3.1-70b\",\n",
    "        model_source=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "    ),\n",
    "    accelerator_type=\"L40S\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1,\n",
    "            max_replicas=4,\n",
    "        )\n",
    "    ),\n",
    "    runtime_env=dict(\n",
    "        env_vars={\n",
    "            \"HF_TOKEN\": os.environ.get(\"HF_TOKEN\"),\n",
    "        }\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len=32768,\n",
    "        tensor_parallel_size=8,\n",
    "    ),\n",
    "    # Enable detailed engine metrics\n",
    "    log_engine_metrics=True\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyscale provides an easy way to visualize your LLM metrics on an integrated Grafana dashboard.\n",
    "\n",
    "On your Anyscale Workspace or Service page, go to Metrics, then click on the View on Grafana dropdown and select Ray Serve LLM Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run serve_llama_3_1_70b:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember shutting down your service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Concurrency\n",
    "\n",
    "Ray Serve LLM uses vLLM as its backend engine, which logs the maximum concurrency it can support.  \n",
    "Example log for 8xL40S:\n",
    "```console\n",
    "INFO 08-19 20:57:37 [kv_cache_utils.py:837] Maximum concurrency for 32,768 tokens per request: 17.79x\n",
    "```\n",
    "Let's explore optimization strategies:\n",
    "\n",
    "### Concurrency Optimization Strategies\n",
    "\n",
    "Below are key strategies to improve model concurrency and performance when serving LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "**Example log (8\u00d7L40S setup):**\n",
    "\n",
    "```\n",
    "INFO: Maximum concurrency for 32,768 tokens per request: 17.79x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Reduce `max_model_len`\n",
    "\n",
    "* `32,768` tokens \u2192 concurrency \u2248 **18**\n",
    "* `16,384` tokens \u2192 concurrency \u2248 **36**\n",
    "* **Trade-off:** shorter context window but higher concurrency\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Use Quantized Models\n",
    "\n",
    "* **FP16 \u2192 FP8:** ~50% memory reduction\n",
    "* **FP8 \u2192 INT4:** ~75% memory reduction\n",
    "* Frees up memory for the KV cache, enabling more concurrent requests\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Enable Pipeline Parallelism\n",
    "\n",
    "* Distribute layers across multiple nodes, set `pipeline_parallel_size > 1`\n",
    "* This increase the size of your KV cache, trading off on your latency due to the multi-node communication overhead\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Scale with More Replicas\n",
    "\n",
    "* Horizontally scale across multiple nodes\n",
    "* Each replica runs an independent model instance\n",
    "* **Total concurrency = per-replica concurrency \u00d7 number of replicas**\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Upgrade Hardware\n",
    "\n",
    "* Example: **L40S (48 GB) \u2192 A100 (80 GB)**\n",
    "* More GPU memory allows higher concurrency\n",
    "* Faster interconnects (e.g., NVLink) reduce latency\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}