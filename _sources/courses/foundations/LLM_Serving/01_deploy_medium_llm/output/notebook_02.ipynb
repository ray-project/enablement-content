{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: Why Medium-Sized Models?\n",
    "\n",
    "A medium LLM typically runs on a single node with 4-8 GPUs. It offers a balance between performance and efficiency. These models provide stronger accuracy and reasoning than small models while remaining more affordable and resource-friendly than very large ones.\n",
    "\n",
    "### Model Size Comparison\n",
    "\n",
    "Let's understand how different model sizes compare:\n",
    "\n",
    "| Model Size | Parameters | Memory (FP16) | Typical Use Case | Hardware Requirements |\n",
    "|------------|------------|---------------|------------------|----------------------|\n",
    "| **Small** | 7B-13B | 14-26 GB | Prototyping, simple tasks | 1-2 GPUs |\n",
    "| **Medium** | 70B-80B | 140-160 GB | Production workloads, complex reasoning | 4-8 GPUs |\n",
    "| **Large** | 400B+ | 800+ GB | Research, maximum capability | Multiple nodes |\n",
    "\n",
    "### Why Choose Medium-Sized Models?\n",
    "\n",
    "**Advantages:**\n",
    "- **Balanced Performance**: Strong accuracy and reasoning capabilities\n",
    "- **Cost-Effective**: More affordable than very large models\n",
    "- **Resource Efficient**: Can run on single-node multi-GPU setups\n",
    "- **Production Ready**: Ideal for scaling applications where large models would be too slow or expensive\n",
    "\n",
    "**Perfect for:**\n",
    "- Production workloads requiring good quality at lower cost\n",
    "- Applications needing stronger reasoning than small models\n",
    "- Scaling scenarios where large models are too resource-intensive\n",
    "\n",
    "### Our Example: Llama-3.1-70B\n",
    "\n",
    "In this tutorial, we'll deploy **Meta's Llama-3.1-70B-Instruct** model, which:\n",
    "- Has 70 billion parameters\n",
    "- Requires ~140GB memory in FP16 precision\n",
    "- Needs 4-8 GPUs for efficient serving\n",
    "- Provides excellent reasoning and instruction-following capabilities\n",
    "\n",
    "### Related Examples\n",
    "\n",
    "- **Small Models**: [Deploy a small-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/small-size-llm/README.html) - 1-2 GPUs\n",
    "- **Large Models**: [Deploy a large-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/large-size-llm/README.html) - Multiple nodes\n",
    "- **Workspace Template**: [Run on Anyscale](https://console.anyscale.com/template-preview/deployment-serve-llm?file=%252Ffiles%252Fmedium-size-llm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}