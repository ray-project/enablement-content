{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy a Medium-Sized LLM with Ray Serve LLM\n",
        "\n",
        "Â© 2025, Anyscale. All Rights Reserved\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’» **Launch Locally**: You can run this notebook locally, but you'll need access to multiple GPUs.\n",
        "\n",
        "ðŸš€ **Launch on Cloud**: A Ray Cluster with 4-8 GPUs (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates how to deploy a medium-sized LLM using Ray Serve LLM. We'll walk through the complete process from configuration to production deployment, covering both local development and cloud deployment with Anyscale Services.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b> Here is the roadmap for this notebook:</b>\n",
        "<ul>\n",
        "    <li>Overview: Why Medium-Sized Models?</li>\n",
        "    <li>Setting up Ray Serve LLM</li>\n",
        "    <li>Local Deployment & Inference</li>\n",
        "    <li>Deploying to Anyscale Services</li>\n",
        "    <li>Advanced Topics: Monitoring & Optimization</li>\n",
        "    <li>Summary & Outlook</li>\n",
        "</ul>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview: Why Medium-Sized Models?\n",
        "\n",
        "A medium LLM typically runs on a single node with 4-8 GPUs. It offers a balance between performance and efficiency. These models provide stronger accuracy and reasoning than small models while remaining more affordable and resource-friendly than very large ones.\n",
        "\n",
        "### Model Size Comparison\n",
        "\n",
        "Let's understand how different model sizes compare:\n",
        "\n",
        "| Model Size | Parameters | Memory (FP16) | Typical Use Case | Hardware Requirements |\n",
        "|------------|------------|---------------|------------------|----------------------|\n",
        "| **Small** | 7B-13B | 14-26 GB | Prototyping, simple tasks | 1-2 GPUs |\n",
        "| **Medium** | 70B-80B | 140-160 GB | Production workloads, complex reasoning | 4-8 GPUs |\n",
        "| **Large** | 400B+ | 800+ GB | Research, maximum capability | Multiple nodes |\n",
        "\n",
        "### Why Choose Medium-Sized Models?\n",
        "\n",
        "**Advantages:**\n",
        "- **Balanced Performance**: Strong accuracy and reasoning capabilities\n",
        "- **Cost-Effective**: More affordable than very large models\n",
        "- **Resource Efficient**: Can run on single-node multi-GPU setups\n",
        "- **Production Ready**: Ideal for scaling applications where large models would be too slow or expensive\n",
        "\n",
        "**Perfect for:**\n",
        "- Production workloads requiring good quality at lower cost\n",
        "- Applications needing stronger reasoning than small models\n",
        "- Scaling scenarios where large models are too resource-intensive\n",
        "\n",
        "### Our Example: Llama-3.1-70B\n",
        "\n",
        "In this tutorial, we'll deploy **Meta's Llama-3.1-70B-Instruct** model, which:\n",
        "- Has 70 billion parameters\n",
        "- Requires ~140GB memory in FP16 precision\n",
        "- Needs 4-8 GPUs for efficient serving\n",
        "- Provides excellent reasoning and instruction-following capabilities\n",
        "\n",
        "### Related Examples\n",
        "\n",
        "- **Small Models**: [Deploy a small-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/small-size-llm/README.html) - 1-2 GPUs\n",
        "- **Large Models**: [Deploy a large-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/large-size-llm/README.html) - Multiple nodes\n",
        "- **Workspace Template**: [Run on Anyscale](https://console.anyscale.com/template-preview/deployment-serve-llm?file=%252Ffiles%252Fmedium-size-llm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up Ray Serve LLM\n",
        "\n",
        "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. The main abstractions we'll work with are:\n",
        "\n",
        "### Key Components\n",
        "\n",
        "1. **`LLMConfig`**: Configuration object that defines your model, hardware, and deployment settings\n",
        "2. **`build_openai_app`**: Public function that creates an OpenAI-compatible application from your configuration\n",
        "3. **Ray Serve**: The underlying orchestration layer that handles scaling and load balancing\n",
        "\n",
        "### Configuration for Medium-Sized Models\n",
        "\n",
        "For medium-sized models, we need to:\n",
        "- Set appropriate `accelerator_type` for the hardware\n",
        "- Configure **tensor parallelism** with `tensor_parallel_size` to match the number of GPUs\n",
        "\n",
        "Let's create our configuration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# serve_llama_3_1_70b.py\n",
        "from ray.serve.llm import LLMConfig, build_openai_app\n",
        "import os\n",
        "\n",
        "llm_config = LLMConfig(\n",
        "    model_loading_config=dict(\n",
        "        model_id=\"my-llama-3.1-70b\",\n",
        "        # Or unsloth/Meta-Llama-3.1-70B-Instruct for an ungated model\n",
        "        model_source=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
        "    ),\n",
        "    deployment_config=dict(\n",
        "        autoscaling_config=dict(\n",
        "            min_replicas=1,\n",
        "            max_replicas=4,\n",
        "        )\n",
        "    ),\n",
        "    accelerator_type=\"L40S\", # Or with similar VRAM like \"A100-40G\"\n",
        "    # Type `export HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>` in a terminal\n",
        "    runtime_env=dict(env_vars={\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}),\n",
        "    engine_kwargs=dict(\n",
        "        max_model_len=32768, # See model's Hugging Face card for max context length\n",
        "        # Split weights among 8 GPUs in the node\n",
        "        tensor_parallel_size=8,\n",
        "    ),\n",
        "    log_engine_metrics=True,\n",
        ")\n",
        "\n",
        "app = build_openai_app({\"llm_configs\": [llm_config]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration Breakdown\n",
        "\n",
        "Let's understand each part of our configuration:\n",
        "\n",
        "**Model Loading:**\n",
        "- `model_id`: Unique identifier for your model in the API\n",
        "- `model_source`: Hugging Face model path (gated model requires HF token)\n",
        "- `HF_TOKEN`: Hugging Face token for accessing gated models\n",
        "\n",
        "**Hardware Configuration:**\n",
        "- `accelerator_type`: GPU type (L40S, A100-40G, etc.)\n",
        "- `tensor_parallel_size`: Number of GPUs to split the model across\n",
        "\n",
        "**Deployment Settings:**\n",
        "- `autoscaling_config`: Min/max replicas for horizontal scaling\n",
        "\n",
        "**Monitoring**\n",
        "- `log_engine_metrics`: Display LLM-specific metrics (Time to First Toke, Time Per Output Token, Request Per Second...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Local Deployment & Inference\n",
        "\n",
        "Now let's deploy our medium-sized LLM locally and query it.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "**Hardware Requirements:**\n",
        "- Access to 4-8 GPUs (L40S, A100-40G, or similar with sufficient GPU memory for the 70B model (~140GB))\n",
        "\n",
        "**Software Requirements:**\n",
        "- Ray Serve LLM\n",
        "- For gated models, an Hugging Face token with authorization to access the model\n",
        "\n",
        "**Installation:**\n",
        "```bash\n",
        "pip install \"ray[serve,llm]\"\n",
        "```\n",
        "\n",
        "**Hugging Face Token:**\n",
        "```bash\n",
        "export HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>\n",
        "```\n",
        "\n",
        "### Launching Ray Serve\n",
        "\n",
        "Let's start our LLM service:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve run serve_llama_3_1_70b:app --non-blocking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sending Requests\n",
        "\n",
        "Once deployed, your endpoint is available at `http://localhost:8000`. You can use a placeholder authentication token like `\"FAKE_KEY\"`.\n",
        "\n",
        "Let's test our model with some example requests:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from urllib.parse import urljoin\n",
        "from openai import OpenAI\n",
        "\n",
        "API_KEY = \"FAKE_KEY\"\n",
        "BASE_URL = \"http://localhost:8000\"\n",
        "\n",
        "client = OpenAI(base_url=urljoin(BASE_URL, \"v1\"), api_key=API_KEY)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"my-llama-3.1-70b\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content:\n",
        "        print(content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shutting Down\n",
        "\n",
        "When you're done testing, shut down the service:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve shutdown -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploying to Anyscale Services\n",
        "\n",
        "For production deployment, we'll use Anyscale Services to deploy our Ray Serve app to a dedicated cluster. The great news is that **no code changes are needed** - we can use the exact same LLM configuration!\n",
        "\n",
        "### What is an Anyscale Service?\n",
        "\n",
        "An **Anyscale Service** is a managed deployment that provides:\n",
        "- **Dedicated Infrastructure**: Your own Ray cluster in the cloud\n",
        "- **Automatic Scaling**: Handles traffic spikes and load balancing\n",
        "- **Fault Tolerance**: Resilient against node failures and rolling updates\n",
        "- **Enterprise Features**: Security, monitoring, and compliance\n",
        "\n",
        "### Setting up the Configuration File\n",
        "\n",
        "Let's create the service configuration:\n",
        "```yaml\n",
        "# service.yaml\n",
        "name: deploy-llama-3-70b\n",
        "image_uri: anyscale/ray-llm:2.49.0-py311-cu128 # Anyscale Ray Serve LLM image. Use `containerfile: ./Dockerfile` to use a custom Dockerfile.\n",
        "compute_config:\n",
        "  auto_select_worker_config: true \n",
        "working_dir: .\n",
        "cloud:\n",
        "applications:\n",
        "  # Point to your app in your Python module\n",
        "  - import_path: serve_llama_3_1_70b:app\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Launching the Service\n",
        "\n",
        "Now let's deploy our service to Anyscale:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!anyscale service deploy -f service.yaml --env HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running Inference on Anyscale\n",
        "\n",
        "Once deployed, you'll get an endpoint and authentication token. Let's see how to use them:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://deploy-llama-3-70b-jgz99.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com/v1\",\n",
        "    api_key=\"2YKUt_IJZ8q8GWT5VPHVitzsHKsddoL6mSszJxzwe5A\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"my-llama-3.1-70b\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me about Anyscale!\"}],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content:\n",
        "        print(content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shutting Down the Service\n",
        "\n",
        "When you're done with your service:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!anyscale service terminate -n deploy-llama-3-70b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Topics: Monitoring & Optimization\n",
        "\n",
        "Now let's explore advanced features for production deployments.\n",
        "\n",
        "### Enabling LLM Monitoring\n",
        "\n",
        "The Serve LLM Dashboard offers deep visibility into model performance. Let's enable comprehensive monitoring:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# serve_llama_3_1_70b.py\n",
        "from ray.serve.llm import LLMConfig, build_openai_app\n",
        "import os\n",
        "\n",
        "llm_config = LLMConfig(\n",
        "    model_loading_config=dict(\n",
        "        model_id=\"my-llama-3.1-70b\",\n",
        "        model_source=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
        "    ),\n",
        "    accelerator_type=\"L40S\",\n",
        "    deployment_config=dict(\n",
        "        autoscaling_config=dict(\n",
        "            min_replicas=1,\n",
        "            max_replicas=4,\n",
        "        )\n",
        "    ),\n",
        "    runtime_env=dict(\n",
        "        env_vars={\n",
        "            \"HF_TOKEN\": os.environ.get(\"HF_TOKEN\"),\n",
        "        }\n",
        "    ),\n",
        "    engine_kwargs=dict(\n",
        "        max_model_len=32768,\n",
        "        tensor_parallel_size=8,\n",
        "    ),\n",
        "    # Enable detailed engine metrics\n",
        "    log_engine_metrics=True\n",
        ")\n",
        "\n",
        "app = build_openai_app({\"llm_configs\": [llm_config]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anyscale provides an easy way to visualize your LLM metrics on an integrated Grafana dashboard.\n",
        "\n",
        "On your Anyscale Workspace or Service page, go to Metrics, then click on the View on Grafana dropdown and select Ray Serve LLM Dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve run serve_llama_3_1_70b:app --non-blocking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember shutting down your service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve shutdown -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Improving Concurrency\n",
        "\n",
        "Ray Serve LLM uses vLLM as its backend engine, which logs the maximum concurrency it can support.  \n",
        "Example log for 8xL40S:\n",
        "```console\n",
        "INFO 08-19 20:57:37 [kv_cache_utils.py:837] Maximum concurrency for 32,768 tokens per request: 17.79x\n",
        "```\n",
        "Let's explore optimization strategies:\n",
        "\n",
        "### Concurrency Optimization Strategies\n",
        "\n",
        "Below are key strategies to improve model concurrency and performance when serving LLMs.\n",
        "\n",
        "---\n",
        "\n",
        "**Example log (8Ã—L40S setup):**\n",
        "\n",
        "```\n",
        "INFO: Maximum concurrency for 32,768 tokens per request: 17.79x\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. Reduce `max_model_len`\n",
        "\n",
        "* `32,768` tokens â†’ concurrency â‰ˆ **18**\n",
        "* `16,384` tokens â†’ concurrency â‰ˆ **36**\n",
        "* **Trade-off:** shorter context window but higher concurrency\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Use Quantized Models\n",
        "\n",
        "* **FP16 â†’ FP8:** ~50% memory reduction\n",
        "* **FP8 â†’ INT4:** ~75% memory reduction\n",
        "* Frees up memory for the KV cache, enabling more concurrent requests\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Enable Pipeline Parallelism\n",
        "\n",
        "* Distribute layers across multiple nodes, set `pipeline_parallel_size > 1`\n",
        "* This increase the size of your KV cache, trading off on your latency due to the multi-node communication overhead\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. Scale with More Replicas\n",
        "\n",
        "* Horizontally scale across multiple nodes\n",
        "* Each replica runs an independent model instance\n",
        "* **Total concurrency = per-replica concurrency Ã— number of replicas**\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. Upgrade Hardware\n",
        "\n",
        "* Example: **L40S (48 GB) â†’ A100 (80 GB)**\n",
        "* More GPU memory allows higher concurrency\n",
        "* Faster interconnects (e.g., NVLink) reduce latency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Outlook\n",
        "\n",
        "Congratulations! You've successfully learned how to deploy a medium-sized LLM with Ray Serve LLM. Let's summarize what we've covered and look ahead to other possibilities.\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "**Module 2 Summary:**\n",
        "1. **Overview**: Understood why medium-sized models (70B parameters) are ideal for production workloads\n",
        "2. **Configuration**: Set up Ray Serve LLM with tensor parallelism across 8 GPUs\n",
        "3. **Local Deployment**: Deployed locally and tested with various inference scenarios\n",
        "4. **Anyscale Services**: Deployed to production with zero code changes\n",
        "5. **Advanced Topics**: Enabled monitoring, optimized concurrency\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **No Code Changes**: Same configuration works locally and in production\n",
        "- **Tensor Parallelism**: Essential for medium models to distribute across multiple GPUs\n",
        "- **Production Ready**: Anyscale Services provide enterprise-grade deployment\n",
        "- **Monitoring**: Comprehensive dashboards for performance optimization\n",
        "- **Scalability**: Multiple optimization strategies for different use cases\n",
        "\n",
        "### Related Examples & Templates\n",
        "\n",
        "Ray provides many more examples for different scenarios:\n",
        "\n",
        "**Ray Documentation Examples:**\n",
        "- [Deploy a small-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/small-size-llm/README.html) - 1-2 GPUs, prototyping\n",
        "- [Deploy a large-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/large-size-llm/README.html) - Multiple nodes, research\n",
        "\n",
        "**Anyscale Workspace Templates:**\n",
        "- [Anyscale LLM Deployment Templates](https://console.anyscale.com/template-preview/deployment-serve-llm) - Ready-to-run examples\n",
        "\n",
        "### How Other Sizes Differ\n",
        "\n",
        "Now that you've seen a medium model deployment, here's how other sizes would differ:\n",
        "\n",
        "**Small Models (7B-13B):**\n",
        "- No tensor parallelism needed\n",
        "- Single GPU deployment\n",
        "- Faster startup time\n",
        "- Lower concurrency but simpler setup\n",
        "\n",
        "**Large Models (400B+):**\n",
        "- Pipeline parallelism across multiple nodes\n",
        "- More complex infrastructure requirements\n",
        "- Higher costs but maximum capability\n",
        "- Research and specialized use cases\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Ready to explore more? Consider:\n",
        "1. **Try different model sizes** - Deploy small or large models\n",
        "2. **Experiment with optimizations** - Test quantization and concurrency tuning\n",
        "3. **Build applications** - Create end-to-end AI applications\n",
        "4. **Explore advanced features** - Multi-model deployments, custom endpoints\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Ray Serve LLM Documentation](https://docs.ray.io/en/latest/serve/llm/index.html)\n",
        "- [Anyscale LLM Serving Guide](https://docs.anyscale.com/llm/serving)\n",
        "- [vLLM Documentation](https://docs.vllm.ai/)\n",
        "- [Ray Community Forum](https://discuss.ray.io/)\n",
        "\n",
        "\n",
        "You now have the knowledge to deploy medium-sized LLMs in production with Ray Serve LLM!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
