{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges in LLM Serving\n",
    "\n",
    "Serving LLMs in production presents several unique challenges that traditional model serving doesn't face. Let's explore these challenges and understand why they matter.\n",
    "\n",
    "### 1. Memory Management\n",
    "\n",
    "Deploying LLMs is a **memory-intensive** task. A non-exhaustive list of memory constraints are:\n",
    "| Component | Description | Memory Impact |\n",
    "|-----------|-------------|---------------|\n",
    "| **Model Weights** | Model parameters | 7B model \u2248 14GB (FP16) |\n",
    "| **KV Cache** | Token representations | Depends on context length |\n",
    "| **Activations** | Temporary buffers | Varies with batch size |\n",
    "\n",
    "**Example**: A 7B parameter model in FP16 precision requires approximately 14GB just for the model weights, not including the KV cache or activations.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/public-images/ray-serve-llm/diagrams/gpu-memory.png\" width=\"800\">\n",
    "\n",
    "You can distribute your deployment on multiple GPUs or nodes. For example you could split the model accross multiple GPUs on a single node or accross multiple GPUs on multiple nodes.  \n",
    "\n",
    "\n",
    "\n",
    "See examples below for examples of different types of deployment:\n",
    "- Single node, single GPU: [Deploy a small-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/small-size-llm/README.html)\n",
    "- Single node, multiple GPU with tensor parallelism: [Deploy a medium-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/medium-size-llm/README.html)\n",
    "- Multiple nodes, multiple GPU with tensor and pipeline parallelism: [Deploy a large-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/large-size-llm/README.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Latency Requirements\n",
    "\n",
    "Users expect **fast, interactive responses** from LLM applications:\n",
    "\n",
    "- **Time to First Token (TTFT)**: How long until the first token appears\n",
    "- **Time Per Output Token (TPOT)**: How long between subsequent tokens\n",
    "- **Total Response Time**: End-to-end latency\n",
    "\n",
    "### 3. Scalability Demands\n",
    "\n",
    "Production traffic is **unpredictable and bursty**:\n",
    "- Traffic spikes during peak hours\n",
    "- Need to scale up quickly during high demand\n",
    "- Scale down to zero during idle periods to save costs\n",
    "\n",
    "### 4. Cost Optimization\n",
    "\n",
    "GPUs represent **significant infrastructure costs**:\n",
    "- Maximize hardware utilization\n",
    "- Scale to zero during idle periods\n",
    "- Choose appropriate GPU types for your workload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not Kubernetes ?\n",
    "\n",
    "You could either use Ray Serve or Kubernetes microservices to solve the challenges above. They are not mutually exclusive, as Ray Serve can run on Kubernetes. The differences are mostly about who does the orchestration and how much abstraction you want from the inference pipeline.\n",
    "\n",
    "**Ray Serve LLM**\n",
    "\n",
    "* Python-native orchestration (routing, batching, streaming).\n",
    "* Built-in autoscaling, backpressure, health checks or [LLM-optimized routing](https://docs.ray.io/en/latest/serve/llm/prefix-aware-request-router.html).\n",
    "* Actor-based sharding across nodes/GPUs.\n",
    "* Easy multi-model serving behind one endpoint.\n",
    "\n",
    "**Kubernetes**\n",
    "\n",
    "* Pod = unit per node; multi-node model parallelism needs extra controllers/operators.\n",
    "* Batching/routing/backpressure are DIY (app or sidecars).\n",
    "* Strong platform features (networking, security, quotas), but inference control isn\u2019t built-in.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}