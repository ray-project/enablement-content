{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Ray Serve LLM\n",
    "\n",
    "Now that we understand the fundamentals, let's see how to get started with Ray Serve LLM. The process involves three main steps:\n",
    "\n",
    "1. **Configure** your LLM deployment\n",
    "2. **Deploy** the service\n",
    "3. **Query** the deployed model\n",
    "4. **Shutdown** the deployment\n",
    "\n",
    "### Step 1: Configuration\n",
    "\n",
    "Let's create a simple configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#serve_llama.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    # Model loading configuration\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-llama\", # custom name for the model\n",
    "        model_source=\"unsloth/Meta-Llama-3.1-8B-Instruct\", # huggingface model repo\n",
    "    ),\n",
    "    accelerator_type=\"L4\", # device to use (picked from your ray cluster)\n",
    "    ## Optional: configure Ray Serve autoscaling\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1, # keep at least 1 replica up to avoid cold starts\n",
    "            max_replicas=2, # no more than 2 replicas to control cost\n",
    "        )\n",
    "    ),\n",
    "    # Configure your vLLM engine. Follow the same API as vLLM\n",
    "    # https://docs.vllm.ai/en/stable/configuration/engine_args.html\n",
    "    engine_kwargs=dict(max_model_len=8192),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Deployment\n",
    "\n",
    "Deployment can be done locally or on Anyscale Services:\n",
    "\n",
    "**Local Deployment**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run serve_llama:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anyscale Services**:\n",
    "\n",
    "To deploy your LLM with Anyscale Service, configure your cloud and compute configuration and point to your LLM configuration:\n",
    "```yaml\n",
    "# service.yaml\n",
    "name: deploy-llama-3-8b\n",
    "image_uri: anyscale/ray-llm:2.49.0-py311-cu128 # Anyscale Ray Serve LLM image. Use `containerfile: ./Dockerfile` to use a custom Dockerfile.\n",
    "compute_config:\n",
    "  auto_select_worker_config: true \n",
    "working_dir: .\n",
    "cloud:\n",
    "applications:\n",
    "  # Point to your app in your Python module\n",
    "  - import_path: serve_llama:app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy your service:\n",
    "```bash\n",
    "!anyscale service deploy -f service.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Querying\n",
    "\n",
    "Once deployed, you can use the OpenAI Python client with `base_url` pointing to your Ray Serve endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# because deployed locally, we use localhost:8000 and a dummy placeholder API key\n",
    "base_url = \"http://localhost:8000\"\n",
    "token=\"DUMMY_KEY\"\n",
    "client = OpenAI(base_url= urljoin(base_url, \"v1\"), api_key=token)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-llama\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream and print JSON\n",
    "for chunk in response:\n",
    "    data = chunk.choices[0].delta.content\n",
    "    if data:\n",
    "        print(data, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Shutdown\n",
    "\n",
    "Shutdown a local deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminate an Anyscale service:\n",
    "```bash\n",
    "anyscale service terminate deploy-my-llama\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}