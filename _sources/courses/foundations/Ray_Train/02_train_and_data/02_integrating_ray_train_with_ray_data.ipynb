{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ 02 Â· Integrating Ray Train with Ray Data  \n",
    "In this module youâ€™ll extend distributed training with **Ray Train** by adding **Ray Data** to the pipeline. Instead of relying on a local PyTorch DataLoader, youâ€™ll stream batches directly from a distributed **Ray Dataset**, enabling scalable preprocessing and just-in-time data loading across the cluster.  \n",
    "\n",
    "### What youâ€™ll learn & take away  \n",
    "* When to integrate **Ray Data** with Ray Train â€” e.g., for CPU-heavy preprocessing, online augmentations, or multi-format data ingestion  \n",
    "* How to replace `DataLoader` with **`iter_torch_batches()`** to stream batches into your training loop  \n",
    "* How to shard, shuffle, and preprocess data in parallel across the cluster before feeding it into GPUs  \n",
    "* How to define a **training loop** that consumes Ray Dataset shards instead of DataLoader tuples  \n",
    "* How to prepare datasets (For example, Parquet format) so they can be efficiently read and transformed with Ray Data  \n",
    "* How to pass Ray Datasets into the `TorchTrainer` with the `datasets` parameter  \n",
    "\n",
    "> With Ray Data, you can scale preprocessing and training independently â€” CPUs handle input pipelines, GPUs focus on training â€” ensuring **higher utilization and throughput** in your distributed workloads.  \n",
    "\n",
    "Note that the code blocks for this module will depend on the previous module, **Introduction to Ray Train**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Integrating Ray Train with Ray Data  \n",
    "\n",
    "Use both Ray Train and Ray Data when you face one of the following challenges:  \n",
    "| Challenge | Detail | Solution |\n",
    "| --- | --- | --- |\n",
    "| Need to perform online or just-in-time data processing | The training pipeline requires processing data on the fly, such as data augmentation, normalization, or other transformations that may differ for each training epoch. | Ray Train's integration with Ray Data makes it easy to implement just-in-time data processing. |\n",
    "| Need to improve hardware utilization | Training and data processing need to be scaled independently to keep GPUs fully utilized, especially when preprocessing is CPU-intensive. | Ray Data can distribute data processing across multiple CPU nodes, while Ray Train runs the training loop on GPUs. |\n",
    "| Need a consistent interface for loading data | The training process may need to load data from various sources, such as Parquet, CSV, or lakehouses. | Ray Data provides a consistent interface for loading, shuffling, sharding, and batching data for training loops. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Â· Define Training Loop with Ray Data  \n",
    "\n",
    "Here we reimplement the training loop, but this time using **Ray Data** instead of a PyTorch `DataLoader`.  \n",
    "\n",
    "Key differences from the previous version:  \n",
    "- **Data loader** â†’ Built with `build_data_loader_ray_train_ray_data()`, which streams batches from a Ray Dataset shard (details in the following block).  \n",
    "- **Batching** â†’ Still split by `global_batch_size // world_size`, but batches are now **dictionaries** with keys `\"image\"` and `\"label\"`.  \n",
    "- **No device management needed** â†’ Ray Data automatically moves batches to the correct device, so we no longer call `sampler.set_epoch()` or `to(\"cuda\")`.  \n",
    "\n",
    "The rest of the loop (forward pass, loss computation, backward pass, optimizer step, metric logging, and checkpointing) stays the same.  \n",
    "\n",
    "This pattern shows how seamlessly **Ray Data integrates with Ray Train**, replacing `DataLoader` while keeping the training logic identical.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Training loop using Ray Data\n",
    "\n",
    "def train_loop_ray_train_ray_data(config: dict):\n",
    "    # Same as before: define loss, model, optimizer\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Different: build data loader from Ray Data instead of PyTorch DataLoader\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    data_loader = build_data_loader_ray_train_ray_data(batch_size=batch_size) \n",
    "    \n",
    "    # Same: loop over epochs\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        # Different: no sampler.set_epoch(), Ray Data handles shuffling internally\n",
    "\n",
    "        # Different: batches are dicts {\"image\": ..., \"label\": ...} not tuples\n",
    "        for batch in data_loader: \n",
    "            outputs = model(batch[\"image\"])\n",
    "            loss = criterion(outputs, batch[\"label\"])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "        # Same: report metrics and save checkpoint each epoch\n",
    "        metrics = print_metrics_ray_train(loss, epoch)\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Â· Build DataLoader from Ray Data  \n",
    "\n",
    "Instead of using PyTorchâ€™s `DataLoader`, we now build a loader from a **Ray Dataset shard**.  \n",
    "\n",
    "- `ray.train.get_dataset_shard(\"train\")` â†’ retrieves the shard of the training dataset assigned to the current worker.  \n",
    "- `.iter_torch_batches()` â†’ streams the shard as PyTorch-compatible batches.  \n",
    "  * Each batch is a **dictionary** (e.g., `{\"image\": tensor, \"label\": tensor}`).  \n",
    "  * Supports options like `batch_size` and `prefetch_batches` for performance tuning.  \n",
    "\n",
    "This integration ensures that data is **sharded, shuffled, and moved to the right device automatically**, while still looking and feeling like a familiar PyTorch data loader.  \n",
    "\n",
    "**Note:** Use [`iter_torch_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.iter_torch_batches.html) to build a PyTorch-compatible data loader from a Ray Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Build a Ray Dataâ€“backed data loader\n",
    "\n",
    "def build_data_loader_ray_train_ray_data(batch_size: int, prefetch_batches: int = 2):\n",
    "\n",
    "    # Different: instead of creating a PyTorch DataLoader,\n",
    "    # fetch the training dataset shard for this worker\n",
    "    dataset_iterator = ray.train.get_dataset_shard(\"train\")\n",
    "\n",
    "    # Convert the shard into a PyTorch-style iterator\n",
    "    # - Returns dict batches: {\"image\": ..., \"label\": ...}\n",
    "    # - prefetch_batches controls pipeline buffering\n",
    "    data_loader = dataset_iterator.iter_torch_batches(\n",
    "        batch_size=batch_size, prefetch_batches=prefetch_batches\n",
    "    )\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Â· Prepare Dataset for Ray Data  \n",
    "\n",
    "Ray Data works best with data in **tabular formats** such as Parquet.  \n",
    "In this step we:  \n",
    "\n",
    "- Convert the MNIST dataset into a **pandas DataFrame** with two columns:  \n",
    "  * `\"image\"` â†’ raw image arrays  \n",
    "  * `\"label\"` â†’ digit class (0â€“9)  \n",
    "- Write the DataFrame to disk in **Parquet format** under `/mnt/cluster_storage/`.  \n",
    "\n",
    "Parquet is efficient for both reading and distributed processing, making it a good fit for Ray Data pipelines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Convert MNIST dataset into Parquet for Ray Data\n",
    "\n",
    "# Build a DataFrame with image arrays and labels\n",
    "df = pd.DataFrame({\n",
    "    \"image\": dataset.data.tolist(),   # raw image pixels (as lists)\n",
    "    \"label\": dataset.targets          # digit labels 0â€“9\n",
    "})\n",
    "\n",
    "# Persist the dataset in Parquet format (columnar, efficient for Ray Data)\n",
    "df.to_parquet(\"/mnt/cluster_storage/cifar10.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 Â· Load Dataset into Ray Data  \n",
    "\n",
    "Now that the training data is stored as Parquet, we can load it back into a **Ray Dataset**:  \n",
    "\n",
    "- Use `ray.data.read_parquet()` to create a distributed Ray Dataset from the Parquet file.  \n",
    "- Each row has two columns: `\"image\"` (raw pixel array) and `\"label\"` (digit class).  \n",
    "- The dataset is automatically **sharded across the Ray cluster**, so multiple workers can read and process it in parallel.  \n",
    "\n",
    "This Ray Dataset will later be passed to the `TorchTrainer` for distributed training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Load the Parquet dataset into a Ray Dataset\n",
    "\n",
    "# Read the Parquet file â†’ creates a distributed Ray Dataset\n",
    "train_ds = ray.data.read_parquet(\"/mnt/cluster_storage/cifar10.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 Â· Define Image Transformation  \n",
    "\n",
    "To make the dataset usable by PyTorch, we need to preprocess the raw image arrays with the same steps that pytorch data loader does.  \n",
    "\n",
    "- Define a function `transform_images(row)` that:  \n",
    "  * Converts the `\"image\"` array from `numpy` into a PIL image.  \n",
    "  * Applies the standard PyTorch transforms:  \n",
    "    - `ToTensor()` â†’ converts the image to a tensor.  \n",
    "    - `Normalize((0.5,), (0.5,))` â†’ scales pixel values to the range [-1, 1].  \n",
    "  * Replaces the `\"image\"` entry in the row with the transformed tensor.  \n",
    "\n",
    "This function will later be applied in parallel across the Ray Dataset.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Define preprocessing transform for Ray Data\n",
    "\n",
    "def transform_images(row: dict):\n",
    "    # Convert numpy array to a PIL image, then apply TorchVision transforms\n",
    "    transform = Compose([\n",
    "        ToTensor(),              # convert to tensor\n",
    "        Normalize((0.5,), (0.5,)) # normalize to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    # Ensure image is in uint8 before conversion\n",
    "    image_arr = np.array(row[\"image\"], dtype=np.uint8)\n",
    "\n",
    "    # Apply transforms and replace the \"image\" field with tensor\n",
    "    row[\"image\"] = transform(Image.fromarray(image_arr))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note**: Unlike the PyTorch DataLoader, the preprocessing can now occur on any node in the cluster.\n",
    "\n",
    "The data will be passed to training workers via the ray object store (a distributed in-memory object store).\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 Â· Apply Transformations with Ray Data  \n",
    "\n",
    "Now we apply the preprocessing function to the dataset using `map()`:  \n",
    "\n",
    "- `train_ds.map(transform_images)` â†’ runs the `transform_images` function on every row of the dataset.  \n",
    "- Transformations are executed **in parallel across the cluster**, so preprocessing can scale independently of training.  \n",
    "- The transformed dataset now has:  \n",
    "  * `\"image\"` â†’ normalized PyTorch tensors  \n",
    "  * `\"label\"` â†’ unchanged integer labels  \n",
    "\n",
    "This makes the dataset ready to be streamed into the training loop.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Apply the preprocessing transform across the Ray Dataset\n",
    "\n",
    "# Run transform_images() on each row (parallelized across cluster workers)\n",
    "train_ds = train_ds.map(transform_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 Â· Configure `TorchTrainer` with Ray Data  \n",
    "\n",
    "Now we connect the Ray Dataset to the training loop using the `datasets` parameter in `TorchTrainer`:  \n",
    "\n",
    "- **`datasets={\"train\": train_ds}`** â†’ makes the transformed dataset available to the training loop as the `\"train\"` shard.  \n",
    "- **`train_loop_ray_train_ray_data`** â†’ the per-worker training loop that consumes Ray Data batches.  \n",
    "- **`train_loop_config`** â†’ passes hyperparameters (`num_epochs`, `global_batch_size`).  \n",
    "- **`scaling_config`** â†’ specifies the number of workers and GPUs to use (same as before).  \n",
    "- **`run_config`** â†’ defines storage for checkpoints and metrics.  \n",
    "\n",
    "This setup allows Ray Train to automatically shard and stream the Ray Dataset into each worker during training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Configure TorchTrainer with Ray Data integration\n",
    "\n",
    "# Wrap Ray Dataset in a dict â†’ accessible as \"train\" inside the training loop\n",
    "datasets = {\"train\": train_ds}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train_ray_data,  # training loop consuming Ray Data\n",
    "    train_loop_config={             # hyperparameters\n",
    "        \"num_epochs\": 1,\n",
    "        \"global_batch_size\": 512,\n",
    "    },\n",
    "    scaling_config=scaling_config,  # number of workers + GPU/CPU resources\n",
    "    run_config=RunConfig(\n",
    "        storage_path=storage_path, \n",
    "        name=\"dist-cifar-res18-ray-data\"\n",
    "    ),                              # where to store checkpoints/logs\n",
    "    datasets=datasets,              # provide Ray Dataset shards to workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 Â· Launch Training with Ray Data  \n",
    "\n",
    "Finally, call `trainer.fit()` to start the distributed training job.  \n",
    "\n",
    "- Ray will automatically:  \n",
    "  * Launch workers according to the `scaling_config`.  \n",
    "  * Stream sharded, preprocessed batches from the Ray Dataset into each worker.  \n",
    "  * Run the training loop (`train_loop_ray_train_ray_data`) on every worker in parallel.  \n",
    "  * Report metrics and save checkpoints to the configured storage path.  \n",
    "\n",
    "With this call, you now have a fully **end-to-end distributed pipeline** where **Ray Data handles ingestion + preprocessing** and **Ray Train handles multi-GPU training**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Start the distributed training job with Ray Data integration\n",
    "\n",
    "# Launches the training loop across all workers\n",
    "# - Streams preprocessed Ray Dataset batches into each worker\n",
    "# - Reports metrics and checkpoints to cluster storage\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
