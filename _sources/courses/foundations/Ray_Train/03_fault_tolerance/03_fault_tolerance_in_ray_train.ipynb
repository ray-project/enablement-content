{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train  \n",
    "In this module you\u2019ll learn how **Ray Train** handles failures and how to make your training jobs **resilient** with checkpointing and recovery. You\u2019ll see both **automatic retries** and **manual restoration**, and how to modify the training loop so it can safely resume from the latest checkpoint.  \n",
    "\n",
    "### What you\u2019ll learn & take away  \n",
    "* How Ray Train uses **automatic retries** to restart failed workers without losing progress  \n",
    "* How to modify the training loop with **`get_checkpoint()`** to enable checkpoint loading  \n",
    "* How to save additional state (e.g., optimizer and epoch) alongside the model for full recovery  \n",
    "* How to configure **`FailureConfig`** to set retry behavior  \n",
    "* How to perform a **manual restoration** if retries are exhausted, resuming training from the last checkpoint  \n",
    "* Why checkpointing to **persistent storage** is essential for reliable recovery  \n",
    "\n",
    "> With fault tolerance enabled, you can run long, large-scale training jobs confidently \u2014 knowing they can recover from failures without starting over.  \n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/stable-diffusion/diagrams/fault_tolerant_cropped_v2.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 \u00b7 Modify Training Loop to Enable Checkpoint Loading  \n",
    "\n",
    "To support fault tolerance, we extend the training loop so it can **resume from a previously saved checkpoint**.  \n",
    "\n",
    "Key additions:  \n",
    "- Call `ray.train.get_checkpoint()` to check if a checkpoint is available.  \n",
    "- If found, restore:  \n",
    "  * The **model state** (`model.pt`)  \n",
    "  * The **optimizer state** (`optimizer.pt`)  \n",
    "  * The **last completed epoch** (`extra_state.pt`)  \n",
    "- Update `start_epoch` so training resumes from the correct place.  \n",
    "\n",
    "The rest of the loop (forward pass, backward pass, optimizer step, and metrics reporting) is the same, except it now starts from `start_epoch` instead of 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Training loop with checkpoint loading for fault tolerance\n",
    "\n",
    "def train_loop_ray_train_with_checkpoint_loading(config: dict):\n",
    "    # Same setup as before: loss, model, optimizer\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Same data loader logic as before\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    data_loader = build_data_loader_ray_train_ray_data(batch_size=batch_size)\n",
    "\n",
    "    # Default: start at epoch 0 unless a checkpoint is available\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Attempt to load from latest checkpoint\n",
    "    checkpoint = ray.train.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        # Continue training from a previous checkpoint\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            # Restore model + optimizer state\n",
    "            model_state_dict = torch.load(\n",
    "                os.path.join(ckpt_dir, \"model.pt\"),\n",
    "            )\n",
    "            # Load the model and optimizer state\n",
    "            model.module.load_state_dict(model_state_dict)\n",
    "            optimizer.load_state_dict(\n",
    "                torch.load(os.path.join(ckpt_dir, \"optimizer.pt\"))\n",
    "            )\n",
    "\n",
    "            # Resume from last epoch + 1\n",
    "            start_epoch = (\n",
    "                torch.load(os.path.join(ckpt_dir, \"extra_state.pt\"))[\"epoch\"] + 1\n",
    "            )\n",
    "\n",
    "    # Same training loop as before except it starts at a parameterized start_epoch\n",
    "    for epoch in range(start_epoch, config[\"num_epochs\"]):\n",
    "        for batch in data_loader:\n",
    "            outputs = model(batch[\"image\"])\n",
    "            loss = criterion(outputs, batch[\"label\"])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Report metrics and save model + optimizer + epoch state\n",
    "        metrics = print_metrics_ray_train(loss,  epoch)\n",
    "\n",
    "        # We now save the optimizer and epoch state in addition to the model\n",
    "        save_checkpoint_and_metrics_ray_train_with_extra_state(\n",
    "            model, metrics, optimizer, epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 \u00b7 Save Full Checkpoint with Extra State  \n",
    "\n",
    "To support fault-tolerant recovery, we extend checkpoint saving to include not just the model, but also the **optimizer state** and the **current epoch**.  \n",
    "\n",
    "- **`model.pt`** \u2192 model weights (unwrap DDP with `.module`).  \n",
    "- **`optimizer.pt`** \u2192 optimizer state for resuming training seamlessly.  \n",
    "- **`extra_state.pt`** \u2192 stores metadata (here, the current epoch).  \n",
    "\n",
    "Only the **rank-0 worker** writes the checkpoint to avoid duplication, but all workers still call `ray.train.report()` to keep the loop synchronized.  \n",
    "\n",
    "This ensures that if training is interrupted, Ray Train can restore **model weights, optimizer progress, and the correct epoch** before continuing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Save checkpoint with model, optimizer, and epoch state\n",
    "\n",
    "def save_checkpoint_and_metrics_ray_train_with_extra_state(\n",
    "    model: torch.nn.Module,\n",
    "    metrics: dict[str, float],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        checkpoint = None\n",
    "        # Only rank-0 worker saves files to disk\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "                # Save all state required for full recovery\n",
    "                torch.save(\n",
    "                    model.module.state_dict(),  # unwrap DDP before saving\n",
    "                    os.path.join(temp_checkpoint_dir, \"model.pt\"),\n",
    "                )\n",
    "                torch.save(\n",
    "                    optimizer.state_dict(),     # include optimizer state\n",
    "                    os.path.join(temp_checkpoint_dir, \"optimizer.pt\"),\n",
    "                )\n",
    "                torch.save(\n",
    "                    {\"epoch\": epoch},           # store last completed epoch\n",
    "                    os.path.join(temp_checkpoint_dir, \"extra_state.pt\"),\n",
    "                )\n",
    "                # Package into a Ray checkpoint\n",
    "                checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "        \n",
    "        # Report metrics and attach checkpoint (only rank-0 attaches checkpoint)\n",
    "        ray.train.report(  \n",
    "            metrics,  \n",
    "            checkpoint=checkpoint,\n",
    "            )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 \u00b7 Configure Automatic Retries with `FailureConfig`  \n",
    "\n",
    "Now that the training loop can load from checkpoints, we can enable **automatic retries** in case of worker or node failures.  \n",
    "\n",
    "- **`FailureConfig(max_failures=3)`** \u2192 allows the job to retry up to 3 times before giving up.  \n",
    "- Pass this `failure_config` into `RunConfig` so Ray Train knows how to handle failures.  \n",
    "- When a failure happens, Ray will:  \n",
    "  1. Restart the failed workers.  \n",
    "  2. Reload the latest checkpoint.  \n",
    "  3. Resume training from the last saved epoch.  \n",
    "\n",
    "This setup makes training jobs resilient to transient hardware or cluster issues without requiring manual intervention.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Configure TorchTrainer with fault-tolerance enabled\n",
    "\n",
    "# Allow up to 3 automatic retries if workers fail\n",
    "failure_config = ray.train.FailureConfig(max_failures=3)\n",
    "\n",
    "experiment_name = \"fault-tolerant-cifar-vit\"\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_ray_train_with_checkpoint_loading,  # fault-tolerant loop\n",
    "    train_loop_config={   # hyperparameters\n",
    "        \"num_epochs\": 1,\n",
    "        \"global_batch_size\": 512,\n",
    "    },\n",
    "    scaling_config=scaling_config,  # resource scaling as before\n",
    "    run_config=ray.train.RunConfig(\n",
    "        name=\"fault-tolerant-cifar-vit\",\n",
    "        storage_path=storage_path,      # persistent checkpoint storage\n",
    "        failure_config=failure_config,  # enable automatic retries\n",
    "    ),\n",
    "    datasets=datasets,  # Ray Dataset shard for each worker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 \u00b7 Launch Fault-Tolerant Training  \n",
    "\n",
    "Finally, call `trainer.fit()` to start the training job.  \n",
    "\n",
    "With the **fault-tolerant loop** and **`FailureConfig`** in place, Ray Train will:  \n",
    "- Run the training loop on all workers.  \n",
    "- If a failure occurs (e.g., worker crash, node preemption), automatically restart workers.  \n",
    "- Reload the latest checkpoint and continue training without losing progress.  \n",
    "\n",
    "This makes your training job robust against transient infrastructure failures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Start the fault-tolerant training job\n",
    "\n",
    "# Launches training with checkpointing + automatic retries enabled\n",
    "# If workers fail, Ray will reload the latest checkpoint and resume\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 \u00b7 Manual Restoration from Checkpoints  \n",
    "\n",
    "If the maximum number of retries is reached, you can still **manually restore training** by creating a new `TorchTrainer` with the same configuration:  \n",
    "\n",
    "- Use the same `train_loop_ray_train_with_checkpoint_loading` so the loop can resume from a checkpoint.  \n",
    "- Provide the same `run_config` (name, storage path, and failure config).  \n",
    "- Pass in the same dataset and scaling configuration.  \n",
    "\n",
    "Ray Train will detect the latest checkpoint in the specified `storage_path` and resume training from that point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Manually restore a trainer from the last checkpoint\n",
    "\n",
    "restored_trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_ray_train_with_checkpoint_loading,  # loop supports checkpoint loading\n",
    "        train_loop_config={   # hyperparameters must match\n",
    "        \"num_epochs\": 1,\n",
    "        \"global_batch_size\": 512,\n",
    "    },\n",
    "    scaling_config=scaling_config,  # same resource setup as before\n",
    "    run_config=ray.train.RunConfig(\n",
    "        name=\"fault-tolerant-cifar-vit\",  # must match previous run name\n",
    "        storage_path=storage_path,       # path where checkpoints are saved\n",
    "        failure_config=failure_config,   # still allow retries\n",
    "    ),\n",
    "    datasets=datasets,  # same dataset as before\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 \u00b7 Resume Training from the Last Checkpoint  \n",
    "\n",
    "Calling `restored_trainer.fit()` will continue training from the most recent checkpoint found in the specified storage path.  \n",
    "\n",
    "- If all epochs were already completed in the previous run, the trainer will terminate immediately.  \n",
    "- If training was interrupted mid-run, it will resume from the saved epoch, restoring both the **model** and **optimizer** state.  \n",
    "- The returned `Result` object confirms that training picked up correctly and contains metrics, checkpoints, and logs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Resume training from the last checkpoint\n",
    "\n",
    "# Fit the restored trainer \u2192 continues from last saved epoch\n",
    "# If all epochs are already complete, training ends immediately\n",
    "result = restored_trainer.fit()\n",
    "\n",
    "# Display final training results (metrics, checkpoints, etc.)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 \u00b7 Clean Up Cluster Storage  \n",
    "\n",
    "Finally, remove any tutorial artifacts from **persistent cluster storage**:  \n",
    "\n",
    "- Deletes the **downloaded MNIST dataset** (`/mnt/cluster_storage/MNIST`).  \n",
    "- Deletes the **training outputs** (`/mnt/cluster_storage/training`).  \n",
    "- Deletes the **Parquet dataset** used for Ray Data (`/mnt/cluster_storage/cifar10.parquet`).  \n",
    "\n",
    "This keeps your shared storage clean and avoids leftover data or files from occupying space.  \n",
    "Run this only when you\u2019re sure you no longer need the data, checkpoints, or Parquet files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Cleanup Cluster Storage\n",
    "\n",
    "# Paths to remove \u2192 include MNIST data, training outputs, and cifar10.parquet\n",
    "paths_to_delete = [\n",
    "    \"/mnt/cluster_storage/MNIST\",\n",
    "    \"/mnt/cluster_storage/training\",\n",
    "    \"/mnt/cluster_storage/cifar10.parquet\",\n",
    "]\n",
    "\n",
    "for path in paths_to_delete:\n",
    "    if os.path.exists(path):\n",
    "        # Handle directories vs. files\n",
    "        if os.path.isdir(path):\n",
    "            shutil.rmtree(path)       # recursively delete directory\n",
    "        else:\n",
    "            os.remove(path)           # delete single file\n",
    "        print(f\"Deleted: {path}\")\n",
    "    else:\n",
    "        print(f\"Not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Wrapping Up & Next Steps  \n",
    "\n",
    "Nice work -- you completed a full, production-style workflow with **Ray Train on Anyscale**, then extended it with **Ray Data**, and finally added **fault tolerance**. Here\u2019s what you accomplished across the three modules:\n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Module 01 \u00b7 Introduction to Ray Train  \n",
    "- Scaled PyTorch DDP with **`TorchTrainer`** using **`ScalingConfig`** and **`RunConfig`**  \n",
    "- Wrapped code for multi-GPU with **`prepare_model()`** and **`prepare_data_loader()`**  \n",
    "- Reported **metrics** and saved **checkpoints** via `ray.train.report(...)` (rank-0 checkpointing best practice)  \n",
    "- Inspected results from the **`Result`** object and served **GPU inference** with a Ray actor  \n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Module 02 \u00b7 Integrating Ray Train with Ray Data  \n",
    "- Prepared MNIST as **Parquet** and loaded it as a **Ray Dataset**  \n",
    "- Streamed batches with **`iter_torch_batches()`** and consumed dict batches in the training loop  \n",
    "- Passed datasets to the trainer via **`datasets={\"train\": ...}`**  \n",
    "- Decoupled CPU preprocessing from GPU training for **better utilization and throughput**  \n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Module 03 \u00b7 Fault Tolerance in Ray Train  \n",
    "- Enabled resume-from-checkpoint using **`ray.train.get_checkpoint()`**  \n",
    "- Saved full state (model, **optimizer**, **epoch**) for robust restoration  \n",
    "- Configured **`FailureConfig(max_failures=...)`** for automatic retries  \n",
    "- Performed **manual restoration** by re-creating a trainer with the same `RunConfig`  \n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\ude80 Where to go next  \n",
    "- **Scale up**: Increase `num_workers`, try multi-node clusters, or switch to **FSDP** via `prepare_model(parallel_strategy=\"fsdp\")`.  \n",
    "- **Input pipelines**: Add augmentations, caching, and windowed shuffles in **Ray Data**; try multi-file Parquet or lakehouse sources.  \n",
    "- **Experiment tracking**: Log metrics to external systems (Weights & Biases, MLflow) alongside `ray.train.report()`.  \n",
    "- **Larger models**: Integrate **DeepSpeed** or parameter-efficient fine-tuning templates.  \n",
    "- **Productionization**: Store checkpoints in cloud storage (S3/GCS/Azure), wire up alerts/dashboards, and add CI for smoke tests.  \n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcda Next Tutorials in the Course  \n",
    "In the next tutorials, you\u2019ll find **end-to-end workload examples** for using Ray Train on Anyscale (e.g., recommendation systems, vision, NLP, generative models).  \n",
    "\n",
    "\ud83d\udc49 You only need to pick **one** of these workloads to work through in the course \u2014 but you can explore more if you\u2019re curious!  \n",
    "\n",
    "---\n",
    "\n",
    "> With these patterns\u2014**distributed training**, **scalable data ingestion**, and **resilient recovery**\u2014you\u2019re ready to run larger, longer, and more reliable training jobs on Anyscale.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}