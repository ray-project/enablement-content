{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18 \u00b7 Load a Checkpoint for Inference  \n",
    "\n",
    "After training, we often want to reload the model and use it for predictions.  \n",
    "Here we define a **Ray actor** (`ModelWorker`) that loads the checkpointed ResNet-18 onto a GPU and serves inference requests.  \n",
    "\n",
    "- **Initialization (`__init__`)**:  \n",
    "  * Reads the checkpoint directory using `checkpoint.as_directory()`.  \n",
    "  * Loads the model weights into a fresh ResNet-18.  \n",
    "  * Moves the model to GPU and sets it to evaluation mode.  \n",
    "\n",
    "- **Prediction (`predict`)**:  \n",
    "  * Accepts either a single image (`[C,H,W]`) or a batch (`[B,C,H,W]`).  \n",
    "  * Ensures the tensor is correctly shaped and moved to GPU.  \n",
    "  * Runs inference in `torch.inference_mode()` for efficiency.  \n",
    "  * Returns the predicted class indices as a Python list.  \n",
    "\n",
    "Finally, we launch the actor with `ModelWorker.remote(result.checkpoint)`.  \n",
    "This spawns a dedicated process with **1 GPU** attached that can serve predictions using the trained model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Define a Ray actor to load the trained model and run inference\n",
    "\n",
    "@ray.remote(num_gpus=1)  # allocate 1 GPU to this actor\n",
    "class ModelWorker:\n",
    "    def __init__(self, checkpoint):\n",
    "        # Load model weights from the Ray checkpoint (on CPU first)\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            model_path = os.path.join(ckpt_dir, \"model.pt\")\n",
    "            state_dict = torch.load(\n",
    "                model_path,\n",
    "                map_location=torch.device(\"cpu\"),\n",
    "                weights_only=True,\n",
    "            )\n",
    "        # Rebuild the model, load weights, move to GPU, and set to eval mode\n",
    "        self.model = build_resnet18()\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model.to(\"cuda\")\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.inference_mode()  # disable autograd for faster inference\n",
    "    def predict(self, batch):\n",
    "        \"\"\"\n",
    "        batch: torch.Tensor or numpy array with shape [B,C,H,W] or [C,H,W]\n",
    "        returns: list[int] predicted class indices\n",
    "        \"\"\"\n",
    "        x = torch.as_tensor(batch)\n",
    "        if x.ndim == 3:          # single image \u2192 add batch dimension\n",
    "            x = x.unsqueeze(0)   # shape becomes [1,C,H,W]\n",
    "        x = x.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        logits = self.model(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return preds.detach().cpu().tolist()\n",
    "\n",
    "# Create a fresh actor instance (avoid naming conflicts)\n",
    "worker = ModelWorker.remote(result.checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19 \u00b7 Run Inference and Visualize Predictions  \n",
    "\n",
    "With the `ModelWorker` actor running on GPU, we can now generate predictions on random samples from the MNIST dataset and plot them.  \n",
    "\n",
    "Steps in this cell:  \n",
    "1. **Normalization on CPU**  \n",
    "   - Convert each image to a tensor with `ToTensor()`.  \n",
    "   - Apply channel-specific normalization (`0.5` mean / std).  \n",
    "   - Keep this preprocessing on CPU for efficiency.  \n",
    "\n",
    "2. **Prediction on GPU via Actor**  \n",
    "   - Each normalized image is expanded to shape `[1, C, H, W]`.  \n",
    "   - The tensor is sent to the remote `ModelWorker` for inference.  \n",
    "   - `ray.get(worker.predict.remote(x))` retrieves the predicted class index.  \n",
    "\n",
    "3. **Plot Results**  \n",
    "   - Display a 3\u00d73 grid of random MNIST samples.  \n",
    "   - Each subplot shows the **true label** and the **predicted label** from the trained ResNet-18.  \n",
    "\n",
    "This demonstrates a simple but practical workflow: **CPU-based preprocessing + GPU-based inference in a Ray actor**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 19. CPU preprocessing + GPU inference via Ray actor\n",
    "\n",
    "to_tensor = ToTensor()\n",
    "\n",
    "def normalize_cpu(img):\n",
    "    # Convert image (PIL) to tensor on CPU \u2192 shape [C,H,W]\n",
    "    t = to_tensor(img)                # [C,H,W] on CPU\n",
    "    C = t.shape[0]\n",
    "    # Apply channel-wise normalization (grayscale vs RGB)\n",
    "    if C == 3:\n",
    "        norm = Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    else:\n",
    "        norm = Normalize((0.5,), (0.5,))\n",
    "    return norm(t)\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "# Plot a 3x3 grid of random MNIST samples with predictions\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    img, label = dataset[idx]\n",
    "\n",
    "    # Preprocess on CPU, add batch dim \u2192 [1,C,H,W]\n",
    "    x = normalize_cpu(img).unsqueeze(0)    \n",
    "\n",
    "    # Run inference on GPU via Ray actor, fetch result   \n",
    "    pred = ray.get(worker.predict.remote(x))[0]  # int\n",
    "    \n",
    "    # Plot image with true label and predicted label\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f\"label: {label}; pred: {int(pred)}\")\n",
    "    plt.axis(\"off\")\n",
    "    arr = np.array(img)\n",
    "    plt.imshow(arr, cmap=\"gray\" if arr.ndim == 2 else None)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 \u00b7 Clean Up the Ray Actor  \n",
    "\n",
    "Once you\u2019re done running inference, it\u2019s a good practice to free up resources:  \n",
    "\n",
    "- **`ray.kill(worker, no_restart=True)`** \u2192 stops the `ModelWorker` actor and releases its GPU.  \n",
    "- **`del worker` + `gc.collect()`** \u2192 drop local references so Python\u2019s garbage collector can clean up.  \n",
    "\n",
    "This ensures the GPU is no longer pinned by the actor and can be reused for other jobs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20.\n",
    "\n",
    "# stop the actor process and free its GPU\n",
    "ray.kill(worker, no_restart=True)     \n",
    "\n",
    "# drop local references so nothing pins it\n",
    "del worker\n",
    "\n",
    "# Forcing garbage collection is optional:\n",
    "# - Cluster resources are already freed by ray.kill()\n",
    "# - Python will clean up the local handle eventually\n",
    "# - gc.collect() is usually unnecessary unless debugging memory issues\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}