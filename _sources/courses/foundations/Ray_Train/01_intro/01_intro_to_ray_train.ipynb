{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 01 ¬∑ Introduction to Ray Train  \n",
    "In this notebook you‚Äôll learn how to run **distributed data-parallel training with PyTorch** on an Anyscale cluster using **Ray Train V2**. You‚Äôll train a **ResNet-18 model on MNIST** across multiple GPUs, with built-in support for **checkpointing, metrics reporting, and distributed orchestration**.  \n",
    "\n",
    "### What you‚Äôll learn & take away\n",
    "* Why and when to use **Ray Train** for distributed training instead of managing PyTorch DDP manually  \n",
    "* How to wrap your PyTorch code with **`prepare_model()`** and **`prepare_data_loader()`** for multi-GPU execution  \n",
    "* How to configure scale with **`ScalingConfig(num_workers=..., use_gpu=True)`** and track outputs with **`RunConfig(storage_path=...)`**  \n",
    "* How to **report metrics and save checkpoints** using `ray.train.report(...)`, with best practices for rank-0 checkpointing  \n",
    "* How to use **Anyscale storage**: fast local NVMe vs. persistent cluster/cloud storage  \n",
    "* How to **inspect training results** (metrics DataFrame, checkpoints) and load a checkpointed model for inference with Ray  \n",
    "\n",
    "> The entire workflow runs **fully distributed from the start**: you define your training loop once, and Ray handles orchestration, sharding, and checkpointing across the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  üîé When to use Ray Train  \n",
    "\n",
    "Use Ray Train when you face one of the following challenges:\n",
    "\n",
    "|Challenge|Detail|Solution|\n",
    "|---|---|---|\n",
    "|Need to speed up or scale up training| Training jobs might take a long time to complete, or require a lot of compute | Ray Train provides a distributed training framework that allows engineers to scale training jobs to multiple GPUs |\n",
    "|Minimize overhead of setting up distributed training| Engineers need to manage the underlying infrastructure | Ray Train handles the underlying infrastructure via Ray's autoscaling |\n",
    "|Achieve observability| Engineers need to connect to different nodes and GPUs to find the root cause of failures, fetch logs, traces, etc | Ray Train provides observability via Ray's dashboard, metrics, and traces that allow engineers to monitor the training job |\n",
    "|Ensure reliable training| Training jobs can fail due to hardware failures, network issues, or other unexpected events | Ray Train provides fault tolerance via checkpointing, automatic retries, and the ability to resume training from the last checkpoint |\n",
    "|Avoid significant code rewrite| Engineers might need to fully rewrite their training loop to support distributed training | Ray Train provides a suite of integrations with the PyTorch ecosystem, Tree-based methods (XGB, LGBM), and more to minimize the amount of code changes needed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñ•Ô∏è How Distributed Data Parallel (DDP) Works  \n",
    "\n",
    "The diagram above shows the **lifecycle of a single training step** in PyTorch DistributedDataParallel (DDP) when orchestrated by Ray Train:\n",
    "\n",
    "1. **Model Replication**  \n",
    "   The model is initialized on GPU rank 0 and broadcast to all other workers so that each has an identical copy.  \n",
    "\n",
    "2. **Sharded Data Loading**  \n",
    "   The dataset is automatically split into **non-overlapping shards**. Each worker processes only its shard, ensuring efficient parallelism without duplicate samples.  \n",
    "\n",
    "3. **Forward & Backward Passes**  \n",
    "   Each worker runs a forward pass and computes gradients locally during the backward pass.  \n",
    "\n",
    "4. **Gradient Synchronization**  \n",
    "   Gradients are aggregated across workers via an **AllReduce** step, ensuring that model updates stay consistent across all GPUs.  \n",
    "\n",
    "5. **Weight Updates**  \n",
    "   Once gradients are synchronized, each worker applies the update, keeping model replicas in sync.  \n",
    "\n",
    "6. **Checkpointing & Metrics**  \n",
    "   By convention, only the **rank 0 worker** saves checkpoints and logs metrics to persistent storage. This avoids duplication while preserving progress and results.  \n",
    "\n",
    "With Ray Train, you don‚Äôt need to manage process groups or samplers manually‚Äîutilities like `prepare_model()` and `prepare_data_loader()` wrap these details so your code works out of the box in a distributed setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_v4.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Schematic overview of DistributedDataParallel (DDP) training: (1) the model is replicated from the <code>GPU rank 0</code> to all other workers; (2) each worker receives a shard of the dataset and processes a mini-batch; (3) during the backward pass, gradients are averaged across GPUs; (4) checkpoint and metrics from rank 0 GPU are saved to the persistent storage.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 ¬∑ Imports  \n",
    "\n",
    "Start by importing all the libraries you‚Äôll need for this tutorial.  \n",
    "\n",
    "- **Standard utilities**: `os`, `datetime`, `tempfile`, `csv`, `shutil`, and `gc` help with file paths, checkpointing, cleanup, and general housekeeping.  \n",
    "- **Data and visualization**: `pandas`, `numpy`, `matplotlib`, and `PIL` are used for inspecting the dataset and plotting sample images.  \n",
    "- **PyTorch**: core deep learning components (`torch`, `CrossEntropyLoss`, `Adam`) plus `torchvision` for loading MNIST and building a ResNet-18 model.  \n",
    "- **Ray Train**: the key imports for distributed training‚Äî`ScalingConfig`, `RunConfig`, and `TorchTrainer`. These handle cluster scaling, experiment output storage, and execution of your training loop across GPUs.  \n",
    "\n",
    "This notebook assumes Ray is already running (for example, inside an Anyscale cluster), so you don‚Äôt need to call `ray.init()` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# --- Standard library: file IO, paths, timestamps, temp dirs, cleanup ---\n",
    "import csv            # Simple CSV logging for metrics in single-GPU section\n",
    "import datetime       # Timestamps for run directories / filenames\n",
    "import os             # Filesystem utilities (paths, env vars)\n",
    "import tempfile       # Ephemeral dirs for checkpoint staging with ray.train.report()\n",
    "import shutil         # Cleanup of artifacts (later cells)\n",
    "import gc             # Manual garbage collection to cleanup after inference\n",
    "\n",
    "from pathlib import Path  # Convenient, cross-platform path handling\n",
    "\n",
    "# --- Visualization & data wrangling ---\n",
    "import matplotlib.pyplot as plt  # Plot sample digits and metrics curves\n",
    "from PIL import Image            # Image utilities for inspection/debug\n",
    "import numpy as np               # Numeric helpers (random sampling, arrays)\n",
    "import pandas as pd              # Read metrics.csv into a DataFrame\n",
    "\n",
    "# --- PyTorch & TorchVision (model + dataset) ---\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss      # Classification loss for MNIST\n",
    "from torch.optim import Adam               # Optimizer\n",
    "from torchvision.models import resnet18    # Baseline CNN (we‚Äôll adapt for 1-channel input)\n",
    "from torchvision.datasets import MNIST     # Dataset\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose  # Preprocessing pipeline\n",
    "\n",
    "# --- Ray Train (distributed orchestration) ---\n",
    "import ray\n",
    "from ray.train import ScalingConfig, RunConfig      # Configure scale and storage\n",
    "from ray.train.torch import TorchTrainer            # Multi-GPU PyTorch trainer (DDP/FSDP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 ¬∑ Download MNIST Dataset  \n",
    "\n",
    "Next, download the **MNIST dataset** using `torchvision.datasets.MNIST`.  \n",
    "- This will automatically fetch the dataset (if not already present) into a local `./data` directory.  \n",
    "- MNIST consists of **60,000 grayscale images of handwritten digits (0‚Äì9)**, each sized **28√ó28 pixels**.  \n",
    "- By setting `train=True`, we load the training split of the dataset.  \n",
    "\n",
    "Once downloaded, we‚Äôll later wrap this dataset in a `DataLoader` and apply normalization so it can be used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 02. Download MNIST Dataset  \n",
    "\n",
    "dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note about Anyscale storage options</b>\n",
    "\n",
    "In this example, the MNIST dataset is stored under <code>/mnt/cluster_storage/</code>, which is Anyscale‚Äôs **persistent cluster storage**.  \n",
    "\n",
    "* Unlike node-local NVMe volumes, cluster storage is **shared across nodes** in your cluster.  \n",
    "* Data written here will **persist across cluster restarts**, making it a safe place for datasets, checkpoints, and results.  \n",
    "* This is the recommended location for training data and artifacts you want to reuse.  \n",
    "\n",
    "* Anyscale also provides each node with its own volume and disk and doesn‚Äôt share them with other nodes.\n",
    "* Local storage is very fast - Anyscale supports the Non-Volatile Memory Express (NVMe) interface.\n",
    "* Local storage is not a persisent storage, Anyscale deletes data in the local storage after instances are terminated. \n",
    "\n",
    "Read more about available <a href=\"https://docs.anyscale.com/configuration/storage\" target=\"_blank\">storage</a> options.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 ¬∑ Visualize Sample Digits  \n",
    "\n",
    "Before training, let‚Äôs take a quick look at the dataset.  \n",
    "- We‚Äôll randomly sample **9 images** from the MNIST training set.  \n",
    "- Each image is a **28√ó28 grayscale digit**, with its ground-truth label shown above the plot.  \n",
    "- This simple visualization is a good sanity check to confirm that the dataset downloaded correctly and that labels align with the images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 03. Visualize Sample Digits\n",
    "\n",
    "# Create a square figure for plotting 9 samples (3x3 grid)\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "# Loop through grid slots and plot a random digit each time\n",
    "for i in range(1, cols * rows + 1):\n",
    "    # Randomly select an index from the dataset\n",
    "    sample_idx = np.random.randint(0, len(dataset.data))\n",
    "    img, label = dataset[sample_idx]  # image (PIL) and its digit label\n",
    "    \n",
    "    # Add subplot to the figure\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)         # show the digit label above each subplot\n",
    "    plt.axis(\"off\")          # remove axes for cleaner visualization\n",
    "    plt.imshow(img, cmap=\"gray\")  # display as grayscale image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 ¬∑ Define ResNet-18 Model for MNIST  \n",
    "\n",
    "Now let‚Äôs define the **ResNet-18** architecture we‚Äôll use for classification.  \n",
    "\n",
    "- `torchvision.models.resnet18` is preconfigured for **3-channel RGB input** and **ImageNet classes**.  \n",
    "- Since MNIST digits are **1-channel grayscale** images with **10 output classes**, we need two adjustments:  \n",
    "  1. Override the first convolution layer (`conv1`) to accept **`in_channels=1`**.  \n",
    "  2. Set the final layer to output **10 logits**, one per digit class (handled by `num_classes=10`).  \n",
    "\n",
    "This gives us a ResNet-18 tailored for MNIST while preserving the rest of the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Define ResNet-18 Model for MNIST\n",
    "\n",
    "def build_resnet18():\n",
    "    # Start with a torchvision ResNet-18 backbone\n",
    "    # Set num_classes=10 since MNIST has digits 0‚Äì9\n",
    "    model = resnet18(num_classes=10)\n",
    "\n",
    "    # Override the first convolution layer:\n",
    "    # - Default expects 3 channels (RGB images)\n",
    "    # - MNIST is grayscale ‚Üí only 1 channel\n",
    "    # - Keep kernel size/stride/padding consistent with original ResNet\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        in_channels=1,   # input = grayscale\n",
    "        out_channels=64, # number of filters remains the same as original ResNet\n",
    "        kernel_size=(7, 7),\n",
    "        stride=(2, 2),\n",
    "        padding=(3, 3),\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "    # Return the customized ResNet-18\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Migration roadmap: from standalone PyTorch to PyTorch with Ray Train</b>  \n",
    "\n",
    "The following are the steps to take a **regular PyTorch training loop** and run it in a **fully distributed setup with Ray Train**.  \n",
    "\n",
    "<ol>\n",
    "    <li><b>Configure scale and GPUs</b> ‚Äî decide how many workers and whether each should use a GPU.</li>\n",
    "    <li><b>Wrap the model with Ray Train</b> ‚Äî use <code>prepare_model()</code> to move the ResNet to the right device and wrap it in DDP automatically.</li>\n",
    "    <li><b>Wrap the dataset with Ray Train</b> ‚Äî use <code>prepare_data_loader()</code> so each worker gets a distinct shard of MNIST, moved to the correct device.</li>\n",
    "    <li><b>Add metrics & checkpointing</b> ‚Äî report training loss and save checkpoints with <code>ray.train.report()</code> from rank-0.</li>\n",
    "    <li><b>Configure persistent storage</b> ‚Äî store outputs under <code>/mnt/cluster_storage/</code> so that results and checkpoints are available across the cluster.</li>\n",
    "</ol>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Train is built around [four key concepts](https://docs.ray.io/en/latest/train/overview.html):\n",
    "1. **Training function**: (implemented above `train_loop_ray_train`): A Python function that contains your model training logic.\n",
    "1. **Worker**: A process that runs the training function.\n",
    "1. **Scaling config**: specifices number of workers and compute resources (CPUs or GPUs, TPUs).\n",
    "1. **Trainer**: A Python class (Ray Actor) that ties together the training function, workers, and scaling configuration to execute a distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"60%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|High-level architecture of how Ray Train|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 ¬∑ Define the Ray Train Loop (DDP per-worker)\n",
    "\n",
    "This is the **per-worker training function** that Ray executes on each process/GPU. It keeps your PyTorch code intact while Ray handles **process launch, device placement, and data sharding**.\n",
    "\n",
    "Key points:\n",
    "- **Inputs via `config`**: we pass hyperparameters like `num_epochs` and a **`global_batch_size`**.\n",
    "- **Model & optimizer**: `load_model_ray_train()` returns a model already wrapped by Ray Train (DDP + correct device). We use `Adam` and `CrossEntropyLoss` for MNIST.\n",
    "- **Batch sizing**: we split the global batch across workers:  \n",
    "  `per_worker_batch = global_batch_size // world_size`.\n",
    "- **Data sharding**: `build_data_loader_ray_train(...)` returns a DataLoader wrapped with a **DistributedSampler**; each worker sees a disjoint shard.\n",
    "- **Epoch control**: `data_loader.sampler.set_epoch(epoch)` ensures proper shuffling across epochs in distributed mode.\n",
    "- **Training step**: standard PyTorch loop‚Äîforward ‚Üí loss ‚Üí zero_grad ‚Üí backward ‚Üí step.\n",
    "- **Metrics & checkpointing**: `print_metrics_ray_train(...)` logs loss; `save_checkpoint_and_metrics_ray_train(...)` calls `ray.train.report(...)` (rank-0 saves the checkpoint).\n",
    "\n",
    "This function is passed to `TorchTrainer`, which runs it **concurrently on all workers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this data-parallel training loop will look like with Ray Train and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Define the Ray Train per-worker training loop\n",
    "\n",
    "def train_loop_ray_train(config: dict):  # pass in hyperparameters in config\n",
    "    # config holds hyperparameters passed from TorchTrainer (e.g. num_epochs, global_batch_size)\n",
    "\n",
    "    # Define loss function for MNIST classification\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Build and prepare the model for distributed training.\n",
    "    # load_model_ray_train() calls ray.train.torch.prepare_model()\n",
    "    # ‚Üí moves model to GPU and wraps it in DistributedDataParallel (DDP).\n",
    "    model = load_model_ray_train()\n",
    "\n",
    "    # Standard optimizer (learning rate fixed for demo)\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Calculate the batch size for each worker\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    world_size = ray.train.get_context().get_world_size()  # total # of workers in the job\n",
    "    batch_size = global_batch_size // world_size  # split global batch evenly\n",
    "    print(f\"{world_size=}\\n{batch_size=}\")\n",
    "\n",
    "    # Wrap DataLoader with prepare_data_loader()\n",
    "    # ‚Üí applies DistributedSampler (shards data across workers)\n",
    "    # ‚Üí ensures batches are automatically moved to correct device\n",
    "    data_loader = build_data_loader_ray_train(batch_size=batch_size)\n",
    "\n",
    "    # ----------------------- Training loop ----------------------- #\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "\n",
    "        # Ensure each worker shuffles its shard differently every epoch\n",
    "        data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        # Iterate over batches (sharded across workers)\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)            # forward pass\n",
    "            loss = criterion(outputs, labels)  # compute loss\n",
    "            optimizer.zero_grad()              # reset gradients\n",
    "\n",
    "            loss.backward()   # backward pass (grads averaged across workers via DDP)\n",
    "            optimizer.step()  # update model weights\n",
    "\n",
    "        # After each epoch: report loss and log metrics\n",
    "        metrics = print_metrics_ray_train(loss, epoch)\n",
    "\n",
    "        # Save checkpoint (only rank-0 worker persists the model)\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Main training loop</b>\n",
    "<ul>\n",
    "  <li><strong>global_batch_size</strong>: the total number of samples processed in a single training step of the entire training job.\n",
    "    <ul>\n",
    "      <li>It's estimated like this: <code>batch size * DDP workers * gradient accumulation steps</code>.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Notice that images and labels are no longer manually moved to device (<code>images.to(\"cuda\")</code>). This is done by \n",
    "    <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html#ray-train-torch-prepare-data-loader\" target=\"_blank\">\n",
    "      prepare_data_loader()\n",
    "    </a>.\n",
    "  </li>\n",
    "  <li>Config that will be passed here, is defined below. It will be passed to the Ray Train's <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.TorchTrainer.html#ray-train-torch-torchtrainer\" target=\"_blank\">TorchTrainer</a>.</li>\n",
    "  <li>\n",
    "    <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.v2.api.context.TrainContext.html#ray-train-v2-api-context-traincontext\" target=\"_blank\">\n",
    "      TrainContext\n",
    "    </a> lets users get useful information about the training i.e. node rank, world size, world rank, experiment name.\n",
    "  </li>\n",
    "\n",
    "  <li><code>load_model_ray_train</code> and <code>build_data_loader_ray_train</code> are implemented below.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 ¬∑ Define `train_loop_config`  \n",
    "\n",
    "The `train_loop_config` is a simple **dictionary of hyperparameters** that Ray passes into your training loop (`train_loop_ray_train`).  \n",
    "\n",
    "- It acts as the **bridge between the `TorchTrainer` and your per-worker training code**.  \n",
    "- Anything defined here becomes available inside the `config` argument of `train_loop_ray_train`.  \n",
    "\n",
    "In this example we define:  \n",
    "- **`num_epochs`** ‚Üí how many full passes through the dataset to run.  \n",
    "- **`global_batch_size`** ‚Üí the total batch size across all workers (Ray will split this evenly across GPUs).  \n",
    "\n",
    "You can add other parameters here (like `learning_rate`, `embedding_dim`, etc.) and they‚Äôll automatically be accessible in your training loop via `config[\"param_name\"]`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Define the configuration dictionary passed into the training loop\n",
    "\n",
    "# train_loop_config is provided to TorchTrainer and injected into\n",
    "# train_loop_ray_train(config) as the \"config\" argument.\n",
    "# ‚Üí Any values defined here are accessible inside the training loop.\n",
    "\n",
    "train_loop_config = {\n",
    "    \"num_epochs\": 2,           # Number of full passes through the dataset\n",
    "    \"global_batch_size\": 128   # Effective batch size across ALL workers\n",
    "                               # (Ray will split this evenly per worker, e.g.\n",
    "                               # with 8 workers ‚Üí 16 samples/worker/step)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 ¬∑ Configure Scaling with `ScalingConfig`  \n",
    "\n",
    "The `ScalingConfig` tells Ray Train **how many workers to launch** and **what resources each worker should use**.  \n",
    "\n",
    "- **`num_workers=8`** ‚Üí Run the training loop on 8 parallel workers. Each worker runs the same code on a different shard of the data.  \n",
    "- **`use_gpu=True`** ‚Üí Assign one GPU per worker. If you set this to `False`, each worker would train on CPU instead.  \n",
    "\n",
    "This declarative config is what allows Ray to handle cluster orchestration for you ‚Äî you don‚Äôt need to manually start processes or set CUDA devices.  \n",
    "\n",
    "Later, we‚Äôll pass this `scaling_config` into the `TorchTrainer` to launch distributed training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Configure the scaling of the training job\n",
    "\n",
    "# ScalingConfig defines how many parallel training workers Ray should launch\n",
    "# and whether each worker should be assigned a GPU or CPU.\n",
    "# ‚Üí Each worker runs train_loop_ray_train(config) independently,\n",
    "#    with Ray handling synchronization via DDP under the hood.\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=8,   # Launch 8 training workers (1 process per worker)\n",
    "    use_gpu=True     # Allocate 1 GPU to each worker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Docs on <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html#ray-train-scalingconfig\" target=\"_blank\">ScalingConfig</a> can be found with the link in this sentence.\n",
    "\n",
    "See docs on configuring <a href=\"https://docs.ray.io/en/latest/train/user-guides/using-gpus.html\" target=\"_blank\">scale and GPUs</a> for more details.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08 ¬∑ Wrap the Model with `prepare_model()`  \n",
    "\n",
    "Next, we define a helper function to build and prepare the model for Ray Train.  \n",
    "\n",
    "- Start by constructing the **ResNet-18** model adapted for MNIST using `build_resnet18()`.  \n",
    "- Instead of manually calling `model.to(\"cuda\")` and wrapping it in **DistributedDataParallel (DDP)**, we use **`ray.train.torch.prepare_model()`**.  \n",
    "  * This automatically:  \n",
    "    - Moves the model to the correct device (GPU or CPU).  \n",
    "    - Wraps it in DDP or FSDP.  \n",
    "    - Ensures gradients are synchronized across workers.  \n",
    "\n",
    "This means the same code works whether you‚Äôre training on **1 GPU or 100 GPUs** ‚Äî no manual device placement or DDP boilerplate required.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Build and prepare the model for Ray Train\n",
    "\n",
    "def load_model_ray_train() -> torch.nn.Module:\n",
    "    model = build_resnet18()\n",
    "    # prepare_model() ‚Üí move to correct device + wrap in DDP automatically\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_model.html#ray-train-torch-prepare-model\" target=\"_blank\">\n",
    "    prepare_model()\n",
    "  </a> allows users to specify additional parameters:\n",
    "  <ul>\n",
    "    <li><code>parallel_strategy</code>: \"ddp\", \"fsdp\" ‚Äì wrap models in <code>DistributedDataParallel</code> or <code>FullyShardedDataParallel</code></li>\n",
    "    <li><code>parallel_strategy_kwargs</code>: pass additional arguments to \"ddp\" or \"fsdp\"</li>\n",
    "  </ul>\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09 ¬∑ Build the DataLoader with `prepare_data_loader()`  \n",
    "\n",
    "Now let‚Äôs define a helper that builds the **MNIST DataLoader** and makes it Ray Train‚Äìready.  \n",
    "\n",
    "- Apply standard preprocessing:  \n",
    "  * `ToTensor()` ‚Üí convert PIL images to PyTorch tensors  \n",
    "  * `Normalize((0.5,), (0.5,))` ‚Üí center and scale pixel values  \n",
    "\n",
    "- Construct a PyTorch `DataLoader` with batching and shuffling.  \n",
    "\n",
    "- Finally, wrap it with [`prepare_data_loader()`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html#ray-train-torch-prepare-data-loader), which automatically:  \n",
    "  * Moves each batch to the correct device (GPU or CPU).  \n",
    "  * Copies data from host memory to device memory as needed.  \n",
    "  * Injects a PyTorch [`DistributedSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler) when running with multiple workers, so that each worker processes a unique shard of the dataset.  \n",
    "\n",
    "This utility lets you use the **same DataLoader code** whether you‚Äôre training on one GPU or many ‚Äî Ray handles the distributed sharding and device placement for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. Build a Ray Train‚Äìready DataLoader for MNIST\n",
    "\n",
    "def build_data_loader_ray_train(batch_size: int) -> torch.utils.data.DataLoader:\n",
    "    # Define preprocessing: convert to tensor + normalize pixel values\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    # Load the MNIST training set from persistent cluster storage\n",
    "    train_data = MNIST(\n",
    "        root=\"/mnt/cluster_storage/data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    # Standard PyTorch DataLoader (batching, shuffling, drop last incomplete batch)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # prepare_data_loader():\n",
    "    # - Adds a DistributedSampler when using multiple workers\n",
    "    # - Moves batches to the correct device automatically\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Ray Data integration</b>\n",
    "\n",
    "This step isn't necessary if you are integrating your Ray Train workload with Ray Data. It's especially useful if preprocessing is CPU-heavly and user wants to run preprocessing and training of separate instances.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10 ¬∑ Report Training Metrics  \n",
    "\n",
    "During training, it‚Äôs important to log metrics like loss values so you can monitor progress.  \n",
    "\n",
    "This helper function prints metrics from **every worker**:  \n",
    "- Collects the current **loss** and **epoch** into a dictionary.  \n",
    "- Uses `ray.train.get_context().get_world_rank()` to identify which worker is reporting.  \n",
    "- Prints the metrics along with the worker‚Äôs rank for debugging and visibility.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Report training metrics from each worker\n",
    "\n",
    "def print_metrics_ray_train(loss: torch.Tensor, epoch: int) -> None:\n",
    "    metrics = {\"loss\": loss.item(), \"epoch\": epoch}  \n",
    "    world_rank = ray.train.get_context().get_world_rank() # report from all workers\n",
    "    print(f\"{metrics=} {world_rank=}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "If you want to log only from the rank 0 worker, use this code:\n",
    "\n",
    "```python\n",
    "def print_metrics_ray_train(loss: torch.Tensor, epoch: int) -> None:\n",
    "    metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "    if ray.train.get_context().get_world_rank() == 0:  # report only from the rank 0 worker\n",
    "        print(f\"{metrics=} {world_rank=}\")\n",
    "    return metrics\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 ¬∑ Save Checkpoints and Report Metrics  \n",
    "\n",
    "We will report intermediate metrics and checkpoints using the [`ray.train.report`](https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray-train-report) utility function.  \n",
    "\n",
    "This helper function:  \n",
    "- Creates a temporary directory to stage the checkpoint.  \n",
    "- Saves the model weights with `torch.save()`.  \n",
    "  * Since the model is wrapped in **DistributedDataParallel (DDP)**, we call `model.module.state_dict()` to unwrap it.  \n",
    "- Calls `ray.train.report()` to:  \n",
    "  * Log the current metrics (e.g., loss, epoch).  \n",
    "  * Attach a `Checkpoint` object created from the staged directory.  \n",
    "\n",
    "This way, each epoch produces both **metrics for monitoring** and a **checkpoint for recovery or inference**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Save checkpoint and report metrics with Ray Train\n",
    "\n",
    "def save_checkpoint_and_metrics_ray_train(model: torch.nn.Module, metrics: dict[str, float]) -> None:\n",
    "    # Create a temporary directory to stage checkpoint files\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        # Save the model weights.\n",
    "        # Note: under DDP the model is wrapped in DistributedDataParallel,\n",
    "        # so we unwrap it with `.module` before calling state_dict().        \n",
    "        torch.save(\n",
    "            model.module.state_dict(),  # note the `.module` to unwrap the DistributedDataParallel\n",
    "            os.path.join(temp_checkpoint_dir, \"model.pt\"),\n",
    "        )\n",
    "        \n",
    "        # Report metrics and attach a checkpoint to Ray Train.\n",
    "        # ‚Üí metrics are logged centrally\n",
    "        # ‚Üí checkpoint allows resuming training or running inference later\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <p><strong>Quick notes:</strong></p>\n",
    "  <ul>\n",
    "    <li>\n",
    "      Use \n",
    "      <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray.train.report\" target=\"_blank\">\n",
    "        ray.train.report\n",
    "      </a> to save the metrics and checkpoint.\n",
    "    </li>\n",
    "    <li>Only metrics from the rank 0 worker are reported.</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the Checkpoint Lifecycle  \n",
    "\n",
    "The diagram above shows how a checkpoint moves from **local storage** (temporary directory on a worker) to **persistent cluster or cloud storage**.  \n",
    "\n",
    "Key points to remember:  \n",
    "- Since the model is identical across all workers, it‚Äôs enough to **write the checkpoint only on the rank-0 worker**.  \n",
    "  * However, you still need to call [`ray.train.report`](https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray-train-report) on **all workers** to keep the training loop synchronized.  \n",
    "- Ray Train expects every worker to have access to the **same persistent storage location** for writing files.  \n",
    "- For production jobs, **cloud storage** (e.g., S3, GCS, Azure Blob) is the recommended target for checkpoints.  \n",
    "\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/checkpoint_lifecycle.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 ¬∑ Save Checkpoints on Rank-0 Only  \n",
    "\n",
    "To avoid redundant writes, we update the checkpointing function so that **only the rank-0 worker** saves the model weights.  \n",
    "\n",
    "- **Temporary directory** ‚Üí Each worker still creates a temp directory, but only rank-0 writes the model file.  \n",
    "- **Rank check** ‚Üí `ray.train.get_context().get_world_rank()` ensures that only worker 0 performs the checkpointing.  \n",
    "- **All workers report** ‚Üí Every worker still calls [`ray.train.report`](https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray-train-report), but only rank-0 attaches the actual checkpoint. This keeps the training loop synchronized.  \n",
    "\n",
    "This pattern is the recommended best practice:  \n",
    "- Avoids unnecessary duplicate checkpoints from multiple workers.  \n",
    "- Still guarantees that metrics are reported from every worker.  \n",
    "- Ensures checkpoints are cleanly written once per epoch to persistent storage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Save checkpoint only from the rank-0 worker\n",
    "\n",
    "def save_checkpoint_and_metrics_ray_train(model: torch.nn.Module, metrics: dict[str, float]) -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        checkpoint = None\n",
    "\n",
    "        # Only the rank-0 worker writes the checkpoint file\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            torch.save(\n",
    "                model.module.state_dict(),  # unwrap DDP before saving\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\"),\n",
    "            )\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "        # All workers still call ray.train.report()\n",
    "        # ‚Üí keeps training loop synchronized\n",
    "        # ‚Üí metrics are logged from each worker\n",
    "        # ‚Üí only rank-0 attaches a checkpoint\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=checkpoint,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our guide on [saving and loading checkpoints](https://docs.ray.io/en/latest/train/user-guides/checkpoints.html) for more details and best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 ¬∑ Configure Persistent Storage with `RunConfig`  \n",
    "\n",
    "To tell Ray Train **where to store results, checkpoints, and logs**, we use a [`RunConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.RunConfig.html).  \n",
    "\n",
    "- **`storage_path`** ‚Üí Base directory for all outputs of this training run.  \n",
    "  * In this example we use `/mnt/cluster_storage/training/`, which is **persistent shared storage** across all nodes.  \n",
    "  * This ensures checkpoints and metrics remain available even after the cluster shuts down.  \n",
    "- **`name`** ‚Üí A human-readable name for the run (e.g., `\"distributed-mnist-resnet18\"`). This is used to namespace output files.  \n",
    "\n",
    "Together, the `RunConfig` defines how Ray organizes and persists all artifacts from your training job.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Configure persistent storage and run name\n",
    "\n",
    "storage_path = \"/mnt/cluster_storage/training/\"\n",
    "run_config = RunConfig(\n",
    "    storage_path=storage_path,         # where to store checkpoints/logs\n",
    "    name=\"distributed-mnist-resnet18\"  # identifier for this run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Data-Parallel Training with Ray Train  \n",
    "\n",
    "This diagram shows the same DDP workflow as before, but now with **Ray Train utilities** highlighted:  \n",
    "\n",
    "1. **`ray.train.torch.prepare_data_loader()`**  \n",
    "   - Automatically wraps your PyTorch DataLoader with a `DistributedSampler`.  \n",
    "   - Ensures each worker processes a unique shard of the dataset.  \n",
    "   - Moves batches to the correct device (GPU or CPU).  \n",
    "\n",
    "2. **`ray.train.torch.prepare_model()`**  \n",
    "   - Moves your model to the right device.  \n",
    "   - Wraps it in `DistributedDataParallel (DDP)` so gradients are synchronized across workers.  \n",
    "   - Removes the need for manual `.to(\"cuda\")` calls or DDP boilerplate.  \n",
    "\n",
    "3. **`ray.train.report()`**  \n",
    "   - Centralized way to report metrics and attach checkpoints.  \n",
    "   - Keeps the training loop synchronized across all workers, even if only rank-0 saves the actual checkpoint.  \n",
    "\n",
    "By combining these helpers, Ray Train takes care of the **data sharding, model replication, gradient synchronization, and checkpoint lifecycle** ‚Äî letting you keep your training loop clean and close to standard PyTorch.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_annotated_v5.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14 ¬∑ Create the `TorchTrainer`  \n",
    "\n",
    "Now we bring everything together with a [`TorchTrainer`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.TorchTrainer.html).  \n",
    "\n",
    "The `TorchTrainer` is the high-level Ray Train class that:  \n",
    "- Launches the per-worker training loop (`train_loop_ray_train`) across the cluster.  \n",
    "- Applies the scaling setup from `scaling_config` (number of workers, GPUs/CPUs).  \n",
    "- Uses `run_config` to decide where results and checkpoints are stored.  \n",
    "- Passes `train_loop_config` (hyperparameters like `num_epochs` and `global_batch_size`) into the training loop.  \n",
    "\n",
    "This object encapsulates the **distributed orchestration**, so you can start training with a simple call to `trainer.fit()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Set up the TorchTrainer\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train,          # training loop to run on each worker\n",
    "    scaling_config=scaling_config, # number of workers and resource config\n",
    "    run_config=run_config,         # storage path + run name for artifacts\n",
    "    train_loop_config=train_loop_config,  # hyperparameters passed to the loop\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15 ¬∑ Launch Training with `trainer.fit()`  \n",
    "\n",
    "Calling `trainer.fit()` starts the distributed training job and blocks until it completes.  \n",
    "\n",
    "When the job launches, you‚Äôll see logs that confirm:  \n",
    "- **Process group setup** ‚Üí Ray initializes a distributed worker group and assigns ranks (e.g., `world_rank=0` and `world_rank=1`).  \n",
    "- **Worker placement** ‚Üí Each worker is launched on a specific node and device. The logs show IP addresses, process IDs, and rank assignments.  \n",
    "- **Model preparation** ‚Üí Each worker moves the model to its GPU (`cuda:0`) and wraps it in **DistributedDataParallel (DDP)**.  \n",
    "\n",
    "These logs are a quick sanity check that Ray Train is correctly orchestrating multi-GPU training across your cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-intro/ray-train-intro-logs.png\" width=\"80%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Launch distributed training\n",
    "\n",
    "# trainer.fit() starts the training job:\n",
    "# - Spawns workers according to scaling_config\n",
    "# - Runs train_loop_ray_train() on each worker\n",
    "# - Collects metrics and checkpoints into result\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16 ¬∑ Inspect the Training Results  \n",
    "\n",
    "When `trainer.fit()` finishes, it returns a [`Result`](https://docs.ray.io/en/latest/train/api/doc/ray.train.Result.html) object.  \n",
    "\n",
    "This object contains:  \n",
    "- **Final metrics** ‚Üí the most recent values reported from the training loop (e.g., loss at the last epoch).  \n",
    "- **Checkpoint** ‚Üí a reference to the latest saved checkpoint, including its path in cluster storage.  \n",
    "- **Metrics dataframe** ‚Üí a history of all reported metrics across epochs (accessible with `result.metrics_dataframe`).  \n",
    "- **Best checkpoints** ‚Üí Ray automatically tracks checkpoints associated with their reported metrics.  \n",
    "\n",
    "In the output above, you can see:  \n",
    "- The final reported loss at epoch 1.  \n",
    "- The location where checkpoints are stored (`/mnt/cluster_storage/training/distributed-mnist-resnet18/...`).  \n",
    "- A list of best checkpoints with their corresponding metrics.  \n",
    "\n",
    "This makes it easy to both **analyze training performance** and **restore the trained model** later for inference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Show the training results  \n",
    "\n",
    "result  # contains metrics, checkpoints, and run history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17 ¬∑ View Metrics as a DataFrame  \n",
    "\n",
    "The `Result` object also includes a `metrics_dataframe`, which stores the full history of metrics reported during training.  \n",
    "\n",
    "- Each row corresponds to one reporting step (here, each epoch).  \n",
    "- The columns show the metrics you logged in the training loop (e.g., `loss`, `epoch`).  \n",
    "- This makes it easy to plot learning curves or further analyze training progress.  \n",
    "\n",
    "In the example below, you can see the training loss steadily decreasing across two epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Display the full metrics history as a pandas DataFrame\n",
    "\n",
    "result.metrics_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <p>\n",
    "    To learn more about the training results, see this \n",
    "    <a href=\"https://docs.ray.io/en/latest/train/user-guides/results.html\" target=\"_blank\">\n",
    "      docs\n",
    "    </a> on inspecting the training results.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18 ¬∑ Load a Checkpoint for Inference  \n",
    "\n",
    "After training, we often want to reload the model and use it for predictions.  \n",
    "Here we define a **Ray actor** (`ModelWorker`) that loads the checkpointed ResNet-18 onto a GPU and serves inference requests.  \n",
    "\n",
    "- **Initialization (`__init__`)**:  \n",
    "  * Reads the checkpoint directory using `checkpoint.as_directory()`.  \n",
    "  * Loads the model weights into a fresh ResNet-18.  \n",
    "  * Moves the model to GPU and sets it to evaluation mode.  \n",
    "\n",
    "- **Prediction (`predict`)**:  \n",
    "  * Accepts either a single image (`[C,H,W]`) or a batch (`[B,C,H,W]`).  \n",
    "  * Ensures the tensor is correctly shaped and moved to GPU.  \n",
    "  * Runs inference in `torch.inference_mode()` for efficiency.  \n",
    "  * Returns the predicted class indices as a Python list.  \n",
    "\n",
    "Finally, we launch the actor with `ModelWorker.remote(result.checkpoint)`.  \n",
    "This spawns a dedicated process with **1 GPU** attached that can serve predictions using the trained model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Define a Ray actor to load the trained model and run inference\n",
    "\n",
    "@ray.remote(num_gpus=1)  # allocate 1 GPU to this actor\n",
    "class ModelWorker:\n",
    "    def __init__(self, checkpoint):\n",
    "        # Load model weights from the Ray checkpoint (on CPU first)\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            model_path = os.path.join(ckpt_dir, \"model.pt\")\n",
    "            state_dict = torch.load(\n",
    "                model_path,\n",
    "                map_location=torch.device(\"cpu\"),\n",
    "                weights_only=True,\n",
    "            )\n",
    "        # Rebuild the model, load weights, move to GPU, and set to eval mode\n",
    "        self.model = build_resnet18()\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model.to(\"cuda\")\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.inference_mode()  # disable autograd for faster inference\n",
    "    def predict(self, batch):\n",
    "        \"\"\"\n",
    "        batch: torch.Tensor or numpy array with shape [B,C,H,W] or [C,H,W]\n",
    "        returns: list[int] predicted class indices\n",
    "        \"\"\"\n",
    "        x = torch.as_tensor(batch)\n",
    "        if x.ndim == 3:          # single image ‚Üí add batch dimension\n",
    "            x = x.unsqueeze(0)   # shape becomes [1,C,H,W]\n",
    "        x = x.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        logits = self.model(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return preds.detach().cpu().tolist()\n",
    "\n",
    "# Create a fresh actor instance (avoid naming conflicts)\n",
    "worker = ModelWorker.remote(result.checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19 ¬∑ Run Inference and Visualize Predictions  \n",
    "\n",
    "With the `ModelWorker` actor running on GPU, we can now generate predictions on random samples from the MNIST dataset and plot them.  \n",
    "\n",
    "Steps in this cell:  \n",
    "1. **Normalization on CPU**  \n",
    "   - Convert each image to a tensor with `ToTensor()`.  \n",
    "   - Apply channel-specific normalization (`0.5` mean / std).  \n",
    "   - Keep this preprocessing on CPU for efficiency.  \n",
    "\n",
    "2. **Prediction on GPU via Actor**  \n",
    "   - Each normalized image is expanded to shape `[1, C, H, W]`.  \n",
    "   - The tensor is sent to the remote `ModelWorker` for inference.  \n",
    "   - `ray.get(worker.predict.remote(x))` retrieves the predicted class index.  \n",
    "\n",
    "3. **Plot Results**  \n",
    "   - Display a 3√ó3 grid of random MNIST samples.  \n",
    "   - Each subplot shows the **true label** and the **predicted label** from the trained ResNet-18.  \n",
    "\n",
    "This demonstrates a simple but practical workflow: **CPU-based preprocessing + GPU-based inference in a Ray actor**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 19. CPU preprocessing + GPU inference via Ray actor\n",
    "\n",
    "to_tensor = ToTensor()\n",
    "\n",
    "def normalize_cpu(img):\n",
    "    # Convert image (PIL) to tensor on CPU ‚Üí shape [C,H,W]\n",
    "    t = to_tensor(img)                # [C,H,W] on CPU\n",
    "    C = t.shape[0]\n",
    "    # Apply channel-wise normalization (grayscale vs RGB)\n",
    "    if C == 3:\n",
    "        norm = Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    else:\n",
    "        norm = Normalize((0.5,), (0.5,))\n",
    "    return norm(t)\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "# Plot a 3x3 grid of random MNIST samples with predictions\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    img, label = dataset[idx]\n",
    "\n",
    "    # Preprocess on CPU, add batch dim ‚Üí [1,C,H,W]\n",
    "    x = normalize_cpu(img).unsqueeze(0)    \n",
    "\n",
    "    # Run inference on GPU via Ray actor, fetch result   \n",
    "    pred = ray.get(worker.predict.remote(x))[0]  # int\n",
    "    \n",
    "    # Plot image with true label and predicted label\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f\"label: {label}; pred: {int(pred)}\")\n",
    "    plt.axis(\"off\")\n",
    "    arr = np.array(img)\n",
    "    plt.imshow(arr, cmap=\"gray\" if arr.ndim == 2 else None)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 ¬∑ Clean Up the Ray Actor  \n",
    "\n",
    "Once you‚Äôre done running inference, it‚Äôs a good practice to free up resources:  \n",
    "\n",
    "- **`ray.kill(worker, no_restart=True)`** ‚Üí stops the `ModelWorker` actor and releases its GPU.  \n",
    "- **`del worker` + `gc.collect()`** ‚Üí drop local references so Python‚Äôs garbage collector can clean up.  \n",
    "\n",
    "This ensures the GPU is no longer pinned by the actor and can be reused for other jobs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20.\n",
    "\n",
    "# stop the actor process and free its GPU\n",
    "ray.kill(worker, no_restart=True)     \n",
    "\n",
    "# drop local references so nothing pins it\n",
    "del worker\n",
    "\n",
    "# Forcing garbage collection is optional:\n",
    "# - Cluster resources are already freed by ray.kill()\n",
    "# - Python will clean up the local handle eventually\n",
    "# - gc.collect() is usually unnecessary unless debugging memory issues\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
