{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- automatically handles **multi-node, multi-GPU** setup with no manual SSH setup or hostfile configs. \n",
    "- define **per-worker fractional resource requirements**, for example, 2 CPUs and 0.5 GPU per worker.\n",
    "- run on **heterogeneous machines** and scale flexibly, for example, CPU for preprocessing and GPU for training. \n",
    "- built-in **fault tolerance** with retry of failed workers and continue from last checkpoint.\n",
    "- supports Data Parallel, Model Parallel, Parameter Server, and even custom strategies.\n",
    "- [Ray Compiled graphs](https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html) allow you to even define different parallelism for jointly optimizing multiple models like Megatron, DeepSpeed, etc., or only allow for one global setting.\n",
    "- You can also use Torch DDP, FSPD, DeepSpeed, etc., under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udd25 [RayTurbo Train](https://docs.anyscale.com/rayturbo/rayturbo-train) offers even more improvement to the price-performance ratio, performance monitoring and more:\n",
    "- **elastic training** to scale to a dynamic number of workers, continue training on fewer resources, even on spot instances.\n",
    "- **purpose-built dashboard** designed to streamline the debugging of Ray Train workloads:\n",
    "    - Monitoring: View the status of training runs and train workers.\n",
    "    - Metrics: See insights on training throughput and training system operation time.\n",
    "    - Profiling: Investigate bottlenecks, hangs, or errors from individual training worker processes.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/multimodal-ai/refs/heads/main/images/train_dashboard.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view experiment metrics and model artifacts in the model registry. You're using OSS MLflow so you can run the server by pointing to the model registry location:\n",
    "\n",
    "```bash\n",
    "mlflow server -h 0.0.0.0 -p 8080 --backend-store-uri /mnt/cluster_storage/mlflow/doggos\n",
    "```\n",
    "\n",
    "You can view the dashboard by going to the **Overview tab** > **Open Ports**. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/multimodal-ai/refs/heads/main/images/mlflow.png\" width=1000>\n",
    "\n",
    "You also have the preceding Ray Dashboard and Train workload specific dashboards.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/multimodal-ai/refs/heads/main/images/train_metrics.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_id                                      fcb9ef8c96f844f08bcd0185601f3dbd\n",
       "experiment_id                                             858816514880031760\n",
       "status                                                              FINISHED\n",
       "artifact_uri               file:///mnt/cluster_storage/mlflow/doggos/8588...\n",
       "start_time                                  2025-08-22 00:32:11.522000+00:00\n",
       "end_time                                    2025-08-22 00:32:32.895000+00:00\n",
       "metrics.train_loss                                                   0.35504\n",
       "metrics.val_loss                                                    0.593301\n",
       "metrics.lr                                                             0.001\n",
       "params.lr_patience                                                         3\n",
       "params.dropout_p                                                         0.3\n",
       "params.num_epochs                                                         20\n",
       "params.lr                                                              0.001\n",
       "params.num_classes                                                        36\n",
       "params.hidden_dim                                                        256\n",
       "params.experiment_name                                                doggos\n",
       "params.batch_size                                                        256\n",
       "params.model_registry                     /mnt/cluster_storage/mlflow/doggos\n",
       "params.class_to_label      {'border_collie': 0, 'pomeranian': 1, 'basset'...\n",
       "params.lr_factor                                                         0.8\n",
       "params.embedding_dim                                                     512\n",
       "tags.mlflow.user                                                         ray\n",
       "tags.mlflow.source.type                                                LOCAL\n",
       "tags.mlflow.runName                                      enthused-donkey-931\n",
       "tags.mlflow.source.name    /home/ray/anaconda3/lib/python3.12/site-packag...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorted runs\n",
    "mlflow.set_tracking_uri(f\"file:{model_registry}\")\n",
    "sorted_runs = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name], order_by=[\"metrics.val_loss ASC\"]\n",
    ")\n",
    "best_run = sorted_runs.iloc[0]\n",
    "best_run\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}