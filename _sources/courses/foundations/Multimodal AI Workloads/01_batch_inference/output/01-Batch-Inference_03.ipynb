{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section applied a mapping operation using a function to each row in the dataset. Now you're ready to generate embeddings from the data and using Ray Data's [`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) to apply an operation across batches of the data. The operation is in the form of a callable, which is a function or a class with a `__call__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedImages(object):\n",
    "    def __init__(self, model_id, device):\n",
    "        # Load CLIP model and processor\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_id)\n",
    "        self.model = CLIPModel.from_pretrained(model_id)\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Load and preprocess images\n",
    "        images = [\n",
    "            Image.fromarray(np.uint8(img)).convert(\"RGB\") for img in batch[\"image\"]\n",
    "        ]\n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\", padding=True).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # Generate embeddings\n",
    "        with torch.inference_mode():\n",
    "            batch[\"embedding\"] = self.model.get_image_features(**inputs).cpu().numpy()\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Ray object store references</b> \n",
    "\n",
    "Instead of initializing the same model for each instance of the class above, we can instead use references to Ray's [shared memory object store](https://docs.ray.io/en/latest/ray-core/objects.html#objects-in-ray). We can load the model once, store it inside the default object store and then have each instance of our class refer to it.\n",
    "\n",
    "```python\n",
    "model = load_model(...)\n",
    "model_ref = ray.put(model) \n",
    "\n",
    "class Foo:\n",
    "    def __init__(self, model_ref):\n",
    "        self.model = ray.get(model_ref)\n",
    "        ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batch embeddings\n",
    "embeddings_ds = ds.map_batches(\n",
    "    EmbedImages,\n",
    "    fn_constructor_kwargs={\n",
    "        \"model_id\": \"openai/clip-vit-base-patch32\",\n",
    "        \"device\": \"cuda\",\n",
    "    },  # class kwargs\n",
    "    fn_kwargs={},  # __call__ kwargs\n",
    "    concurrency=4,\n",
    "    batch_size=64,\n",
    "    num_gpus=1,\n",
    "    accelerator_type=\"T4\",\n",
    ")\n",
    "embeddings_ds = embeddings_ds.drop_columns([\"image\"])  # remove image column\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}