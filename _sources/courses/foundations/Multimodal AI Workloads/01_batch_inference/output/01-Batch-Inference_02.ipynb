{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "Start by reading the data from a public cloud storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 00:14:08,238\tINFO worker.py:1747 -- Connecting to existing Ray cluster at address: 10.0.52.10:6379...\n",
      "2025-08-22 00:14:08,250\tINFO worker.py:1918 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-466hy7cqu1gzrp8zk8l4byz7l7.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-08-22 00:14:08,255\tINFO packaging.py:588 -- Creating a file package for local module '/home/ray/default/doggos/doggos'.\n",
      "2025-08-22 00:14:08,258\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_0193267f6c9951ce.zip' (0.02MiB) to Ray cluster...\n",
      "2025-08-22 00:14:08,259\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_0193267f6c9951ce.zip'.\n",
      "2025-08-22 00:14:08,262\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_6d26725922931a7a9e87fca928dfafe4f4e5e54b.zip' (1.18MiB) to Ray cluster...\n",
      "2025-08-22 00:14:08,268\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_6d26725922931a7a9e87fca928dfafe4f4e5e54b.zip'.\n",
      "2025-08-22 00:14:08,550\tINFO dataset.py:3057 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2025-08-22 00:14:08,552\tINFO logging.py:295 -- Registered dataset logger for dataset dataset_59_0\n",
      "2025-08-22 00:14:08,641\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_59_0. Full logs are in /tmp/ray/session_2025-08-21_18-48-13_464408_2298/logs/ray-data\n",
      "2025-08-22 00:14:08,642\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_59_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6862146286847ef9294638c1aa3d311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a135d7d9cd45bc8ad9e5f0e5c477ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ListFiles 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfc88e39a7b450c945855a2d3f908e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadFiles 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1071a49d524e424498985dc424a029a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 00:14:08,686\tWARNING resource_manager.py:130 -- \u26a0\ufe0f  Ray's object store is configured to use only 28.2% of available memory (67.8GB out of 240.5GB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2025-08-22 00:15:25,802\tINFO streaming_executor.py:231 -- \u2714\ufe0f  Dataset dataset_59_0 execution finished in 77.16 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'image': array([[[123, 118,  78],\n",
       "          [125, 120,  80],\n",
       "          [128, 120,  83],\n",
       "          ...,\n",
       "          [162, 128,  83],\n",
       "          [162, 128,  83],\n",
       "          [161, 127,  82]],\n",
       "  \n",
       "         [[123, 118,  78],\n",
       "          [125, 120,  80],\n",
       "          [127, 119,  82],\n",
       "          ...,\n",
       "          [162, 128,  83],\n",
       "          [162, 128,  83],\n",
       "          [161, 127,  82]],\n",
       "  \n",
       "         [[123, 118,  78],\n",
       "          [125, 120,  80],\n",
       "          [127, 119,  82],\n",
       "          ...,\n",
       "          [161, 128,  83],\n",
       "          [161, 128,  83],\n",
       "          [160, 127,  82]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[235, 234, 239],\n",
       "          [233, 232, 237],\n",
       "          [221, 220, 225],\n",
       "          ...,\n",
       "          [158,  95,  54],\n",
       "          [150,  85,  53],\n",
       "          [151,  88,  57]],\n",
       "  \n",
       "         [[219, 220, 222],\n",
       "          [227, 228, 230],\n",
       "          [222, 223, 225],\n",
       "          ...,\n",
       "          [153,  91,  54],\n",
       "          [146,  83,  52],\n",
       "          [149,  88,  59]],\n",
       "  \n",
       "         [[213, 217, 216],\n",
       "          [217, 221, 220],\n",
       "          [213, 214, 216],\n",
       "          ...,\n",
       "          [153,  91,  54],\n",
       "          [144,  83,  54],\n",
       "          [149,  88,  60]]], dtype=uint8),\n",
       "  'path': 'doggos-dataset/train/border_collie/border_collie_1055.jpg'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data.\n",
    "ds = ray.data.read_images(\n",
    "    \"s3://doggos-dataset/train\",\n",
    "    include_paths=True,\n",
    "    shuffle=\"files\",\n",
    ")\n",
    "ds.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> \u270d\ufe0f Distributed READ/WRITE</b> \n",
    "\n",
    "Ray Data supports a wide range of data sources for both [loading](https://docs.ray.io/en/latest/data/loading-data.html) and [saving](https://docs.ray.io/en/latest/data/saving-data.html) from generic binary files in cloud storage to structured data formats used by modern data platforms. This example reads data from a public S3 bucket prepared with the dataset. This `read` operation, much like the `write` operation in a later step, runs in a distributed fashion. As a result, Ray Data processes the data in parallel across the cluster and doesn't need to load the data entirely into memory at once, making data loading scalable and memory-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b>\ud83d\udca1 Ray Data best practices</b>\n",
    "\n",
    "- **trigger lazy execution**: use [`take`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.take.html) to trigger the execution because Ray has lazy execution mode, which decreases execution time and memory utilization. But, this approach means that you need an operation like take, count, write, etc., to actually execute the workflow DAG.\n",
    "- **shuffling strategies**: to shuffle the dataset because it's all ordered by class, randomly shuffle the ordering of input files before reading. Ray Data also provides an extensive list of [shuffling strategies](https://docs.ray.io/en/latest/data/shuffling-data.html) such as local shuffles, per-epoch shuffles, etc.\n",
    "- **`materialize` during development**: use [`materialize`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.materialize.html) to execute and materialize the dataset into Ray's [shared memory object store memory](https://docs.ray.io/en/latest/ray-core/objects.html). This way, you save a checkpoint at this point and future operations on the dataset can start from this point. You won't rerun all operations on the dataset again from scratch. This feature is convenient during development, especially in a stateful environment like Jupyter notebooks, because you can run from saved checkpoints.\n",
    "\n",
    "    ```python\n",
    "    ds = ds.map(...)\n",
    "    ds = ds.materialize()\n",
    "    ```\n",
    "\n",
    "    **Note**: only use this during development and use it with small datasets, as it will load it all into memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also want to add the class for each data point. When reading the data with `include_paths` Ray Data saves the filename with each data point. The filename has the class label in it so add that to each data point's row. Use Ray Data's [map](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html) function to apply the function to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_class(row):\n",
    "    row[\"class\"] = row[\"path\"].rsplit(\"/\", 3)[-2]\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add class.\n",
    "ds = ds.map(add_class, num_cpus=1, num_gpus=0, concurrency=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Ray Data streaming execution</b> \n",
    "\n",
    "\u274c Traditional batch execution, for example, non-streaming like Spark without pipelining, SageMaker Batch Transform:\n",
    "- Reads the entire dataset into memory or a persistent intermediate format.\n",
    "- Only then starts applying transformations like .map, .filter, etc.\n",
    "- Higher memory pressure and startup latency.\n",
    "\n",
    "\u2705 Streaming execution with Ray Data:\n",
    "- Starts processing chunks (\"blocks\") as they're loaded. No need to wait for entire dataset to load.\n",
    "- Reduces memory footprint (no OOMs) and speeds up time to first output.\n",
    "- Increase resource utilization by reducing idle time.\n",
    "- Online-style inference pipelines with minimal latency.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/multimodal-ai/refs/heads/main/images/streaming.gif\" width=1000>\n",
    "\n",
    "**Note**: Ray Data isn't a real-time stream processing engine like Flink or Kafka Streams. Instead, it's batch processing with streaming execution, which is especially useful for iterative ML workloads, ETL pipelines, and preprocessing before training or inference. Ray typically has a [**2-17x throughput improvement**](https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker#-results-of-throughput-from-experiments) over solutions like Spark and SageMaker Batch Transform, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}