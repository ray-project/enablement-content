{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports  \n",
    "This cell loads all required libraries for the tutorial: PyData tools for data processing, PyTorch for model building and training, and Ray Train for distributed orchestration. `TorchTrainer` is the main training engine, while `prepare_model` and `prepare_data_loader` help convert vanilla PyTorch code into Ray-aware components that scale seamlessly across nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup \n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "    \"datasets==2.19.2\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "import os, io, math, uuid, shutil, random\n",
    "import requests, sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from datasets import load_dataset   \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import ray\n",
    "import ray.data as rdata\n",
    "import ray.train as train\n",
    "from ray.train import (\n",
    "    ScalingConfig, RunConfig, FailureConfig,\n",
    "    CheckpointConfig, Checkpoint, get_checkpoint, get_context\n",
    ")\n",
    "from ray.train.torch import prepare_model, prepare_data_loader, TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load NYC taxi passenger counts (30-min)  \n",
    "Download and cache a lightweight NYC taxi demand dataset from GitHub. Store the file under the shared `/mnt/cluster_storage` directory so that all Ray workers can read it without duplication. Parse the timestamps and used as the DataFrame index, making the data time-series ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load NYC taxi passenger counts (30-min) from GitHub raw \u2013 no auth, ~1 MB\n",
    "\n",
    "DATA_DIR = \"/mnt/cluster_storage/nyc_taxi_ts\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv\"\n",
    "csv_path = os.path.join(DATA_DIR, \"nyc_taxi.csv\")\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    print(\"Downloading nyc_taxi.csv \u2026\")\n",
    "    df = pd.read_csv(url)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "else:\n",
    "    print(\"File already present.\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "# Parse timestamp and tidy\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\").rename(columns={\"value\": \"passengers\"})\n",
    "\n",
    "print(\"Rows:\", len(df), \"| Time span:\", df.index.min(), \"\u2192\", df.index.max())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Resample to hourly, then normalize  \n",
    "Resample the dataset to 30-minute intervals (if it wasn\u2019t already), then z-score the `passengers` column to get a standardized signal. This helps with training stability, gradient scale, and ensures the model doesn\u2019t learn absolute magnitudes too early. You reverse the normalization after inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Resample to hourly, then normalize\n",
    "hourly = df.resample(\"30min\").mean()\n",
    "\n",
    "mean, std = hourly[\"passengers\"].mean(), hourly[\"passengers\"].std()\n",
    "hourly[\"norm\"] = (hourly[\"passengers\"] - mean) / std\n",
    "\n",
    "print(f\"Half-Hourly rows: {len(hourly)}  |  mean={mean:.1f}, std={std:.1f}\")\n",
    "hourly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Quick visual sanity-check  \n",
    "Before moving to training, it\u2019s good practice to visualise the raw data. Plot the first two weeks of half-hourly taxi demand. This helps confirm that the series exhibits strong seasonality and contains no unexpected gaps or noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Quick visual sanity-check \u2014 first two weeks\n",
    "plt.figure(figsize=(10, 4))\n",
    "hourly[\"passengers\"].iloc[:24*14].plot()\n",
    "plt.title(\"NYC-Taxi passengers - first 2 weeks of 2014\")\n",
    "plt.ylabel(\"# trips in hour\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sliding-window dataset to Parquet  \n",
    "Convert the time-series into a supervised learning format using sliding windows. Each sample consists of a fixed-length input sequence (1 week of past data) and a prediction target (next 24 hours). Write these to columnar Parquet files on shared storage to enable efficient streaming in distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Build sliding-window dataset and write to Parquet\n",
    "# ----------------------------------------------------\n",
    "INPUT_WINDOW = 24 * 7   # 1/2 week history (in 30-min steps = 168)\n",
    "HORIZON      = 48       # predict next 24 h\n",
    "STRIDE       = 12       # slide 6 hours at a time\n",
    "\n",
    "values = hourly[\"norm\"].to_numpy(dtype=\"float32\")  # already normalised\n",
    "\n",
    "# ---- Time-aware split to avoid leakage between train and val ----\n",
    "cut = int(0.9 * len(values))  # split by time index on the original series\n",
    "train_records, val_records = [], []\n",
    "\n",
    "for s in range(0, len(values) - INPUT_WINDOW - HORIZON + 1, STRIDE):\n",
    "    past   = values[s : s + INPUT_WINDOW]\n",
    "    future = values[s + INPUT_WINDOW : s + INPUT_WINDOW + HORIZON]\n",
    "    end    = s + INPUT_WINDOW + HORIZON  # last index consumed by this window\n",
    "\n",
    "    rec = {\n",
    "        \"series_id\": 0,\n",
    "        \"past\":  past.tolist(),\n",
    "        \"future\": future.tolist(),\n",
    "    }\n",
    "\n",
    "    if end <= cut:         # Entire window ends before the cut to train\n",
    "        train_records.append(rec)\n",
    "    elif s >= cut:         # Window starts after the cut to val\n",
    "        val_records.append(rec)\n",
    "    # else: window crosses the cut to drop to prevent leakage\n",
    "\n",
    "print(f\"Windows \u2192 train: {len(train_records)}, val: {len(val_records)}\")\n",
    "\n",
    "# Write to Parquet\n",
    "DATA_DIR     = \"/mnt/cluster_storage/nyc_taxi_ts\"\n",
    "PARQUET_DIR  = os.path.join(DATA_DIR, \"parquet\")\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "schema = pa.schema([\n",
    "    (\"series_id\", pa.int32()),\n",
    "    (\"past\",  pa.list_(pa.float32())),\n",
    "    (\"future\", pa.list_(pa.float32()))\n",
    "])\n",
    "\n",
    "def write_parquet(records, fname):\n",
    "    pq.write_table(pa.Table.from_pylist(records, schema=schema), fname, version=\"2.6\")\n",
    "\n",
    "write_parquet(train_records, os.path.join(PARQUET_DIR, \"train.parquet\"))\n",
    "write_parquet(val_records,   os.path.join(PARQUET_DIR, \"val.parquet\"))\n",
    "print(\"Parquet shards written \u2192\", PARQUET_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. PyTorch Dataset over Parquet  \n",
    "Define a lightweight PyTorch `Dataset` class that reads each window from the Parquet shard. This makes the model training logic agnostic to how you store the data. Your DataLoader receives standard PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. PyTorch Dataset that reads the Parquet shards\n",
    "\n",
    "class TaxiWindowDataset(Dataset):\n",
    "    def __init__(self, parquet_path):\n",
    "        self.table  = pq.read_table(parquet_path)\n",
    "        self.past   = self.table.column(\"past\").to_pylist()\n",
    "        self.future = self.table.column(\"future\").to_pylist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.past)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        past   = torch.tensor(self.past[idx],   dtype=torch.float32).unsqueeze(-1)   # (T, 1)\n",
    "        future = torch.tensor(self.future[idx], dtype=torch.float32)                 # (H,)\n",
    "        return past, future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Inspect one random batch  \n",
    "Always verify shapes before diving into training. This cell uses a basic `DataLoader` to fetch one random batch and prints the dimensions of the input and target tensors. This ensures the encoder and decoder receive tensors of the correct size and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Inspect one random batch\n",
    "loader = DataLoader(TaxiWindowDataset(os.path.join(PARQUET_DIR, \"train.parquet\")),\n",
    "                    batch_size=4, shuffle=True)\n",
    "xb, yb = next(iter(loader))\n",
    "print(\"Past:\", xb.shape, \"Future:\", yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Ray-prepared DataLoader  \n",
    "Ray Train provides a helper to wrap your `DataLoader` so that it integrates seamlessly with distributed training. `prepare_data_loader` takes care of sharding and worker setup, ensuring each process only loads a subset of the data and communicates correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Helper to build Ray-prepared DataLoader\n",
    "from ray.train.torch import prepare_data_loader\n",
    "\n",
    "def build_dataloader(parquet_path, batch_size, shuffle=True):\n",
    "    ds = TaxiWindowDataset(parquet_path)\n",
    "    loader = DataLoader(\n",
    "        ds, batch_size=batch_size, shuffle=shuffle, num_workers=2, drop_last=False,\n",
    "    )\n",
    "    return prepare_data_loader(loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}