{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Model Serving with Ray Serve",
    "\u00a9 2025, Anyscale. All Rights Reserved",
    "",
    "\ud83d\udcbb **Launch Locally**: You can run this notebook locally.",
    "",
    "\ud83d\ude80 **Launch on Cloud**: Think about running this notebook on a Ray Cluster (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale)",
    "",
    "Model serving is the process of deploying machine learning models to production so that they can be accessed and ",
    "used by applications or users. It involves creating an API or interface that allows users to send requests to the model",
    "and receive predictions in response. There are several libraries and frameworks available for model serving, ",
    "each with its own features and capabilities. In this notebook, we showcase Ray Serve and FastAPI to deploy a sentiment analysis",
    "machine learning (ML) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ray Serve?",
    "Ray Serve is a scalable model serving library that allows you to deploy and manage machine learning models in production.",
    "With Ray Serve, you can easily create a scalable and distributed serving architecture that",
    "can handle high traffic and large workloads. It is built on top of Ray, a distributed computing framework",
    "that allows you to run Python code in parallel across multiple machines. ",
    "Ray Serve provides a simple API for deploying and managing models, as well as features like autoscaling,",
    "load balancing, and versioning.",
    "",
    "Ray Serve is designed to be easy to use and integrate with existing machine learning workflows.",
    "It supports a wide range of machine learning frameworks, including TensorFlow, PyTorch, and Scikit-learn.",
    "Ray Serve also provides a simple way to deploy models as REST APIs, using FastAPI,",
    "making it easy to integrate with web applications and other services.",
    "",
    "More information: https://docs.ray.io/en/latest/serve/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not use just FastAPI or Flask?",
    "We could have simply used FastAPI or Flask to create a REST API for the model,",
    "but Ray Serve provides additional features like autoscaling and load balancing that ",
    "make it a better choice for production deployments. Ray Serve also allows you to easily",
    "deploy multiple models and manage their versions, which can be useful in a production environment ",
    "where you may need to deploy multiple models or update existing ones.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline",
    "",
    "",
    "    Architecture",
    "    Import Libraries",
    "    FastAPI service to accept HTTP requests and scaling with Ray Serve",
    "    Simulate Client: Send test requests",
    "    Shutdown the Serve app and the ray cluster",
    "</ul>",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}