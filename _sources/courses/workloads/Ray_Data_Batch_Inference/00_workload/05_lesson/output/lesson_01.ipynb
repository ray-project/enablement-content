{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on the entire dataset",
    "Execute and materialize this dataset into object store memory. This operation will trigger execution of the lazy transformations performed on this dataset. The embedding model 'TextEmbedder' in map_batches() is called on the entire dataset.",
    "",
    "# Run inference on the entire dataset",
    "# Note that this does not mutate the original Dataset.",
    "materialized_ds = ds.materialize()",
    "",
    "# metadata after inference",
    "print('** Original dataset:', ds)",
    "print('\\n** Materialized dataset:', materialized_ds)",
    "",
    "# Show a few rows of the materialized dataset with embeddings",
    "materialized_ds.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of memory errors",
    "GPU (or MPS or CPU) memory has to keep the machine learning model and the batch of data in memory during the inference. If the batch_size is too large, it can run out of memory and throw out of memory errors. In that case, reduce the batch_size.",
    "",
    "### Shutdown Ray cluster",
    "",
    "# avoids collisons with other notebooks running ray jobs on the same machine",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary",
    "This notebook demonstrates how to perform efficient batch inference on large datasets using Ray Data. It walks through loading a public dataset from Hugging Face, converting it into a Ray Dataset, and defining a callable class to load and apply a machine learning model (SentenceTransformer) for embedding text. The notebook shows how to use Ray Data\u2019s `map_batches` API to process data in parallel batches, leveraging available CPUs or GPUs for high-throughput inference. It also covers best practices for scaling, handling memory constraints, and summarizes how Ray Data enables scalable, distributed batch inference for modern ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}