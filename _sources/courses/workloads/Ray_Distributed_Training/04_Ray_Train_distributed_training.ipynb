{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training with Ray Train, PyTorch and Hugging Face\n",
    "¬© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª **Launch Locally**: You can run this notebook locally.\n",
    "\n",
    "üöÄ **Launch on Cloud**: Think about running this notebook on a Ray Cluster (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates how to perform distributed training of a BERT model for sequence classification using Ray Train, PyTorch, and Hugging Face libraries. The goal is to classify Yelp reviews into categories by leveraging the power of distributed computing, which allows you to train large models efficiently across multiple CPUs or GPUs.\n",
    "\n",
    "The notebook starts by importing all the necessary libraries, including PyTorch for deep learning, Hugging Face Transformers for model and tokenizer utilities, and Ray Train for distributed training. It then sets up the evaluation metric (accuracy) and defines a function to compute this metric during model evaluation.\n",
    "\n",
    "A key part of the notebook is the training function, which is executed by each worker in the distributed setup. This function handles loading the Yelp review dataset, tokenizing the text data, preparing data loaders for batching, and setting up the BERT model for training. The function is designed to automatically use the best available hardware, whether that's a CPU, GPU, or Apple Silicon's MPS.\n",
    "\n",
    "The main training function, `train_bert`, configures the distributed environment using Ray, sets up the training parameters, and launches the training process across multiple workers. This approach allows you to scale up your training easily, making it suitable for both local machines and cloud platforms. After training, Ray is properly shut down to free up resources.\n",
    "\n",
    "Overall, this notebook provides a practical introduction to distributed deep learning with modern Python tools, making it easier for machine learning engineers to train large models on big datasets efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<ol>\n",
    "    <li>Architecture Diagram\n",
    "    <li>Library Imports\n",
    "        <ul>\n",
    "            <li>Importing PyTorch, Hugging Face Transformers, Ray Train, and other dependencies\n",
    "        </ul>\n",
    "    <li>Metrics Setup\n",
    "        <ul>\n",
    "            <li>Defining accuracy as the evaluation metric\n",
    "            <li>Function to compute metrics during evaluation\n",
    "        </ul>\n",
    "    <li>Training Function Per Worker\n",
    "        <ul>\n",
    "            <li>Data loading and preprocessing (tokenization)\n",
    "            <li>Preparing data loaders for batching\n",
    "            <li>Model initialization (BERT for sequence classification)\n",
    "            <li>Device selection (CPU, GPU, or MPS)\n",
    "            <li>Training and evaluation loop\n",
    "        </ul>\n",
    "    <li>Main Training Function\n",
    "        <ul>\n",
    "            <li>Setting up distributed training configuration with Ray\n",
    "            <li>Scaling configuration for CPUs/GPUs\n",
    "            <li>Initializing and running the Ray TorchTrainer\n",
    "        </ul>\n",
    "    <li>Running the Training\n",
    "        <ul>\n",
    "            <li>Executing the main training function with a specified number of workers\n",
    "        </ul>\n",
    "    <li>Shutdown Ray Cluster\n",
    "    <li>Summary\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture Diagram](https://lz-public-demo.s3.us-east-1.amazonaws.com/anyscale101/01_examples/04_Ray_Train_architecture.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "from typing import Dict # For type hinting\n",
    "\n",
    "import torch # PyTorch for tensor operations\n",
    "from torch import nn # PyTorch for deep learning\n",
    "from torch.utils.data import DataLoader # DataLoader for batching and shuffling data\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset # To load datasets from Hugging Face\n",
    "import transformers # Transformers library for model and tokenizer\n",
    "from transformers import (\n",
    "    Trainer, # \n",
    "    TrainingArguments,\n",
    "    AutoTokenizer, # Tokenizer for Hugging Face models\n",
    "    AutoModelForSequenceClassification, # Model for sequence classification\n",
    ")\n",
    "\n",
    "import ray.train # Ray Train for distributed training\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer # Trainer for PyTorch\n",
    "from ray.train.torch import TorchConfig # Configuration for PyTorch training\n",
    "from ray.runtime_env import RuntimeEnv # Runtime environment for Ray tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metrics Setup\n",
    "We will use accuracy as our evaluation metric. The compute_metrics function will calculate the accuracy of our model‚Äôs predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "metric = evaluate.load(\"accuracy\") # Load accuracy metric from Hugging Face evaluate library\n",
    "\n",
    "# Function to compute metrics\n",
    "# This function takes the evaluation predictions and computes the accuracy metric\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training function per worker\n",
    "This function will be executed by each worker during training. It handles data loading, tokenization, model initialization, and the training loop. This will automatically select GPU, MPS (on Apple Silicon), or CPU.\n",
    "\n",
    "### Tokenizer\n",
    "Tokenizer function is used to convert text into input IDs and attention masks.\n",
    "\n",
    "Padding and truncation are applied to ensure uniform input size. This is essential for training models that require fixed-size inputs. The function is applied to the dataset using the map method. The map method applies the function to each example in the dataset. The batched=True argument allows processing multiple examples at once, which is more efficient.\n",
    "\n",
    "The resulting dataset will have the tokenized inputs ready for training. This is a crucial step in preparing the dataset for model training. It ensures that the text data is converted into a format that the model can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "Dataloaders are used to load the dataset in batches for training and evaluation. This is essential for efficient training, especially with large datasets. The DataLoader will shuffle the training data and collate it into batches\n",
    "The collate_fn is set to transformers.default_data_collator, which handles padding and batching automatically. The batch_size is set to the batch size per worker, which is defined in the config. This allows each worker to process a subset of the data in parallel. This is crucial for distributed training, where each worker processes a portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func_per_worker(config: Dict):\n",
    "    \n",
    "    # Datasets\n",
    "    dataset = load_dataset(\"yelp_review_full\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        \"\"\"    \n",
    "        This function will tokenize the text data in the dataset\n",
    "        It uses the tokenizer to convert text into input IDs and attention masks\n",
    "        Padding and truncation are applied to ensure uniform input size\n",
    "        This is essential for training models that require fixed-size inputs\n",
    "        \"\"\"\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    batch_size = config[\"batch_size_per_worker\"]\n",
    "\n",
    "    # select a subset of the dataset for training and evaluation\n",
    "    # In a real-world scenario, you would use the entire dataset\n",
    "    SMALL_SIZE = 100\n",
    "    # The map method applies the function to each example in the dataset\n",
    "    # The batched=True argument allows processing multiple examples at once, which is more efficient\n",
    "    # The resulting dataset will have the tokenized inputs ready for training\n",
    "    # This is a crucial step in preparing the dataset for model training\n",
    "    # It ensures that the text data is converted into a format that the model can understand\n",
    "    train_dataset = dataset[\"train\"].select(range(SMALL_SIZE)).map(tokenize_function, batched=True)\n",
    "    eval_dataset = dataset[\"test\"].select(range(SMALL_SIZE)).map(tokenize_function, batched=True)\n",
    "\n",
    "    # Prepare dataloader for each worker\n",
    "    # Dataloaders are used to load the dataset in batches for training and evaluation\n",
    "    # The dataloaders dictionary will hold the training and evaluation dataloaders\n",
    "    # This allows for easy access to the dataloaders during training and evaluation\n",
    "    # The dataloaders will be used in the training loop to fetch batches of data for each worker\n",
    "    dataloaders = {}\n",
    "    dataloaders[\"train\"] = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        shuffle=True, \n",
    "        collate_fn=transformers.default_data_collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    dataloaders[\"test\"] = torch.utils.data.DataLoader(\n",
    "        eval_dataset, \n",
    "        shuffle=True, \n",
    "        collate_fn=transformers.default_data_collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Obtain GPU device automatically\n",
    "    # device = ray.train.torch.get_device()\n",
    "    \n",
    "    # Alternatively, you can specify the device manually\n",
    "    # Check if CUDA or MPS is available and set device accordingly\n",
    "    # This is useful for running on different hardware configurations\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") # For Apple Silicon Macs\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Prepare model and optimizer\n",
    "    # Load a pre-trained BERT model for sequence classification\n",
    "    # The model is initialized with the number of labels for classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-cased\", num_labels=5\n",
    "    )\n",
    "    # The model is moved to the selected device (GPU, MPS, or CPU)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # The optimizer is set to SGD with momentum\n",
    "    # This is essential for training the model\n",
    "    # The optimizer will update the model parameters during training\n",
    "    # The learning rate and momentum are set based on the configuration\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Start training loops\n",
    "    # The model will be trained for the specified number of epochs\n",
    "    # The model will be trained using the training dataloader\n",
    "    # The model will be evaluated using the evaluation dataloader\n",
    "    # The training loop will iterate over the epochs and batches\n",
    "    for epoch in range(epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            # breakpoint()\n",
    "            for batch in dataloaders[phase]: # Iterate over batches in the dataloader\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # The model processes the input batch and returns outputs\n",
    "                    # The outputs include the loss and logits\n",
    "                    # The loss is calculated based on the model's predictions and the true labels\n",
    "                    # The logits are the raw predictions from the model\n",
    "                    # The loss is used to update the model parameters during training\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward() # Backpropagate the loss to compute gradients\n",
    "                        # The optimizer updates the model parameters based on the computed gradients\n",
    "                        optimizer.step()\n",
    "                        print(f\"train epoch:[{epoch}]\\tloss:{loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Training Function\n",
    "The *train_bert* function sets up the distributed training environment using Ray and starts the training process. To enable training using GPU, we only need to make the following changes:\n",
    "\n",
    "* Require an GPU for each worker in ScalingConfig\n",
    "* Set backend to ‚Äúnccl‚Äù in TorchConfig\n",
    "\n",
    "This function is designed to train a BERT model using Ray Train. It sets up the training configuration, scaling, and starts the Ray cluster. The function initializes the Ray Train environment, configures the trainer, and starts the training process.\n",
    "* It is intended to be run in a distributed setting with multiple workers, allowing for efficient training of large models on large datasets by leveraging Ray's distributed computing capabilities.\n",
    "* The function uses the Ray Train library to manage distributed training and the TorchTrainer for PyTorch models.\n",
    "* It supports both GPU and CPU training, making it flexible for different hardware configurations. \n",
    "* Additionally, it can be easily adapted for different models and datasets by changing the model and dataset loading parts. \n",
    "* This approach provides a scalable solution for training deep learning models in a distributed manner and can be used in various environments, including local machines and cloud platforms.\n",
    "* It is a powerful tool for researchers and developers working with large-scale machine learning tasks, enabling efficient training on large datasets and easy integration into existing machine learning workflows with minimal changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train BERT model using Ray Train\n",
    "# This function sets up the training configuration, scaling, and starts the Ray cluster.\n",
    "# It initializes the Ray Train environment, configures the trainer, and starts the training process.\n",
    "def train_bert(num_workers=2):\n",
    "    global_batch_size = 8 # This is the total batch size across all workers\n",
    "\n",
    "    # Define the training configuration\n",
    "    # This configuration includes the learning rate, number of epochs, and batch size per worker\n",
    "    train_config = {\n",
    "        \"lr\": 1e-3,  # Learning rate\n",
    "        \"epochs\": 2,  # Reduced for faster testing\n",
    "        \"batch_size_per_worker\": global_batch_size // num_workers,\n",
    "    }\n",
    "\n",
    "    # Configure computation resources\n",
    "    # if using CPUs or MPS\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers, resources_per_worker={\"CPU\": 1,})\n",
    "    \n",
    "    # If using GPUs, you can specify resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    "    # scaling_config = ScalingConfig(num_workers=num_workers, resources_per_worker={\"CPU\": 1, \"GPU\": 1})\n",
    "    # Set backend to nccl in TorchConfig\n",
    "    # torch_config = TorchConfig(backend = \"nccl\")\n",
    "    \n",
    "    # start your ray cluster\n",
    "    ray.init() \n",
    "    \n",
    "    # Initialize a Ray TorchTrainer\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func_per_worker,\n",
    "        train_loop_config=train_config,\n",
    "        # torch_config=torch_config, # Uncomment if using nccl backend\n",
    "        scaling_config=scaling_config,\n",
    "    )\n",
    "\n",
    "    result = trainer.fit() # Start the training process\n",
    "    print(f\"Training result: {result}\") # This will print the training result, which includes metrics like loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Training\n",
    "Finally, we call the train_bert function to start the training process. You can adjust the number of workers to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 10:16:10,519\tINFO worker.py:1908 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-07-11 10:16:11,092\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:11 (running for 00:00:00.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:16 (running for 00:00:05.14)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=41599)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TorchTrainer pid=41521)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=41521)\u001b[0m - (node_id=0eca5bdc14957219701f50108487dbd39f13987d253f812c0d6b29a9, ip=127.0.0.1, pid=41599) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=41521)\u001b[0m - (node_id=0eca5bdc14957219701f50108487dbd39f13987d253f812c0d6b29a9, ip=127.0.0.1, pid=41598) world_rank=1, local_rank=1, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:21 (running for 00:00:10.22)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 5090.92 examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=41599)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "\u001b[36m(RayTrainWorker pid=41599)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=41599)\u001b[0m train epoch:[0]\tloss:1.764641\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:26 (running for 00:00:15.28)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=41598)\u001b[0m train epoch:[0]\tloss:1.949393\u001b[32m [repeated 27x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:31 (running for 00:00:20.35)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:36 (running for 00:00:25.46)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=41599)\u001b[0m train epoch:[1]\tloss:1.799808\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:41 (running for 00:00:30.54)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=41598)\u001b[0m train epoch:[1]\tloss:1.422321\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:46 (running for 00:00:35.63)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 10:16:49,397\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/maxpumperla/ray_results/TorchTrainer_2025-07-11_10-16-11' in 0.0033s.\n",
      "2025-07-11 10:16:49,400\tINFO tune.py:1041 -- Total run time: 38.31 seconds (38.29 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial TorchTrainer_4dd7a_00000 completed. Last result: \n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:49 (running for 00:00:38.30)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n",
      "Training result: Result(\n",
      "  metrics={},\n",
      "  path='/Users/maxpumperla/ray_results/TorchTrainer_2025-07-11_10-16-11/TorchTrainer_4dd7a_00000_0_2025-07-11_10-16-11',\n",
      "  filesystem='local',\n",
      "  checkpoint=None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Run the training function with the specified number of workers\n",
    "# You can adjust the number of workers based on your hardware configuration\n",
    "train_bert(num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Shutdown Ray Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray after training is complete\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Summary\n",
    "This notebook demonstrates how to use Ray Train, PyTorch, and Hugging Face Transformers to perform distributed training of a BERT model for sequence classification on the Yelp review dataset. It covers data loading, tokenization, model setup, and distributed training configuration, allowing you to efficiently train large models across multiple CPUs or GPUs. The notebook is designed to be accessible for machine learning engineers who want to learn scalable deep learning workflows using modern Python tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
