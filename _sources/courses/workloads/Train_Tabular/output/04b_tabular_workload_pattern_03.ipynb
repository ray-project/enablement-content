{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define the Ray Train worker loop (Arrow-based, memory-efficient)  \n",
    "\n",
    "Each Ray Train worker runs its own copy of `train_func`.  \n",
    "Inside the loop, the worker pulls its **shard** of the train and validation datasets directly as **Arrow tables**. \n",
    "\n",
    "You then:  \n",
    "1. **Materialize each shard** into a `pyarrow.Table` and drop any accidental index columns (like `__index_level_0__`)  \n",
    "   that might have been added during Parquet serialization.  \n",
    "2. **Convert Arrow \u2192 NumPy \u2192 XGBoost DMatrix** with explicit `feature_names`, ensuring consistent column order  \n",
    "   across all workers and splits.  \n",
    "3. **Optionally resume** from a prior checkpoint using `get_checkpoint()`.  \n",
    "4. **Train the booster** with `xgb.train`, using the built-in `RayTrainReportCallback()` to automatically stream  \n",
    "   per-round metrics and checkpoints back to Ray Train.  \n",
    "\n",
    "This design keeps the data path fully distributed and avoids unnecessary copies or manual metric handling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_COLS = {\"__index_level_0__\"}  # extend if needed\n",
    "\n",
    "def _arrow_table_from_shard(name: str) -> pa.Table:\n",
    "    \"\"\"Collect this worker's Ray Dataset shard into one pyarrow. Table and\n",
    "    drop accidental index columns (e.g., from pandas Parquet).\"\"\"\n",
    "    ds_iter = get_dataset_shard(name)\n",
    "    arrow_refs = ds_iter.materialize().to_arrow_refs()\n",
    "    tables = [ray.get(r) for r in arrow_refs]\n",
    "    tbl = pa.concat_tables(tables, promote_options=\"none\") if tables else pa.table({})\n",
    "    # Drop index columns if present\n",
    "    keep = [c for c in tbl.column_names if c not in INDEX_COLS]\n",
    "    if len(keep) != len(tbl.column_names):\n",
    "        tbl = tbl.select(keep)\n",
    "    return tbl\n",
    "\n",
    "def _dmat_from_arrow(table: pa.Table, feature_cols, label_col: str):\n",
    "    \"\"\"Build XGBoost DMatrix from pyarrow.Table with explicit feature_names.\"\"\"\n",
    "    X = np.column_stack([table[c].to_numpy(zero_copy_only=False) for c in feature_cols])\n",
    "    y = table[label_col].to_numpy(zero_copy_only=False)\n",
    "    return xgb.DMatrix(X, label=y, feature_names=feature_cols)\n",
    "\n",
    "def train_func(config):\n",
    "    label_col = config[\"label_column\"]\n",
    "\n",
    "    # Arrow tables \n",
    "    train_arrow = _arrow_table_from_shard(\"train\")\n",
    "    eval_arrow  = _arrow_table_from_shard(\"evaluation\")\n",
    "\n",
    "    # Use the SAME ordered feature list for both splits\n",
    "    feature_cols = [c for c in train_arrow.column_names if c != label_col]\n",
    "\n",
    "    dtrain = _dmat_from_arrow(train_arrow, feature_cols, label_col)\n",
    "    deval  = _dmat_from_arrow(eval_arrow,  feature_cols, label_col)\n",
    "\n",
    "    # -------- 2) Optional resume from checkpoint ------------------------------\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as d:\n",
    "            model_path = os.path.join(d, RayTrainReportCallback.CHECKPOINT_NAME)\n",
    "            booster = xgb.Booster()\n",
    "            booster.load_model(model_path)\n",
    "            print(f\"[Rank {get_context().get_world_rank()}] Resumed from checkpoint\")\n",
    "    else:\n",
    "        booster = None\n",
    "\n",
    "    # -------- 3) Train with per-round reporting & checkpointing ---------------\n",
    "    evals_result = {}\n",
    "    xgb.train(\n",
    "        params          = config[\"params\"],\n",
    "        dtrain          = dtrain,\n",
    "        evals           = [(dtrain, \"train\"), (deval, \"validation\")],\n",
    "        num_boost_round = config[\"num_boost_round\"],\n",
    "        xgb_model       = booster,\n",
    "        evals_result    = evals_result,\n",
    "        callbacks       = [RayTrainReportCallback()],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Configure XGBoost and build the Trainer  \n",
    "Next, define the XGBoost hyperparameters and wrap the `train_func` in an `XGBoostTrainer` for distributed execution.  \n",
    "Each worker is assigned an entire CPU node (`resources_per_worker={\"CPU\": CPUS_PER_WORKER}`),  \n",
    "allowing XGBoost to use all local cores efficiently through the `nthread` parameter.  \n",
    "\n",
    "Key settings:  \n",
    "- **`ScalingConfig`** \u2014 controls how many workers to launch and their CPU/GPU allocation.  \n",
    "- **`CheckpointConfig`** \u2014 saves a checkpoint every 10 boosting rounds and scores each checkpoint by  \n",
    "  validation log-loss (`validation-mlogloss`), retaining only the best model.  \n",
    "- **`FailureConfig`** \u2014 automatically retries failed workers up to one time for fault tolerance.  \n",
    "\n",
    "By passing the Ray Datasets directly into the trainer, Ray handles dataset sharding and distributed streaming automatically,  \n",
    "so each worker trains on its own slice of the data without manual coordination.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. XGBoost config and Trainer (full-node CPU workers)\n",
    "\n",
    "# Adjust this to your node size if different (e.g., 16, 32, etc.)\n",
    "CPUS_PER_WORKER = 4\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 7,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eta\": 0.3,\n",
    "    \"max_depth\": 8,\n",
    "    \"nthread\": CPUS_PER_WORKER,  \n",
    "}\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    train_func,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=2,\n",
    "        use_gpu=False,\n",
    "        resources_per_worker={\"CPU\": CPUS_PER_WORKER},\n",
    "    ),\n",
    "    datasets={\"train\": train_ds, \"evaluation\": val_ds},\n",
    "    train_loop_config={\n",
    "        \"label_column\": \"label\",\n",
    "        \"params\": xgb_params,\n",
    "        \"num_boost_round\": 50,\n",
    "    },\n",
    "    run_config=RunConfig(\n",
    "        name=\"covtype_xgb_cpu\",\n",
    "        storage_path=\"/mnt/cluster_storage/covtype/results\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            checkpoint_frequency=10,\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"validation-mlogloss\",  # score by val loss\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "        failure_config=FailureConfig(max_failures=1),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Start distributed training  \n",
    "`trainer.fit()` blocks until all boosting rounds finish, or until Ray exhausts retries.  The result object contains the last reported metrics and the best checkpoint found so far. Print the final validation log-loss and keep a handle to the checkpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Fit the trainer (reports eval metrics every boosting round)\n",
    "result = trainer.fit()\n",
    "best_ckpt = result.checkpoint            # saved automatically by Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Evaluate the trained model  \n",
    "Pull the XGBoost `Booster` back from the checkpoint, run predictions on the entire validation set, and compute overall accuracy. Converting the Ray Dataset to pandas keeps the example short. In production you stream batches instead of materializing the whole frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Retrieve Booster object from Ray checkpoint\n",
    "booster = RayTrainReportCallback.get_model(best_ckpt)\n",
    "\n",
    "# Convert Ray Dataset to pandas for quick local scoring\n",
    "val_pd = val_ds.to_pandas()\n",
    "dmatrix = xgb.DMatrix(val_pd[feature_columns])\n",
    "pred_prob = booster.predict(dmatrix)\n",
    "pred_labels = np.argmax(pred_prob, axis=1)\n",
    "\n",
    "acc = accuracy_score(val_pd.label, pred_labels)\n",
    "print(f\"Validation accuracy: {acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}