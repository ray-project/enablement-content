{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Reverse diffusion sampler  \n",
    "A simple Euler-style loop that starts from Gaussian noise and iteratively subtracts the model\u2019s predicted noise.  \n",
    "This isn't production-grade sampling, but it's suitable for illustrating inference after training. Use **Ray Data** when performing inference at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Reverse diffusion sampling\n",
    "\n",
    "def sample_image(model, steps=50, device=\"cpu\"):\n",
    "    \"\"\"Generate an image by iteratively de-noising random noise.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = torch.randn(1, 3, 224, 224, device=device)\n",
    "        for step in reversed(range(steps)):\n",
    "            t = torch.tensor([step], device=device)\n",
    "            pred_noise = model(img, t)\n",
    "            img = img - pred_noise * 0.1                      # simple Euler update\n",
    "        # Rescale back to [0,1]\n",
    "        img = torch.clamp((img * 0.5 + 0.5), 0.0, 1.0)\n",
    "        return img.squeeze(0).cpu().permute(1,2,0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Generate and display samples from the best checkpoint \n",
    "Load the model weights from `best_ckpt`, move to GPU if available, generate three images, and show them side-by-side.  \n",
    "Remember that when using a tiny CNN and only 10 epochs, these samples look noise-like. If you replace the backbone or train longer, you will to see better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Generate and display samples\n",
    "\n",
    "import glob\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "assert best_ckpt is not None, \"Checkpoint is missing. Did training run and complete?\"\n",
    "\n",
    "# Restore model weights from Ray Train checkpoint (Lightning-first)\n",
    "model = PixelDiffusion()\n",
    "\n",
    "with best_ckpt.as_directory() as ckpt_dir:\n",
    "    # Prefer Lightning checkpoints (*.ckpt) saved by ModelCheckpoint\n",
    "    ckpt_files = glob.glob(os.path.join(ckpt_dir, \"*.ckpt\"))\n",
    "    if ckpt_files:\n",
    "        pl_ckpt = torch.load(ckpt_files[0], map_location=\"cpu\")\n",
    "        state = pl_ckpt.get(\"state_dict\", pl_ckpt)\n",
    "        model.load_state_dict(state, strict=False)\n",
    "    elif os.path.exists(os.path.join(ckpt_dir, \"model.pt\")):\n",
    "        # Fallback for older/manual checkpoints\n",
    "        state = torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No Lightning .ckpt or model.pt found in: {ckpt_dir}\"\n",
    "        )\n",
    "\n",
    "# Move to device and sample\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Generate three images\n",
    "samples = [sample_image(model, steps=50, device=device) for _ in range(3)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "for ax, img in zip(axs, samples):\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Food-101 Diffusion Samples (unconditional)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Clean up shared storage  \n",
    "Reclaim cluster disk space by deleting the entire tutorial output directory.  \n",
    "Run this only when you\u2019re **sure** you don\u2019t need the checkpoints or metrics anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Cleanup -- delete checkpoints and metrics from model training\n",
    "\n",
    "TARGET_PATH = \"/mnt/cluster_storage/generative_cv\"\n",
    "\n",
    "if os.path.exists(TARGET_PATH):\n",
    "    shutil.rmtree(TARGET_PATH)\n",
    "    print(f\"\u2705 Deleted everything under {TARGET_PATH}\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f Path does not exist: {TARGET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up and next steps\n",
    "\n",
    "In this tutorial, you used **Ray Train and Ray Data on Anyscale** to scale a compact diffusion-policy workload, from raw JPEG bytes to distributed training and sampling, without changing the core PyTorch logic. You should now feel confident:\n",
    "\n",
    "* Using **Ray Data** to decode, normalize, and shard large image datasets in parallel  \n",
    "* Scaling training across multiple GPUs using **TorchTrainer** and a Ray-native `train_loop`  \n",
    "* Managing distributed training state with **Ray Checkpoints** and automatic resume  \n",
    "* Running fault-tolerant multi-node jobs on Anyscale without orchestration scripts  \n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?\n",
    "\n",
    "Below are a few directions you might explore to adapt or extend the pattern:\n",
    "\n",
    "1. **Backbones and architecture upgrades**  \n",
    "   * Swap in a larger ResNet or another vision model for much better generative performance.  \n",
    "   * Try pre-trained encoders and fine-tune only the diffusion-specific layers.\n",
    "\n",
    "2. **Conditional diffusion**  \n",
    "   * Use the `label` column to condition the model (for example, class-conditioning).  \n",
    "   * Compare unconditional versus conditional generation side by side.\n",
    "\n",
    "3. **Sampling improvements**  \n",
    "   * Replace naive reverse diffusion with De-noising Diffusion Implicit Models (DDIM), Pseudo Numerical Methods for Diffusion Models (PNDM), or learned de-noisers.  \n",
    "   * Add timestep embeddings or noise schedules to increase model expressiveness.\n",
    "\n",
    "4. **Longer training and mixed precision**  \n",
    "   * Increase the `max_epochs` and enable Automatic Mixed Precision (AMP) for faster training with less memory.  \n",
    "   * Visualize convergence and training stability across longer runs.\n",
    "\n",
    "5. **Hyperparameter sweeps**  \n",
    "   * Use **Ray Tune** to search over learning rates, model size, or sampling steps.  \n",
    "   * Leverage Tune\u2019s reporting to schedule early stopping or checkpoint pruning.\n",
    "\n",
    "6. **Data handling and scaling**  \n",
    "   * Shard the dataset into multiple Parquet files and distribute across more workers.  \n",
    "   * Store and load datasets from S3 or other cloud storage.\n",
    "\n",
    "7. **Image quality evaluation**  \n",
    "   * Log Fr\u00e9chet Inception Distance (FID) scores, perceptual similarity, or diffusion-specific metrics.  \n",
    "   * Compare generated samples from different checkpoints or backbones.\n",
    "\n",
    "8. **Model serving**  \n",
    "   * Package the reverse sampler into a Ray task or **Ray Serve** endpoint.  \n",
    "   * Run a demo app that generates images on demand from a class name or random seed.\n",
    "\n",
    "9. **End-to-end MLOps**  \n",
    "   * Register the best checkpoint with MLflow or Weights & Biases.  \n",
    "   * Wrap the training loop in a Ray Job and run it on a schedule with Anyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}