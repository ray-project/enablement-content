{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training function per worker\n",
    "This function will be executed by each worker during training. It handles data loading, tokenization, model initialization, and the training loop. This will automatically select GPU, MPS (on Apple Silicon), or CPU.\n",
    "\n",
    "### Tokenizer\n",
    "Tokenizer function is used to convert text into input IDs and attention masks.\n",
    "\n",
    "Padding and truncation are applied to ensure uniform input size. This is essential for training models that require fixed-size inputs. The function is applied to the dataset using the map method. The map method applies the function to each example in the dataset. The batched=True argument allows processing multiple examples at once, which is more efficient.\n",
    "\n",
    "The resulting dataset will have the tokenized inputs ready for training. This is a crucial step in preparing the dataset for model training. It ensures that the text data is converted into a format that the model can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "Dataloaders are used to load the dataset in batches for training and evaluation. This is essential for efficient training, especially with large datasets. The DataLoader will shuffle the training data and collate it into batches\n",
    "The collate_fn is set to transformers.default_data_collator, which handles padding and batching automatically. The batch_size is set to the batch size per worker, which is defined in the config. This allows each worker to process a subset of the data in parallel. This is crucial for distributed training, where each worker processes a portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func_per_worker(config: Dict):\n",
    "    \n",
    "    # Datasets\n",
    "    dataset = load_dataset(\"yelp_review_full\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        \"\"\"    \n",
    "        This function will tokenize the text data in the dataset\n",
    "        It uses the tokenizer to convert text into input IDs and attention masks\n",
    "        Padding and truncation are applied to ensure uniform input size\n",
    "        This is essential for training models that require fixed-size inputs\n",
    "        \"\"\"\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    batch_size = config[\"batch_size_per_worker\"]\n",
    "\n",
    "    # select a subset of the dataset for training and evaluation\n",
    "    # In a real-world scenario, you would use the entire dataset\n",
    "    SMALL_SIZE = 100\n",
    "    # The map method applies the function to each example in the dataset\n",
    "    # The batched=True argument allows processing multiple examples at once, which is more efficient\n",
    "    # The resulting dataset will have the tokenized inputs ready for training\n",
    "    # This is a crucial step in preparing the dataset for model training\n",
    "    # It ensures that the text data is converted into a format that the model can understand\n",
    "    train_dataset = dataset[\"train\"].select(range(SMALL_SIZE)).map(tokenize_function, batched=True)\n",
    "    eval_dataset = dataset[\"test\"].select(range(SMALL_SIZE)).map(tokenize_function, batched=True)\n",
    "\n",
    "    # Prepare dataloader for each worker\n",
    "    # Dataloaders are used to load the dataset in batches for training and evaluation\n",
    "    # The dataloaders dictionary will hold the training and evaluation dataloaders\n",
    "    # This allows for easy access to the dataloaders during training and evaluation\n",
    "    # The dataloaders will be used in the training loop to fetch batches of data for each worker\n",
    "    dataloaders = {}\n",
    "    dataloaders[\"train\"] = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        shuffle=True, \n",
    "        collate_fn=transformers.default_data_collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    dataloaders[\"test\"] = torch.utils.data.DataLoader(\n",
    "        eval_dataset, \n",
    "        shuffle=True, \n",
    "        collate_fn=transformers.default_data_collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Obtain GPU device automatically\n",
    "    # device = ray.train.torch.get_device()\n",
    "    \n",
    "    # Alternatively, you can specify the device manually\n",
    "    # Check if CUDA or MPS is available and set device accordingly\n",
    "    # This is useful for running on different hardware configurations\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\") # For Apple Silicon Macs\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Prepare model and optimizer\n",
    "    # Load a pre-trained BERT model for sequence classification\n",
    "    # The model is initialized with the number of labels for classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-cased\", num_labels=5\n",
    "    )\n",
    "    # The model is moved to the selected device (GPU, MPS, or CPU)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # The optimizer is set to SGD with momentum\n",
    "    # This is essential for training the model\n",
    "    # The optimizer will update the model parameters during training\n",
    "    # The learning rate and momentum are set based on the configuration\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Start training loops\n",
    "    # The model will be trained for the specified number of epochs\n",
    "    # The model will be trained using the training dataloader\n",
    "    # The model will be evaluated using the evaluation dataloader\n",
    "    # The training loop will iterate over the epochs and batches\n",
    "    for epoch in range(epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            # breakpoint()\n",
    "            for batch in dataloaders[phase]: # Iterate over batches in the dataloader\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # The model processes the input batch and returns outputs\n",
    "                    # The outputs include the loss and logits\n",
    "                    # The loss is calculated based on the model's predictions and the true labels\n",
    "                    # The logits are the raw predictions from the model\n",
    "                    # The loss is used to update the model parameters during training\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward() # Backpropagate the loss to compute gradients\n",
    "                        # The optimizer updates the model parameters based on the computed gradients\n",
    "                        optimizer.step()\n",
    "                        print(f\"train epoch:[{epoch}]\\tloss:{loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}