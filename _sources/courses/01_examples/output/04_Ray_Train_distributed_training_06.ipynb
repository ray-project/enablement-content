{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Training\n",
    "Finally, we call the train_bert function to start the training process. You can adjust the number of workers to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 10:16:10,519\tINFO worker.py:1908 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-07-11 10:16:11,092\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:11 (running for 00:00:00.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:16 (running for 00:00:05.14)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=41599)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TorchTrainer pid=41521)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=41521)\u001b[0m - (node_id=0eca5bdc14957219701f50108487dbd39f13987d253f812c0d6b29a9, ip=127.0.0.1, pid=41599) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=41521)\u001b[0m - (node_id=0eca5bdc14957219701f50108487dbd39f13987d253f812c0d6b29a9, ip=127.0.0.1, pid=41598) world_rank=1, local_rank=1, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:21 (running for 00:00:10.22)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 5090.92 examples/s]\n",
      "\u001b[36m(RayTrainWorker pid=41599)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "\u001b[36m(RayTrainWorker pid=41599)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=41599)\u001b[0m train epoch:[0]\tloss:1.764641\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:26 (running for 00:00:15.28)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=41598)\u001b[0m train epoch:[0]\tloss:1.949393\u001b[32m [repeated 27x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:31 (running for 00:00:20.35)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:36 (running for 00:00:25.46)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=41599)\u001b[0m train epoch:[1]\tloss:1.799808\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:41 (running for 00:00:30.54)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=41598)\u001b[0m train epoch:[1]\tloss:1.422321\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:46 (running for 00:00:35.63)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 10:16:49,397\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/maxpumperla/ray_results/TorchTrainer_2025-07-11_10-16-11' in 0.0033s.\n",
      "2025-07-11 10:16:49,400\tINFO tune.py:1041 -- Total run time: 38.31 seconds (38.29 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial TorchTrainer_4dd7a_00000 completed. Last result: \n",
      "== Status ==\n",
      "Current time: 2025-07-11 10:16:49 (running for 00:00:38.30)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n",
      "Training result: Result(\n",
      "  metrics={},\n",
      "  path='/Users/maxpumperla/ray_results/TorchTrainer_2025-07-11_10-16-11/TorchTrainer_4dd7a_00000_0_2025-07-11_10-16-11',\n",
      "  filesystem='local',\n",
      "  checkpoint=None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Run the training function with the specified number of workers\n",
    "# You can adjust the number of workers based on your hardware configuration\n",
    "train_bert(num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Shutdown Ray Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray after training is complete\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Summary\n",
    "This notebook demonstrates how to use Ray Train, PyTorch, and Hugging Face Transformers to perform distributed training of a BERT model for sequence classification on the Yelp review dataset. It covers data loading, tokenization, model setup, and distributed training configuration, allowing you to efficiently train large models across multiple CPUs or GPUs. The notebook is designed to be accessible for machine learning engineers who want to learn scalable deep learning workflows using modern Python tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}