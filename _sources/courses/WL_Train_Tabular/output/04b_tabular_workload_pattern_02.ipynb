{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports  \n",
    "Before you touch any data, import every tool you need.  \n",
    "Alongside the standard scientific-Python stack, bring in **XGBoost** for gradient-boosted decision trees and **Ray** for distributed data loading and training. Ray Train\u2019s helper classes (RunConfig, ScalingConfig, CheckpointConfig, FailureConfig) give you fault-tolerant, CPU training with almost no extra code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup \n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"scikit-learn==1.7.2\",\n",
    "    \"pyarrow==14.0.2\",    \n",
    "    \"xgboost==3.0.5\",\n",
    "    \"seaborn==0.13.2\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "import os, shutil, json, uuid, tempfile, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import pyarrow as pa\n",
    "\n",
    "import ray\n",
    "import ray.data as rd\n",
    "from ray.data import ActorPoolStrategy\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig, FailureConfig, get_dataset_shard, get_checkpoint, get_context\n",
    "from ray.train.xgboost import XGBoostTrainer, RayTrainReportCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the University of California, Irvine (UCI) Cover type dataset  \n",
    "The Cover type dataset contains ~580 000 forest-cover observations with 54 tabular features and a 7-class label. Fetch it from `sklearn.datasets`, rename the target column to `label` (Ray\u2019s default), and shift the classes from **1-7** to **0-6** so they're zero-indexed as XGBoost expects. A quick `value_counts` sanity-check confirms the mapping worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load the UCI Cover type dataset (~580k rows, 54 features)\n",
    "data = fetch_covtype(as_frame=True)\n",
    "df = data.frame\n",
    "df.rename(columns={\"Cover_Type\": \"label\"}, inplace=True)   # Ray expects \"label\"\n",
    "df[\"label\"] = df[\"label\"] - 1          # 1-7  \u2192  0-6\n",
    "assert df[\"label\"].between(0, 6).all()\n",
    "print(df.shape, df.label.value_counts(normalize=True).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize class balance  \n",
    "Highly imbalanced targets can bias tree-based models, so plot the raw label counts. The cover type distribution shows skew, but not much\u2014the bar chart lets you judge whether extra re-scaling or class-weighting is necessary. Rely on XGBoost\u2019s built-in handling for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Visualize class distribution\n",
    "df.label.value_counts().plot(kind=\"bar\", figsize=(6,3), title=\"Cover Type distribution\")\n",
    "plt.ylabel(\"Frequency\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write train / validation Parquet files  \n",
    "\n",
    "Rather than splitting a large dataset in memory later, you persist **train** and **validation** splits up front.  \n",
    "Each split is written to the cluster\u2019s shared volume (`/mnt/cluster_storage`) so that all Ray workers can access it directly.  \n",
    "This approach keeps the workflow reproducible and avoids rematerializing the dataset during distributed training.  \n",
    "\n",
    "You perform a **stratified 80 / 20 split** to preserve class balance across splits, then write each subset to its own Parquet file.  \n",
    "Parquet is columnar and compressed, making it ideal for Ray Data ingestion and parallel reads.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Write separate train/val Parquets to /mnt/cluster_storage/covtype/\n",
    "\n",
    "PARQUET_DIR = \"/mnt/cluster_storage/covtype/parquet\"\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_PARQUET = os.path.join(PARQUET_DIR, \"train.parquet\")\n",
    "VAL_PARQUET   = os.path.join(PARQUET_DIR, \"val.parquet\")\n",
    "\n",
    "# Stratified 80/20 split for reproducibility\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_df.to_parquet(TRAIN_PARQUET, index=False)\n",
    "val_df.to_parquet(VAL_PARQUET, index=False)\n",
    "\n",
    "print(f\"Wrote Train \u2192 {TRAIN_PARQUET} ({len(train_df):,} rows)\")\n",
    "print(f\"Wrote Val   \u2192 {VAL_PARQUET}   ({len(val_df):,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load the train and validation splits as Ray Datasets  \n",
    "\n",
    "Now that the data is stored in Parquet, you load each split directly with `ray.data.read_parquet`.  \n",
    "Each call returns a **lazy, columnar Ray Dataset** that supports distributed reads and transformations across the cluster.  \n",
    "\n",
    "Calling `.random_shuffle()` on the training split ensures balanced sampling during training,  \n",
    "while leaving the validation split unshuffled preserves its deterministic order for evaluation.  \n",
    "\n",
    "From this point forward, all data access is **parallel and streaming**, eliminating single-node I/O bottlenecks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Load the two splits as Ray Datasets (lazy, columnar)\n",
    "train_ds = rd.read_parquet(TRAIN_PARQUET).random_shuffle()\n",
    "val_ds   = rd.read_parquet(VAL_PARQUET)\n",
    "\n",
    "print(train_ds)\n",
    "print(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Inspect dataset sizes (optional)\n",
    "\n",
    "After loading the Parquet files, quickly confirm that both splits were read correctly by counting their rows.  \n",
    "This step triggers a lightweight distributed count across the cluster and verifies that the  \n",
    "**train / validation partitioning** matches the expected 80 / 20 ratio before moving on to distributed training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train rows: {train_ds.count():,},  Val rows: {val_ds.count():,}\")  # Note that this will materialize the dataset (skip at scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Inspect a mini-batch  \n",
    "Taking a tiny pandas batch helps verify that feature columns and labels have the expected shapes and types. You also build `feature_columns`, a list you reuse when building XGBoost\u2019s `DMatrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Look into one batch to confirm feature dimensionality\n",
    "batch = train_ds.take_batch(batch_size=5, batch_format=\"pandas\")\n",
    "print(batch.head())\n",
    "feature_columns = [c for c in batch.columns if c != \"label\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}