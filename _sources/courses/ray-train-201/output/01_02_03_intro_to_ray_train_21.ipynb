{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 \u00b7 Modify Training Loop to Enable Checkpoint Loading  \n",
    "\n",
    "To support fault tolerance, we extend the training loop so it can **resume from a previously saved checkpoint**.  \n",
    "\n",
    "Key additions:  \n",
    "- Call `ray.train.get_checkpoint()` to check if a checkpoint is available.  \n",
    "- If found, restore:  \n",
    "  * The **model state** (`model.pt`)  \n",
    "  * The **optimizer state** (`optimizer.pt`)  \n",
    "  * The **last completed epoch** (`extra_state.pt`)  \n",
    "- Update `start_epoch` so training resumes from the correct place.  \n",
    "\n",
    "The rest of the loop (forward pass, backward pass, optimizer step, and metrics reporting) is the same, except it now starts from `start_epoch` instead of 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Training loop with checkpoint loading for fault tolerance\n",
    "\n",
    "def train_loop_ray_train_with_checkpoint_loading(config: dict):\n",
    "    # Same setup as before: loss, model, optimizer\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Same data loader logic as before\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    data_loader = build_data_loader_ray_train_ray_data(batch_size=batch_size)\n",
    "\n",
    "    # Default: start at epoch 0 unless a checkpoint is available\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Attempt to load from latest checkpoint\n",
    "    checkpoint = ray.train.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        # Continue training from a previous checkpoint\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            # Restore model + optimizer state\n",
    "            model_state_dict = torch.load(\n",
    "                os.path.join(ckpt_dir, \"model.pt\"),\n",
    "            )\n",
    "            # Load the model and optimizer state\n",
    "            model.module.load_state_dict(model_state_dict)\n",
    "            optimizer.load_state_dict(\n",
    "                torch.load(os.path.join(ckpt_dir, \"optimizer.pt\"))\n",
    "            )\n",
    "\n",
    "            # Resume from last epoch + 1\n",
    "            start_epoch = (\n",
    "                torch.load(os.path.join(ckpt_dir, \"extra_state.pt\"))[\"epoch\"] + 1\n",
    "            )\n",
    "\n",
    "    # Same training loop as before except it starts at a parameterized start_epoch\n",
    "    for epoch in range(start_epoch, config[\"num_epochs\"]):\n",
    "        for batch in data_loader:\n",
    "            outputs = model(batch[\"image\"])\n",
    "            loss = criterion(outputs, batch[\"label\"])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Report metrics and save model + optimizer + epoch state\n",
    "        metrics = print_metrics_ray_train(loss,  epoch)\n",
    "\n",
    "        # We now save the optimizer and epoch state in addition to the model\n",
    "        save_checkpoint_and_metrics_ray_train_with_extra_state(\n",
    "            model, metrics, optimizer, epoch\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}