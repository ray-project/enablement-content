{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Continue training from the latest checkpoint  \n",
    "Because `train_func` always checks for `get_checkpoint()`, re-invoking `trainer.fit()` automatically resumes boosting from where you left off. Simply call `fit()` a second time and print the new best validation log-loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Run 50 more training iterations from the last saved checkpoint\n",
    "result = trainer.fit()\n",
    "best_ckpt = result.checkpoint            # Saved automatically by Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Verify post-training inference  \n",
    "\n",
    "Rerun the Ray Data inference pipeline with the latest checkpoint to confirm that  \n",
    "additional boosting rounds improved validation accuracy.  \n",
    "This reuses the same distributed actors, ensuring consistent and scalable evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Rerun Ray Data inference to verify improved accuracy after continued training\n",
    "\n",
    "# Reuse the existing Ray Data inference setup with the latest checkpoint\n",
    "pred_ds = val_ds.map_batches(\n",
    "    XGBPredictor,\n",
    "    fn_constructor_args=(best_ckpt, feature_columns),\n",
    "    batch_format=\"pandas\",\n",
    "    compute=ActorPoolStrategy(),\n",
    "    num_cpus=1,\n",
    ")\n",
    "\n",
    "# Aggregate accuracy across all batches\n",
    "stats_ds = pred_ds.map_batches(\n",
    "    lambda df: pd.DataFrame({\n",
    "        \"correct\": [int((df[\"pred\"] == df[\"label\"]).sum())],\n",
    "        \"n\": [int(len(df))]\n",
    "    }),\n",
    "    batch_format=\"pandas\",\n",
    ")\n",
    "\n",
    "correct = int(stats_ds.sum(\"correct\"))\n",
    "n = int(stats_ds.sum(\"n\"))\n",
    "print(f\"Validation accuracy after continued training: {correct / n:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Clean up  \n",
    "Finally, tidy up by deleting temporary checkpoint folders, the metrics CSV, and any intermediate result directories. Clearing out old artifacts frees disk space and leaves your workspace clean for whatever comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Optional cleanup to free space\n",
    "ARTIFACT_DIR = \"/mnt/cluster_storage/covtype\"\n",
    "if os.path.exists(ARTIFACT_DIR):\n",
    "    shutil.rmtree(ARTIFACT_DIR)\n",
    "    print(f\"Deleted {ARTIFACT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up and next steps\n",
    "\n",
    "You built a fast and fault-tolerant XGBoost training loop that runs on real data, scales across CPUs, recovers from worker failures, and supports batch inference, all inside a single notebook.\n",
    "\n",
    "You should now feel confident:\n",
    "\n",
    "* Using **Ray Data** to ingest, shuffle, and shard large tabular datasets across a cluster  \n",
    "* Defining custom `train_func`s that run on **Ray Train** workers and resume seamlessly from checkpoints  \n",
    "* Tracking per-round metrics and saving checkpoints with **RayTrainReportCallback**  \n",
    "* Leveraging **Ray\u2019s distributed execution model** to evaluate and monitor models without manual orchestration  \n",
    "* Launching remote CPU-powered inference tasks using **Ray Data** for scalable batch scoring\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?\n",
    "\n",
    "Below are a few directions you might explore to adapt or extend the pattern:\n",
    "\n",
    "1. **Early stopping and best iteration tracking**  \n",
    "   * Add `early_stopping_rounds=10` to `xgb.train` and log the best round.  \n",
    "   * Track performance delta across resumed runs.\n",
    "\n",
    "2. **Hyperparameter sweeps**  \n",
    "   * Wrap the trainer with **Ray Tune** and search over `eta`, `max_depth`, or `subsample`.  \n",
    "   * Use Tune\u2019s built-in checkpoint pruning and log callbacks.\n",
    "\n",
    "3. **Feature engineering at scale**  \n",
    "   * Create new features using `Ray Dataset.map_batches`, such as terrain interactions or log-scaled distances.  \n",
    "   * Materialize multiple Parquet shards and benchmark load time.\n",
    "\n",
    "4. **Model interpretability**  \n",
    "   * Use XGBoost\u2019s built-in `Booster.get_score` for feature attributions.  \n",
    "   * Rank features by importance and validate with domain knowledge.\n",
    "\n",
    "5. **Serving the model**  \n",
    "   * Package the Booster as a Ray task or **Ray Serve** endpoint.  \n",
    "   * Deploy an API that takes a feature vector and returns the predicted cover type.\n",
    "\n",
    "6. **Real-time logging**  \n",
    "   * Integrate with MLflow or Weights & Biases to store logs, plots, and checkpoints.  \n",
    "   * Use tags and metadata to track experiments over time.\n",
    "\n",
    "7. **Alternative objectives**  \n",
    "   * Try a binary objective (for example, presence versus absence of a species) or regression target (for example, canopy height).  \n",
    "   * Fine-tune loss functions for specific ecological tasks.\n",
    "\n",
    "8. **End-to-end MLOps**  \n",
    "   * Schedule retraining with Ray Jobs or Anyscale Jobs.  \n",
    "   * Upload new data snapshots and trigger daily training runs with automatic checkpoint cleanup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}