{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)\n",
    "\n",
    "Define the `train_loop_per_worker`, the core function executed by each Ray Train worker.  \n",
    "This loop handles distributed training, validation, and checkpointing with Ray-managed metrics.\n",
    "\n",
    "Each worker receives its own shard of the training and validation datasets using `get_dataset_shard()`.  \n",
    "Batches are streamed directly into PyTorch via `iter_torch_batches()`, ensuring efficient, fully distributed data loading.\n",
    "\n",
    "During each epoch:\n",
    "- Compute average **training** and **validation** MSE losses.  \n",
    "- On **rank 0** only, save a temporary checkpoint (model weights + epoch metadata) using `tempfile.TemporaryDirectory()`.  \n",
    "- Call `ray.train.report()` to report metrics and attach the checkpoint; other workers report metrics only.\n",
    "\n",
    "All metrics are automatically captured by Ray and made available in `result.metrics_dataframe`, enabling progress tracking and fault-tolerant recovery without extra logging logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Define Ray Train loop (with val loss, checkpointing, and Ray-managed metrics)\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    import tempfile\n",
    "    # ---------------- Dataset shards -> PyTorch-style iterators ---------------- #\n",
    "    train_ds = get_dataset_shard(\"train\")\n",
    "    val_ds   = get_dataset_shard(\"val\")\n",
    "    train_loader = train_ds.iter_torch_batches(batch_size=512, dtypes=torch.float32)\n",
    "    val_loader   = val_ds.iter_torch_batches(batch_size=512, dtypes=torch.float32)\n",
    "\n",
    "    # ---------------- Model / Optimizer ---------------- #\n",
    "    model = MatrixFactorizationModel(\n",
    "        num_users=config[\"num_users\"],\n",
    "        num_items=config[\"num_items\"],\n",
    "        embedding_dim=config.get(\"embedding_dim\", 64),\n",
    "    )\n",
    "    model = prepare_model(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 1e-3))\n",
    "\n",
    "    # ---------------- Checkpointing setup ---------------- #\n",
    "    rank = get_context().get_world_rank()\n",
    "    start_epoch = 0\n",
    "\n",
    "    # If a checkpoint exists (auto-resume), load it\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as ckpt_dir:\n",
    "            model.load_state_dict(\n",
    "                torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\")\n",
    "            )\n",
    "            start_epoch = torch.load(os.path.join(ckpt_dir, \"meta.pt\")).get(\"epoch\", 0) + 1\n",
    "        if rank == 0:\n",
    "            print(f\"[Rank {rank}] \u2705 Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    # ---------------- Training loop ---------------- #\n",
    "    for epoch in range(start_epoch, config.get(\"epochs\", 5)):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            user = batch[\"user_idx\"].long()\n",
    "            item = batch[\"item_idx\"].long()\n",
    "            rating = batch[\"rating\"].float()\n",
    "\n",
    "            pred = model(user, item)\n",
    "            loss = F.mse_loss(pred, rating)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / max(1, len(train_losses))\n",
    "\n",
    "        # ---- Validate ----\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                user = batch[\"user_idx\"].long()\n",
    "                item = batch[\"item_idx\"].long()\n",
    "                rating = batch[\"rating\"].float()\n",
    "\n",
    "                pred = model(user, item)\n",
    "                loss = F.mse_loss(pred, rating)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / max(1, len(val_losses))\n",
    "\n",
    "        # Console log (optional)\n",
    "        if rank == 0:\n",
    "            print(f\"[Epoch {epoch}] Train MSE: {avg_train_loss:.4f} | Val MSE: {avg_val_loss:.4f}\")\n",
    "\n",
    "        metrics = {\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "        }\n",
    "\n",
    "        # ---- Save checkpoint & report (rank 0 attaches checkpoint; others report metrics only) ----\n",
    "        if rank == 0:\n",
    "            with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                torch.save(model.state_dict(), os.path.join(tmpdir, \"model.pt\"))\n",
    "                torch.save({\"epoch\": epoch}, os.path.join(tmpdir, \"meta.pt\"))\n",
    "                ckpt_out = Checkpoint.from_directory(tmpdir)\n",
    "                report(metrics, checkpoint=ckpt_out)\n",
    "        else:\n",
    "            report(metrics, checkpoint=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Launch distributed training with Ray Train  \n",
    "Now, launch distributed training using `TorchTrainer`, Ray Train\u2019s high-level orchestration interface. Provide it with:\n",
    "\n",
    "- Your custom `train_loop_per_worker` function\n",
    "- A `train_config` dictionary that specifies model dimensions, learning rate, and number of epochs\n",
    "- The sharded `train` and `val` Ray Datasets\n",
    "- A `ScalingConfig` that sets the number of workers and GPU usage\n",
    "\n",
    "Also, configure checkpointing and fault tolerance:\n",
    "- Ray keeps all checkpoints checkpoints for later plotting\n",
    "- Failed workers retry up to two times\n",
    "\n",
    "Calling `trainer.fit()` kicks off training across the cluster. If any workers fail or disconnect, Ray restarts them and resume from the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. Launch distributed training with Ray TorchTrainer\n",
    "\n",
    "# Define config params (use Ray-derived counts)\n",
    "train_config = {\n",
    "    \"num_users\": NUM_USERS,\n",
    "    \"num_items\": NUM_ITEMS,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"lr\": 1e-3,\n",
    "    \"epochs\": 20,\n",
    "}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=8,       # Increase as needed\n",
    "        use_gpu=True         # Set to True if training on GPUs\n",
    "    ),\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    run_config=RunConfig(\n",
    "        name=\"mf_ray_train\",\n",
    "        storage_path=\"/mnt/cluster_storage/rec_sys_tutorial/results\",\n",
    "        checkpoint_config=CheckpointConfig(num_to_keep=20),\n",
    "        failure_config=FailureConfig(max_failures=2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run distributed training\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Plot train and validation loss curves\n",
    "\n",
    "After training, retrieve the full metrics history directly from **Ray Train\u2019s internal tracking** via `result.metrics_dataframe`.\n",
    "\n",
    "This DataFrame automatically includes all reported metrics across epochs (e.g., `train_loss`, `val_loss`) for every call to `ray.train.report()`.  \n",
    "You use it to visualize model convergence and ensure the training loop, checkpointing, and reporting worked correctly.\n",
    "\n",
    "The plotted curves show how the **training** and **validation** MSE losses evolve over time\u2014confirming whether the model is learning effectively and when it begins to stabilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Plot train/val loss curves (from Ray Train results)\n",
    "\n",
    "# Pull the full metrics history Ray stored for this run\n",
    "df = result.metrics_dataframe.copy()\n",
    "\n",
    "# Keep only the columns we need (guard against extra columns)\n",
    "cols = [c for c in [\"epoch\", \"train_loss\", \"val_loss\"] if c in df.columns]\n",
    "df = df[cols].dropna()\n",
    "\n",
    "# If multiple rows per epoch exist, keep the last report per epoch\n",
    "if \"epoch\" in df.columns:\n",
    "    df = df.sort_index().groupby(\"epoch\", as_index=False).last()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "if \"train_loss\" in df.columns:\n",
    "    plt.plot(df[\"epoch\"], df[\"train_loss\"], marker=\"o\", label=\"Train\")\n",
    "if \"val_loss\" in df.columns:\n",
    "    plt.plot(df[\"epoch\"], df[\"val_loss\"], marker=\"o\", label=\"Val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Matrix Factorization - Loss per Epoch\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}