{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 \u00b7 Imports  \n",
    "Before you touch any data, import every tool you need.  \n",
    "Alongside the standard scientific-Python stack, bring in **XGBoost** for gradient-boosted decision trees and **Ray** for distributed data loading and training. Ray Train\u2019s helper classes (RunConfig, ScalingConfig, CheckpointConfig, FailureConfig) give you fault-tolerant, CPU training with almost no extra code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "import os, shutil, json, uuid, tempfile, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "import ray\n",
    "import ray.data as rd\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig, FailureConfig, get_dataset_shard, get_checkpoint, get_context\n",
    "from ray.train.xgboost import XGBoostTrainer, RayTrainReportCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 \u00b7 Load the University of California, Irvine (UCI) Cover type dataset  \n",
    "The Cover type dataset contains ~580 000 forest-cover observations with 54 tabular features and a 7-class label. Fetch it from `sklearn.datasets`, rename the target column to `label` (Ray\u2019s default), and shift the classes from **1-7** to **0-6** so they're zero-indexed as XGBoost expects. A quick `value_counts` sanity-check confirms the mapping worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load the UCI Cover type dataset (~580k rows, 54 features)\n",
    "data = fetch_covtype(as_frame=True)\n",
    "df = data.frame\n",
    "df.rename(columns={\"Cover_Type\": \"label\"}, inplace=True)   # Ray expects \"label\"\n",
    "df[\"label\"] = df[\"label\"] - 1          # 1-7  \u2192  0-6\n",
    "assert df[\"label\"].between(0, 6).all()\n",
    "print(df.shape, df.label.value_counts(normalize=True).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 \u00b7 Visualise class balance  \n",
    "Highly imbalanced targets can bias tree-based models, so plot the raw label counts. The cover type distribution shows skew, but not much\u2014the bar chart lets you judge whether extra re-scaling or class-weighting is necessary (You rely on XGBoost\u2019s built-in handling for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Visualize class distribution\n",
    "df.label.value_counts().plot(kind=\"bar\", figsize=(6,3), title=\"Cover Type distribution\")\n",
    "plt.ylabel(\"Frequency\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 \u00b7 Persist the dataset to Parquet  \n",
    "Storing the data-frame once on the cluster\u2019s shared file-system keeps later steps fast and reproducible. Parquet is columnar, compressed, and lazily readable by Ray Data, which means you can stream partitions to workers without loading everything into RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Write to /mnt/cluster_storage/covtype/\n",
    "PARQUET_DIR = \"/mnt/cluster_storage/covtype/parquet\"\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "file_path = os.path.join(PARQUET_DIR, \"covtype.parquet\")\n",
    "df.to_parquet(file_path)\n",
    "print(f\"Wrote Parquet -> {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 \u00b7 Read the data as a Ray Dataset  \n",
    "`ray.data.read_parquet` gives you a **lazy, columnar dataset** and shuffles it on-the-fly. From this point on, every split, batch, or transformation executes in parallel across the cluster, so you avoid a single-node bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Load dataset into a Ray Dataset (lazy, columnar)\n",
    "ds_full = rd.read_parquet(file_path).random_shuffle()      \n",
    "print(ds_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 \u00b7 Create train / validation splits  \n",
    "Perform an 80 / 20 split directly on the Ray Dataset, preserving the lazy execution plan. Each subset remains a Ray Dataset object, so they can later stream to the training workers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Split to train / validation Ray Datasets\n",
    "train_ds, val_ds = ds_full.split_proportionately([0.8])\n",
    "print(f\"Train rows: {train_ds.count()},  Val rows: {val_ds.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 \u00b7 Inspect a mini-batch  \n",
    "Taking a tiny pandas batch helps verify that feature columns and labels have the expected shapes and types. You also build `feature_columns`, a list you reuse when building XGBoost\u2019s `DMatrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Look into one batch to confirm feature dimensionality\n",
    "batch = train_ds.take_batch(batch_size=5, batch_format=\"pandas\")\n",
    "print(batch.head())\n",
    "feature_columns = [c for c in batch.columns if c != \"label\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}