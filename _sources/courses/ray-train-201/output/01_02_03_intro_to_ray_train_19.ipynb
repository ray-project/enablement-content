{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 \u00b7 Configure `TorchTrainer` with Ray Data  \n",
    "\n",
    "Now we connect the Ray Dataset to the training loop using the `datasets` parameter in `TorchTrainer`:  \n",
    "\n",
    "- **`datasets={\"train\": train_ds}`** \u2192 makes the transformed dataset available to the training loop as the `\"train\"` shard.  \n",
    "- **`train_loop_ray_train_ray_data`** \u2192 the per-worker training loop that consumes Ray Data batches.  \n",
    "- **`train_loop_config`** \u2192 passes hyperparameters (`num_epochs`, `global_batch_size`).  \n",
    "- **`scaling_config`** \u2192 specifies the number of workers and GPUs to use (same as before).  \n",
    "- **`run_config`** \u2192 defines storage for checkpoints and metrics.  \n",
    "\n",
    "This setup allows Ray Train to automatically shard and stream the Ray Dataset into each worker during training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Configure TorchTrainer with Ray Data integration\n",
    "\n",
    "# Wrap Ray Dataset in a dict \u2192 accessible as \"train\" inside the training loop\n",
    "datasets = {\"train\": train_ds}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train_ray_data,  # training loop consuming Ray Data\n",
    "    train_loop_config={             # hyperparameters\n",
    "        \"num_epochs\": 1,\n",
    "        \"global_batch_size\": 512,\n",
    "    },\n",
    "    scaling_config=scaling_config,  # number of workers + GPU/CPU resources\n",
    "    run_config=RunConfig(\n",
    "        storage_path=storage_path, \n",
    "        name=\"dist-cifar-res18-ray-data\"\n",
    "    ),                              # where to store checkpoints/logs\n",
    "    datasets=datasets,              # provide Ray Dataset shards to workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 \u00b7 Launch Training with Ray Data  \n",
    "\n",
    "Finally, call `trainer.fit()` to start the distributed training job.  \n",
    "\n",
    "- Ray will automatically:  \n",
    "  * Launch workers according to the `scaling_config`.  \n",
    "  * Stream sharded, preprocessed batches from the Ray Dataset into each worker.  \n",
    "  * Run the training loop (`train_loop_ray_train_ray_data`) on every worker in parallel.  \n",
    "  * Report metrics and save checkpoints to the configured storage path.  \n",
    "\n",
    "With this call, you now have a fully **end-to-end distributed pipeline** where **Ray Data handles ingestion + preprocessing** and **Ray Train handles multi-GPU training**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Start the distributed training job with Ray Data integration\n",
    "\n",
    "# Launches the training loop across all workers\n",
    "# - Streams preprocessed Ray Dataset batches into each worker\n",
    "# - Reports metrics and checkpoints to cluster storage\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}