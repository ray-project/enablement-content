{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Inference helper \u2014 Ray Data batch predictor on GPU  \n",
    "\n",
    "Define a Ray Data based batch predictor class that loads the trained `TimeSeriesTransformer` once per GPU actor and keeps it resident in memory for efficient inference.  \n",
    "Each actor processes batches of input windows (e.g., recent time series segments) in parallel, producing forecasts for the next horizon.  \n",
    "\n",
    "Ray Data inference enables scalable, fault-tolerant prediction pipelines that reuse loaded models across many requests, making it ideal for large-scale batch or near-real-time forecasting workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Ray Data inference helper \u2014 stateful per-actor predictor\n",
    "\n",
    "class TimeSeriesBatchPredictor:\n",
    "    \"\"\"\n",
    "    Keeps the TimeSeriesTransformer in memory per actor (GPU if available).\n",
    "    Expects a Pandas batch with a 'past' column containing np.ndarray of shape (INPUT_WINDOW,).\n",
    "    Returns a batch with a 'pred' column (np.ndarray of shape (HORIZON,)).\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_path: str, model_kwargs: dict):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Recreate model with the *same* hyperparams used during training\n",
    "        self.model = TimeSeriesTransformer(\n",
    "            input_window=model_kwargs[\"input_window\"],\n",
    "            horizon=model_kwargs[\"horizon\"],\n",
    "            d_model=model_kwargs[\"d_model\"],\n",
    "            nhead=model_kwargs[\"nhead\"],\n",
    "            num_layers=model_kwargs[\"num_layers\"],\n",
    "        ).to(self.device).eval()\n",
    "\n",
    "        # Load checkpoint weights once per actor\n",
    "        ckpt = Checkpoint.from_directory(checkpoint_path)\n",
    "        with ckpt.as_directory() as ckpt_dir:\n",
    "            state_dict = torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\")\n",
    "            # Strip DDP prefix if present\n",
    "            state_dict = {k.replace(\"module.\", \"\", 1): v for k, v in state_dict.items()}\n",
    "            self.model.load_state_dict(state_dict)\n",
    "\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        import pandas as pd\n",
    "\n",
    "        past_list = batch[\"past\"]  # each entry: np.ndarray shape (INPUT_WINDOW,)\n",
    "        # Stack into (B, T, 1)\n",
    "        x = np.stack([p.astype(np.float32) for p in past_list], axis=0)\n",
    "        x = torch.from_numpy(x).unsqueeze(-1).to(self.device)  # (B, INPUT_WINDOW, 1)\n",
    "\n",
    "        # Inference path uses the model's \"zeros as decoder input\" forward\n",
    "        preds = self.model(x).detach().cpu().numpy()  # (B, HORIZON)\n",
    "\n",
    "        out = batch.copy()\n",
    "        out[\"pred\"] = list(preds)  # each row: np.ndarray (HORIZON,)\n",
    "        return out[[\"pred\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Run distributed inference and visualize results  \n",
    "\n",
    "Use **Ray Data** to perform GPU-based batch inference with the trained model.  \n",
    "The model runs on a Ray worker, generates a forecast for the latest input window, and returns predictions to the driver.  \n",
    "De-normalize and plot the forecast against the ground truth to visually assess model performance.\n",
    "This tutorial uses a very small amount of data. As a result, you can see that the model learns a near-constant solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Run inference on the latest window with Ray Data and plot\n",
    "\n",
    "# 1) Prepare the latest window on the driver\n",
    "past_norm = hourly[\"norm\"].iloc[-INPUT_WINDOW:].to_numpy().astype(np.float32)\n",
    "future_true = hourly[\"passengers\"].iloc[-HORIZON:].to_numpy()  # for visualization only\n",
    "\n",
    "# 2) Get the best checkpoint directory selected by Ray\n",
    "with result.checkpoint.as_directory() as ckpt_dir:\n",
    "    best_ckpt_path = ckpt_dir  # path visible to workers\n",
    "\n",
    "# 3) Build a tiny Ray Dataset and run inference on a GPU actor\n",
    "model_kwargs = {\n",
    "    \"input_window\": INPUT_WINDOW,\n",
    "    \"horizon\": HORIZON,\n",
    "    \"d_model\": 128,\n",
    "    \"nhead\": 4,\n",
    "    \"num_layers\": 3,\n",
    "}\n",
    "\n",
    "ds = rdata.from_items([{\"past\": past_norm}])\n",
    "pred_ds = ds.map_batches(\n",
    "    TimeSeriesBatchPredictor,\n",
    "    fn_constructor_args=(best_ckpt_path, model_kwargs),\n",
    "    batch_size=1,\n",
    "    batch_format=\"pandas\",\n",
    "    concurrency=1,\n",
    "    num_gpus=1,  # force placement on a GPU worker if available\n",
    ")\n",
    "\n",
    "pred_row = pred_ds.take(1)[0]\n",
    "pred_norm = pred_row[\"pred\"]  # np.ndarray (HORIZON,)\n",
    "\n",
    "# 4) De-normalize on the driver\n",
    "mean, std = hourly[\"passengers\"].mean(), hourly[\"passengers\"].std()\n",
    "pred = pred_norm * std + mean\n",
    "past = past_norm * std + mean\n",
    "\n",
    "# 5) Plot\n",
    "\n",
    "t_past   = np.arange(-INPUT_WINDOW, 0)\n",
    "STEP_SIZE_HOURS = 0.5  # you mentioned 30-min data\n",
    "t_future = np.arange(0, HORIZON) * STEP_SIZE_HOURS\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(t_past, past, label=\"History\", marker=\"o\")\n",
    "plt.plot(t_future, future_true, \"--\", label=\"Ground Truth\")\n",
    "plt.plot(t_future, pred, \"-.\", label=\"Forecast\")\n",
    "plt.axvline(0)\n",
    "plt.xlabel(\"Hours relative\")\n",
    "plt.ylabel(\"# trips\")\n",
    "plt.title(\"NYC-Taxi Forecast (Ray Data Inference)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Cleanup: remove all training artifacts  \n",
    "Finally, tidy up by deleting temporary checkpoint folders and any intermediate result directories. Clearing out old artifacts frees disk space and leaves your workspace clean for whatever comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Cleanup \u2013 optionally remove all artifacts to free space\n",
    "if os.path.exists(DATA_DIR):\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "    print(f\"Deleted {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up and next steps\n",
    "\n",
    "You built a robust, distributed forecasting workflow using **Ray Train on Anyscale** that:\n",
    "\n",
    "* Trains a Transformer model across **multiple GPUs** using **Ray Train with Distributed Data Parallel (DDP)**, abstracting away low-level orchestration.\n",
    "* Recovers automatically from failures with **built-in checkpointing and resume**, even across re-launches or node churn.\n",
    "* Logs and reports per-epoch metrics using **Ray Train\u2019s reporting APIs**, enabling real-time monitoring and seamless plotting.\n",
    "* Performs inference using **Ray Data**. This allows you to scale forecasting across GPUs or nodes without changing model code.\n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?\n",
    "\n",
    "The following are a few directions you can explore to extend or adapt this workload:\n",
    "\n",
    "1. **Hyperparameter sweeps**  \n",
    "   * Wrap the `TorchTrainer` with **Ray Tune** to search over `d_model`, `nhead`, learning rate, and window sizes.  \n",
    "\n",
    "2. **Probabilistic forecasting**  \n",
    "   * Output percentiles or fit a distribution head (for example, Gaussian) to capture prediction uncertainty.  \n",
    "\n",
    "3. **Multivariate and exogenous features**  \n",
    "   * Add weather, holidays, or ride-sharing surge multipliers as extra input channels.  \n",
    "\n",
    "4. **Early-stopping and LR scheduling**  \n",
    "   * Monitor val-loss and reduce LR on plateau, or stop when improvement < 1 %.  \n",
    "\n",
    "5. **Model compression**  \n",
    "   * Distill the large Transformer into a lightweight LSTM or Tiny-Transformer for edge deployment.  \n",
    "\n",
    "6. **Streaming and online learning**  \n",
    "   * Use **Ray Serve** to deploy the model and update weights periodically with the latest data.  \n",
    "\n",
    "7. **Interpretability**  \n",
    "   * Visualize attention maps to see which time lags the model focuses on\u2014effective for stakeholder trust.  \n",
    "\n",
    "8. **End-to-end MLOps**  \n",
    "   * Schedule nightly retraining with **Ray jobs**, log artifacts to MLflow or Weights & Biases, and automate model promotion.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}