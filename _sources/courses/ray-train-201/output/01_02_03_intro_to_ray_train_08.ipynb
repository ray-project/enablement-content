{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09 \u00b7 Build the DataLoader with `prepare_data_loader()`  \n",
    "\n",
    "Now let\u2019s define a helper that builds the **MNIST DataLoader** and makes it Ray Train\u2013ready.  \n",
    "\n",
    "- Apply standard preprocessing:  \n",
    "  * `ToTensor()` \u2192 convert PIL images to PyTorch tensors  \n",
    "  * `Normalize((0.5,), (0.5,))` \u2192 center and scale pixel values  \n",
    "\n",
    "- Construct a PyTorch `DataLoader` with batching and shuffling.  \n",
    "\n",
    "- Finally, wrap it with [`prepare_data_loader()`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html#ray-train-torch-prepare-data-loader), which automatically:  \n",
    "  * Moves each batch to the correct device (GPU or CPU).  \n",
    "  * Copies data from host memory to device memory as needed.  \n",
    "  * Injects a PyTorch [`DistributedSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler) when running with multiple workers, so that each worker processes a unique shard of the dataset.  \n",
    "\n",
    "This utility lets you use the **same DataLoader code** whether you\u2019re training on one GPU or many \u2014 Ray handles the distributed sharding and device placement for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. Build a Ray Train\u2013ready DataLoader for MNIST\n",
    "\n",
    "def build_data_loader_ray_train(batch_size: int) -> torch.utils.data.DataLoader:\n",
    "    # Define preprocessing: convert to tensor + normalize pixel values\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    # Load the MNIST training set from persistent cluster storage\n",
    "    train_data = MNIST(\n",
    "        root=\"/mnt/cluster_storage/data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    # Standard PyTorch DataLoader (batching, shuffling, drop last incomplete batch)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # prepare_data_loader():\n",
    "    # - Adds a DistributedSampler when using multiple workers\n",
    "    # - Moves batches to the correct device automatically\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Ray Data integration</b>\n",
    "\n",
    "This step isn't necessary if you are integrating your Ray Train workload with Ray Data. It's especially useful if preprocessing is CPU-heavly and user wants to run preprocessing and training of separate instances.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}