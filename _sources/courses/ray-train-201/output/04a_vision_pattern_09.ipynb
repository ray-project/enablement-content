{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 \u00b7 Remote Inference Helper  \n",
    "Define a lightweight Ray remote function that loads a chosen checkpoint into a fresh `resnet18`, runs inference on one image, and returns both the predicted and true labels. Because the function requests one GPU, Ray schedules it on an appropriate node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 15. Define batch inference function\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def run_inference_from_checkpoint(checkpoint_path, parquet_path, idx=0):\n",
    "\n",
    "    # === Load model ===\n",
    "    model = resnet18(num_classes=101)\n",
    "    checkpoint = Checkpoint.from_directory(checkpoint_path)\n",
    "\n",
    "    with checkpoint.as_directory() as ckpt_dir:\n",
    "        state_dict = torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cuda\")\n",
    "\n",
    "        # Strip \"module.\" prefix from distributed data parallelism trained weights\n",
    "        state_dict = {k.replace(\"module.\", \"\", 1): v for k, v in state_dict.items()}\n",
    "\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    model.eval().cuda()\n",
    "\n",
    "    # === Define transform ===\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "    # === Load dataset ===\n",
    "    dataset = Food101Dataset(parquet_path, transform=transform)\n",
    "    img, label = dataset[idx]\n",
    "    img = img.unsqueeze(0).cuda()  # batch size 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(img)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    return {\"predicted_label\": pred, \"true_label\": int(label), \"index\": idx}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16 \u00b7 Run and Visualise Inference  \n",
    "Using the checkpoint with the lowest validation loss, invoke your remote inference helper on a validation image. Then plot the image while displaying both the model\u2019s prediction and the ground-truth class, giving you an immediate, intuitive sense of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Perform inference with best trained model (i.e. lowest validation loss for a checkpointed model)\n",
    " \n",
    "checkpoint_root = \"/mnt/cluster_storage/food101_lite/results/food101_ft_resume\"\n",
    "\n",
    "checkpoint_dirs = sorted(\n",
    "    [\n",
    "        d for d in os.listdir(checkpoint_root)\n",
    "        if d.startswith(\"checkpoint_\") and os.path.isdir(os.path.join(checkpoint_root, d))\n",
    "    ],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "if not checkpoint_dirs:\n",
    "    raise FileNotFoundError(\"No checkpoint directories found.\")\n",
    "\n",
    "with result.checkpoint.as_directory() as ckpt_dir:\n",
    "    print(\"Best checkpoint contents:\", os.listdir(ckpt_dir))\n",
    "    best_ckpt_path = ckpt_dir \n",
    "parquet_path = \"/mnt/cluster_storage/food101_lite/val.parquet\"\n",
    "\n",
    "# Define which image to use\n",
    "idx = 2\n",
    "\n",
    "# Launch GPU inference task with Ray\n",
    "result = ray.get(run_inference_from_checkpoint.remote(best_ckpt_path, parquet_path, idx=idx))\n",
    "print(result)\n",
    "\n",
    "# Load label map from Hugging Face\n",
    "ds = load_dataset(\"food101\", split=\"train[:1%]\")  # load just to get label names\n",
    "label_names = ds.features[\"label\"].names\n",
    "\n",
    "# Load image from the same dataset locally\n",
    "dataset = Food101Dataset(parquet_path, transform=None)  # No transform = raw image\n",
    "img, _ = dataset[idx]\n",
    "\n",
    "# Plot the image with predicted and true labels\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Pred: {label_names[result['predicted_label']]}\\nTrue: {label_names[result['true_label']]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17 \u00b7 Cleanup  \n",
    "Finally, tidy up by deleting temporary checkpoint folders, the metrics CSV, and any intermediate result directories. Clearing out old artefacts frees disk space and leaves your workspace clean for whatever comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Cleanup---delete checkpoints and metrics from model training\n",
    "\n",
    "# Base directory\n",
    "BASE_DIR = \"/mnt/cluster_storage/food101_lite\"\n",
    "\n",
    "# Paths to clean\n",
    "paths_to_delete = [\n",
    "    os.path.join(BASE_DIR, \"tmp_checkpoints\"),           # custom checkpoints\n",
    "    os.path.join(BASE_DIR, \"results\", \"history.csv\"),    # metrics history file\n",
    "    os.path.join(BASE_DIR, \"results\", \"food101_ft_resume\"),  # ray trainer run dir\n",
    "    os.path.join(BASE_DIR, \"results\", \"food101_ft_run\"),\n",
    "    os.path.join(BASE_DIR, \"results\", \"food101_single_run\"),\n",
    "]\n",
    "\n",
    "# Delete each path if it exists\n",
    "for path in paths_to_delete:\n",
    "    if os.path.exists(path):\n",
    "        if os.path.isfile(path):\n",
    "            os.remove(path)\n",
    "            print(f\"Deleted file: {path}\")\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"Deleted directory: {path}\")\n",
    "    else:\n",
    "        print(f\"Not found (skipped): {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83c\udf89 Wrapping Up & Next Steps\n",
    "\n",
    "Great job making it to the end. You've taken a realistic computer-vision workload, from raw images all the way to distributed training and GPU inference, and run it on Ray Train with zero boilerplate around GPUs, data parallelism, or fault-tolerance. You should now feel comfortable:\n",
    "\n",
    "* Using **Ray Train\u2019s TorchTrainer** to scale PyTorch training across multiple GPUs and nodes with minimal code changes  \n",
    "* Wrapping models and data loaders with **`prepare_model()`** and **`prepare_data_loader()`** to enable Ray-managed device placement and distributed execution  \n",
    "* Sharding data across workers using **`DistributedSampler`**, and coordinating training epochs across Ray workers  \n",
    "* Configuring **automatic checkpointing and failure recovery** using Ray Train\u2019s built-in `Checkpoint`, `RunConfig`, and `FailureConfig` APIs  \n",
    "* Running **GPU-backed Ray tasks** for distributed inference, showing how to serve and scale model predictions across a Ray cluster  \n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\ude80 Where can you take this next?\n",
    "\n",
    "Below are a few directions you might explore to adapt or extend the pattern:\n",
    "\n",
    "1. **Larger or Custom Datasets**  \n",
    "   * Swap in the full 75 k-image Food-101 split\u2014or your own dataset in any storage backend (S3, GCS, Azure Blob).  \n",
    "   * Add multi-file Parquet sharding and let each worker read a different shard.\n",
    "\n",
    "2. **Model Architectures**  \n",
    "   * Drop in Vision Transformers (`vit_b_16`, `vit_l_32`) or ConvNeXt; the prepare helpers work exactly the same.  \n",
    "   * Experiment with transfer learning vs. training from scratch.\n",
    "\n",
    "3. **Mixed Precision & Performance Tuning**  \n",
    "   * Enable automatic mixed precision (`torch.cuda.amp`) or bfloat16 to speed up training and save memory.  \n",
    "   * Profile data-loading throughput and play with `num_workers`, prefetching, and caching.\n",
    "\n",
    "4. **Hyperparameter Sweeps**  \n",
    "   * Wrap the training loop in **Ray Tune** to search over learning rates, augmentations, or optimisers.  \n",
    "   * Use Ray\u2019s integrated reporting to schedule early stopping.\n",
    "\n",
    "5. **Data Augmentation Pipelines**  \n",
    "   * Integrate additional transforms inside the dataset class for image augmentation.  \n",
    "   * Compare CPU vs. GPU-side augmentations for throughput.\n",
    "\n",
    "6. **Distributed Validation & Metrics**  \n",
    "   * Replace your simple accuracy printout with more advanced metrics (F1, top-5 accuracy, confusion matrices).  \n",
    "\n",
    "7. **Model Serving**  \n",
    "   * Convert the remote inference helper into a **Ray Serve** deployment for low-latency online predictions.  \n",
    "   * Auto-scale replicas based on request volume.\n",
    "\n",
    "8. **End-to-End MLOps**  \n",
    "   * Register checkpoints in a model registry (For example, MLflow, Weights & Biases, or Ray\u2019s built-in MLflow integration).  \n",
    "   * Schedule the notebook as a Ray Job or CI/CD pipeline for regular retraining runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}