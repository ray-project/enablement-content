{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 \u00b7 Save Full Checkpoint with Extra State  \n",
    "\n",
    "To support fault-tolerant recovery, we extend checkpoint saving to include not just the model, but also the **optimizer state** and the **current epoch**.  \n",
    "\n",
    "- **`model.pt`** \u2192 model weights (unwrap DDP with `.module`).  \n",
    "- **`optimizer.pt`** \u2192 optimizer state for resuming training seamlessly.  \n",
    "- **`extra_state.pt`** \u2192 stores metadata (here, the current epoch).  \n",
    "\n",
    "Only the **rank-0 worker** writes the checkpoint to avoid duplication, but all workers still call `ray.train.report()` to keep the loop synchronized.  \n",
    "\n",
    "This ensures that if training is interrupted, Ray Train can restore **model weights, optimizer progress, and the correct epoch** before continuing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Save checkpoint with model, optimizer, and epoch state\n",
    "\n",
    "def save_checkpoint_and_metrics_ray_train_with_extra_state(\n",
    "    model: torch.nn.Module,\n",
    "    metrics: dict[str, float],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        checkpoint = None\n",
    "        # Only rank-0 worker saves files to disk\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "                # Save all state required for full recovery\n",
    "                torch.save(\n",
    "                    model.module.state_dict(),  # unwrap DDP before saving\n",
    "                    os.path.join(temp_checkpoint_dir, \"model.pt\"),\n",
    "                )\n",
    "                torch.save(\n",
    "                    optimizer.state_dict(),     # include optimizer state\n",
    "                    os.path.join(temp_checkpoint_dir, \"optimizer.pt\"),\n",
    "                )\n",
    "                torch.save(\n",
    "                    {\"epoch\": epoch},           # store last completed epoch\n",
    "                    os.path.join(temp_checkpoint_dir, \"extra_state.pt\"),\n",
    "                )\n",
    "                # Package into a Ray checkpoint\n",
    "                checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "        \n",
    "        # Report metrics and attach checkpoint (only rank-0 attaches checkpoint)\n",
    "        ray.train.report(  \n",
    "            metrics,  \n",
    "            checkpoint=checkpoint,\n",
    "            )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 \u00b7 Configure Automatic Retries with `FailureConfig`  \n",
    "\n",
    "Now that the training loop can load from checkpoints, we can enable **automatic retries** in case of worker or node failures.  \n",
    "\n",
    "- **`FailureConfig(max_failures=3)`** \u2192 allows the job to retry up to 3 times before giving up.  \n",
    "- Pass this `failure_config` into `RunConfig` so Ray Train knows how to handle failures.  \n",
    "- When a failure happens, Ray will:  \n",
    "  1. Restart the failed workers.  \n",
    "  2. Reload the latest checkpoint.  \n",
    "  3. Resume training from the last saved epoch.  \n",
    "\n",
    "This setup makes training jobs resilient to transient hardware or cluster issues without requiring manual intervention.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Configure TorchTrainer with fault-tolerance enabled\n",
    "\n",
    "# Allow up to 3 automatic retries if workers fail\n",
    "failure_config = ray.train.FailureConfig(max_failures=3)\n",
    "\n",
    "experiment_name = \"fault-tolerant-cifar-vit\"\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_ray_train_with_checkpoint_loading,  # fault-tolerant loop\n",
    "    train_loop_config={   # hyperparameters\n",
    "        \"num_epochs\": 1,\n",
    "        \"global_batch_size\": 512,\n",
    "    },\n",
    "    scaling_config=scaling_config,  # resource scaling as before\n",
    "    run_config=ray.train.RunConfig(\n",
    "        name=\"fault-tolerant-cifar-vit\",\n",
    "        storage_path=storage_path,      # persistent checkpoint storage\n",
    "        failure_config=failure_config,  # enable automatic retries\n",
    "    ),\n",
    "    datasets=datasets,  # Ray Dataset shard for each worker\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}