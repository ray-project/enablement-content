{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and setup\n",
    "\n",
    "Standard scientific-Python stack, plus **Ray** for distributed data/training\n",
    "and **Lightning** for ergonomic model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup \n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"lightning==2.5.5\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# Standard Python packages for math, plotting, and data handling\n",
    "import os, shutil, glob\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "\n",
    "# Ray libraries for distributed data and training\n",
    "import ray\n",
    "import ray.data\n",
    "from ray.train.lightning import RayLightningEnvironment  \n",
    "from ray.train import ScalingConfig, RunConfig, FailureConfig, CheckpointConfig, get_context, get_checkpoint, report, Checkpoint\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# PyTorch Lightning and base PyTorch for model definition and training\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate a real pendulum dataset\n",
    "\n",
    "Roll out a random policy for **10 000 steps**, logging:\n",
    "\n",
    "| field | shape | description |\n",
    "|-------|-------|-------------|\n",
    "| `obs`          | `(3,)`  | `[cos \u03b8, sin \u03b8, \u03b8\u0307]` |\n",
    "| `noisy_action` | `(1,)`  | ground-truth action + Gaussian noise |\n",
    "| `noise`        | `(1,)`  | the injected noise (supervision target) |\n",
    "| `timestep`     | `()`    | random diffusion step \u2208 [0, 999] |\n",
    "\n",
    "You wrap the list of dicts in a **Ray Dataset** for automatic sharding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Generate Pendulum offline dataset \n",
    "\n",
    "def make_pendulum_dataset(n_steps: int = 10_000):\n",
    "    \"\"\"\n",
    "    Roll out a random policy in Pendulum-v1 and log (obs, noisy_action, noise, timestep).\n",
    "    Returns a Ray Dataset ready for sharding.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    obs, _ = env.reset(seed=0)\n",
    "    data = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        action = env.action_space.sample().astype(np.float32)      # shape (1,)\n",
    "        noise   = np.random.randn(*action.shape).astype(np.float32)\n",
    "        noisy_action = action + noise                              # add Gaussian noise\n",
    "        timestep = np.random.randint(0, 1000, dtype=np.int64)\n",
    "\n",
    "        data.append(\n",
    "            {\n",
    "                \"obs\":        obs.astype(np.float32),              # shape (3,)\n",
    "                \"noisy_action\": noisy_action,                      # shape (1,)\n",
    "                \"noise\":        noise,                             # shape (1,)\n",
    "                \"timestep\":     timestep,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Step environment\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    return ray.data.from_items(data)\n",
    "\n",
    "ds = make_pendulum_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalize and split\n",
    "\n",
    "Pendulum states lie roughly in **[\u2013\u03c0, \u03c0]**.  \n",
    "Scale to **[\u20131, 1]**, then **shuffle** and split 80 / 20 into train and val shards.\n",
    "All transformations execute in parallel across the Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Normalize and split (vector obs \u2208 [-\u03c0, \u03c0])\n",
    "\n",
    "# Normalize pixel values from [0, 1] to [-1, 1] for training\n",
    "def normalize(batch):\n",
    "    # Pendulum observations are roughly in [-\u03c0, \u03c0] \u2192 scale to [-1, 1]\n",
    "    batch[\"obs\"] = batch[\"obs\"] / np.pi\n",
    "    return batch\n",
    "\n",
    "# Apply normalization in parallel using Ray Data\n",
    "ds = ds.map_batches(normalize, batch_format=\"numpy\")\n",
    "\n",
    "# Count total number of items (triggers actual execution)\n",
    "total = ds.count()\n",
    "print(\"Total dataset size:\", total)\n",
    "\n",
    "# Shuffle and split dataset into 80% training and 20% validation\n",
    "split_idx = int(total * 0.8)\n",
    "ds = ds.random_shuffle()\n",
    "train_ds, val_ds = ds.split_at_indices([split_idx])\n",
    "\n",
    "print(\"Train size:\", train_ds.count())\n",
    "print(\"Val size:\", val_ds.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}