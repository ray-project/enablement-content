{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09 \u00b7 Ray Train `train_loop` (Checkpoint + Resume)  \n",
    "Core training logic run **once per Ray worker**:  \n",
    "1. Shard-aware DataLoaders with `get_dataset_shard`.  \n",
    "2. Auto-resume from the latest Ray Checkpoint (if present).  \n",
    "3. Manual per-epoch checkpointing: save `model.pt` + `meta.pt`, then call `report(metrics, checkpoint=\u2026)`.  \n",
    "This makes the run fully **fault-tolerant**---if a worker crashes, Ray restarts the group and re-enters the loop with the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. Train Loop for Ray TorchTrainer\n",
    "\n",
    "def train_loop(config):\n",
    "    \"\"\"Ray Train per-worker function with checkpointing and resume support.\"\"\"\n",
    "    import os, torch, uuid, json\n",
    "    from ray.train import get_checkpoint, get_context, report, Checkpoint\n",
    "\n",
    "    # Paths\n",
    "    LOG_PATH = \"/mnt/cluster_storage/generative_cv/epoch_metrics.json\"\n",
    "    CKPT_ROOT = \"/mnt/cluster_storage/generative_cv/food101_diffusion_ckpts\"\n",
    "\n",
    "    rank = get_context().get_world_rank()\n",
    "    if rank == 0:\n",
    "        os.makedirs(CKPT_ROOT, exist_ok=True)\n",
    "        if not get_checkpoint() and os.path.exists(LOG_PATH):\n",
    "            os.remove(LOG_PATH)\n",
    "\n",
    "    # Data\n",
    "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
    "    val_ds   = ray.train.get_dataset_shard(\"val\")\n",
    "    train_loader = train_ds.iter_torch_batches(batch_size=32)\n",
    "    val_loader   = val_ds.iter_torch_batches(batch_size=32)\n",
    "\n",
    "    # Model\n",
    "    model = PixelDiffusion()\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Resume from checkpoint if present\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as d:\n",
    "            model.load_state_dict(torch.load(os.path.join(d, \"model.pt\"), map_location=\"cpu\"))\n",
    "            start_epoch = torch.load(os.path.join(d, \"meta.pt\")).get(\"epoch\", 0) + 1\n",
    "        if rank == 0:\n",
    "            print(f\"[Rank {rank}] Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.get(\"epochs\", 10),\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        enable_progress_bar=False,\n",
    "        check_val_every_n_epoch=1,\n",
    "    )\n",
    "\n",
    "    # Train loop: run each epoch, checkpoint manually\n",
    "    for epoch in range(start_epoch, config.get(\"epochs\", 10)):\n",
    "        trainer.fit_loop.max_epochs = epoch + 1\n",
    "        trainer.fit_loop.current_epoch = epoch\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "        if rank == 0:\n",
    "            # Save model checkpoint\n",
    "            out_dir = os.path.join(CKPT_ROOT, f\"epoch_{epoch}_{uuid.uuid4().hex}\")\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(out_dir, \"model.pt\"))\n",
    "            torch.save({\"epoch\": epoch}, os.path.join(out_dir, \"meta.pt\"))\n",
    "            ckpt_out = Checkpoint.from_directory(out_dir)\n",
    "        else:\n",
    "            ckpt_out = None\n",
    "\n",
    "        # Report with checkpoint so Ray saves it\n",
    "        report({\"epoch\": epoch}, checkpoint=ckpt_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 \u00b7 Launch Distributed Training with TorchTrainer  \n",
    "Ask for **8 GPU workers**, keep the five most-recent checkpoints, and allow up to three automatic retries.  \n",
    "`result.checkpoint` captures the checkpoint from the highest epoch (because you used `epoch` as the score attribute---you can change this to other metrics such as validation loss or training loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Launch Distributed Training\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop,\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=True),\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    run_config=RunConfig(\n",
    "        name=\"food101_diffusion_ft\",\n",
    "        storage_path=\"/mnt/cluster_storage/generative_cv/food101_diffusion_results\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            checkpoint_frequency=1,\n",
    "            num_to_keep=5,\n",
    "            checkpoint_score_attribute=\"epoch\",\n",
    "            checkpoint_score_order=\"max\",\n",
    "        ),\n",
    "        failure_config=FailureConfig(max_failures=3),\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(\"Training complete \u2192\", result.metrics)\n",
    "best_ckpt = result.checkpoint  # checkpoint from highest reported epoch (you can change score attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 \u00b7 Plot Loss Curves  \n",
    "Parse the JSON written by `PixelDiffusion.on_train_epoch_end`, convert to a DataFrame, and render Train vs. Val MSE loss.  \n",
    "Good practice for quick health checks without external tooling.\n",
    "\n",
    "**Why is validation loss lower than training loss?**  \n",
    "You measure training loss *before* weights update and includes fresh noise every step, while validation runs in `eval()` mode with no gradient updates, often making it slightly lower, especially early in training.  \n",
    "This is normal behavior in this sort of scenario and usually means the model is generalizing well, and not over-fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Plot Train/Val Loss Curves\n",
    "\n",
    "LOG_PATH = \"/mnt/cluster_storage/generative_cv/epoch_metrics.json\"\n",
    "with open(LOG_PATH, \"r\") as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df[\"val_loss\"] = pd.to_numeric(df[\"val_loss\"], errors=\"coerce\")\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df[\"epoch\"], df[\"train_loss\"], marker=\"o\", label=\"Train\")\n",
    "plt.plot(df[\"epoch\"], df[\"val_loss\"],   marker=\"o\", label=\"Val\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE Loss\"); plt.title(\"Pixel Diffusion - Loss per Epoch\")\n",
    "plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}