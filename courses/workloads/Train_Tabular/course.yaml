title: Distributed XGBoost Training with Ray Train
description: Learn how to scale a tabular XGBoost forest-cover classification workflow
  from local training to a distributed Ray Train V2 job on an Anyscale cluster. Youâ€™ll
  ingest the UCI Cover Type dataset, write train/validation Parquet splits to shared
  storage, and train and evaluate the model using Ray Datasets with distributed execution.
author: ''
mediaStorage: ''
category: foundation
thumbnail: thumbnail.png
