{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba89087",
   "metadata": {},
   "source": [
    "# Introduction to Ray Data: Industry Landscape\n",
    "\u00a9 2025, Anyscale. All Rights Reserved\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79181767",
   "metadata": {},
   "source": [
    "This document is meant to provide a landscape of the industry.\n",
    "<b> Here is the roadmap: </b>\n",
    "<ul> \n",
    "    <li> The Data Layer </li>\n",
    "    <li> The Compute Layer </li>\n",
    "    <li> The Orchestration Layer </li>\n",
    "    <li> Commercial Distributed Computing Platforms </li>\n",
    "    <li> Distributed Computing execution models </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85270d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The data layer\n",
    "\n",
    "This document provides an overview of the data layer, including its components and commonly used tools. The evolution of data storage patterns reflects the changing needs of businesses and advancements in technology. Below is a breakdown of these patterns and their applications.\n",
    "\n",
    "\n",
    "## Databases\n",
    "\n",
    "- **Purpose**: Built for handling transactional data.\n",
    "- **Optimization**: Designed for online transaction processing (OLTP).\n",
    "- **Key characteristics**:\n",
    "  - Handles frequent, small-scale, atomic transactions.\n",
    "  - Ensures data consistency and integrity using **ACID properties**: atomicity, consistency, isolation, and durability.\n",
    "- **Examples**: MySQL, PostgreSQL, MongoDB.\n",
    "- **Specialized Databases**:\n",
    "  - **Vector Databases**: Databases that are optimized for storing and querying vector data.\n",
    "    - **Examples**: Pinecone, Zilliz, ChromaDB, Weaviate.\n",
    "\n",
    "## Data warehouses\n",
    "\n",
    "- **Purpose**: Designed for analyzing large datasets.\n",
    "- **Optimization**: Suited for online analytical processing (OLAP).\n",
    "- **Key characteristics**:\n",
    "  - Stores vast amounts of structured and historical data optimized for analytics.\n",
    "  - Uses indexing and sorting to accelerate query performance.\n",
    "  - Often employs proprietary storage formats (e.g., Snowflake\u2019s columnar format) for efficiency.\n",
    "  - Supports SQL-based querying and analytical functions for business intelligence.\n",
    "  - Relies on ETL (extract, transform, load) or ELT (extract, load, transform) pipelines to preprocess and structure data.\n",
    "- **Examples**: Amazon Redshift, Google BigQuery, Snowflake.\n",
    "\n",
    "\n",
    "## Data lakes\n",
    "\n",
    "- **Purpose**: Store large volumes of raw, semi-structured, or unstructured data.\n",
    "- **Key characteristics**:\n",
    "  - Maintains raw data in its native format without requiring prior transformation.\n",
    "  - Supports diverse data types (e.g., text, images, videos, logs).\n",
    "  - Ideal for machine learning, big data analytics, and scenarios requiring data exploration.\n",
    "  - Lacks built-in mechanisms for enforcing data quality, transactional consistency, or indexing.\n",
    "\n",
    "\n",
    "### Structure of a data lake\n",
    "\n",
    "Data lakes are organized into layers that enable efficient storage and processing:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/storage-layer.png\" width=\"500\">\n",
    "\n",
    "1. **Storage layer**: The foundation of the data lake, implemented using scalable systems like AWS S3, Cloudflare R2, or distributed file systems such as Ceph and Lustre.\n",
    "2. **File formats**: Stores data in multiple formats, including Parquet, CSV, JSON, JPEG, and Protocol Buffers, to support varied workloads.\n",
    "3. **Table format layer**: Adds features like schema management, ACID transactions, and indexing to provide database-like capabilities within the data lake. Examples of table formats include:\n",
    "   - **Apache Iceberg**: Focuses on versioning, schema evolution, and large-scale analytics.\n",
    "   - **Delta Lake**: Built with ACID transactions and data quality controls for analytics and machine learning.\n",
    "   - **Apache Hudi**: Offers incremental processing capabilities and efficient updates for data lakes.\n",
    "\n",
    "### Lakehouses\n",
    "\n",
    "Lakehouses build on the foundation of data lakes, addressing their limitations while incorporating features of data warehouses.\n",
    "\n",
    "- **Purpose**: Provide a unified platform for both analytical and transactional workloads.\n",
    "- **Key characteristics**:\n",
    "  - Combines the scalability and flexibility of data lakes with the structure and performance of data warehouses.\n",
    "  - Implements ACID transactions, indexing, and schema enforcement directly on lake-stored data.\n",
    "  - Built on open table formats (e.g., Apache Iceberg, Delta Lake, Apache Hudi) for robust querying and data management.\n",
    "  - Bridges the gap between data engineering and machine learning workflows, enabling seamless data sharing.\n",
    "  - Eliminates the need to move data between separate lake and warehouse systems, reducing complexity and cost.\n",
    "\n",
    "- **Examples**: Databricks Lakehouse Platform, Delta Lake, and other systems leveraging modern table formats.\n",
    "\n",
    "\n",
    "## In-memory data formats\n",
    "\n",
    "### Apache Arrow\n",
    "\n",
    "Apache Arrow is a high-performance in-memory data processing framework designed to enhance analytical workflows across languages and platforms.\n",
    "\n",
    "- **Purpose**: Provides a standard for in-memory data that minimizes serialization costs and maximizes interoperability.\n",
    "- **Language bindings**:\n",
    "  - Examples: PyArrow for Python, Arrow-rs for Rust.\n",
    "- **Key capabilities**:\n",
    "  - **Zero-copy data sharing**: Supports efficient shared memory access and RPC-based data transfers.\n",
    "  - **File format support**: Reads and writes formats like CSV, Apache ORC, and Apache Parquet.\n",
    "  - **In-memory analytics**: Facilitates high-speed operations using the Arrow Table abstraction for query processing and computation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}