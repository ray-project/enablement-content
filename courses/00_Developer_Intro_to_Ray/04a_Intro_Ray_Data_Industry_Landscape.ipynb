{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba89087",
   "metadata": {},
   "source": [
    "# Introduction to Ray Data: Industry Landscape\n",
    "© 2025, Anyscale. All Rights Reserved\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79181767",
   "metadata": {},
   "source": [
    "This document is meant to provide a landscape of the industry.\n",
    "<b> Here is the roadmap: </b>\n",
    "<ul> \n",
    "    <li> The Data Layer </li>\n",
    "    <li> The Compute Layer </li>\n",
    "    <li> The Orchestration Layer </li>\n",
    "    <li> Commercial Distributed Computing Platforms </li>\n",
    "    <li> Distributed Computing execution models </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85270d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The data layer\n",
    "\n",
    "This document provides an overview of the data layer, including its components and commonly used tools. The evolution of data storage patterns reflects the changing needs of businesses and advancements in technology. Below is a breakdown of these patterns and their applications.\n",
    "\n",
    "\n",
    "## Databases\n",
    "\n",
    "- **Purpose**: Built for handling transactional data.\n",
    "- **Optimization**: Designed for online transaction processing (OLTP).\n",
    "- **Key characteristics**:\n",
    "  - Handles frequent, small-scale, atomic transactions.\n",
    "  - Ensures data consistency and integrity using **ACID properties**: atomicity, consistency, isolation, and durability.\n",
    "- **Examples**: MySQL, PostgreSQL, MongoDB.\n",
    "- **Specialized Databases**:\n",
    "  - **Vector Databases**: Databases that are optimized for storing and querying vector data.\n",
    "    - **Examples**: Pinecone, Zilliz, ChromaDB, Weaviate.\n",
    "\n",
    "## Data warehouses\n",
    "\n",
    "- **Purpose**: Designed for analyzing large datasets.\n",
    "- **Optimization**: Suited for online analytical processing (OLAP).\n",
    "- **Key characteristics**:\n",
    "  - Stores vast amounts of structured and historical data optimized for analytics.\n",
    "  - Uses indexing and sorting to accelerate query performance.\n",
    "  - Often employs proprietary storage formats (e.g., Snowflake’s columnar format) for efficiency.\n",
    "  - Supports SQL-based querying and analytical functions for business intelligence.\n",
    "  - Relies on ETL (extract, transform, load) or ELT (extract, load, transform) pipelines to preprocess and structure data.\n",
    "- **Examples**: Amazon Redshift, Google BigQuery, Snowflake.\n",
    "\n",
    "\n",
    "## Data lakes\n",
    "\n",
    "- **Purpose**: Store large volumes of raw, semi-structured, or unstructured data.\n",
    "- **Key characteristics**:\n",
    "  - Maintains raw data in its native format without requiring prior transformation.\n",
    "  - Supports diverse data types (e.g., text, images, videos, logs).\n",
    "  - Ideal for machine learning, big data analytics, and scenarios requiring data exploration.\n",
    "  - Lacks built-in mechanisms for enforcing data quality, transactional consistency, or indexing.\n",
    "\n",
    "\n",
    "### Structure of a data lake\n",
    "\n",
    "Data lakes are organized into layers that enable efficient storage and processing:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/storage-layer.png\" width=\"500\">\n",
    "\n",
    "1. **Storage layer**: The foundation of the data lake, implemented using scalable systems like AWS S3, Cloudflare R2, or distributed file systems such as Ceph and Lustre.\n",
    "2. **File formats**: Stores data in multiple formats, including Parquet, CSV, JSON, JPEG, and Protocol Buffers, to support varied workloads.\n",
    "3. **Table format layer**: Adds features like schema management, ACID transactions, and indexing to provide database-like capabilities within the data lake. Examples of table formats include:\n",
    "   - **Apache Iceberg**: Focuses on versioning, schema evolution, and large-scale analytics.\n",
    "   - **Delta Lake**: Built with ACID transactions and data quality controls for analytics and machine learning.\n",
    "   - **Apache Hudi**: Offers incremental processing capabilities and efficient updates for data lakes.\n",
    "\n",
    "### Lakehouses\n",
    "\n",
    "Lakehouses build on the foundation of data lakes, addressing their limitations while incorporating features of data warehouses.\n",
    "\n",
    "- **Purpose**: Provide a unified platform for both analytical and transactional workloads.\n",
    "- **Key characteristics**:\n",
    "  - Combines the scalability and flexibility of data lakes with the structure and performance of data warehouses.\n",
    "  - Implements ACID transactions, indexing, and schema enforcement directly on lake-stored data.\n",
    "  - Built on open table formats (e.g., Apache Iceberg, Delta Lake, Apache Hudi) for robust querying and data management.\n",
    "  - Bridges the gap between data engineering and machine learning workflows, enabling seamless data sharing.\n",
    "  - Eliminates the need to move data between separate lake and warehouse systems, reducing complexity and cost.\n",
    "\n",
    "- **Examples**: Databricks Lakehouse Platform, Delta Lake, and other systems leveraging modern table formats.\n",
    "\n",
    "\n",
    "## In-memory data formats\n",
    "\n",
    "### Apache Arrow\n",
    "\n",
    "Apache Arrow is a high-performance in-memory data processing framework designed to enhance analytical workflows across languages and platforms.\n",
    "\n",
    "- **Purpose**: Provides a standard for in-memory data that minimizes serialization costs and maximizes interoperability.\n",
    "- **Language bindings**:\n",
    "  - Examples: PyArrow for Python, Arrow-rs for Rust.\n",
    "- **Key capabilities**:\n",
    "  - **Zero-copy data sharing**: Supports efficient shared memory access and RPC-based data transfers.\n",
    "  - **File format support**: Reads and writes formats like CSV, Apache ORC, and Apache Parquet.\n",
    "  - **In-memory analytics**: Facilitates high-speed operations using the Arrow Table abstraction for query processing and computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832158fe",
   "metadata": {},
   "source": [
    "## The Compute Layer\n",
    "\n",
    "Here is one way in which compute can be categorized:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/compute-layer-overview.png\" width=\"500\">\n",
    "\n",
    "### Compute by Function\n",
    "\n",
    "- **Data Engineering**: Transforming a dataset (X) into a new dataset (X') through various operations.\n",
    "- **Analytics**: Using datasets to create visualizations, dashboards, and reports.\n",
    "- **Machine Learning and AI**: Training models using the data to produce predictive models.\n",
    "\n",
    "### Data Engineering Compute\n",
    "\n",
    "If we want to drill deeper, here is how the data engineering compute space is categorized:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/compute-layer-data-eng.png\" width=\"500\">\n",
    "\n",
    "### Machine Learning and AI Compute\n",
    "\n",
    "Similarly, here is how the machine learning and AI compute space is categorized:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/compute-layer-data-eng-v2.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec559c5b",
   "metadata": {},
   "source": [
    "## The Orchestration Layer\n",
    "\n",
    "To orchestrate between different stages of compute, we usually use a workflow engine or orchestration platform.\n",
    "\n",
    "Here are some of the most popular ones:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/orchestration-layer-v2.png\" width=\"500\">\n",
    "\n",
    "Orchestration platforms usually differ in the following ways:\n",
    "- **Language**: Some are natively written in Python, others are not.\n",
    "- **Ease of use**: Some have a rigid DAG/DSL interface, others have a more flexible API-based interface to creating tasks.\n",
    "- **Tilt towards ML vs Data Engineering**: Some are more focused on ML, some are more focused on Data Engineering.\n",
    "  - Some have native dataset concepts meant for data engineering whereas others have native workflow concepts meant for ML.\n",
    "- **Cost**: Some are cheaper than others depending on the commercial offering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a64fb",
   "metadata": {},
   "source": [
    "## Distributed Computing Frameworks\n",
    "\n",
    "Distributed computing frameworks enable scalable data processing and analysis by dividing workloads across multiple machines. These frameworks are often categorized based on their primary use case, such as stream processing, batch processing, or general-purpose computing.\n",
    "\n",
    "### Streaming Applications\n",
    "\n",
    "Streaming frameworks process data as it arrives in real-time or near-real-time. They are essential for applications requiring immediate insights, such as fraud detection, monitoring, and recommendation systems.\n",
    "\n",
    "- **Apache Kafka** (JVM-Based):\n",
    "  - Distributed event streaming platform.\n",
    "  - Fault-tolerant and scalable messaging.\n",
    "  - Widely used for real-time data pipelines and log aggregation.\n",
    "  - Python integration relies on external libraries like `confluent-kafka`.\n",
    "\n",
    "- **Apache Flink** (JVM-Based):\n",
    "  - Unified engine for stream and batch processing.\n",
    "  - Features low-latency, event-time processing, and stateful computation.\n",
    "  - Commonly used for real-time dashboards, fraud detection, and IoT analytics.\n",
    "  - PyFlink API provides Python support but has fewer features compared to Java/Scala.\n",
    "\n",
    "### Batch Processing\n",
    "\n",
    "Batch processing frameworks handle large data volumes in discrete chunks, often for ETL workflows, batch inference and last-mile data processing.\n",
    "\n",
    "- **Apache Hadoop** (JVM-Based):\n",
    "  - Pioneer in distributed storage (HDFS) and batch processing (MapReduce).\n",
    "  - Reliable for large-scale, fault-tolerant data processing.\n",
    "  - Historically used for batch ETL and analytics.\n",
    "\n",
    "- **Apache Spark** (JVM-Based):\n",
    "  - Unified engine for batch and streaming workloads.\n",
    "  - Features in-memory computation, scalable processing, and rich APIs.\n",
    "  - Popular for data transformation, machine learning pipelines, and analytics.\n",
    "  - Python integration via PySpark can introduce overhead due to JVM interaction.\n",
    "\n",
    "### General-Purpose Distributed Computing\n",
    "\n",
    "These frameworks are designed for diverse tasks, including machine learning, reinforcement learning, and data processing.\n",
    "\n",
    "- **Dask**:\n",
    "  - Python-native framework for parallel and distributed computing.\n",
    "  - Frequently used in scientific computing and dataframe-based workflows.\n",
    "  - Scales efficiently from single machines to distributed clusters.\n",
    "\n",
    "- **Ray**:\n",
    "  - Flexible platform for scalable distributed applications.\n",
    "  - Rich in high-level libraries for:\n",
    "    - Reinforcement Learning (Ray RLlib)\n",
    "    - Distributed Data processing (Ray Data)\n",
    "    - Distributed Training (Ray Train)\n",
    "    - Distributed Hyperparameter Tuning (Ray Tune)\n",
    "    - Distributed Serving (Ray Serve)\n",
    "  - Seamless integration across the Ray ecosystem to build end-to-end data pipelines.\n",
    "  - Ideal for Python-centric teams needing high-performance distributed computing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e44e6d5",
   "metadata": {},
   "source": [
    "### Challenges with JVM\n",
    "\n",
    "JVM-based frameworks like Spark and Flink have historically dominated the distributed computing landscape. However, they present several challenges:\n",
    "\n",
    "Here is a diagram of the data flow in Spark:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/data-flow-jvm.png\" width=\"500\">\n",
    "   \n",
    "Here are the issues highlighted in the diagram:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/data-flow-jvm-issues.png\" width=\"500\">\n",
    "\n",
    "1. **Painful local development UX**:  \n",
    "   Getting a local development environment setup is difficult given the complexity of the dependencies.\n",
    "\n",
    "2. **Inscrutable error traces between Python and JVM**:  \n",
    "   Some tracebacks are not helpful in debugging given failures can occur across the language boundary (e.g. socket errors, JVM crashes vs Python application crashes).\n",
    "\n",
    "3. **Data/Memory Overhead**:  \n",
    "   The onus is on the user to properly type and design their UDFs and to minimize the data/memory overhead in serializing and deserializing data between Python and JVM.\n",
    "\n",
    "By contrast, frameworks like Ray and Dask avoid the JVM overhead entirely, offering Python-native performance and better alignment with modern data science workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c25a29",
   "metadata": {},
   "source": [
    "## Data Processing with Ray Data\n",
    "\n",
    "### What is Ray Data ?\n",
    "\n",
    "Ray Data is a distributed data processing library designed for high-performance workloads.\n",
    "\n",
    "- Initially developed as a last-mile data processing solution to seamlessly integrate with model training workflows.\n",
    "- Facilitates efficient execution of GPU-intensive batch inference tasks.\n",
    "- Currently being enhanced to support structured data processing, including advanced functionalities for operations such as joins and groupby.\n",
    "\n",
    "### Why Ray Data ?\n",
    "* Ray Data is natively designed to support:\n",
    "    - Heterogeneous computational workloads\n",
    "    - Pass data dependencies via a distributed in-memory object store\n",
    "* Ray's support for stateful computation through Actors is a core feature. In contrast:\n",
    "    - Spark lacks native support for stateful computations\n",
    "    - Dask documents stateful capabilities, it does not guarantee execution.\n",
    "* Ray Data's seamless integration with the broader Ray ecosystem (including Train, Tune, Core, and Serve) offers significant advantages in integration engineering, which often incurs higher costs and performance impacts than application engineering.\n",
    "* Ray's advanced resource tagging, accounting, and scalability capabilities are more sophisticated than those of other tools. \n",
    "\n",
    "### When to use Ray Core over Ray Data ?\n",
    "If a user is:\n",
    "- an expert in Ray Core\n",
    "- knows their data distribution very well\n",
    "- has very complex data processing logic\n",
    "\n",
    "Then perhaps trying out Ray Data won't lead to a win, given they will be able to optimize their workflow while implementing complex logic to handle object store backpressure.\n",
    "\n",
    "### On Ray Data vs Spark\n",
    "On the positive side:\n",
    "- It handles running on heterogeneous compute much nicer than Spark where GPU support has been patched in.\n",
    "- Its Python-native API and integration with the end-to-end Ray and ML ecosystem is a big win.\n",
    "\n",
    "On the negative side:\n",
    "- Still immature to claim it can compete with Spark on SQL-like operations.\n",
    "- Spark is a much more mature product with a much larger community and ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041490c5",
   "metadata": {},
   "source": [
    "## Ray Serve\n",
    "\n",
    "### What is Ray Serve ?\n",
    "\n",
    "Ray Serve is a framework for building distributed ML inference services.\n",
    "\n",
    "#### Data flow\n",
    "\n",
    "Here is a diagram of the request lifecycle in Ray Serve:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/geotab/request_lifecycle.jpg\" width=\"800\">\n",
    "\n",
    "When an HTTP or gRPC request is sent to the corresponding HTTP or gRPC proxy, the following happens:\n",
    "\n",
    "1. The request is received and parsed.\n",
    "2. Ray Serve looks up the correct deployment associated with the HTTP URL path or application name metadata. Serve places the request in a queue.\n",
    "3. For each request in a deployment's queue, an available replica is looked up and the request is sent to it.\n",
    "4. If no replicas are available (that is, more than `max_ongoing_requests` requests are outstanding at each replica), the request is left in the queue until a replica becomes available. \n",
    "5. Each replica maintains a queue of requests and executes requests one at a time, possibly using asyncio to process them concurrently.\n",
    "\n",
    "### Why Ray Serve ?\n",
    "\n",
    "Ray Serve enables scaling services and is a good choice given it:\n",
    "* allows for an intuitive approach to autoscaling based on request load.\n",
    "* has integrations with tools like FastAPI to make it easier to develop and document APIs.\n",
    "* allows for easy composition of a complex DAG of models and data processing steps.\n",
    "* provides support for both grpc and http protocols.\n",
    "\n",
    "### Ray Serve vs Ray Data\n",
    "\n",
    "Rules of thumb:\n",
    "- When dealing with continuous/streaming applications where low-latency is critical, Ray Serve is a good choice.\n",
    "- Otherwise, if data can be batched, processed at longer time intervals, Ray Data is a good choice to maximize throughput.\n",
    "\n",
    "In terms of implementation:\n",
    "- Ray Data implements complex logic to handle object store backpressure and perform dynamic resource allocation\n",
    "- whereas Ray Serve relies on simple logic to batch and queue requests and statically allocates resources.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
