{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Ray Data:  Ray Data + Unstructured Data\n",
    "\n",
    "Â© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’» **Launch Locally**: You can run this notebook locally, but performance will be reduced.\n",
    "\n",
    "ðŸš€ **Launch on Cloud**: A Ray Cluster (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will provide an overview of Ray Data and how to use it to read, transform and write data in a distributed manner.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "  <li>When and why to use Ray Data?</li>\n",
    "  <li>How to work with Ray Data</li>\n",
    "  <li>Loading data</li>\n",
    "  <li>Ray Data key concepts</li>\n",
    "  <li>Lazy execution mode</li>\n",
    "  <li>Transforming data</li>\n",
    "  <li>Stateful transformations with Ray Actors</li>\n",
    "  <li>Materializing data</li>\n",
    "  <li>Data operations: grouping, aggregation, and shuffling</li>\n",
    "  <li>Persisting data</li>\n",
    "  <li>Ray Data in production</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. When to Consider Ray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider using Ray Data for your project if it meets one or more of the following criteria:\n",
    "\n",
    "| **Challenge** | **Details** | **Ray Data Solution** |\n",
    "|---------------|-------------|------------------------|\n",
    "| **Operating on large datasets and/or models** | - Having to load and process massive datasets or models (e.g., >10â€¯TB) <br>- Having to perform inference with large models (e.g., LLMs) using inference engines | - Distributes data loading and processing across a Ray cluster <br>- Supports large model inference workloads via [ray.data.llm](https://docs.ray.io/en/latest/data/working-with-llms.html) |\n",
    "| **Efficient hardware utilization across CPUs and GPUs** | - Over-provisioning compute to naively partition data <br>- Performing static resource allocation <br>- Running execution in full across CPU and GPU stages <br>- Passing data between heteregenous stages by persisting intermediate results to disk  | - [Streams data](https://docs.ray.io/en/latest/data/data-internals.html#streaming-execution) to avoid full materialization in memory <br>- Enables resource multiplexing across pipeline stages <br>- Supports autoscaling for both CPU and GPU resources <br>- Enables pipeline parallelism across heterogeneous hardware with [configurable batch sizes](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray-data-dataset-map-batches) |\n",
    "| **Building reliable pipelines** | - Needing to handle failures such as network errors, spot instance preemptions, and hardware faults | - Leverages [Ray Coreâ€™s fault-tolerance mechanisms](https://docs.ray.io/en/latest/ray-core/fault-tolerance.html) to recover from failed tasks <br>- Supports [driver checkpointing](https://docs.anyscale.com/rayturbo/rayturbo-data/#job-level-checkpointing) (via RayTurbo) for comprehensive pipeline reliability |\n",
    "| **Handling unstructured data efficiently** | - Suboptimal resource allocation due to data skew in input data sizes (e.g. vary input video lengths)  | - Automatically [reshapes data into uniformly sized blocks](https://docs.ray.io/en/latest/data/data-internals.html) to improve processing efficiency |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://docs.ray.io/en/releases-2.6.1/_images/stream-example.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Example batch inference pipeline with Ray Data over a large dataset using heterogeneous cluster of CPUs and GPUs.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to work with Ray Data\n",
    "\n",
    "It is commonly a three step process when using Ray Data:\n",
    "1. Create your Dataset (most commonly using an IO connector)\n",
    "2. Apply transformations to your Dataset\n",
    "3. Consume your Dataset by either:\n",
    "   1. Writing it out to a sink (file-based or database)\n",
    "   2. Iterating over it (connecting it to a training process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some `MNIST` data from s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is our dataset it contains 50 images per class\n",
    "!aws s3 ls s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `read_images` function to load the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ray.data.read_images(\"s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/\", include_paths=True)\n",
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <p><strong>Ray Data supports a variety of data sources for loading data</strong></p>\n",
    "  <ul>\n",
    "    <li>\n",
    "      Reading files from common file formats (e.g. Parquet, CSV, JSON, etc.)\n",
    "      <ul>\n",
    "          <li><code>ds = ray.data.read_parquet(\"s3://...\")</code></li>\n",
    "      </ul>\n",
    "    </li>\n",
    "    <li>Loading from in-memory data structures (e.g. NumPy, PyTorch, etc.)\n",
    "      <ul>\n",
    "          <li><code>ray.data.from_torch(torch_ds)</code></li>\n",
    "      </ul>\n",
    "    <li>Loading from data lakehouses and warehouses such as Snowflake, Iceberg, and Databricks.</li>\n",
    "      <ul>\n",
    "          <li><code>ds = ray.data.read_databricks_tables()</code></li>\n",
    "      </ul>\n",
    "  </ul>\n",
    "  <p>\n",
    "    Start with an extensive list of <a href=\"https://docs.ray.io/en/latest/data/api/input_output.html#input-output\" target=\"_blank\">supported formats</a> and review further options in our <a href=\"https://docs.ray.io/en/latest/data/loading-data.html#loading-data\" target=\"_blank\">data loading guide</a>.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, Ray Data uses Ray tasks to read data from remote storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/Ray+Data+Internals+-+reading.png\" width=\"500px\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|When reading from a file-based datasource, Ray Data starts with a number of read tasks proportional to the number of CPUs in the cluster. |\n",
    "|Each read task reads its assigned files and produces output blocks.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Note on blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://assets-training.s3.us-west-2.amazonaws.com/ray-intro/block.png\" width=\"700px\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|A Dataset when materialized is a distributed collection of blocks. This example illustrates a materialized dataset with three blocks, each block holding 1000 rows.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Block</strong> is a contiguous subset of rows from a dataset. Blocks are distributed across the cluster and processed independently in parallel. By default blocks are PyArrow tables.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lazy execution mode\n",
    "\n",
    "In Ray Data, operations are not executed immediately. Most transformations are **lazy**, meaning they build up an execution plan rather than running right away. \n",
    "\n",
    "The execution plan is only **executed** when you call a method that *materializes* or *consumes* the dataset.\n",
    "\n",
    "To materialize a small subset of the data, you can use the `take_batch` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = ds.take_batch(batch_size=3)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize an example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = batch[\"image\"][0]\n",
    "title = batch[\"path\"][0]\n",
    "\n",
    "plt.title(title)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Note on execution triggering methods in Ray Dataset</b>\n",
    "\n",
    "To determine if an operation will trigger execution, look for the methods with the `ConsumptionAPI` decorator in the [`Dataset.py`](https://github.com/ray-project/ray/blob/master/python/ray/data/dataset.py).\n",
    "\n",
    "These categories of operations trigger execution (with some examples):\n",
    "* Method designed to consume Datasets for **writing**:\n",
    "  * [`write_parquet`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.write_parquet.html#ray.data.Dataset.write_parquet)\n",
    "* Method designed to consume Datasets for **distributed training**:\n",
    "  * [`streaming_split`](https://github.com/ray-project/ray/blob/master/python/ray/data/dataset.py#L1694)\n",
    "* Methods that attempt to **show** data, for example:\n",
    "  * [`take`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.take.html#ray.data.Dataset.take)\n",
    "  * [`show`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.show.html#ray-data-dataset-show)\n",
    "* **Aggregations**, which attempt to reduce a dataset to a single value per column:\n",
    "  * [`min`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.min.html#ray.data.Dataset.min)\n",
    "  * [`sum`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.sum.html#ray.data.Dataset.sum)\n",
    "\n",
    "Another way to trigger execution is to explicitly call <a href=\"https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.materialize.html#ray-data-dataset-materialize\" target=\"_blank\">materialize()</a>. This will execute the underlying plan and generate the entire data blocks onto the cluster's memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transforming data\n",
    "\n",
    "To transform data, we can use the [`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html#ray-data-dataset-map-batches) API. \n",
    "\n",
    "`map_batches` takes a user-defined function which accepts a batch of data and returns a batch of transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    batch[\"image\"] = [transform(image) for image in batch[\"image\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `ds.map_batches` will add a `map_batches` operator to the execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_normalized = ds.map_batches(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the batch size for the transformation, specify the `batch_size` parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_normalized = ds.map_batches(normalize, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note:** batching only helps with performance when a transformation is vectorized - i.e. benefits from processing multiple rows at once.\n",
    "\n",
    "Finding the optimal batch size depends on the hardware available and the target utilization.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 On resource specification\n",
    "\n",
    "To specify the exact resources to use for a `map_batches` transformation, specify the `num_cpus`, `num_gpus`, `memory`, and `resources` parameters.\n",
    "\n",
    "- `num_cpus`: Number of CPUs to use for each task (use >1 if task performs multithreaded operations).\n",
    "- `num_gpus`: Number of GPUs to use for each task.\n",
    "- `memory`: Amount of RAM to use for each task (in bytes).\n",
    "- `resources`: What is referred to as custom resources in Ray. It is a way to specify which node types to use for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_normalized = ds.map_batches(normalize, batch_size=32, num_cpus=1, memory=100 * 1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note:** Ray only performs a logical allocation of resources and does not physically enforce resource limits.\n",
    "\n",
    "By default, Ray will [retry OOM errors](https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html#retry-policy) and Ray Data will infinitely retry tasks that fail due to system failures.\n",
    "\n",
    "Specifying resources helps avoid resource contention, avoiding unnecessary retries and confusing OOM errors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 On concurrency limiting\n",
    "\n",
    "Ray Data will attempt to use all the resources available in the cluster. \n",
    "\n",
    "In particular, it will schedule as many tasks as there are input blocks for each operator (stage) in the pipeline.\n",
    "\n",
    "To limit the concurrency for a particular transformation, specify the `concurrency` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concurrency_limit = 10  # Don't schedule more than 10 tasks at a time\n",
    "\n",
    "ds_normalized = ds.map_batches(\n",
    "    normalize,\n",
    "    batch_size=32,\n",
    "    num_cpus=1,\n",
    "    memory=100 * 1024**2,\n",
    "    concurrency=concurrency_limit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note:** Limiting concurrency might be helpful when you have an unbounded compute configuration (max number of nodes is too high) and you want to avoid aggressive scaling for a fast step in the pipeline (e.g. light preprocessing of data).\n",
    "\n",
    "Additionally, note that Ray Data will attempt to fuse transformations together to reduce data transfer between stages. Setting different concurrency limits for different transformations might prevent this optimization.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the output of `normalize()`, call [`take_batch()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.take_batch.html#ray.data.Dataset.take_batch) on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalized_batch = ds_normalized.take_batch(batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the normalized pixel value range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for image in normalized_batch[\"image\"]:\n",
    "    assert image.shape == (1, 28, 28) # channel, height, width\n",
    "    assert image.min() >= -1 and image.max() <= 1 # normalized to [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Activity**\n",
    "\n",
    "Add the ground truth label extracted from the image path.\n",
    "\n",
    "Starting point:\n",
    "\n",
    "```python\n",
    "ds = ray.data.read_images(\"s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/\", include_paths=True)\n",
    "ds_normalized = ds.map_batches(normalize)\n",
    "# batch = ds_normalized.take_batch(batch_size=3)\n",
    "\n",
    "# add_label\n",
    "ds_labeled = ds_normalized.map_batches(add_label)\n",
    "labeled_batch = ds_labeled.take_batch(10)\n",
    "print(labeled_batch)\n",
    "```\n",
    "\n",
    "\n",
    "**The task**\n",
    "\n",
    "Implement `add_label` function that takes batch of data and return batch with image label.\n",
    "\n",
    "The image path is in the format:`s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/{label}/{image_id}.png`.\n",
    "\n",
    "**Hint**: Define the add_label function; use `take_batch()` to better understand the data format.\n",
    "```\n",
    "def add_label(batch):\n",
    "    ...\n",
    "    return batch\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click to view solution</summary>\n",
    "\n",
    "```python\n",
    "def add_label(batch):\n",
    "    labels = []\n",
    "    for item in batch[\"path\"]:\n",
    "        label = int(item.split(\"/\")[-2])\n",
    "        labels.append(label)\n",
    "\n",
    "    batch[\"label\"] = np.array(labels)\n",
    "    return batch\n",
    "\n",
    "ds = ray.data.read_images(\"s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/\", include_paths=True)\n",
    "ds_normalized = ds.map_batches(normalize)\n",
    "ds_labeled = ds_normalized.map_batches(add_label)\n",
    "labeled_batch = ds_labeled.take_batch(10)\n",
    "print(labeled_batch)\n",
    "```\n",
    "\n",
    "</details>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stateful transformations with Ray Actors\n",
    "\n",
    "In cases like batch inference, you want to spin up a number of actor processes that are **initialized once** with your model **and reused** to process multiple batches.\n",
    "\n",
    "To implement this, you can use the `map_batches` API with a \"Callable\" class method that implements:\n",
    "\n",
    "- `__init__`: Initialize any expensive state.\n",
    "- `__call__`: Perform the stateful transformation.\n",
    "\n",
    "For example, we can implement a `MNISTClassifier` that:\n",
    "- loads a pre-trained model from a local file\n",
    "- accepts a batch of images and generates the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cpu\" # or \"cpu\" if you want to run it locally on CPU\n",
    "class MNISTClassifier:\n",
    "    def __init__(self, remote_path: str, local_path: str):\n",
    "        subprocess.run(f\"aws s3 cp --no-sign-request {remote_path} {local_path}\", shell=True, check=True)\n",
    "\n",
    "        self.model = torch.jit.load(local_path).to(device).eval()\n",
    "\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `map_batches` API to apply the transformation to each batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storage_folder = 'mnt/cluster_storage'  # Modify this path to your local folder if it runs on your local environment\n",
    "local_path = f\"{storage_folder}/model.pt\"\n",
    "\n",
    "mnist_classifier_args = {\n",
    "    \"remote_path\": \"s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt\",\n",
    "    \"local_path\": local_path,\n",
    "}\n",
    "\n",
    "ds_preds = ds_normalized.map_batches(\n",
    "    MNISTClassifier,\n",
    "    fn_constructor_kwargs=mnist_classifier_args,\n",
    "    num_gpus= 1 if device == 'cuda' else None,  # Use GPU if available\n",
    "    concurrency=3,\n",
    "    batch_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Resource specification for stateful transformations\n",
    "\n",
    "It is common when you have varying hardware types in your cluster to want to further specify which accelerators to use for each stage of your pipeline.\n",
    "\n",
    "Let's show how to achieve this with the `resources` parameter.\n",
    "\n",
    "To use a GPU for following examples, we suggest to run them on Anyscale Ray Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds = ds_normalized.map_batches(\n",
    "    MNISTClassifier,\n",
    "    fn_constructor_kwargs=mnist_classifier_args,\n",
    "    num_gpus= 1 if device == 'cuda' else None,  # Use GPU if available\n",
    "    concurrency=3,\n",
    "    batch_size=100,\n",
    "    resources={\"accelerator_type:T4\": 0.0001},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Note:</b> Pass in the Callable class uninitialized. Your driver will not execute the class constructor. Ray will pass in the arguments to the class constructor when the class is actually used in a transformation.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Note on autoscaling for stateful transformations\n",
    "\n",
    "For stateless transformations, Ray Data will automatically scale up the number of tasks to match the number of input blocks.\n",
    "\n",
    "For stateful transformations, Ray Data will schedule tasks proportional to the number of actors (workers) in the pool. \n",
    "\n",
    "To specify an autoscaling pool, use a tuple of `(min_size, max_size)` for the `concurrency` parameter.\n",
    "\n",
    "Ray Data will start with `min_size` actors and automatically scale up to `max_size` as needed.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds = ds_normalized.map_batches(\n",
    "    MNISTClassifier,\n",
    "    fn_constructor_kwargs=mnist_classifier_args,\n",
    "    num_gpus= 1 if device == 'cuda' else None,  # Use GPU if available\n",
    "    concurrency=(1, 4),  # Autoscale pool based on blocks, resources and limits\n",
    "    batch_size=100,\n",
    "    #resources={\"accelerator_type:T4\": 0.0001}, # Optional if you run it locally\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_preds = ds_preds.take_batch(100)\n",
    "batch_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Materializing data\n",
    "\n",
    "You can choose to materialize the entire dataset into the Ray object store which is distributed across the cluster, primarily in memory and secondarily spilling to disk.\n",
    "\n",
    "To materialize the dataset, we can use the `materialize()` method.\n",
    "\n",
    "Use this **only** when you require the full dataset to compute downstream outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`materialize()` triggers the execution. The logs should show the execution plan of Dataset:\n",
    "\n",
    "```\n",
    "Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ListFiles] -> TaskPoolMapOperator[ReadFiles] -> TaskPoolMapOperator[MapBatches(normalize)] -> ActorPoolMapOperator[MapBatches(MNISTClassifier)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Operations: grouping, aggregation, and shuffling\n",
    "\n",
    "Let's look at some more involved transformations.\n",
    "\n",
    "Some operations require all inputs to be materialized in object store. To determinte this, look for the methods with the `AllToAllAPI` decorator in the [`Dataset.py`](https://github.com/ray-project/ray/blob/master/python/ray/data/dataset.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Custom batching using `groupby`. \n",
    "\n",
    "In case you want to generate batches according to a specific key, you can use `groupby` to group the data by the key and then use `map_groups` to apply the transformation.\n",
    "\n",
    "For instance, let's compute the accuracy of the model by \"ground truth label\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    batch[\"ground_truth_label\"] = [int(path.split(\"/\")[-2]) for path in batch[\"path\"]]\n",
    "    return batch\n",
    "\n",
    "def compute_accuracy(group: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    return {\n",
    "        \"accuracy\": [np.mean(group[\"predicted_label\"] == group[\"ground_truth_label\"])],\n",
    "        \"ground_truth_label\": group[\"ground_truth_label\"][:1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds.map_batches(add_label).groupby(\"ground_truth_label\").map_groups(compute_accuracy).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Note:</b> `ds_preds` is not re-computed given we have already materialized the dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Aggregations\n",
    "\n",
    "Ray Data also supports a variety of aggregations. For instance, we can compute the mean accuracy across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds.map_batches(add_label).map_batches(compute_accuracy).mean(on=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is ConsumptionAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Data provides collection of aggregation functions including:\n",
    "\n",
    "- `count`\n",
    "- `max`\n",
    "- `mean`\n",
    "- `min`\n",
    "- `sum`\n",
    "- `std`\n",
    "\n",
    "See relevant [docs page here](https://docs.ray.io/en/latest/data/api/grouped_data.html#ray.data.aggregate.AggregateFn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Shuffling data \n",
    "\n",
    "There are different options to shuffle data in Ray Data of varying degrees of randomness and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.1. File based shuffle on read\n",
    "\n",
    "To randomly shuffle the ordering of input files before reading, call a read function that supports shuffling, such as `read_images()`, and use the shuffle=\"files\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.data.read_images(\"s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/\", shuffle=\"files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2. Shuffling block order\n",
    "This option randomizes the order of blocks in a dataset. Blocks are the basic unit of data chunk that Ray Data stores in the object store. Applying this operation alone doesnâ€™t involve heavy computation and communication. However, it requires Ray Data to materialize all blocks in memory before applying the operation. Only use this option when your dataset is small enough to fit into the object store memory.\n",
    "\n",
    "To perform block order shuffling, use `randomize_block_order`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_randomized_blocks = ds_preds.randomize_block_order()\n",
    "ds_randomized_blocks.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.3. Shuffle all rows globally\n",
    "To randomly shuffle all rows globally, call `random_shuffle()`. This is the slowest option for shuffle, and requires transferring data across network between workers. This option achieves the best randomness among all options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_randomized_rows = ds_preds.random_shuffle()\n",
    "ds_randomized_rows.materialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Persisting data\n",
    "\n",
    "Finally, you can persist a dataset to storage using any of the \"write\" functions that Ray Data supports.\n",
    "\n",
    "Lets write our predictions to a parquet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_pred_folder = f\"{storage_folder}/mnist_preds\" # Change this to your local path if needed\n",
    "ds_preds.write_parquet(local_pred_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [Input/Output docs](https://docs.ray.io/en/latest/data/api/input_output.html) for a comprehensive list of write functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ray Data in production\n",
    "\n",
    "- Netflix is using Ray Data for multi-modal inference pipelines. See [this talk at the Ray Summit 2024](https://raysummit.anyscale.com/flow/anyscale/raysummit2024/landing/page/sessioncatalog/session/1722028596844001bCg0) to learn more.\n",
    "- Pinterest is using Ray Data and Ray Train for their recommendation system model training. They leverage Ray Data to improve GPU utilization and training throughput by disaggregating their compute across a heterogeneous cluster.  [this talk at the Ray Summit 2024](https://www.classcentral.com/course/youtube-pinterest-s-ml-evolution-distributed-training-with-ray-ray-summit-2024-353672) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm -rf {storage_folder}/mnist_preds\n",
    "!rm {storage_folder}/model.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
