{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682e224e-1bb9-470c-b363-386ede0785a4",
   "metadata": {},
   "source": [
    "# Introduction to Ray Serve with PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55345a83",
   "metadata": {},
   "source": [
    "ðŸ’» **Launch Locally**: You can run this notebook locally, but performance will be reduced.\n",
    "\n",
    "ðŸš€ **Launch on Cloud**: A Ray Cluster (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1060aea0",
   "metadata": {},
   "source": [
    "This notebook will introduce you to Ray Serve with PyTorch, a framework for building and deploying scalable ML applications.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>1.</b> When to consider Ray Serve</li>\n",
    "    <li><b>2.</b> Overview of Ray Serve</li>\n",
    "    <li><b>3.</b> Implement an image classification service</li>\n",
    "    <li><b>4.</b> Development workflow with Ray Serve </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d528147",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from typing import Any\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from ray import serve\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b34936",
   "metadata": {},
   "source": [
    "## 1. When to Consider Ray Serve\n",
    "\n",
    "Consider using Ray Serve for your project if it meets one or more of the following criteria:\n",
    "\n",
    "| **Challenge** | **Details** | **Ray Serve Solution** |\n",
    "|---------------|------------------|--------------------------|\n",
    "| **Slow iteration speed for ML engineers** | - Developers need to containerize and rollout components on Kubernetes to test changes<br>- Developers need to use complex protocols (e.g. gRPC) to achieve acceptable performance | - Provides a Python-first API to develop lightweight services<br>- Services are lightweight [Ray actors](https://docs.ray.io/en/latest/ray-core/actors.html)<br>- Ray Serve can be run locally for development |\n",
    "| **Need to efficiently compose multiple components** | - Requires efficient data sharing between components<br>- Implementing performant streaming protocols (e.g. gRPC) is a complex task | - Relies on [Ray's object store](https://docs.ray.io/en/latest/ray-core/objects.html) to share data optimally<br>- Avoids the need to implement gRPC streaming |\n",
    "| **Poor utilization of expensive hardware** | Suffering from poor utilization due to naive request handling | - Offers [dynamic batching of requests](https://docs.ray.io/en/latest/serve/advanced-guides/dyn-req-batch.html) to improve hardware utilization<br>- Leverages Ray Core's support for accelerators and custom resources:<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ [Multi-node/multi-GPU serving](https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html)<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ [Fractional compute resource usage](https://docs.ray.io/en/latest/serve/configure-serve-deployment.html)<br>- RayTurbo Serve offers [replica compaction](https://www.anyscale.com/blog/new-feature-replica-compaction?_gl=1*lrhlou*_gcl_au*OTY4NjkwODIzLjE3Mzg1Mjc2MzA.) |\n",
    "| **High-latency outliers when juggling many models** | Stuck with naive load balancing and expensive state loading (e.g. ML models) | - Provides [model multiplexing](https://docs.ray.io/en/latest/serve/model-multiplexing.html) to avoid unnecessary load times<br>- Routes to replicas that already have a model loaded |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250cc03-e52c-4e30-a262-8d8e0a5a0837",
   "metadata": {},
   "source": [
    "## 2. Overview of Ray Serve\n",
    "\n",
    "Serve is a framework for serving ML applications. \n",
    "\n",
    "### Applications\n",
    "\n",
    "Here is a high-level overview of the architecture of a Ray Serve Application.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "A Ray Serve cluster is made up of one or more Applications.\n",
    "\n",
    "An Application is composed of one or more Deployments that work together. Key characteristics:\n",
    "- Applications are coarse-grained units of functionality\n",
    "- They can be **independently upgraded** without affecting other applications running on the same cluster\n",
    "- They provide isolation and separate deployment lifecycles\n",
    "\n",
    "### Deployments\n",
    "\n",
    "A Deployment is the fundamental building block in Ray Serve's architecture.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "Deployments enable:\n",
    "- Separation of concerns (e.g., different models, business logic, data transformations)\n",
    "- **Independent scaling**, including autoscaling capabilities\n",
    "- Multiple replicas for handling concurrent requests\n",
    "\n",
    "\n",
    "### Replicas\n",
    "Each Replica is a worker process (Ray actor) with its own request processing queue. Replicas offer flexible configuration options:\n",
    "\n",
    "- Specify its own hardware and resource requirements (e.g., GPUs)\n",
    "- Specify its own runtime environments (e.g., libraries)\n",
    "- Maintain state (e.g., models)\n",
    "\n",
    "This architecture provides a clean separation of concerns while enabling high scalability and efficient resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43da1a6",
   "metadata": {},
   "source": [
    "## 3. Implement an image classification service\n",
    "\n",
    "Letâ€™s jump right in and get a simple ML service up and running on Ray Serve. \n",
    "\n",
    "Here is an image classification service that performs inference on a batch of handwritten digits using an `MNISTClassifier` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb17a6-a71c-4a11-8ea8-b1b350a5fa1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTClassifier:\n",
    "    def __init__(self, remote_path: str, local_path: str, device: str):\n",
    "        subprocess.run(f\"aws s3 cp {remote_path} {local_path} --no-sign-request\", shell=True, check=True)\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = torch.jit.load(local_path).to(device).eval()\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        return self.predict(batch)\n",
    "    \n",
    "    def predict(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d148b8",
   "metadata": {},
   "source": [
    "First we need to load the classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a79961",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_folder = '/mnt/cluster_storage'  # Modify this path to your local folder if it runs on your local environment\n",
    "model_path = f\"{storage_folder}/model.pt\" # Use your local path\n",
    "classifier = MNISTClassifier(remote_path=\"s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt\", local_path=model_path, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b7dbc",
   "metadata": {},
   "source": [
    "Then we can run inference to generate predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b16400",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = classifier({\"image\": np.random.rand(1, 1, 28, 28).astype(np.float32)})  # Example input (B, C, H, W)\n",
    "output[\"predicted_label\"]  # Should be a numpy array with the predicted label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1a687",
   "metadata": {},
   "source": [
    "Now, if we want to migrate to an online inference setting, we can transform this into a Ray Serve Deployment by applying the `@serve.deployment` decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68888dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment() # this is the decorator to add\n",
    "class OnlineMNISTClassifier:\n",
    "    # same code as MNISTClassifier.__init__\n",
    "    def __init__(self, remote_path: str, local_path: str, device: str):\n",
    "        subprocess.run(f\"aws s3 cp {remote_path} {local_path} --no-sign-request\", shell=True, check=True)\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = torch.jit.load(local_path).to(device).eval()\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict[str, Any]:  # __call__ now takes a Request object\n",
    "        batch = json.loads(await request.json()) # we will need to parse the JSON body of the request\n",
    "        return await self.predict(batch)\n",
    "    \n",
    "    # same code as MNISTClassifier.predict\n",
    "    async def predict(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033619e",
   "metadata": {},
   "source": [
    "We have now defined our Ray Serve deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OnlineMNISTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf85ff1",
   "metadata": {},
   "source": [
    "We can now build an Application using `OnlineMNISTClassifier` deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{storage_folder}/model.pt\" # Use your local path\n",
    "mnist_app = OnlineMNISTClassifier.bind(remote_path=\"s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt\", local_path=model_path, device=\"cpu\")\n",
    "mnist_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e8ac4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note:** `.bind` is a method that takes in the arguments to pass to the Deployment constructor.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e70529",
   "metadata": {},
   "source": [
    "We can then run the application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96056cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_app_handle = serve.run(mnist_app, name='mnist_classifier', blocking=False)\n",
    "mnist_app_handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a0cdb-822a-4439-aeab-9916dd8d059c",
   "metadata": {},
   "source": [
    "We can test it as an HTTP endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a80e9-c26f-48d2-8985-ef4eab4dc580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = np.random.rand(2, 1, 28, 28).tolist()\n",
    "json_request = json.dumps({\"image\": images})\n",
    "response = requests.post(\"http://localhost:8000/\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2cb01",
   "metadata": {},
   "source": [
    "We can also test it as a gRPC endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342928ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\"image\": np.random.rand(10, 1, 28, 28)}\n",
    "response = await mnist_app_handle.predict.remote(batch)\n",
    "response[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da966d55",
   "metadata": {},
   "source": [
    "## 4. Development workflow\n",
    "\n",
    "1. Define application in a `main.py` file\n",
    "2. Deploy the application with `serve run`\n",
    "3. Optionally specify configuration in a `config.yaml` file\n",
    "    - you can use `serve build` to scaffold a basic config.yaml file\n",
    "    - useful if you want to decouple the deployment configuration from the code\n",
    "4. After making a change \n",
    "    - you can re-run the application with `serve run`\n",
    "    - Note there is experimental support for hot-reloading of changes to the application (using `serve run --reload`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef75ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the app with default config\n",
    "!cd intro/ && serve run main:mnist_app --non-blocking --name app1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca83adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and optionally customize config\n",
    "!cd intro/ && serve build -o config.yaml main:mnist_app "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a20b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the running app\n",
    "!cd intro/ && serve run config.yaml --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba25d08",
   "metadata": {},
   "source": [
    "In case you want to **parameterize the application building**, use an \"application builder\" pattern - i.e. set the import path to point to a callable that will return an application.\n",
    "\n",
    "To view an example, see `app_builder.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89696b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd intro/ && serve run app_builder:build_app --non-blocking --name app1 device=cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6a9fc",
   "metadata": {},
   "source": [
    "For more details on the recommended development workflow, read the [docs here](https://docs.ray.io/en/latest/serve/advanced-guides/dev-workflow.html#development-workflow)\n",
    "\n",
    "\n",
    "For unit testing and debugging, Ray Serve provides a local testing mode. For more details, see the [docs here](https://docs.ray.io/en/latest/serve/advanced-guides/dev-workflow.html#local-testing-mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e18b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm {storage_folder}/model.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
