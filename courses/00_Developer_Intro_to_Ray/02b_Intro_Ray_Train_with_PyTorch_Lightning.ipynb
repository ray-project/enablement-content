{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7078ab58-6ca4-4255-8050-b7c5fe7eae1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Ray Train: Ray Train + PyTorch Lightning\n",
    "\n",
    "Â© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58510ed8",
   "metadata": {},
   "source": [
    "ðŸ’» **Launch Locally**: Not recommended. You may encounter out-of-memory (OOM) errors when running certain cells locally.\n",
    "\n",
    "ðŸš€ **Cloud Required**: A Ray Cluster with 4 GPUs (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommanded to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c2710",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train a Stable Diffusion model using PyTorch Lightning and Ray Train. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ol>\n",
    "    <li>When to use Ray Train</li>\n",
    "    <li>Single GPU Training with PyTorch Lightning</li>\n",
    "    <li>Distributed Training with Ray Train and PyTorch Lightning</li>\n",
    "    <li>Ray Train in Production\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f5851",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecea11-fc44-4bc2-af6d-09db4753d78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
    "from lightning.pytorch.utilities.types import OptimizerLRScheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import ray.train\n",
    "from ray.train.lightning import (\n",
    "    RayDDPStrategy,\n",
    "    RayLightningEnvironment,\n",
    "    RayTrainReportCallback,\n",
    ")\n",
    "from ray.train.torch import TorchTrainer, get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18937b56",
   "metadata": {},
   "source": [
    "## 1. When to use Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cbfe1",
   "metadata": {},
   "source": [
    "\n",
    "Use Ray Train when you face one of the following challenges:\n",
    "\n",
    "|Challenge|Detail|Solution|\n",
    "|---|---|---|\n",
    "|**Need to speed up or scale up training**| Training jobs might take a long time to complete, or require a lot of compute | Ray Train provides a **distributed training** framework that allows engineers to scale training jobs to multiple GPUs |\n",
    "|**Minimize overhead of setting up clusters**| Engineers need to manage the underlying infrastructure | Ray Train **provisions the underlying infrastructure** via Ray's cluster autoscaler. |\n",
    "|**Achieve observability**| Engineers need to connect to different nodes and GPUs to find the root cause of failures, fetch logs, traces, etc | Ray Train **provides observability** via Ray's dashboard, metrics, and traces that allow engineers to monitor the training job |\n",
    "|**Ensure reliable training**| Training jobs can fail due to hardware failures, network issues, or other unexpected events | Ray Train **ensures fault tolerance** via checkpointing, automatic retries, and the ability to resume training from the last checkpoint |\n",
    "|**Avoid significant code rewrite**| Engineers might need to fully rewrite their training loop to support distributed training | Ray Train has **built-in integrations** with the PyTorch ecosystem (Torch, Lightning, Huggingface), Tree-based methods (XGB, LGBM), and more to minimize the amount of code changes needed |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b883e7",
   "metadata": {},
   "source": [
    "## 2. Single GPU Training with PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1288014b",
   "metadata": {},
   "source": [
    "### 2.1 Overview\n",
    "\n",
    "We will start by fitting a Stable Diffusion `Unet` model to a preprocessed image and text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1537cf",
   "metadata": {},
   "source": [
    "This diagram shows the full-scale training architecture.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/stable-diffusion/training_architecture_v3.jpeg\" width=\"700px\">\n",
    "\n",
    "Regardless of scale, the process is primarily composed of three main stages:\n",
    "1. **Loading the preprocessed data**\n",
    "2. **Training the model**\n",
    "3. **Storing the model checkpoints**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c50ba",
   "metadata": {},
   "source": [
    "### 2.2. Create a torch dataloader \n",
    "\n",
    "Let's start by defining a datasets we want to use. \n",
    "\n",
    "We'll use `parquet` data that was generated using the same preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40392506-334f-4b05-9bb0-f2815daff428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParquetDataset(Dataset):\n",
    "    \"\"\"Minimal PyTorch Dataset for data stored in parquet.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        s3_path: str,\n",
    "        dtype: np.dtype = np.float16,\n",
    "    ):\n",
    "        self.columns = [\"image_latents_256\", \"caption_latents\"]\n",
    "        loaded_df = pd.read_parquet(\n",
    "            s3_path,\n",
    "            columns=[\"image_latents_256\", \"caption_latents\"],\n",
    "            filesystem=s3fs.S3FileSystem(anon=False),\n",
    "        )\n",
    "        loaded_df[\"image_latents_256\"] = loaded_df[\"image_latents_256\"].apply(\n",
    "            lambda x: x.reshape(4, 32, 32).astype(dtype)\n",
    "        )\n",
    "        loaded_df[\"caption_latents\"] = loaded_df[\"caption_latents\"].apply(\n",
    "            lambda x: x.reshape(77, 1024).astype(dtype)\n",
    "        )\n",
    "        self.df = loaded_df\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Convert to tensors so default collate stacks them automatically.\n",
    "        return {col: torch.as_tensor(row[col]) for col in self.columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c4905",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Note** this Dataset implementation is very simple and loads the entire dataset into memory. Not recommended for large datasets.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b56c12",
   "metadata": {},
   "source": [
    "Let's proceed to build our custom map-style dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifcat_storage_path = os.environ[\"ANYSCALE_ARTIFACT_STORAGE\"]\n",
    "s3_path = f\"{artifcat_storage_path}/stable-diffusion/256/6_000108_000000.parquet\"\n",
    "dataset = ParquetDataset(s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f26cf6",
   "metadata": {},
   "source": [
    "We construct a torch dataloader that will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc36a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Adjust based on memory constraints\n",
    "    shuffle=True,\n",
    "    num_workers=2,  # Adjust based on system's CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd9333",
   "metadata": {},
   "source": [
    "We can inspect the batches to verify their shape and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    print(batch[\"image_latents_256\"].shape, batch[\"caption_latents\"].shape)\n",
    "    print(batch[\"caption_latents\"].dtype, batch[\"image_latents_256\"].dtype)\n",
    "    break  # Remove this to process the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d8b57-ded8-42c2-84a1-60e8102d17ba",
   "metadata": {},
   "source": [
    "### 2.3 Define a stable diffusion model\n",
    "\n",
    "This \"standard\" LightningModule does not explicitly refer to Ray or Ray Train, which makes migrating workloads easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7e93d-4e8f-4053-86d0-0fd5f44b5f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_unet_model_config = {\n",
    "    \"_class_name\": \"UNet2DConditionModel\",\n",
    "    \"_diffusers_version\": \"0.2.2\",\n",
    "    \"act_fn\": \"silu\",\n",
    "    \"attention_head_dim\": 8,\n",
    "    \"block_out_channels\": [160, 320, 640, 640],\n",
    "    \"center_input_sample\": False,\n",
    "    \"cross_attention_dim\": 1024,\n",
    "    \"down_block_types\": [\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ],\n",
    "    \"downsample_padding\": 1,\n",
    "    \"flip_sin_to_cos\": True,\n",
    "    \"freq_shift\": 0,\n",
    "    \"in_channels\": 4,\n",
    "    \"layers_per_block\": 2,\n",
    "    \"mid_block_scale_factor\": 1,\n",
    "    \"norm_eps\": 1e-05,\n",
    "    \"norm_num_groups\": 32,\n",
    "    \"out_channels\": 4,\n",
    "    \"sample_size\": 64,\n",
    "    \"up_block_types\": [\n",
    "        \"UpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "class StableDiffusion(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float,\n",
    "        resolution: int,\n",
    "        weight_decay: float,\n",
    "        num_warmup_steps: int,\n",
    "        model_name: str,\n",
    "    ) -> None:\n",
    "        self.lr = lr\n",
    "        self.resolution = resolution\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # Initialize U-Net.\n",
    "        # model_config = PretrainedConfig.get_config_dict(model_name, subfolder=\"unet\")[0]\n",
    "        model_config = small_unet_model_config\n",
    "        self.unet = UNet2DConditionModel(**model_config)\n",
    "        # Define the training noise scheduler.\n",
    "        self.noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "            model_name, subfolder=\"scheduler\"\n",
    "        )\n",
    "        # Setup loss function.\n",
    "        self.loss_fn = F.mse_loss\n",
    "        self.current_training_steps = 0\n",
    "\n",
    "    def on_fit_start(self) -> None:\n",
    "        \"\"\"Move cumprod tensor to GPU in advance to avoid data movement on each step.\"\"\"\n",
    "        self.noise_scheduler.alphas_cumprod = self.noise_scheduler.alphas_cumprod.to(\n",
    "            get_device()\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, batch: dict[str, torch.Tensor]\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        # Extract inputs.\n",
    "        latents = batch[\"image_latents_256\"]\n",
    "        conditioning = batch[\"caption_latents\"]\n",
    "        # Sample the diffusion timesteps.\n",
    "        timesteps = self._sample_timesteps(latents)\n",
    "        # Add noise to the inputs (forward diffusion).\n",
    "        noise = torch.randn_like(latents)\n",
    "        noised_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        # Forward through the model.\n",
    "        outputs = self.unet(noised_latents, timesteps, conditioning)[\"sample\"]\n",
    "        return outputs, noise\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: dict[str, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Training step of the model.\"\"\"\n",
    "        outputs, targets = self.forward(batch)\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        self.log(\n",
    "            \"train/loss_mse\", loss.item(), prog_bar=False, on_step=True, sync_dist=False\n",
    "        )\n",
    "        self.current_training_steps += 1\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        \"\"\"Configure the optimizer and learning rate scheduler.\"\"\"\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.trainer.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        # Set a large training step here to keep lr constant after warm-up.\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.num_warmup_steps,\n",
    "            num_training_steps=100000000000,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _sample_timesteps(self, latents: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.randint(\n",
    "            0, len(self.noise_scheduler), (latents.shape[0],), device=latents.device\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822926ee-bd2b-4977-8ffd-725881160da3",
   "metadata": {},
   "source": [
    "### 2.4. Define a PyTorch Lightning training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109df3ab",
   "metadata": {},
   "source": [
    "Here is a training loop that is specific to PyTorch Lightning.\n",
    "\n",
    "It performs the following steps:\n",
    "1. **Model Initialization:**\n",
    "   - Instantiate the diffusion model.\n",
    "2. **Trainer Setup:**\n",
    "   - Instantiate the Lightning Trainer with a `DDPStrategy` to perform data parallel training.\n",
    "3. **Training Execution:**\n",
    "   - Run the trainer using the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightning_training_loop(\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    storage_path: str,\n",
    "    model_name: str = \"stabilityai/stable-diffusion-2-base\",\n",
    "    resolution: int = 256,\n",
    "    lr: float = 1e-4,\n",
    "    max_epochs: int = 1,\n",
    "    num_warmup_steps: int = 10_000,\n",
    "    weight_decay: float = 1e-2,\n",
    ") -> None:\n",
    "    # 1. Initialize the model\n",
    "    model = StableDiffusion(\n",
    "        model_name=model_name,\n",
    "        resolution=resolution,\n",
    "        lr=lr,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    # 2. Initialize the Lightning Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=\"auto\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        max_epochs=max_epochs,\n",
    "        default_root_dir=storage_path,\n",
    "        log_every_n_steps=8,\n",
    "    )\n",
    "\n",
    "    # 3. Run the trainer\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423b358",
   "metadata": {},
   "source": [
    "Here is how we would run the lightning training loop on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5575eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = \"/mnt/local_storage/lightning/stable-diffusion-pretraining/\"\n",
    "lightning_training_loop(train_loader=data_loader, storage_path=storage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a09b6",
   "metadata": {},
   "source": [
    "Let's inspect the storage path to see what files were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree {storage_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c2063-8988-4687-b1d4-b24e0a5a3d66",
   "metadata": {},
   "source": [
    "## 3. Distributed Training with Ray Train and PyTorch Lightning\n",
    "\n",
    "Let's consider the case where we have a very large dataset of images that would take a long time to train on a single GPU. We would now like to scale this training job to run on multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49711c5",
   "metadata": {},
   "source": [
    "### 3.1 Distributed Data Parallel Training\n",
    "Here is a diagram showing the standard distributed data parallel training loop.\n",
    "\n",
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_v4.png\" width=\"900px\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Schematic overview of DistributedDataParallel (DDP) training: (1) the model is replicated from the <code>GPU rank 0</code> to all other workers; (2) each worker receives a shard of the dataset and processes a mini-batch; (3) during the backward pass, gradients are averaged across GPUs; (4) checkpoint and metrics from rank 0 GPU are saved to the persistent storage.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f8204",
   "metadata": {},
   "source": [
    "### 3.2 Ray Train Migration\n",
    "\n",
    "Here are the changes we need to make to the training loop to migrate it to Ray Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42be9f-fad5-4f3f-8a1c-5812c5573eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop_per_worker(\n",
    "    config: dict,  # Update the function signature to comply with Ray Train\n",
    "):\n",
    "    # Note lightning prepare the dataloader (adding a distriuted sampler) for us\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config[\"batch_size_per_worker\"],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    # Same model initialization as vanilla lightning\n",
    "    model = StableDiffusion(\n",
    "        lr=config[\"lr\"],\n",
    "        resolution=config[\"resolution\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        num_warmup_steps=config[\"num_warmup_steps\"],\n",
    "        model_name=config[\"model_name\"],\n",
    "    )\n",
    "\n",
    "    # Same trainer setup as vanilla lightning except we add Ray Train specific arguments\n",
    "    trainer = pl.Trainer(\n",
    "        max_steps=config[\"max_steps\"],\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        accelerator=\"gpu\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        devices=\"auto\",  # Set devices to \"auto\" to use all available GPUs\n",
    "        strategy=RayDDPStrategy(),  # Use RayDDPStrategy for distributed data parallel training\n",
    "        plugins=[\n",
    "            RayLightningEnvironment()\n",
    "        ],  # Use RayLightningEnvironment to run the Lightning Trainer\n",
    "        callbacks=[\n",
    "            RayTrainReportCallback()\n",
    "        ],  # Use RayTrainReportCallback to report metrics and checkpoints\n",
    "        enable_checkpointing=False,  # Disable lightning checkpointing\n",
    "    )\n",
    "\n",
    "    # 4. Same as vanilla lightning\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5328b5",
   "metadata": {},
   "source": [
    "Here is the same diagram as before but with the Ray Train specific components highlighted.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_lightning_annotated_no_data_v2.png\" width=900 loading=\"lazy\">\n",
    "\n",
    "We made use of:\n",
    "- `ray.train.get_dataset_shard(\"train\")` to get the training dataset shard.\n",
    "- `RayDDPStrategy` to perform distributed data parallel training.\n",
    "- `RayLightningEnvironment` to run the Lightning Trainer.\n",
    "- `RayTrainReportCallback` to report metrics and checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af7dd4",
   "metadata": {},
   "source": [
    "### 3.3. Configure scale and GPUs\n",
    "Outside of our training function, we create a `ScalingConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a327630",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ray.train.ScalingConfig(num_workers=2, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14744be2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html#ray-train-scalingconfig\" target=\"_blank\">ScalingConfig</a> configures:\n",
    "\n",
    "<ul>\n",
    "  <li><code>num_workers</code>: The number of distributed training worker processes.</li>\n",
    "  <li><code>use_gpu</code>: Whether each worker should use a GPU (or CPU).</li>\n",
    "</ul>\n",
    "\n",
    "See docs on configuring <a href=\"https://docs.ray.io/en/latest/train/user-guides/using-gpus.html\" target=\"_blank\">scale and GPUs</a> for more details.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74098a02",
   "metadata": {},
   "source": [
    "#### 3.3.1. Note on Ray Train key concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77815f",
   "metadata": {},
   "source": [
    "Ray Train is built around [four key concepts](https://docs.ray.io/en/latest/train/overview.html):\n",
    "1. **Training function**: (implemented above `train_loop_ray_train`): A Python function that contains your model training logic.\n",
    "1. **Worker**: A process that runs the training function.\n",
    "1. **Scaling config**: specifices number of workers and compute resources (CPUs or GPUs, TPUs).\n",
    "1. **Trainer**: A Python class (Ray Actor) that ties together the training function, workers, and scaling configuration to execute a distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288fe598",
   "metadata": {},
   "source": [
    "|<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"700px\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|High-level architecture of how Ray Train|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d8ccb-30c2-4491-a381-63ac3330bc2e",
   "metadata": {},
   "source": [
    "### 3.4 Create and fit a Ray Train TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b32a89",
   "metadata": {},
   "source": [
    "We first specify the run configuration to tell Ray Train where to store the checkpoints and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = \"/mnt/cluster_storage/\"\n",
    "experiment_name = \"stable-diffusion-pretraining\"\n",
    "\n",
    "run_config = ray.train.RunConfig(name=experiment_name, storage_path=storage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d2f69",
   "metadata": {},
   "source": [
    "Now we can create our Ray Train `TorchTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop_config = {\n",
    "    \"batch_size_per_worker\": 8,\n",
    "    \"prefetch_batches\": 1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"num_warmup_steps\": 10_000,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_steps\": 550_000,\n",
    "    \"max_epochs\": 1,\n",
    "    \"resolution\": 256,\n",
    "    \"model_name\": \"stabilityai/stable-diffusion-2-base\",\n",
    "}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81045b35",
   "metadata": {},
   "source": [
    "We call `.fit()` to start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e338b7",
   "metadata": {},
   "source": [
    "### 3.5. Access the training results\n",
    "\n",
    "We can check the metrics produced by the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.metrics_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916616d",
   "metadata": {},
   "source": [
    "### 3.6. Load the checkpointed model to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89719e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = result.checkpoint\n",
    "with ckpt.as_directory() as ckpt_dir:\n",
    "    ckpt_path = os.path.join(ckpt_dir, \"checkpoint.ckpt\")\n",
    "    loaded_model_ray_train = StableDiffusion.load_from_checkpoint(\n",
    "        checkpoint_path=ckpt_path,\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "        weights_only=True,\n",
    "    )\n",
    "    loaded_model_ray_train.eval()\n",
    "\n",
    "loaded_model_ray_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ee2ff",
   "metadata": {},
   "source": [
    "### 3.7. Activity: Run the distributed training with more workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797a47f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "1. Update the scaling configuration to make use of 4 GPU workers\n",
    "2. Run the trainer using the same hypeparameters\n",
    "\n",
    "Use the following code snippets to guide you:\n",
    "\n",
    "```python\n",
    "# Hint: Update the scaling configuration\n",
    "scaling_config = ...\n",
    "\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    train_loop_config=train_loop_config,\n",
    ")\n",
    "result = trainer.fit()\n",
    "result.metrics_dataframe\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80e92c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary> Click here to see the solution </summary>\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(num_workers=4, use_gpu=True)\n",
    "\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    train_loop_config=train_loop_config,\n",
    ")\n",
    "result = trainer.fit()\n",
    "result.metrics_dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49befe33",
   "metadata": {},
   "source": [
    "## 4. Ray Train in Production\n",
    "\n",
    "Here are some use-cases of using Ray Train in production:\n",
    "1. Canva uses Ray Train + Ray Data to cut down Stable Diffusion training costs by 3.7x. Read this [Anyscale blog post here](https://www.anyscale.com/blog/scalable-and-cost-efficient-stable-diffusion-pre-training-with-ray) and the [Canva  case study here](https://www.anyscale.com/resources/case-study/how-canva-built-a-modern-ai-platform-using-anyscale)\n",
    "2. Anyscale uses Ray Train + Deepspeed to finetune language models. Read more [here](https://github.com/ray-project/ray/tree/master/doc/source/templates/04_finetuning_llms_with_deepspeed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ad2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup\n",
    "!rm -rf /mnt/cluster_storage/stable-diffusion-pretraining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
