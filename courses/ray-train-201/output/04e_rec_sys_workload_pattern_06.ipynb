{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Inference: recommend top-N items for a user  \n",
    "To demonstrate inference, generate top-10 item recommendations for a randomly selected user. Please note that the following method is meant for this small example, and **Ray Data** should be used for inference at scale.\n",
    "\n",
    "First, reload the original `ratings.csv` and rebuild the user and item ID mappings used during training. Then, load the latest model checkpoint and restore the trained embedding weights. If you trained the model with DDP, strip the `'module.'` prefix from checkpoint keys.\n",
    "\n",
    "Next, select a user, compute their embedding, and take the dot product against all item embeddings to produce predicted scores. Finally, extract the top-N items with the highest scores and print their IDs and associated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Inference: recommend top-N items for a user\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 1: Reload original ratings CSV and mappings\n",
    "# ---------------------------------------------\n",
    "df = pd.read_csv(\"/mnt/cluster_storage/rec_sys_tutorial/raw/ratings.csv\")\n",
    "\n",
    "# Recompute ID mappings (same as during preprocessing)\n",
    "unique_users = sorted(df[\"user_id\"].unique())\n",
    "unique_items = sorted(df[\"item_id\"].unique())\n",
    "\n",
    "user2idx = {uid: j for j, uid in enumerate(unique_users)}\n",
    "item2idx = {iid: j for j, iid in enumerate(unique_items)}\n",
    "idx2item = {v: k for k, v in item2idx.items()}\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 2: Load model from checkpoint\n",
    "# ---------------------------------------------\n",
    "model = MatrixFactorizationModel(\n",
    "    num_users=len(user2idx),\n",
    "    num_items=len(item2idx),\n",
    "    embedding_dim=train_config[\"embedding_dim\"]\n",
    ")\n",
    "\n",
    "with result.checkpoint.as_directory() as ckpt_dir:\n",
    "    state_dict = torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\")\n",
    "\n",
    "    # Remove 'module.' prefix if using DDP-trained model\n",
    "    if any(k.startswith(\"module.\") for k in state_dict):\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 3: Select a user and generate recommendations\n",
    "# ---------------------------------------------\n",
    "# Choose a random user from the original dataset\n",
    "original_user_id = df[\"user_id\"].sample(1).iloc[0]\n",
    "user_idx = user2idx[original_user_id]\n",
    "\n",
    "print(f\"Generating recommendations for user_id={original_user_id} (internal idx={user_idx})\")\n",
    "\n",
    "# Compute scores for all items for this user\n",
    "with torch.no_grad():\n",
    "    user_vector = model.user_embedding(torch.tensor([user_idx]))           # [1, D]\n",
    "    item_vectors = model.item_embedding.weight                             # [num_items, D]\n",
    "    scores = torch.matmul(user_vector, item_vectors.T).squeeze(0)          # [num_items]\n",
    "\n",
    "    topk = torch.topk(scores, k=10)\n",
    "    top_item_ids = [idx2item[j.item()] for j in topk.indices]\n",
    "    top_scores = topk.values.tolist()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 4: Print top-N recommendations\n",
    "# ---------------------------------------------\n",
    "print(\"\\nTop 10 Recommended Item IDs:\")\n",
    "for i, (item_id, score) in enumerate(zip(top_item_ids, top_scores), 1):\n",
    "    print(f\"{i:2d}. Item ID: {item_id} | Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Join top-N item IDs with movie titles  \n",
    "To make your recommendations more interpretable, join the top-10 recommended `item_id`s with movie titles from the original `u.item` metadata file.\n",
    "\n",
    "Load only the relevant columns\u2014`item_id` and `title`\u2014from `u.item`, then merge them with the top-N predictions you computed in the previous step. The result is a user-friendly list of movie titles with associated predicted scores, rather than raw item IDs.\n",
    "\n",
    "This small addition makes the model outputs easier to understand and more useful for downstream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Join top-N item IDs with movie titles from u.item\n",
    "\n",
    "item_metadata = pd.read_csv(\n",
    "    \"/mnt/cluster_storage/rec_sys_tutorial/ml-100k/u.item\",\n",
    "    sep=\"|\",\n",
    "    encoding=\"latin-1\",\n",
    "    header=None,\n",
    "    usecols=[0, 1],  # Only item_id and title\n",
    "    names=[\"item_id\", \"title\"]\n",
    ")\n",
    "\n",
    "# Join with top-N items\n",
    "top_items_df = pd.DataFrame({\n",
    "    \"item_id\": top_item_ids,\n",
    "    \"score\": top_scores\n",
    "})\n",
    "\n",
    "merged = top_items_df.merge(item_metadata, on=\"item_id\", how=\"left\")\n",
    "\n",
    "print(\"\\nTop 10 Recommended Movies:\")\n",
    "for j, row in merged.iterrows():\n",
    "    print(f\"{j+1:2d}. {row['title']} | Score: {row['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Clean up shared storage  \n",
    "Reclaim cluster disk space by deleting the entire tutorial output directory.  \n",
    "Run this only when you\u2019re **sure** you don\u2019t need the checkpoints or metrics anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Cleanup -- delete checkpoints and metrics from model training\n",
    "\n",
    "TARGET_PATH = \"/mnt/cluster_storage/rec_sys_tutorial\"  # please note, that /mnt/cluster_storage/ only exists on Anyscale\n",
    "\n",
    "if os.path.exists(TARGET_PATH):\n",
    "    shutil.rmtree(TARGET_PATH)\n",
    "    print(f\"\u2705 Deleted everything under {TARGET_PATH}\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f Path does not exist: {TARGET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up and next steps\n",
    "\n",
    "In this tutorial, you used **Ray Train and Ray Data on Anyscale** to scale a full matrix factorization recommendation system, end-to-end, from a raw CSV to multi-GPU distributed training and personalized top-N item recommendations.\n",
    "\n",
    "You should now feel confident:\n",
    "\n",
    "* Using **Ray Data** to preprocess, encode, and shard large tabular datasets  \n",
    "* Streaming data into PyTorch with `iter_torch_batches()` for efficient training  \n",
    "* Scaling matrix factorization across multiple GPUs with **Ray Train\u2019s `TorchTrainer`**  \n",
    "* Saving and resuming training with **Ray Checkpoints**  \n",
    "* Running multi-node, fault-tolerant jobs without touching orchestration code  \n",
    "* Performing post-training inference using Ray-restored model checkpoints and learned user and item embeddings\n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?\n",
    "\n",
    "The following are a few directions you can explore to extend or adapt this workload:\n",
    "\n",
    "1. **Ranking metrics and evaluation**  \n",
    "   * Add metrics like **Root Mean Squared Error (RMSE)**, **Normalized Discounted Cumulative Gain (NDCG)**, or **Hit@K** to evaluate recommendation quality.  \n",
    "   * Filter out already-rated items during inference to measure novelty.\n",
    "\n",
    "2. **Two-tower and deep models**  \n",
    "   * Replace dot product with a **two-tower neural model** or a **deep MLP**.  \n",
    "   * Add side features (for example, timestamp, genre) into each tower for better personalization.\n",
    "\n",
    "3. **Recommendation personalization**  \n",
    "   * Store and cache user embeddings after training.  \n",
    "   * Run lightweight inference tasks to generate recommendations in real-time.\n",
    "\n",
    "4. **Content-based or hybrid models**  \n",
    "   * Join movie metadata (genres, tags) and build a hybrid collaborative\u2013content model.  \n",
    "   * Embed titles or genres using pre-trained language models.\n",
    "\n",
    "5. **Hyperparameter optimization**  \n",
    "   * Use **Ray Tune** to sweep embedding sizes, learning rates, or regularization.  \n",
    "   * Track performance over epochs and checkpoint the best models automatically.\n",
    "\n",
    "6. **Data scaling**  \n",
    "   * Switch from MovieLens 100K to 1M or 10M as Ray Data handles it seamlessly.  \n",
    "   * Save and load from cloud object storage (S3, GCS) for real-world deployments.\n",
    "\n",
    "7. **Production inference**  \n",
    "   * Wrap the recommendation system into a **Ray Serve** endpoint for serving top-N results using **Ray Data** based inference.  \n",
    "   * Build a simple demo that recommends movies to live users.\n",
    "\n",
    "8. **End-to-end MLOps**  \n",
    "   * Register the best model with MLflow or Weights & Biases.  \n",
    "   * Package the training job as a Ray job and schedule it with Anyscale.\n",
    "\n",
    "9. **Multi-tenant recommendation systems**  \n",
    "   * Extend this to support **multiple audiences** or contexts (for example, multi-country, A/B groups).  \n",
    "   * Train and serve context-aware models in parallel using Ray.\n",
    "\n",
    "This pattern gives you a solid foundation for scaling recommendation workloads across real datasets and real infrastructure\u2014without rewriting your model or managing your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}