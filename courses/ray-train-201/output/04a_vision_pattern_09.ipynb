{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Batch inference with Ray Data  \n",
    "\n",
    "Define a **stateful, GPU-backed batch inference pipeline** using Ray Data.  \n",
    "Each actor loads the model **once per GPU**, keeps it in memory, and performs inference on incoming batches in parallel.  \n",
    "This pattern scales efficiently across multiple GPUs and avoids redundant model loading for every prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Batch inference with Ray Data (force GPU actors if available on the cluster)\n",
    "\n",
    "import ray.data as rdata\n",
    "\n",
    "class ImageBatchPredictor:\n",
    "    \"\"\"Stateful per-actor batch predictor that keeps the model in memory.\"\"\"\n",
    "    def __init__(self, checkpoint_path: str):\n",
    "        # Pick the best available device on the ACTOR (worker), not the driver.\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # === Load model & weights once per actor ===\n",
    "        model = resnet18(num_classes=101)\n",
    "        checkpoint = Checkpoint.from_directory(checkpoint_path)\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            state_dict = torch.load(\n",
    "                os.path.join(ckpt_dir, \"model.pt\"),\n",
    "                map_location=self.device,\n",
    "            )\n",
    "            # Strip DDP \"module.\" prefix if present\n",
    "            state_dict = {k.replace(\"module.\", \"\", 1): v for k, v in state_dict.items()}\n",
    "            model.load_state_dict(state_dict)\n",
    "\n",
    "        self.model = model.eval().to(self.device)\n",
    "        self.transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "        ])\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"batch: Pandas DataFrame with columns ['image_bytes', 'label']\"\"\"\n",
    "        imgs = []\n",
    "        for b in batch[\"image_bytes\"]:\n",
    "            img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "            imgs.append(self.transform(img).numpy())  # (C,H,W) as numpy\n",
    "        x = torch.from_numpy(np.stack(imgs, axis=0)).to(self.device)  # (N,C,H,W)\n",
    "\n",
    "        logits = self.model(x)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        out = batch.copy()\n",
    "        out[\"predicted_label\"] = preds.astype(int)\n",
    "        return out[[\"predicted_label\", \"label\"]]\n",
    "\n",
    "def build_inference_dataset(\n",
    "    checkpoint_path: str,\n",
    "    parquet_path: str,\n",
    "    *,\n",
    "    num_actors: int = 1,\n",
    "    batch_size: int = 64,\n",
    "    use_gpu_actors: bool = True,   # <\u2014 default to GPU actors on the cluster\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a Ray Dataset pipeline that performs batch inference using\n",
    "    stateful per-actor model loading. By default, requests 1 GPU per actor\n",
    "    so each actor runs on a GPU worker (driver may have no GPU).\n",
    "    \"\"\"\n",
    "    ds = rdata.read_parquet(parquet_path, columns=[\"image_bytes\", \"label\"])\n",
    "\n",
    "    pred_ds = ds.map_batches(\n",
    "        ImageBatchPredictor,                     # pass the CLASS (stateful actors)\n",
    "        fn_constructor_args=(checkpoint_path,),  # ctor args for each actor\n",
    "        batch_size=batch_size,\n",
    "        batch_format=\"pandas\",\n",
    "        concurrency=num_actors,                  # number of actor workers\n",
    "        num_gpus=1 if use_gpu_actors else 0,     # <\u2014 force GPU placement on workers\n",
    "    )\n",
    "    return pred_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Run and visualize Ray Data inference  \n",
    "\n",
    "Use the best checkpoint to run **Ray Data Inference** on a validation sample.  \n",
    "The model loads once per GPU actor, predictions are batched and parallelized, and the result is visualized alongside the ground-truth label for quick qualitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Perform inference with Ray Data using the best checkpoint\n",
    "\n",
    "checkpoint_root = \"/mnt/cluster_storage/food101_lite/results/food101_ft_resume\"\n",
    "\n",
    "checkpoint_dirs = sorted(\n",
    "    [\n",
    "        d for d in os.listdir(checkpoint_root)\n",
    "        if d.startswith(\"checkpoint_\") and os.path.isdir(os.path.join(checkpoint_root, d))\n",
    "    ],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "if not checkpoint_dirs:\n",
    "    raise FileNotFoundError(\"No checkpoint directories found.\")\n",
    "\n",
    "# Use the best checkpoint from the training result\n",
    "with result.checkpoint.as_directory() as ckpt_dir:\n",
    "    print(\"Best checkpoint contents:\", os.listdir(ckpt_dir))\n",
    "    best_ckpt_path = ckpt_dir\n",
    "\n",
    "parquet_path = \"/mnt/cluster_storage/food101_lite/val.parquet\"\n",
    "\n",
    "# Which item to visualize\n",
    "idx = 2\n",
    "\n",
    "# Build a Ray Data inference pipeline (model is loaded once per GPU actor)\n",
    "pred_ds = build_inference_dataset(\n",
    "    checkpoint_path=best_ckpt_path,\n",
    "    parquet_path=parquet_path,\n",
    "    num_actors=1,       # adjust to scale out\n",
    "    batch_size=64,      # adjust for throughput\n",
    ")\n",
    "\n",
    "# Materialize predictions up to the desired index and grab the row\n",
    "pred_rows = pred_ds.take(idx + 1)\n",
    "inference_row = pred_rows[-1]  # {\"predicted_label\": ..., \"label\": ...}\n",
    "print(inference_row)\n",
    "\n",
    "# Load label map from Hugging Face (for pretty titles)\n",
    "ds_tmp = load_dataset(\"food101\", split=\"train[:1%]\")  # just to get label names\n",
    "label_names = ds_tmp.features[\"label\"].names\n",
    "\n",
    "# Load the raw image locally for visualization\n",
    "dataset = Food101Dataset(parquet_path, transform=None)\n",
    "img, _ = dataset[idx]\n",
    "\n",
    "# Plot the image with predicted and true labels\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\n",
    "    f\"Pred: {label_names[int(inference_row['predicted_label'])]}\\n\"\n",
    "    f\"True: {label_names[int(inference_row['label'])]}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Clean up  \n",
    "Finally, tidy up by deleting temporary checkpoint folders, the metrics CSV, and any intermediate result directories. Clearing out old artifacts frees disk space and leaves your workspace clean for whatever comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Cleanup---delete checkpoints and metrics from model training\n",
    "\n",
    "# Base directory\n",
    "BASE_DIR = \"/mnt/cluster_storage/food101_lite\"\n",
    "\n",
    "# Paths to clean\n",
    "paths_to_delete = [\n",
    "    os.path.join(BASE_DIR, \"tmp_checkpoints\"),           # custom checkpoints\n",
    "    os.path.join(BASE_DIR, \"results\", \"history.csv\"),    # metrics history file\n",
    "    os.path.join(BASE_DIR, \"results\", \"food101_ft_resume\"),  # ray trainer run dir\n",
    "    os.path.join(BASE_DIR, \"results\", \"food101_ft_run\"),\n",
    "    os.path.join(BASE_DIR, \"results\", \"food101_single_run\"),\n",
    "]\n",
    "\n",
    "# Delete each path if it exists\n",
    "for path in paths_to_delete:\n",
    "    if os.path.exists(path):\n",
    "        if os.path.isfile(path):\n",
    "            os.remove(path)\n",
    "            print(f\"Deleted file: {path}\")\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"Deleted directory: {path}\")\n",
    "    else:\n",
    "        print(f\"Not found (skipped): {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up and next steps\n",
    "\n",
    "You've taken a realistic computer-vision workload, from raw images all the way to distributed training and GPU inference, and run it on Ray Train with zero boilerplate around GPUs, data parallelism, or fault-tolerance. You should now feel comfortable:\n",
    "\n",
    "* Using **Ray Train\u2019s TorchTrainer** to scale PyTorch training across multiple GPUs and nodes with minimal code changes  \n",
    "* Wrapping models and data loaders with **`prepare_model()`** and **`prepare_data_loader()`** to enable Ray-managed device placement and distributed execution  \n",
    "* Sharding data across workers using and coordinating training epochs across Ray workers  \n",
    "* Configuring **automatic checkpointing and failure recovery** using Ray Train\u2019s built-in `Checkpoint`, `RunConfig`, and `FailureConfig` APIs  \n",
    "* Running **Ray Data based Inference** for distributed inference, showing how to serve and scale model predictions across a Ray cluster  \n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?\n",
    "\n",
    "Below are a few directions you might explore to adapt or extend the pattern:\n",
    "\n",
    "1. **Larger or custom datasets**  \n",
    "   * Swap in the full 75 k-image Food-101 split\u2014or your own dataset in any storage backend (S3, GCS, Azure Blob).  \n",
    "   * Add multi-file Parquet sharding and let each worker read a different shard.\n",
    "\n",
    "2. **Model architectures**  \n",
    "   * Drop in Vision Transformers (`vit_b_16`, `vit_l_32`) or ConvNeXt; the prepare helpers work exactly the same.  \n",
    "   * Experiment with transfer learning versus training from scratch.\n",
    "\n",
    "3. **Mixed precision and performance tuning**  \n",
    "   * Enable automatic mixed precision (`torch.cuda.amp`) or bfloat16 to speed up training and save memory.  \n",
    "   * Profile data-loading throughput and play with `num_workers`, prefetching, and caching.\n",
    "\n",
    "4. **Hyperparameter sweeps**  \n",
    "   * Wrap the training loop in **Ray Tune** to search over learning rates, augmentations, or optimizers.  \n",
    "   * Use Ray\u2019s integrated reporting to schedule early stopping.\n",
    "\n",
    "5. **Data augmentation pipelines**  \n",
    "   * Integrate additional transforms inside the dataset class for image augmentation.  \n",
    "   * Compare CPU versus GPU-side augmentations for throughput.\n",
    "\n",
    "6. **Distributed validation and metrics**  \n",
    "   * Replace your simple accuracy printout with more advanced metrics (F1, top-5 accuracy, confusion matrices).  \n",
    "\n",
    "7. **Model serving**  \n",
    "   * Convert the remote inference helper into a **Ray Serve** deployment for low-latency online predictions.  \n",
    "   * Auto-scale replicas based on request volume.\n",
    "\n",
    "8. **End-to-end MLOps**  \n",
    "   * Register checkpoints in a model registry (for example, MLflow, Weights & Biases, or Ray\u2019s built-in MLflow integration).  \n",
    "   * Schedule the notebook as a Ray Job or CI/CD pipeline for regular retraining runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}