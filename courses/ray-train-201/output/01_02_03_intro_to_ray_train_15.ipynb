{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 \u00b7 Define Training Loop with Ray Data  \n",
    "\n",
    "Here we reimplement the training loop, but this time using **Ray Data** instead of a PyTorch `DataLoader`.  \n",
    "\n",
    "Key differences from the previous version:  \n",
    "- **Data loader** \u2192 Built with `build_data_loader_ray_train_ray_data()`, which streams batches from a Ray Dataset shard (details in the following block).  \n",
    "- **Batching** \u2192 Still split by `global_batch_size // world_size`, but batches are now **dictionaries** with keys `\"image\"` and `\"label\"`.  \n",
    "- **No device management needed** \u2192 Ray Data automatically moves batches to the correct device, so we no longer call `sampler.set_epoch()` or `to(\"cuda\")`.  \n",
    "\n",
    "The rest of the loop (forward pass, loss computation, backward pass, optimizer step, metric logging, and checkpointing) stays the same.  \n",
    "\n",
    "This pattern shows how seamlessly **Ray Data integrates with Ray Train**, replacing `DataLoader` while keeping the training logic identical.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Training loop using Ray Data\n",
    "\n",
    "def train_loop_ray_train_ray_data(config: dict):\n",
    "    # Same as before: define loss, model, optimizer\n",
    "    criterion = CrossEntropyLoss()\n",
    "    model = load_model_ray_train()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Different: build data loader from Ray Data instead of PyTorch DataLoader\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    batch_size = global_batch_size // ray.train.get_context().get_world_size()\n",
    "    data_loader = build_data_loader_ray_train_ray_data(batch_size=batch_size) \n",
    "    \n",
    "    # Same: loop over epochs\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        # Different: no sampler.set_epoch(), Ray Data handles shuffling internally\n",
    "\n",
    "        # Different: batches are dicts {\"image\": ..., \"label\": ...} not tuples\n",
    "        for batch in data_loader: \n",
    "            outputs = model(batch[\"image\"])\n",
    "            loss = criterion(outputs, batch[\"label\"])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "        # Same: report metrics and save checkpoint each epoch\n",
    "        metrics = print_metrics_ray_train(loss, epoch)\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}