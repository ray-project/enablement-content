{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08 \u00b7 Define Ray Train Loop (with Validation, Logging, and Checkpointing)  \n",
    "Define the `train_loop_per_worker`, the core function Ray executes on each worker. This loop handles everything from dataset loading and model training to validation, logging, and checkpointing.\n",
    "\n",
    "Each worker receives its own shard of the training and validation sets with `get_dataset_shard`. Use `iter_torch_batches` to stream batches directly into PyTorch.\n",
    "\n",
    "For each epoch:\n",
    "- Compute the average training loss across all batches.\n",
    "- Evaluate the model on the validation set and compute the average validation loss.\n",
    "- On rank 0 only, save a checkpoint (model weights + epoch metadata) and append the losses to a shared JSON log.\n",
    "\n",
    "Finally, call `ray.train.report` to expose metrics and checkpoints to the Ray controller. This enables fault tolerance, auto-resume, and metrics tracking with zero additional setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Define Ray Train Loop (with Val Loss + Checkpointing + Logging)\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "\n",
    "    # Get dataset shards for this worker\n",
    "    train_ds = get_dataset_shard(\"train\")\n",
    "    val_ds   = get_dataset_shard(\"val\")\n",
    "    train_loader = train_ds.iter_torch_batches(batch_size=512, dtypes=torch.float32)\n",
    "    val_loader   = val_ds.iter_torch_batches(batch_size=512, dtypes=torch.float32)\n",
    "\n",
    "    # Create model and optimizer\n",
    "    model = MatrixFactorizationModel(\n",
    "        num_users=config[\"num_users\"],\n",
    "        num_items=config[\"num_items\"],\n",
    "        embedding_dim=config.get(\"embedding_dim\", 64)\n",
    "    )\n",
    "    model = prepare_model(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 1e-3))\n",
    "\n",
    "    # Paths for checkpointing and logging\n",
    "    CKPT_DIR = \"/mnt/cluster_storage/rec_sys_tutorial/checkpoints\"\n",
    "    LOG_PATH = \"/mnt/cluster_storage/rec_sys_tutorial/epoch_metrics.json\"\n",
    "    os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))  # Worker rank\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Resume from checkpoint if available\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as ckpt_dir:\n",
    "            model.load_state_dict(torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\"))\n",
    "            start_epoch = torch.load(os.path.join(ckpt_dir, \"meta.pt\")).get(\"epoch\", 0) + 1\n",
    "        if rank == 0:\n",
    "            print(f\"[Rank {rank}] \u2705 Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    # Clean up log file on first run (only if not resuming)\n",
    "    if rank == 0 and start_epoch == 0 and os.path.exists(LOG_PATH):\n",
    "        os.remove(LOG_PATH)\n",
    "\n",
    "    # ----------------- Training Loop ----------------- #\n",
    "    for epoch in range(start_epoch, config.get(\"epochs\", 5)):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        # Train over each batch\n",
    "        for batch in train_loader:\n",
    "            user = batch[\"user_idx\"].long()\n",
    "            item = batch[\"item_idx\"].long()\n",
    "            rating = batch[\"rating\"].float()\n",
    "\n",
    "            pred = model(user, item)\n",
    "            loss = F.mse_loss(pred, rating)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "\n",
    "        # ---------- Validation Pass ----------\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                user = batch[\"user_idx\"].long()\n",
    "                item = batch[\"item_idx\"].long()\n",
    "                rating = batch[\"rating\"].float()\n",
    "\n",
    "                pred = model(user, item)\n",
    "                loss = F.mse_loss(pred, rating)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "        # Log to stdout\n",
    "        print(f\"[Epoch {epoch}] Train MSE: {avg_train_loss:.4f} | Val MSE: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ---------- Save Checkpoint (Rank 0 Only) ----------\n",
    "        if rank == 0:\n",
    "            out_dir = os.path.join(CKPT_DIR, f\"epoch_{epoch}_{uuid.uuid4().hex}\")\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(out_dir, \"model.pt\"))\n",
    "            torch.save({\"epoch\": epoch}, os.path.join(out_dir, \"meta.pt\"))\n",
    "            ckpt_out = Checkpoint.from_directory(out_dir)\n",
    "        else:\n",
    "            ckpt_out = None\n",
    "\n",
    "        # ---------- Append Metrics to JSON Log (Rank 0) ----------\n",
    "        if rank == 0:\n",
    "            logs = []\n",
    "            if os.path.exists(LOG_PATH):\n",
    "                try:\n",
    "                    with open(LOG_PATH, \"r\") as f:\n",
    "                        logs = json.load(f)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"\u26a0\ufe0f  JSON log unreadable. Starting fresh.\")\n",
    "                    logs = []\n",
    "\n",
    "            logs.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss\n",
    "            })\n",
    "            with open(LOG_PATH, \"w\") as f:\n",
    "                json.dump(logs, f)\n",
    "\n",
    "        # ---------- Report to Ray Train ----------\n",
    "        report({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss\n",
    "        }, checkpoint=ckpt_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09 \u00b7 Launch Distributed Training with Ray Train  \n",
    "Now, launch distributed training using `TorchTrainer`, Ray Train\u2019s high-level orchestration interface. Provide it with:\n",
    "\n",
    "- Your custom `train_loop_per_worker` function\n",
    "- A `train_config` dictionary that specifies model dimensions, learning rate, and number of epochs\n",
    "- The sharded `train` and `val` Ray Datasets\n",
    "- A `ScalingConfig` that sets the number of workers and GPU usage\n",
    "\n",
    "Also, configure checkpointing and fault tolerance:\n",
    "- Ray keeps the 3 most recent checkpoints\n",
    "- Failed workers retry up to two times\n",
    "\n",
    "Calling `trainer.fit()` kicks off training across the cluster. If any workers fail or disconnect, Ray restarts them and resume from the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. Launch Distributed Training with Ray TorchTrainer\n",
    "\n",
    "# Define config params\n",
    "train_config = {\n",
    "    \"num_users\": df[\"user_idx\"].nunique(),\n",
    "    \"num_items\": df[\"item_idx\"].nunique(),\n",
    "    \"embedding_dim\": 64,\n",
    "    \"lr\": 1e-3,\n",
    "    \"epochs\": 20,\n",
    "}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=8,       # Increase as needed\n",
    "        use_gpu=True         # Set to True if training on GPUs\n",
    "    ),\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    run_config=RunConfig(\n",
    "        name=\"mf_ray_train\",\n",
    "        storage_path=\"/mnt/cluster_storage/rec_sys_tutorial/results\",\n",
    "        checkpoint_config=CheckpointConfig(num_to_keep=3),\n",
    "        failure_config=FailureConfig(max_failures=2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run distributed training\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 \u00b7 Plot Train and Validation Loss Curves  \n",
    "After training, load the logged epoch metrics from the shared JSON file and plot the train and validation loss curves.\n",
    "\n",
    "This visualization helps you evaluate model behavior across epochs, whether it\u2019s under-fitting, over-fitting, or converging steadily. You compute both curves using MSE (Mean Squared Error), which is the same loss function used during training.\n",
    "\n",
    "Plotting these curves also serves as a sanity check to ensure that checkpointing, logging, and training progressed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Plot Train/Val Loss Curves\n",
    "\n",
    "# Path to training metrics log\n",
    "LOG_PATH = \"/mnt/cluster_storage/rec_sys_tutorial/epoch_metrics.json\"\n",
    "\n",
    "# Load and convert to DataFrame\n",
    "with open(LOG_PATH, \"r\") as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df[\"train_loss\"] = pd.to_numeric(df[\"train_loss\"], errors=\"coerce\")\n",
    "df[\"val_loss\"] = pd.to_numeric(df[\"val_loss\"], errors=\"coerce\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(df[\"epoch\"], df[\"train_loss\"], marker=\"o\", label=\"Train\")\n",
    "plt.plot(df[\"epoch\"], df[\"val_loss\"], marker=\"o\", label=\"Val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Matrix Factorization - Loss per Epoch\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}