{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 \u00b7 Run inference and visualise prediction  \n",
    "Grab the most recent week of taxi data, run it through your trained model, and plot the predicted future demand against the actual values. This gives you a visual check of model quality and allows you to verify whether the model has learned temporal patterns like daily or weekly cycles.\n",
    "\n",
    "Due to the small size of the dataset, the model in this tutorial learns the mean of the data (a constant solution). To improve these results requires more training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Run inference on the latest window & plot\n",
    "\n",
    "# Get last week of data\n",
    "past_norm = hourly[\"norm\"].iloc[-INPUT_WINDOW:].to_numpy()\n",
    "future_true = hourly[\"passengers\"].iloc[-HORIZON:].to_numpy()\n",
    "\n",
    "with best_ckpt.as_directory() as p:\n",
    "    pred_norm = ray.get(forecast_from_checkpoint.remote(p, past_norm))\n",
    "\n",
    "# de-normalise\n",
    "mean, std = hourly[\"passengers\"].mean(), hourly[\"passengers\"].std()\n",
    "pred = pred_norm * std + mean\n",
    "past = past_norm * std + mean\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "t_past   = np.arange(-INPUT_WINDOW, 0)\n",
    "STEP_SIZE_HOURS = 0.5  # because you're now using 30min data\n",
    "t_future = np.arange(0, HORIZON) * STEP_SIZE_HOURS\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_past, past, label=\"History\")\n",
    "plt.plot(t_future, future_true, \"--\", label=\"Ground Truth\")\n",
    "plt.plot(t_future, pred,  \"-.\", label=\"Forecast\")\n",
    "plt.axvline(0, color=\"black\"); plt.xlabel(\"Hours relative\"); plt.ylabel(\"# trips\")\n",
    "plt.title(\"NYC-Taxi 24 h Forecast\"); plt.legend(); plt.grid(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16 \u00b7 Cleanup: remove all training artifacts  \n",
    "Finally, tidy up by deleting temporary checkpoint folders, the metrics CSV, and any intermediate result directories. Clearing out old artefacts frees disk space and leaves your workspace clean for whatever comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Cleanup \u2013 optionally remove all artefacts to free space\n",
    "if os.path.exists(DATA_DIR):\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "    print(f\"Deleted {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83c\udf89 Wrapping Up & Next Steps\n",
    "\n",
    "Nice work. You've built a robust, distributed forecasting workflow using **Ray Train on Anyscale** that:\n",
    "\n",
    "* Trains a Transformer model across **multiple GPUs** using **Ray Train with Distributed Data Parallel (DDP)**, abstracting away low-level orchestration.\n",
    "* Recovers automatically from failures with **built-in checkpointing and resume**, even across re-launches or node churn.\n",
    "* Logs and reports per-epoch metrics using **Ray Train\u2019s reporting APIs**, enabling real-time monitoring and seamless plotting.\n",
    "* Performs inference using **Ray remote tasks**, allowing you to scale forecasting across GPUs or nodes without changing model code.\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\ude80 Where can you take this next?\n",
    "\n",
    "1. **Hyperparameter Sweeps**  \n",
    "   * Wrap the `TorchTrainer` with **Ray Tune** to search over `d_model`, `nhead`, learning rate, and window sizes.  \n",
    "\n",
    "2. **Probabilistic Forecasting**  \n",
    "   * Output percentiles or fit a distribution head (For example, Gaussian) to capture prediction uncertainty.  \n",
    "\n",
    "3. **Multivariate & Exogenous Features**  \n",
    "   * Add weather, holidays, or ride-sharing surge multipliers as extra input channels.  \n",
    "\n",
    "4. **Early-Stopping & LR Scheduling**  \n",
    "   * Monitor val-loss and reduce LR on plateau, or stop when improvement < 1 %.  \n",
    "\n",
    "5. **Model Compression**  \n",
    "   * Distil the large Transformer into a lightweight LSTM or Tiny-Transformer for edge deployment.  \n",
    "\n",
    "6. **Streaming / Online Learning**  \n",
    "   * Use **Ray Serve** to deploy the model and update weights periodically with the latest data.  \n",
    "\n",
    "7. **Interpretability**  \n",
    "   * Visualise attention maps to see which time lags the model focuses on\u2014great for stakeholder trust.  \n",
    "\n",
    "8. **End-to-End MLOps**  \n",
    "   * Schedule nightly retraining with **Ray Jobs**, log artifacts to MLflow or Weights & Biases, and automate model promotion.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}