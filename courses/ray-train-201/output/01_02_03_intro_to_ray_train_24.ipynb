{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 \u00b7 Manual Restoration from Checkpoints  \n",
    "\n",
    "If the maximum number of retries is reached, you can still **manually restore training** by creating a new `TorchTrainer` with the same configuration:  \n",
    "\n",
    "- Use the same `train_loop_ray_train_with_checkpoint_loading` so the loop can resume from a checkpoint.  \n",
    "- Provide the same `run_config` (name, storage path, and failure config).  \n",
    "- Pass in the same dataset and scaling configuration.  \n",
    "\n",
    "Ray Train will detect the latest checkpoint in the specified `storage_path` and resume training from that point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Manually restore a trainer from the last checkpoint\n",
    "\n",
    "restored_trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_ray_train_with_checkpoint_loading,  # loop supports checkpoint loading\n",
    "        train_loop_config={   # hyperparameters must match\n",
    "        \"num_epochs\": 1,\n",
    "        \"global_batch_size\": 512,\n",
    "    },\n",
    "    scaling_config=scaling_config,  # same resource setup as before\n",
    "    run_config=ray.train.RunConfig(\n",
    "        name=\"fault-tolerant-cifar-vit\",  # must match previous run name\n",
    "        storage_path=storage_path,       # path where checkpoints are saved\n",
    "        failure_config=failure_config,   # still allow retries\n",
    "    ),\n",
    "    datasets=datasets,  # same dataset as before\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 \u00b7 Resume Training from the Last Checkpoint  \n",
    "\n",
    "Calling `restored_trainer.fit()` will continue training from the most recent checkpoint found in the specified storage path.  \n",
    "\n",
    "- If all epochs were already completed in the previous run, the trainer will terminate immediately.  \n",
    "- If training was interrupted mid-run, it will resume from the saved epoch, restoring both the **model** and **optimizer** state.  \n",
    "- The returned `Result` object confirms that training picked up correctly and contains metrics, checkpoints, and logs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Resume training from the last checkpoint\n",
    "\n",
    "# Fit the restored trainer \u2192 continues from last saved epoch\n",
    "# If all epochs are already complete, training ends immediately\n",
    "result = restored_trainer.fit()\n",
    "\n",
    "# Display final training results (metrics, checkpoints, etc.)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}