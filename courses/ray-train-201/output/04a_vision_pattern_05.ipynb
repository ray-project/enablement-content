{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. `train_loop_per_worker`  \n",
    "\n",
    "This function defines the **per-worker training logic** that Ray Train executes on each distributed worker.  \n",
    "\n",
    "Each worker builds its own model, optimizer, and dataloaders; resumes automatically from the most recent Ray-managed checkpoint (if available); and then trains and validates the model across epochs.  \n",
    "\n",
    "Key behaviors to note:\n",
    "\n",
    "- **Checkpoints** are first written to a fast **temporary local directory** on each worker, then safely persisted to the run\u2019s configured `storage_path` by `train.report()`\u2014ensuring reliability and retry support even under transient node failures.  \n",
    "- **Metrics** (train and validation loss) are automatically collected and stored by Ray Train\u2014no need for manual file writes or JSON logging.  \n",
    "- **Fault tolerance** is fully handled by Ray Train\u2019s checkpointing and retry mechanism via `RunConfig` and `FailureConfig`.  \n",
    "- **Final accuracy** is computed using `torchmetrics.MulticlassAccuracy`, which performs synchronized, **distributed accuracy aggregation** across all workers, ensuring a correct global metric instead of rank-0-only evaluation.  \n",
    "\n",
    "This design keeps the training loop clean, fault-tolerant, and fully aligned with Ray Train\u2019s built-in distributed orchestration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Define Ray Train train_loop_per_worker (tempdir checkpoints + Ray-managed metrics)\n",
    "def train_loop_per_worker(config):\n",
    "    import tempfile\n",
    "\n",
    "    rank = get_context().get_world_rank()\n",
    "\n",
    "    # === Model ===\n",
    "    net = resnet18(num_classes=101)\n",
    "    model = prepare_model(net)\n",
    "\n",
    "    # === Optimizer / Loss ===\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # === Resume from Checkpoint ===\n",
    "    start_epoch = 0\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as ckpt_dir:\n",
    "            # Map to CPU is fine; prepare_model will handle device placement.\n",
    "            model.load_state_dict(torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\"))\n",
    "            opt_path = os.path.join(ckpt_dir, \"optimizer.pt\")\n",
    "            if os.path.exists(opt_path):\n",
    "                optimizer.load_state_dict(torch.load(opt_path, map_location=\"cpu\"))\n",
    "            meta_path = os.path.join(ckpt_dir, \"meta.pt\")\n",
    "            if os.path.exists(meta_path):\n",
    "                # Continue from the next epoch after the saved one\n",
    "                start_epoch = int(torch.load(meta_path).get(\"epoch\", -1)) + 1\n",
    "        if rank == 0:\n",
    "            print(f\"[Rank {rank}] Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    # === DataLoaders ===\n",
    "    train_loader = build_dataloader(\n",
    "        \"/mnt/cluster_storage/food101_lite/train.parquet\", config[\"batch_size\"], shuffle=True\n",
    "    )\n",
    "    val_loader = build_dataloader(\n",
    "        \"/mnt/cluster_storage/food101_lite/val.parquet\", config[\"batch_size\"], shuffle=False\n",
    "    )\n",
    "\n",
    "    # === Training Loop ===\n",
    "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "        # Required when using DistributedSampler\n",
    "        if hasattr(train_loader, \"sampler\") and hasattr(train_loader.sampler, \"set_epoch\"):\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train()\n",
    "        train_loss_total, train_batches = 0.0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_total += loss.item()\n",
    "            train_batches += 1\n",
    "        train_loss = train_loss_total / max(train_batches, 1)\n",
    "\n",
    "        # === Validation Loop ===\n",
    "        model.eval()\n",
    "        val_loss_total, val_batches = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for val_xb, val_yb in val_loader:\n",
    "                val_loss_total += criterion(model(val_xb), val_yb).item()\n",
    "                val_batches += 1\n",
    "        val_loss = val_loss_total / max(val_batches, 1)\n",
    "\n",
    "        metrics = {\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss}\n",
    "        if rank == 0:\n",
    "            print(metrics)\n",
    "\n",
    "        # ---- Save checkpoint to fast local temp dir; Ray persists it via report() ----\n",
    "        if rank == 0:\n",
    "            with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                torch.save(model.state_dict(), os.path.join(tmpdir, \"model.pt\"))\n",
    "                torch.save(optimizer.state_dict(), os.path.join(tmpdir, \"optimizer.pt\"))\n",
    "                torch.save({\"epoch\": epoch}, os.path.join(tmpdir, \"meta.pt\"))\n",
    "                ckpt_out = Checkpoint.from_directory(tmpdir)\n",
    "                train.report(metrics, checkpoint=ckpt_out)\n",
    "        else:\n",
    "            # Non-zero ranks report metrics only (no checkpoint attachment)\n",
    "            train.report(metrics)\n",
    "\n",
    "    # === Final validation accuracy (distributed via TorchMetrics) ===\n",
    "    from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    # Sync across DDP workers when computing the final value\n",
    "    acc_metric = MulticlassAccuracy(\n",
    "        num_classes=101, average=\"micro\", sync_on_compute=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            logits = model(xb)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            acc_metric.update(preds, yb)\n",
    "\n",
    "    dist_val_acc = acc_metric.compute().item()\n",
    "    if rank == 0:\n",
    "        print(f\"Val Accuracy (distributed): {dist_val_acc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}