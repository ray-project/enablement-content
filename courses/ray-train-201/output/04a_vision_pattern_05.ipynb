{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 \u00b7 `train_loop_per_worker`  \n",
    "This is the workhorse called by each Ray worker. Inside, build the model, optimiser, and loss, try to resume from any existing checkpoint, run the training and validation loops, log metrics, and (on rank 0) save new checkpoints and append results to a history file. At the end, compute a final validation accuracy for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Define Ray Train train_loop_per_worker\n",
    "def train_loop_per_worker(config):\n",
    "\n",
    "    # === Model ===\n",
    "    net = resnet18(num_classes=101)\n",
    "    model = prepare_model(net)\n",
    "\n",
    "    # === Optimizer / Loss ===\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # === Resume from Checkpoint ===\n",
    "    checkpoint = get_checkpoint()\n",
    "    start_epoch = 0\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            model.load_state_dict(torch.load(os.path.join(ckpt_dir, \"model.pt\")))\n",
    "            optimizer.load_state_dict(torch.load(os.path.join(ckpt_dir, \"optimizer.pt\")))\n",
    "            start_epoch = torch.load(os.path.join(ckpt_dir, \"extra.pt\"))[\"epoch\"]\n",
    "        print(f\"[Rank {get_context().get_world_rank()}] Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    # === DataLoaders ===\n",
    "    train_loader = build_dataloader(\n",
    "        \"/mnt/cluster_storage/food101_lite/train.parquet\", config[\"batch_size\"], shuffle=True\n",
    "    )\n",
    "    val_loader = build_dataloader(\n",
    "        \"/mnt/cluster_storage/food101_lite/val.parquet\", config[\"batch_size\"], shuffle=False\n",
    "    )\n",
    "\n",
    "    # === Training Loop ===\n",
    "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "        train_loader.sampler.set_epoch(epoch)  # required when using DistributedSampler\n",
    "        model.train()\n",
    "        train_loss_total = 0.0\n",
    "        train_batches = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_total += loss.item()\n",
    "            train_batches += 1\n",
    "\n",
    "        train_loss = train_loss_total / train_batches\n",
    "\n",
    "        # === Validation Loop ===\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for val_xb, val_yb in val_loader:\n",
    "                val_loss_total += criterion(model(val_xb), val_yb).item()\n",
    "                val_batches += 1\n",
    "        val_loss = val_loss_total / val_batches\n",
    "\n",
    "        metrics = {\"train_loss\": train_loss, \"val_loss\": val_loss, \"epoch\": epoch}\n",
    "        if train.get_context().get_world_rank() == 0:\n",
    "            print(metrics)\n",
    "\n",
    "        # === Save checkpoint only on rank 0 ===\n",
    "        if get_context().get_world_rank() == 0:\n",
    "            ckpt_dir = f\"/mnt/cluster_storage/food101_lite/tmp_checkpoints/epoch_{epoch}_{uuid.uuid4().hex}\"\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(ckpt_dir, \"model.pt\"))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(ckpt_dir, \"optimizer.pt\"))\n",
    "            torch.save({\"epoch\": epoch}, os.path.join(ckpt_dir, \"extra.pt\"))\n",
    "            checkpoint = Checkpoint.from_directory(ckpt_dir)\n",
    "        else:\n",
    "            checkpoint = None\n",
    "\n",
    "        # Append metrics to a file (only on rank 0)\n",
    "        if train.get_context().get_world_rank() == 0:\n",
    "            with open(\"/mnt/cluster_storage/food101_lite/results/history.csv\", \"a\") as f:\n",
    "                f.write(f\"{epoch},{train_loss},{val_loss}\\n\")\n",
    "        train.report(metrics, checkpoint=checkpoint)\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.cuda(), yb.cuda()\n",
    "        pred = model(xb).argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Val Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}