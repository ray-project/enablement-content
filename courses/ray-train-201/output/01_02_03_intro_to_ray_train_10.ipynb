{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 \u00b7 Save Checkpoints and Report Metrics  \n",
    "\n",
    "We will report intermediate metrics and checkpoints using the [`ray.train.report`](https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray-train-report) utility function.  \n",
    "\n",
    "This helper function:  \n",
    "- Creates a temporary directory to stage the checkpoint.  \n",
    "- Saves the model weights with `torch.save()`.  \n",
    "  * Since the model is wrapped in **DistributedDataParallel (DDP)**, we call `model.module.state_dict()` to unwrap it.  \n",
    "- Calls `ray.train.report()` to:  \n",
    "  * Log the current metrics (e.g., loss, epoch).  \n",
    "  * Attach a `Checkpoint` object created from the staged directory.  \n",
    "\n",
    "This way, each epoch produces both **metrics for monitoring** and a **checkpoint for recovery or inference**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Save checkpoint and report metrics with Ray Train\n",
    "\n",
    "def save_checkpoint_and_metrics_ray_train(model: torch.nn.Module, metrics: dict[str, float]) -> None:\n",
    "    # Create a temporary directory to stage checkpoint files\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        # Save the model weights.\n",
    "        # Note: under DDP the model is wrapped in DistributedDataParallel,\n",
    "        # so we unwrap it with `.module` before calling state_dict().        \n",
    "        torch.save(\n",
    "            model.module.state_dict(),  # note the `.module` to unwrap the DistributedDataParallel\n",
    "            os.path.join(temp_checkpoint_dir, \"model.pt\"),\n",
    "        )\n",
    "        \n",
    "        # Report metrics and attach a checkpoint to Ray Train.\n",
    "        # \u2192 metrics are logged centrally\n",
    "        # \u2192 checkpoint allows resuming training or running inference later\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <p><strong>Quick notes:</strong></p>\n",
    "  <ul>\n",
    "    <li>\n",
    "      Use \n",
    "      <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray.train.report\" target=\"_blank\">\n",
    "        ray.train.report\n",
    "      </a> to save the metrics and checkpoint.\n",
    "    </li>\n",
    "    <li>Only metrics from the rank 0 worker are reported.</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the Checkpoint Lifecycle  \n",
    "\n",
    "The diagram above shows how a checkpoint moves from **local storage** (temporary directory on a worker) to **persistent cluster or cloud storage**.  \n",
    "\n",
    "Key points to remember:  \n",
    "- Since the model is identical across all workers, it\u2019s enough to **write the checkpoint only on the rank-0 worker**.  \n",
    "  * However, you still need to call [`ray.train.report`](https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray-train-report) on **all workers** to keep the training loop synchronized.  \n",
    "- Ray Train expects every worker to have access to the **same persistent storage location** for writing files.  \n",
    "- For production jobs, **cloud storage** (e.g., S3, GCS, Azure Blob) is the recommended target for checkpoints.  \n",
    "\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/checkpoint_lifecycle.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 \u00b7 Save Checkpoints on Rank-0 Only  \n",
    "\n",
    "To avoid redundant writes, we update the checkpointing function so that **only the rank-0 worker** saves the model weights.  \n",
    "\n",
    "- **Temporary directory** \u2192 Each worker still creates a temp directory, but only rank-0 writes the model file.  \n",
    "- **Rank check** \u2192 `ray.train.get_context().get_world_rank()` ensures that only worker 0 performs the checkpointing.  \n",
    "- **All workers report** \u2192 Every worker still calls [`ray.train.report`](https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html#ray-train-report), but only rank-0 attaches the actual checkpoint. This keeps the training loop synchronized.  \n",
    "\n",
    "This pattern is the recommended best practice:  \n",
    "- Avoids unnecessary duplicate checkpoints from multiple workers.  \n",
    "- Still guarantees that metrics are reported from every worker.  \n",
    "- Ensures checkpoints are cleanly written once per epoch to persistent storage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Save checkpoint only from the rank-0 worker\n",
    "\n",
    "def save_checkpoint_and_metrics_ray_train(model: torch.nn.Module, metrics: dict[str, float]) -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        checkpoint = None\n",
    "\n",
    "        # Only the rank-0 worker writes the checkpoint file\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            torch.save(\n",
    "                model.module.state_dict(),  # unwrap DDP before saving\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\"),\n",
    "            )\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "        # All workers still call ray.train.report()\n",
    "        # \u2192 keeps training loop synchronized\n",
    "        # \u2192 metrics are logged from each worker\n",
    "        # \u2192 only rank-0 attaches a checkpoint\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=checkpoint,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our guide on [saving and loading checkpoints](https://docs.ray.io/en/latest/train/user-guides/checkpoints.html) for more details and best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 \u00b7 Configure Persistent Storage with `RunConfig`  \n",
    "\n",
    "To tell Ray Train **where to store results, checkpoints, and logs**, we use a [`RunConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.RunConfig.html).  \n",
    "\n",
    "- **`storage_path`** \u2192 Base directory for all outputs of this training run.  \n",
    "  * In this example we use `/mnt/cluster_storage/training/`, which is **persistent shared storage** across all nodes.  \n",
    "  * This ensures checkpoints and metrics remain available even after the cluster shuts down.  \n",
    "- **`name`** \u2192 A human-readable name for the run (e.g., `\"distributed-mnist-resnet18\"`). This is used to namespace output files.  \n",
    "\n",
    "Together, the `RunConfig` defines how Ray organizes and persists all artifacts from your training job.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Configure persistent storage and run name\n",
    "\n",
    "storage_path = \"/mnt/cluster_storage/training/\"\n",
    "run_config = RunConfig(\n",
    "    storage_path=storage_path,         # where to store checkpoints/logs\n",
    "    name=\"distributed-mnist-resnet18\"  # identifier for this run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Data-Parallel Training with Ray Train  \n",
    "\n",
    "This diagram shows the same DDP workflow as before, but now with **Ray Train utilities** highlighted:  \n",
    "\n",
    "1. **`ray.train.torch.prepare_data_loader()`**  \n",
    "   - Automatically wraps your PyTorch DataLoader with a `DistributedSampler`.  \n",
    "   - Ensures each worker processes a unique shard of the dataset.  \n",
    "   - Moves batches to the correct device (GPU or CPU).  \n",
    "\n",
    "2. **`ray.train.torch.prepare_model()`**  \n",
    "   - Moves your model to the right device.  \n",
    "   - Wraps it in `DistributedDataParallel (DDP)` so gradients are synchronized across workers.  \n",
    "   - Removes the need for manual `.to(\"cuda\")` calls or DDP boilerplate.  \n",
    "\n",
    "3. **`ray.train.report()`**  \n",
    "   - Centralized way to report metrics and attach checkpoints.  \n",
    "   - Keeps the training loop synchronized across all workers, even if only rank-0 saves the actual checkpoint.  \n",
    "\n",
    "By combining these helpers, Ray Train takes care of the **data sharding, model replication, gradient synchronization, and checkpoint lifecycle** \u2014 letting you keep your training loop clean and close to standard PyTorch.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_annotated_v5.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "||"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}