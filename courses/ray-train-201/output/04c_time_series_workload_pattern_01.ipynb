{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd52 04c \u00b7 Time-Series Workload Pattern with Ray Train  \n",
    "In this notebook you tackle **New York City (NYC) taxi-demand forecasting** (2014 half-hourly counts) and scale a *sequence-to-sequence Transformer* across an Anyscale cluster using **Ray Train V2**.\n",
    "\n",
    "### What you\u2019ll learn & take away  \n",
    "- **Ray Train V2 distributed loops** \u2013 wrap a PyTorch Transformer in `TorchTrainer` and run it across 8 GPUs with a *single* `ScalingConfig` line.  \n",
    "- **Fault-tolerant checkpointing on Anyscale** \u2013 recover seamlessly from pre-emptions or node failures with automatic epoch-level checkpoints.  \n",
    "- **Remote GPU inference from checkpoints** \u2013 spin up transient GPU actors for batch forecasts without redeploying the whole trainer.  \n",
    "By the end you know exactly how to take a single-node notebook forecast and scale it\u2014data, training, and inference\u2014on any Anyscale cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd22 What problem are you solving? (NYC Taxi Demand Forecasting with a Transformer)\n",
    "\n",
    "YOu want to predict the **next 24 hours (48 half-hour slots)** of taxi pickups in NYC, given one week of historical demand.  \n",
    "Accurate short-term forecasts help ride-hailing fleets, traffic planners, and dynamic pricing engines allocate resources efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### What's a Sequence-to-Sequence Transformer?\n",
    "\n",
    "A **Transformer** models the joint distribution of a sequence by stacking self-attention layers that capture long-range dependencies without recurrence.  \n",
    "Your architecture learns a function  \n",
    "\n",
    "$$\n",
    "f_\\theta : \\underbrace{\\mathbb{R}^{T\\times 1}}_{\\text{past}} \\;\\longrightarrow\\; \\underbrace{\\mathbb{R}^{F}}_{\\text{future}}\n",
    "$$\n",
    "\n",
    "where $T=168$ half-hours (one week) and $F=48$.  \n",
    "During training you use **teacher forcing**, feeding the shifted ground truth to the decoder, so the model can focus on learning residual patterns rather than inventing an initial context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83e\udded How you Migrate This Time-Series Workload to a Distributed Multi-Node Setup using Ray on Anyscale\n",
    "This tutorial walks through the end-to-end process of **migrating a single-GPU PyTorch forecasting pipeline to a distributed Ray cluster running on Anyscale**.\n",
    "\n",
    "Here\u2019s how you make that transition:\n",
    "\n",
    "1. **Local CSV \u2192 Shared Parquet**  \n",
    "   Download the NYC Taxi dataset as a CSV, resample it to 30-minute intervals, normalize the values, and save it as **Parquet shards** in a shared filesystem (`/mnt/cluster_storage`) \u2014 the default storage for Anyscale clusters.\n",
    "\n",
    "2. **Single-loop preprocessing \u2192 Sliding window generation for Distributed Data Parallel (DDP)**  \n",
    "   Create overlapping input/output windows (past \u2192 future) to train a forecasting model. While this preprocessing is local and sequential here, it mirrors pipelines that parallelize with **Ray Data** in large-scale settings. (See other tutorials in this module that incorporate Ray Data for reference)\n",
    "\n",
    "3. **Vanilla PyTorch \u2192 Distributed Ray Train**  \n",
    "   Define a `train_loop_per_worker()` function and use **Ray Train** to launch **8 GPU workers** across the cluster. Each worker loads its own Parquet shard, trains independently under Distributed Data Parallel (DDP), and reports live metrics.\n",
    "\n",
    "4. **Manual device logic \u2192 Scalable cluster orchestration**  \n",
    "   Instead of managing GPUs or process groups manually, configure `ScalingConfig`, `RunConfig`, and `FailureConfig`. **Ray + Anyscale handle fault-tolerant execution across nodes.**\n",
    "\n",
    "5. **Offline inference \u2192 Distributed forecasting with remote Ray tasks**  \n",
    "   Define a `@ray.remote` forecasting function that loads a trained checkpoint and runs prediction on the latest data window. This allows **parallel, stateless inference** on any GPU in the cluster.\n",
    "\n",
    "This pattern takes a local academic-style time-series workflow and scales it into a **cluster-resilient, fault-tolerant forecasting pipeline**, all while preserving your native PyTorch modeling code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}