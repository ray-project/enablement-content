{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04c Time-Series workload pattern with Ray Train  \n",
    "In this notebook you tackle **New York City (NYC) taxi-demand forecasting** (2014 half-hourly counts) and scale a *sequence-to-sequence Transformer* across an Anyscale cluster using **Ray Train V2**.\n",
    "\n",
    "### What you learn and take away  \n",
    "- **Ray Train V2 distributed loops**: wrap a PyTorch Transformer in `TorchTrainer` and run it across 8 GPUs with a *single* `ScalingConfig` line.  \n",
    "- **Fault-tolerant checkpointing on Anyscale**: recover seamlessly from pre-emptions or node failures with automatic epoch-level checkpoints.  \n",
    "- **Inference from checkpoints**: use **Ray Data** with stateful GPU actors to perform scalable batch forecasts directly from saved checkpoints.  \n",
    "By the end, you\u2019ll know how to take a single-node notebook forecast and scale data loading, training, and inference seamlessly across an Anyscale cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What problem are you solving? (NYC taxi demand forecasting with a Transformer)\n",
    "\n",
    "You want to predict the **next 24 hours (48 half-hour slots)** of taxi pickups in NYC, given one week of historical demand.  \n",
    "Accurate short-term forecasts help ride-hailing fleets, traffic planners, and dynamic pricing engines allocate resources efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### What's a sequence-to-sequence Transformer?\n",
    "\n",
    "A **Transformer** models the joint distribution of a sequence by stacking self-attention layers that capture long-range dependencies without recurrence.  \n",
    "Your architecture learns a function  \n",
    "\n",
    "$$\n",
    "f_\\theta : \\underbrace{\\mathbb{R}^{T\\times 1}}_{\\text{past}} \\;\\longrightarrow\\; \\underbrace{\\mathbb{R}^{F}}_{\\text{future}}\n",
    "$$\n",
    "\n",
    "where $T=168$ half-hours (one week) and $F=48$.  \n",
    "During training you use **teacher forcing** (a design choice), feeding the shifted ground truth to the decoder, so the model can focus on learning residual patterns rather than inventing an initial context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to migrate this time-series workload to a distributed multi-node setup using Ray on Anyscale\n",
    "This tutorial walks through the end-to-end process of **migrating a single-GPU PyTorch forecasting pipeline to a distributed Ray cluster running on Anyscale**.\n",
    "\n",
    "Follow these steps to make the transition:\n",
    "\n",
    "1. **Migrate local CSV data to shared Parquet**  \n",
    "   Download the NYC taxi dataset as a CSV, resample it to 30-minute intervals, normalize the values, and save it as **Parquet shards** in a shared filesystem (`/mnt/cluster_storage`)\u2014the default storage for Anyscale clusters.\n",
    "\n",
    "2. **Create sliding window generation for Distributed Data Parallel (DDP)**  \n",
    "   Create overlapping input and output windows (past to future) to train a forecasting model. While this preprocessing is local and sequential in this tutorial, it mirrors pipelines that parallelize with **Ray Data** in large-scale settings. \n",
    "\n",
    "3. **Define a vanilla PyTorch function to use distributed Ray Train**  \n",
    "   Define a `train_loop_per_worker()` function and use **Ray Train** to launch **8 GPU workers** across the cluster. Each worker loads its own Parquet shard, trains independently under Distributed Data Parallel (DDP), and reports live metrics.\n",
    "\n",
    "4. **Configure Ray for scalable cluster orchestration**  \n",
    "   Instead of managing GPUs or process groups manually, configure `ScalingConfig`, `RunConfig`, and `FailureConfig`. **Ray and Anyscale handle fault-tolerant execution across nodes.**\n",
    "\n",
    "5. **Perform distributed batch inference with Ray Data**  \n",
    "   Use **Ray Data** with stateful GPU actors to load the trained checkpoint once per worker and run scalable, parallel forecasts on the latest data windows.  \n",
    "   This enables **efficient, reusable, and fault-tolerant** inference across the cluster.\n",
    "\n",
    "This pattern takes a local academic-style time-series workflow and scales it into a **cluster-resilient, fault-tolerant forecasting pipeline**, all while preserving your native PyTorch modeling code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}