{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 \u00b7 Define the Ray Train Loop (DDP per-worker)\n",
    "\n",
    "This is the **per-worker training function** that Ray executes on each process/GPU. It keeps your PyTorch code intact while Ray handles **process launch, device placement, and data sharding**.\n",
    "\n",
    "Key points:\n",
    "- **Inputs via `config`**: we pass hyperparameters like `num_epochs` and a **`global_batch_size`**.\n",
    "- **Model & optimizer**: `load_model_ray_train()` returns a model already wrapped by Ray Train (DDP + correct device). We use `Adam` and `CrossEntropyLoss` for MNIST.\n",
    "- **Batch sizing**: we split the global batch across workers:  \n",
    "  `per_worker_batch = global_batch_size // world_size`.\n",
    "- **Data sharding**: `build_data_loader_ray_train(...)` returns a DataLoader wrapped with a **DistributedSampler**; each worker sees a disjoint shard.\n",
    "- **Epoch control**: `data_loader.sampler.set_epoch(epoch)` ensures proper shuffling across epochs in distributed mode.\n",
    "- **Training step**: standard PyTorch loop\u2014forward \u2192 loss \u2192 zero_grad \u2192 backward \u2192 step.\n",
    "- **Metrics & checkpointing**: `print_metrics_ray_train(...)` logs loss; `save_checkpoint_and_metrics_ray_train(...)` calls `ray.train.report(...)` (rank-0 saves the checkpoint).\n",
    "\n",
    "This function is passed to `TorchTrainer`, which runs it **concurrently on all workers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this data-parallel training loop will look like with Ray Train and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Define the Ray Train per-worker training loop\n",
    "\n",
    "def train_loop_ray_train(config: dict):  # pass in hyperparameters in config\n",
    "    # config holds hyperparameters passed from TorchTrainer (e.g. num_epochs, global_batch_size)\n",
    "\n",
    "    # Define loss function for MNIST classification\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Build and prepare the model for distributed training.\n",
    "    # load_model_ray_train() calls ray.train.torch.prepare_model()\n",
    "    # \u2192 moves model to GPU and wraps it in DistributedDataParallel (DDP).\n",
    "    model = load_model_ray_train()\n",
    "\n",
    "    # Standard optimizer (learning rate fixed for demo)\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Calculate the batch size for each worker\n",
    "    global_batch_size = config[\"global_batch_size\"]\n",
    "    world_size = ray.train.get_context().get_world_size()  # total # of workers in the job\n",
    "    batch_size = global_batch_size // world_size  # split global batch evenly\n",
    "    print(f\"{world_size=}\\n{batch_size=}\")\n",
    "\n",
    "    # Wrap DataLoader with prepare_data_loader()\n",
    "    # \u2192 applies DistributedSampler (shards data across workers)\n",
    "    # \u2192 ensures batches are automatically moved to correct device\n",
    "    data_loader = build_data_loader_ray_train(batch_size=batch_size)\n",
    "\n",
    "    # ----------------------- Training loop ----------------------- #\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "\n",
    "        # Ensure each worker shuffles its shard differently every epoch\n",
    "        data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        # Iterate over batches (sharded across workers)\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)            # forward pass\n",
    "            loss = criterion(outputs, labels)  # compute loss\n",
    "            optimizer.zero_grad()              # reset gradients\n",
    "\n",
    "            loss.backward()   # backward pass (grads averaged across workers via DDP)\n",
    "            optimizer.step()  # update model weights\n",
    "\n",
    "        # After each epoch: report loss and log metrics\n",
    "        metrics = print_metrics_ray_train(loss, epoch)\n",
    "\n",
    "        # Save checkpoint (only rank-0 worker persists the model)\n",
    "        save_checkpoint_and_metrics_ray_train(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Main training loop</b>\n",
    "<ul>\n",
    "  <li><strong>global_batch_size</strong>: the total number of samples processed in a single training step of the entire training job.\n",
    "    <ul>\n",
    "      <li>It's estimated like this: <code>batch size * DDP workers * gradient accumulation steps</code>.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Notice that images and labels are no longer manually moved to device (<code>images.to(\"cuda\")</code>). This is done by \n",
    "    <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html#ray-train-torch-prepare-data-loader\" target=\"_blank\">\n",
    "      prepare_data_loader()\n",
    "    </a>.\n",
    "  </li>\n",
    "  <li>Config that will be passed here, is defined below. It will be passed to the Ray Train's <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.TorchTrainer.html#ray-train-torch-torchtrainer\" target=\"_blank\">TorchTrainer</a>.</li>\n",
    "  <li>\n",
    "    <a href=\"https://docs.ray.io/en/latest/train/api/doc/ray.train.v2.api.context.TrainContext.html#ray-train-v2-api-context-traincontext\" target=\"_blank\">\n",
    "      TrainContext\n",
    "    </a> lets users get useful information about the training i.e. node rank, world size, world rank, experiment name.\n",
    "  </li>\n",
    "\n",
    "  <li><code>load_model_ray_train</code> and <code>build_data_loader_ray_train</code> are implemented below.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}