{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Wrapping Up & Next Steps  \n",
    "\n",
    "Nice work -- you completed a full, production-style workflow with **Ray Train on Anyscale**, then extended it with **Ray Data**, and finally added **fault tolerance**. Here\u2019s what you accomplished across the three modules:\n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Module 01 \u00b7 Introduction to Ray Train  \n",
    "- Scaled PyTorch DDP with **`TorchTrainer`** using **`ScalingConfig`** and **`RunConfig`**  \n",
    "- Wrapped code for multi-GPU with **`prepare_model()`** and **`prepare_data_loader()`**  \n",
    "- Reported **metrics** and saved **checkpoints** via `ray.train.report(...)` (rank-0 checkpointing best practice)  \n",
    "- Inspected results from the **`Result`** object and served **GPU inference** with a Ray actor  \n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Module 02 \u00b7 Integrating Ray Train with Ray Data  \n",
    "- Prepared MNIST as **Parquet** and loaded it as a **Ray Dataset**  \n",
    "- Streamed batches with **`iter_torch_batches()`** and consumed dict batches in the training loop  \n",
    "- Passed datasets to the trainer via **`datasets={\"train\": ...}`**  \n",
    "- Decoupled CPU preprocessing from GPU training for **better utilization and throughput**  \n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Module 03 \u00b7 Fault Tolerance in Ray Train  \n",
    "- Enabled resume-from-checkpoint using **`ray.train.get_checkpoint()`**  \n",
    "- Saved full state (model, **optimizer**, **epoch**) for robust restoration  \n",
    "- Configured **`FailureConfig(max_failures=...)`** for automatic retries  \n",
    "- Performed **manual restoration** by re-creating a trainer with the same `RunConfig`  \n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\ude80 Where to go next  \n",
    "- **Scale up**: Increase `num_workers`, try multi-node clusters, or switch to **FSDP** via `prepare_model(parallel_strategy=\"fsdp\")`.  \n",
    "- **Input pipelines**: Add augmentations, caching, and windowed shuffles in **Ray Data**; try multi-file Parquet or lakehouse sources.  \n",
    "- **Experiment tracking**: Log metrics to external systems (Weights & Biases, MLflow) alongside `ray.train.report()`.  \n",
    "- **Larger models**: Integrate **DeepSpeed** or parameter-efficient fine-tuning templates.  \n",
    "- **Productionization**: Store checkpoints in cloud storage (S3/GCS/Azure), wire up alerts/dashboards, and add CI for smoke tests.  \n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcda Next Tutorials in the Course  \n",
    "In the next tutorials, you\u2019ll find **end-to-end workload examples** for using Ray Train on Anyscale (e.g., recommendation systems, vision, NLP, generative models).  \n",
    "\n",
    "\ud83d\udc49 You only need to pick **one** of these workloads to work through in the course \u2014 but you can explore more if you\u2019re curious!  \n",
    "\n",
    "---\n",
    "\n",
    "> With these patterns\u2014**distributed training**, **scalable data ingestion**, and **resilient recovery**\u2014you\u2019re ready to run larger, longer, and more reliable training jobs on Anyscale.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orphan": true,
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}