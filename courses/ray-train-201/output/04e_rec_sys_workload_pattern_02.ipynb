{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 \u00b7 Imports  \n",
    "Start by importing all the libraries you need for the rest of the notebook. These include standard utilities like `os`, `json`, and `pandas`, as well as deep learning libraries like PyTorch and visualization tools like `matplotlib`.\n",
    "\n",
    "Also, import everything needed for **distributed training and data processing with Ray**:\n",
    "- `ray` and `ray.data` provide the high-level distributed data API.\n",
    "- `ray.train` gives you `TorchTrainer`, `ScalingConfig`, checkpointing, and metrics reporting.\n",
    "- `prepare_model` wraps your PyTorch model for multi-worker training with Distributed Data Parallel (DDP).\n",
    "\n",
    "A few extra helpers like `tqdm` and `train_test_split` round out the list for progress bars and quick offline preprocessing.\n",
    "\n",
    "This notebook assumes Ray is already running (For example, with Anyscale), so you don\u2019t call `ray.init()` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ray\n",
    "import ray\n",
    "import ray.data\n",
    "from ray.train import ScalingConfig, RunConfig, CheckpointConfig, FailureConfig, Checkpoint, get_checkpoint, get_context,  get_dataset_shard, report\n",
    "from ray.train.torch import TorchTrainer, prepare_model\n",
    "\n",
    "# Other\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 \u00b7 Load MovieLens 100K Dataset  \n",
    "Download and extract the [MovieLens 100K](https://grouplens.org/datasets/movielens/100k/) dataset and persist a cleaned version to cluster storage under `/mnt/cluster_storage/rec_sys_tutorial/raw/ratings.csv`.\n",
    "\n",
    "The MovieLens 100K dataset contains 100,000 ratings across 943 users and 1,682 movies. It\u2019s small enough to train quickly, but realistic enough to demonstrate scaling and checkpointing with Ray Train.\n",
    "\n",
    "If you already downloaded and extracted the dataset, skip both steps to save time. The output is a CSV with four columns: `user_id`, `item_id`, `rating`, and `timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load MovieLens 100K Dataset and store in /mnt/cluster_storage/\n",
    "\n",
    "# Define clean working paths\n",
    "DATA_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "LOCAL_ZIP = \"/mnt/cluster_storage/rec_sys_tutorial/ml-100k.zip\"\n",
    "EXTRACT_DIR = \"/mnt/cluster_storage/rec_sys_tutorial/ml-100k\"\n",
    "OUTPUT_CSV = \"/mnt/cluster_storage/rec_sys_tutorial/raw/ratings.csv\"\n",
    "\n",
    "# Ensure target directories exist\n",
    "os.makedirs(\"/mnt/cluster_storage/rec_sys_tutorial/raw\", exist_ok=True)\n",
    "\n",
    "# Download only if not already done\n",
    "if not os.path.exists(LOCAL_ZIP):\n",
    "    !wget -q $DATA_URL -O $LOCAL_ZIP\n",
    "\n",
    "# Extract cleanly\n",
    "if not os.path.exists(EXTRACT_DIR):\n",
    "    with zipfile.ZipFile(LOCAL_ZIP, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"/mnt/cluster_storage/rec_sys_tutorial\")\n",
    "\n",
    "# Load raw file\n",
    "raw_path = os.path.join(EXTRACT_DIR, \"u.data\")\n",
    "df = pd.read_csv(raw_path, sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "\n",
    "# Save cleaned version\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\u2705 Loaded {len(df):,} ratings \u2192 {OUTPUT_CSV}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 \u00b7 Preprocess IDs and Create Ray Dataset  \n",
    "Begin preprocessing by encoding `user_id` and `item_id` into contiguous integer indices required for embedding layers. These encoded columns\u2014`user_idx` and `item_idx`\u2014are what your model uses during training.\n",
    "\n",
    "After encoding, drop the original IDs and split the dataset into 64 chunks. Serialize each chunk and push to Ray\u2019s object store using `ray.put(...)`. This allows Ray Data to construct a distributed dataset in the next step without creating a bottleneck on a single worker or process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Preprocess IDs and create Ray Dataset in parallel\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"/mnt/cluster_storage/rec_sys_tutorial/raw/ratings.csv\")\n",
    "\n",
    "# Encode user_id and item_id\n",
    "user2idx = {uid: j for j, uid in enumerate(sorted(df[\"user_id\"].unique()))}\n",
    "item2idx = {iid: j for j, iid in enumerate(sorted(df[\"item_id\"].unique()))}\n",
    "\n",
    "df[\"user_idx\"] = df[\"user_id\"].map(user2idx)\n",
    "df[\"item_idx\"] = df[\"item_id\"].map(item2idx)\n",
    "df = df[[\"user_idx\", \"item_idx\", \"rating\", \"timestamp\"]]\n",
    "\n",
    "# Split into multiple chunks for parallel ingestion\n",
    "NUM_SPLITS = 64  # adjust based on cluster size\n",
    "dfs = np.array_split(df, NUM_SPLITS)\n",
    "object_refs = [ray.put(split) for split in dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 \u00b7 Visualize Dataset: Ratings, Users, and Items  \n",
    "Before training, visualize the distribution of ratings, user activity, and item popularity. These quick checks help you verify that the dataset parses correctly and reveal useful patterns:\n",
    "\n",
    "- The first plot shows the overall rating distribution (1\u20135 stars). As expected, you see a skew toward 4 and 5.\n",
    "- The second plot shows how many ratings each user has submitted. There\u2019s a long tail: a few power users, but many light users.\n",
    "- The third plot shows how often users rated each item. Again, you see a long-tail distribution common in recommendation settings.\n",
    "\n",
    "These histograms give you a sense of sparsity and coverage, both of which influence model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Visualize Dataset: Ratings, User & Item Activity\n",
    "\n",
    "# Plot rating distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "df[\"rating\"].hist(bins=[0.5,1.5,2.5,3.5,4.5,5.5], edgecolor='black')\n",
    "plt.title(\"Rating Distribution\")\n",
    "plt.xlabel(\"Rating\"); plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Plot number of ratings per user\n",
    "plt.subplot(1, 3, 2)\n",
    "df[\"user_idx\"].value_counts().hist(bins=30, edgecolor='black')\n",
    "plt.title(\"Ratings per User\")\n",
    "plt.xlabel(\"# Ratings\"); plt.ylabel(\"Users\")\n",
    "\n",
    "# Plot number of ratings per item\n",
    "plt.subplot(1, 3, 3)\n",
    "df[\"item_idx\"].value_counts().hist(bins=30, edgecolor='black')\n",
    "plt.title(\"Ratings per Item\")\n",
    "plt.xlabel(\"# Ratings\"); plt.ylabel(\"Items\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 \u00b7 Create Ray Dataset from Encoded Chunks  \n",
    "Now, convert your list of encoded pandas chunks into a Ray Dataset using `from_pandas_refs(...)`. This method ensures that each chunk becomes its own block, enabling parallel data processing across the cluster.\n",
    "\n",
    "The result is a distributed Ray Dataset with one block per chunk, which is ideal for streaming batches during training. Confirm the number of blocks and show a few rows to verify the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Create Ray Dataset from refs (uses multiple blocks/workers)\n",
    "\n",
    "ratings_ds = ray.data.from_pandas_refs(object_refs)\n",
    "print(\"\u2705 Ray Dataset created with\", ratings_ds.num_blocks(), \"blocks\")\n",
    "ratings_ds.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 \u00b7 Train/Validation Split using Ray Data  \n",
    "Next, split the dataset into training and validation sets. First, shuffle the entire Ray Dataset to ensure randomization, then split by row index, using 80% for training and 20% for validation.\n",
    "\n",
    "This approach is simple and scalable: Ray handles the shuffling and slicing in parallel across blocks. Also, set a fixed seed to ensure the split is reproducible. After you split it, each dataset remains a fully distributed Ray Dataset, ready to stream into workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Train/Val Split using Ray Data\n",
    "\n",
    "# Parameters\n",
    "TRAIN_FRAC = 0.8\n",
    "SEED = 42  # for reproducibility\n",
    "\n",
    "# Shuffle + split by index\n",
    "total_rows = ratings_ds.count()\n",
    "train_size = int(total_rows * TRAIN_FRAC)\n",
    "\n",
    "ratings_ds = ratings_ds.random_shuffle(seed=SEED)\n",
    "train_ds, val_ds = ratings_ds.split_at_indices([train_size])\n",
    "\n",
    "print(f\"\u2705 Train/Val Split:\")\n",
    "print(f\"  Train \u2192 {train_ds.count():,} rows\")\n",
    "print(f\"  Val   \u2192 {val_ds.count():,} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}