{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports  \n",
    "Start by importing all the libraries you need for the rest of the notebook. These include standard utilities like `os`, `json`, and `pandas`, as well as deep learning libraries like PyTorch and visualization tools like `matplotlib`.\n",
    "\n",
    "Also, import everything needed for **distributed training and data processing with Ray**:\n",
    "- `ray` and `ray.data` provide the high-level distributed data API.\n",
    "- `ray.train` gives you `TorchTrainer`, `ScalingConfig`, checkpointing, and metrics reporting.\n",
    "- `prepare_model` wraps your PyTorch model for multi-worker training with Distributed Data Parallel (DDP).\n",
    "\n",
    "A few extra helpers like `tqdm` and `train_test_split` round out the list for progress bars and quick offline preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ray\n",
    "import ray\n",
    "import ray.data\n",
    "from ray.train import ScalingConfig, RunConfig, CheckpointConfig, FailureConfig, Checkpoint, get_checkpoint, get_context,  get_dataset_shard, report\n",
    "from ray.train.torch import TorchTrainer, prepare_model\n",
    "\n",
    "# Other\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load MovieLens 100K dataset\n",
    "\n",
    "Download and extract the [MovieLens 100K](https://grouplens.org/datasets/movielens/100k/) dataset, then persist a cleaned copy under `/mnt/cluster_storage/rec_sys_tutorial/raw/` in **two formats**:\n",
    "\n",
    "- **CSV:** `ratings.csv` (kept for later inference cells).  \n",
    "- **Parquet dataset:** `ratings_parquet/` as multiple shards (production-style blob store layout) so Ray Data can **stream** reads in parallel without materializing the full dataset.\n",
    "\n",
    "The output has four columns: `user_id`, `item_id`, `rating`, and `timestamp`.\n",
    "\n",
    "The MovieLens 100K dataset contains **100,000 ratings** across **943 users** and **1,682 movies** \u2014 small enough for quick iteration, yet realistic for demonstrating distributed streaming and training with **Ray Data + Ray Train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load MovieLens 100K Dataset and store in /mnt/cluster_storage/ as CSV + Parquet\n",
    "\n",
    "# Define clean working paths\n",
    "DATA_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "LOCAL_ZIP = \"/mnt/cluster_storage/rec_sys_tutorial/ml-100k.zip\"\n",
    "EXTRACT_DIR = \"/mnt/cluster_storage/rec_sys_tutorial/ml-100k\"\n",
    "OUTPUT_CSV = \"/mnt/cluster_storage/rec_sys_tutorial/raw/ratings.csv\"\n",
    "PARQUET_DIR = \"/mnt/cluster_storage/rec_sys_tutorial/raw/ratings_parquet\"\n",
    "\n",
    "# Ensure target directories exist\n",
    "os.makedirs(\"/mnt/cluster_storage/rec_sys_tutorial/raw\", exist_ok=True)\n",
    "\n",
    "# Download only if not already done\n",
    "if not os.path.exists(LOCAL_ZIP):\n",
    "    !wget -q $DATA_URL -O $LOCAL_ZIP\n",
    "\n",
    "# Extract cleanly\n",
    "if not os.path.exists(EXTRACT_DIR):\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(LOCAL_ZIP, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"/mnt/cluster_storage/rec_sys_tutorial\")\n",
    "\n",
    "# Load raw file\n",
    "raw_path = os.path.join(EXTRACT_DIR, \"u.data\")\n",
    "df = pd.read_csv(raw_path, sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "\n",
    "# Persist CSV (kept for later inference cell that expects CSV)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# Persist a Parquet *dataset* (multiple files) to simulate blob storage layout\n",
    "if os.path.exists(PARQUET_DIR):\n",
    "    shutil.rmtree(PARQUET_DIR)\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "NUM_PARQUET_SHARDS = 8\n",
    "for i, shard in enumerate(np.array_split(df, NUM_PARQUET_SHARDS)):\n",
    "    shard.to_parquet(os.path.join(PARQUET_DIR, f\"part-{i:02d}.parquet\"), index=False)\n",
    "\n",
    "print(f\"\u2705 Loaded {len(df):,} ratings \u2192 CSV: {OUTPUT_CSV}\")\n",
    "print(f\"\u2705 Wrote Parquet dataset with {NUM_PARQUET_SHARDS} shards \u2192 {PARQUET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Point to Parquet dataset URI\n",
    "\n",
    "Instead of creating a Ray Dataset from in-memory pandas objects, this tutorial now reads data directly from a **Parquet dataset** stored in persistent cluster storage.\n",
    "\n",
    "This URI will be used by Ray Data to **stream** Parquet shards efficiently across workers without loading the full dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Point to Parquet dataset URI \n",
    "DATASET_URI = os.environ.get(\n",
    "    \"RATINGS_PARQUET_URI\",\n",
    "    \"/mnt/cluster_storage/rec_sys_tutorial/raw/ratings_parquet\",\n",
    ")\n",
    "\n",
    "print(\"Parquet dataset URI:\", DATASET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize dataset: ratings, users, and items\n",
    "\n",
    "Before training, visualize the distribution of ratings, user activity, and item popularity.  \n",
    "These plots serve as a quick sanity check to confirm the dataset loaded correctly and to highlight patterns in user\u2013item interactions:\n",
    "\n",
    "- **Rating distribution:** shows how often each rating (1\u20135 stars) occurs, typically skewed toward higher scores.  \n",
    "- **Ratings per user:** reveals the long-tail behavior where a few users rate many items, while most rate only a few.  \n",
    "- **Ratings per item:** similarly shows that a handful of popular items receive most of the ratings.\n",
    "\n",
    "This visualization works with either raw IDs (`user_id`, `item_id`) or encoded indices (`user_idx`, `item_idx`), depending on what\u2019s available in the current DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Visualize dataset: ratings, user and item activity\n",
    "\n",
    "# Use encoded indices if present; otherwise fall back to raw IDs\n",
    "user_col = \"user_idx\" if \"user_idx\" in df.columns else \"user_id\"\n",
    "item_col = \"item_idx\" if \"item_idx\" in df.columns else \"item_id\"\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Rating distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "df[\"rating\"].hist(bins=[0.5,1.5,2.5,3.5,4.5,5.5], edgecolor='black')\n",
    "plt.title(\"Rating Distribution\")\n",
    "plt.xlabel(\"Rating\"); plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Number of ratings per user\n",
    "plt.subplot(1, 3, 2)\n",
    "df[user_col].value_counts().hist(bins=30, edgecolor='black')\n",
    "plt.title(\"Ratings per User\")\n",
    "plt.xlabel(\"# Ratings\"); plt.ylabel(\"Users\")\n",
    "\n",
    "# Number of ratings per item\n",
    "plt.subplot(1, 3, 3)\n",
    "df[item_col].value_counts().hist(bins=30, edgecolor='black')\n",
    "plt.title(\"Ratings per Item\")\n",
    "plt.xlabel(\"# Ratings\"); plt.ylabel(\"Items\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create Ray Dataset from Parquet and encode IDs\n",
    "\n",
    "Read the MovieLens ratings directly from the **Parquet dataset** using `ray.data.read_parquet()`. This keeps data in a **streaming, non-materialized** form suitable for large-scale distributed processing.\n",
    "\n",
    "Next, build lightweight **global ID mappings** for users and items on the driver to convert raw `user_id` and `item_id` values into contiguous integer indices (`user_idx`, `item_idx`) required for embedding layers.  \n",
    "This mapping step materializes only the distinct IDs (a small subset of the data) while keeping the main dataset lazy and scalable.\n",
    "\n",
    "Finally, apply a `map_batches()` transformation to encode each batch of rows in parallel across the cluster.  \n",
    "The resulting **Ray Dataset** remains distributed and ready for streaming batches directly into the Ray Train workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Create Ray Dataset by reading Parquet, then encode IDs via Ray\n",
    "\n",
    "# Read Parquet dataset directly\n",
    "ratings_ds = ray.data.read_parquet(DATASET_URI)\n",
    "print(\"\u2705 Parquet dataset loaded (streaming, non-materialized)\")\n",
    "ratings_ds.show(3)\n",
    "\n",
    "# ---- Build global ID mappings on the driver ----\n",
    "user_ids = sorted([r[\"user_id\"] for r in ratings_ds.groupby(\"user_id\").count().take_all()])\n",
    "item_ids = sorted([r[\"item_id\"] for r in ratings_ds.groupby(\"item_id\").count().take_all()])\n",
    "\n",
    "user2idx = {uid: j for j, uid in enumerate(user_ids)}\n",
    "item2idx = {iid: j for j, iid in enumerate(item_ids)}\n",
    "\n",
    "NUM_USERS = len(user2idx)\n",
    "NUM_ITEMS = len(item2idx)\n",
    "print(f\"Users: {NUM_USERS:,} | Items: {NUM_ITEMS:,}\")\n",
    "\n",
    "# ---- Encode to contiguous indices within Ray (keeps everything distributed) ----\n",
    "def encode_batch(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    pdf[\"user_idx\"] = pdf[\"user_id\"].map(user2idx).astype(\"int64\")\n",
    "    pdf[\"item_idx\"] = pdf[\"item_id\"].map(item2idx).astype(\"int64\")\n",
    "    return pdf[[\"user_idx\", \"item_idx\", \"rating\", \"timestamp\"]]\n",
    "\n",
    "ratings_ds = ratings_ds.map_batches(encode_batch, batch_format=\"pandas\")\n",
    "print(\"\u2705 Encoded Ray Dataset schema:\", ratings_ds.schema())\n",
    "ratings_ds.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train/validation split using Ray Data  \n",
    "Next, split the dataset into training and validation sets. First, shuffle the entire Ray Dataset to ensure randomization, then split by row index, using 80% for training and 20% for validation.\n",
    "\n",
    "This approach is simple and scalable: Ray handles the shuffling and slicing in parallel across blocks. Also, set a fixed seed to ensure the split is reproducible. After you split it, each dataset remains a fully distributed Ray Dataset, ready to stream into workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Train/val split using Ray Data (lazy, avoids materialization)\n",
    "\n",
    "TRAIN_FRAC = 0.8\n",
    "SEED = 42  # for reproducibility\n",
    "\n",
    "# Block-level shuffle + proportional split (approximate by block, lazy)\n",
    "train_ds, val_ds = (\n",
    "    ratings_ds\n",
    "    .randomize_block_order(seed=SEED)   # lightweight; no row-level materialization\n",
    "    .split_proportionately([TRAIN_FRAC])  # returns [train, remainder]\n",
    ")\n",
    "\n",
    "print(\"\u2705 Train/Val Split:\")\n",
    "print(f\"  Train \u2192 {train_ds.count():,} rows\")\n",
    "print(f\"  Val   \u2192 {val_ds.count():,} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}