{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf00 04-d2 \u00b7 Diffusion-Policy Pattern with Ray Train  \n",
    "In this notebook you build a **mini diffusion-policy pipeline** on a **real Pendulum-v1 offline dataset** and run it end-to-end on an Anyscale cluster with **Ray Train V2**.\n",
    "\n",
    "### What you\u2019ll learn & take away  \n",
    "* How to use **Ray Data** to stream and preprocess Gymnasium rollouts in parallel across CPU workers  \n",
    "* How to scale training across **multiple A10G GPUs** using `TorchTrainer` with a minimal `LightningModule`  \n",
    "* How to **checkpoint every epoch** with `ray.train.report()` for robust fault tolerance and auto-resume  \n",
    "* How to log and visualize metrics using **Ray\u2019s built-in results and observability tooling**  \n",
    "* How to generate actions from a trained policy directly in-notebook, with **no need to repackage or redeploy**  \n",
    "* How to run the full pipeline on **Anyscale Workspaces** with no infrastructure setup or cluster config required  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd22 What problem are you solving? (Inverted Pendulum, Diffusion-Style)\n",
    "\n",
    "You\u2019re training a policy to **swing up and balance an inverted pendulum** \u2014 a classic control problem.  \n",
    "In the Gym `Pendulum-v1` env|ironment, the agent sees the current state of the pendulum and must decide what **torque** to apply at the pivot.\n",
    "\n",
    "---\n",
    "\n",
    "### What's a policy?\n",
    "\n",
    "A **policy** is a function that maps the current state to an action:\n",
    "\n",
    "$$\n",
    "\\pi_\\theta(s_{k}) \\;\\longrightarrow\\; u_{k}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- The **state** $s_k$ describes where the pendulum is and how fast it\u2019s moving  \n",
    "- The **action** $u_k$ is the torque you apply to influence future motion  \n",
    "- The **goal** is to learn a policy that keeps the pendulum upright by generating the right torque at every step\n",
    "\n",
    "---\n",
    "\n",
    "### Environment state and action\n",
    "\n",
    "At each timestep:\n",
    "\n",
    "| Symbol        | Dim    | Meaning                           |\n",
    "|---------------|--------|-----------------------------------|\n",
    "| $\\theta_{k}$    | scalar | Angle of the pendulum             |\n",
    "| $\\dot\\theta_{k}$| scalar | Angular velocity                  |\n",
    "| $u_{k}$         | scalar | Torque applied to the base        |\n",
    "\n",
    "The pendulum starts hanging down and must swing up and balanced.\n",
    "\n",
    "Encode the state as:\n",
    "\n",
    "$$\n",
    "s_{k} = [\\cos\\theta_{k},\\ \\sin\\theta_{k},\\ \\dot\\theta_{k}] \\in \\mathbb{R}^3\n",
    "$$\n",
    "\n",
    "This avoids angle discontinuities (no $\\pm\\pi$ jumps) and keeps values in $[-1, 1]$.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Dataset tuples\n",
    "\n",
    "Train on a **log of actions** from a random policy, then inject artificial noise to simulate the diffusion process:\n",
    "\n",
    "$$\n",
    "\\varepsilon_{k} \\sim \\mathcal{N}(0, 1), \\quad t_{k} \\sim \\text{Uniform}\\{0,\\dots,T{-}1\\}\n",
    "$$\n",
    "\n",
    "and construct a noisy action:\n",
    "\n",
    "$$\n",
    "\\tilde{u}_k = u_{k} + \\varepsilon_{k}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Training objective\n",
    "\n",
    "Train a model $f_\\theta$ to predict the injected noise, given the state, the noisy action, and the timestep:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{s_{k},\\varepsilon_k,t_{k}}\\ \\big\\|f_\\theta(s_k, \\tilde{u}_k, t_{k}) - \\varepsilon_k\\big\\|_2^2\n",
    "$$\n",
    "\n",
    "Minimizing this loss teaches the model to **de-noise** $\\tilde{u}_{k}$ back toward the expert action $u_k$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Reverse diffusion (sampling)\n",
    "\n",
    "At inference time, start from noise $x_T \\sim \\mathcal{N}(0, 1)$ and de-noise step by step:\n",
    "\n",
    "$$\n",
    "x_{t} \\;\\leftarrow\\; x_{t} - \\eta \\cdot f_\\theta(s, x_{t}, t), \\quad t = T{-}1, \\dots, 0\n",
    "$$\n",
    "\n",
    "After $T$ steps:\n",
    "\n",
    "$$\n",
    "x_0 \\approx u^\\star\n",
    "$$\n",
    "\n",
    "is a valid torque for the current state \u2014 a sample from your learned diffusion policy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83e\udded How you\u2019ll scale this policy learning workload using Ray on Anyscale\n",
    "\n",
    "This tutorial shows how to take a **local PyTorch + Gymnasium workflow** and migrate it to a fully **distributed, fault-tolerant Ray pipeline running on Anyscale** with minimal code changes.\n",
    "\n",
    "Here\u2019s how the transition works:\n",
    "\n",
    "1. **Gym rollouts \u2192 Ray Dataset**  \n",
    "   Generate simulation rollouts from `Pendulum-v1` and stream them directly into a **Ray Dataset**, enabling distributed preprocessing (For example, normalization) and automatic partitioning across workers.\n",
    "\n",
    "2. **Local Training \u2192 Cluster-scale Distributed Training**  \n",
    "   Wrap a minimal `LightningModule` in a Ray Train `train_loop`, then launch training with **TorchTrainer** across 8 A10G GPUs. Ray handles data sharding, worker setup, and device placement without boilerplate.\n",
    "\n",
    "3. **Manual State Saving \u2192 Structured Checkpointing & Resumption**  \n",
    "   At the end of each epoch, save model weights and metadata with `ray.train.report(checkpoint=...)`. Ray then **auto-resumes training** from the latest checkpoint after restarts. This requires no further logic.\n",
    "\n",
    "4. **Ad-hoc Coordination \u2192 Declarative Orchestration**  \n",
    "   Replace manual logging, retry logic, and resource management with **Ray-native configs** (`ScalingConfig`, `CheckpointConfig`, `FailureConfig`), letting Ray + Anyscale own the orchestration.\n",
    "\n",
    "5. **Notebook-only Inference \u2192 Cluster-aware Evaluation**  \n",
    "   After training, perform **reverse diffusion sampling** in-notebook using the latest checkpoint\u2014but this can easily scale to distributed Ray tasks or serve as the basis for a production rollout.\n",
    "\n",
    "This flow upgrades a local notebook into a **multi-node, resilient training + inference pipeline**, using Ray\u2019s native abstractions and running seamlessly inside an Anyscale Workspace, without sacrificing dev agility.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}