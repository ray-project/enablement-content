{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAPI webservice and deploy a model\n",
    "FastAPI is used to create a webservice 'app' to accept HTTP requests.\n",
    "\n",
    "MySentimentModel class loads the ML model and defines *predict* function for online inference. @serve.deployment decorator defines the Ray Serve deployment.\n",
    "\n",
    "*@app.get()* is used to create a GET '/predict' route. Similarly, @app.post() can be used POST requests. See https://docs.ray.io/en/latest/serve/http-guide.html for more details.\n",
    "\n",
    "In this example, *application_logic()* function is used to define a sample transformation or business logic that can be applied before sending the input to the ML model for inference. See inline comments for further explanation.\n",
    "\n",
    "### Scaling deployment\n",
    "*num_replicas* parameter sets the number of instances of the deployment. FastAPI and RayServe automatically load balances to send requests to each instance. There are more options to set the *accelerator_type* to GPU and even use fractional GPUs. See configuration options here: https://docs.ray.io/en/latest/serve/configure-serve-deployment.html ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define a Ray Serve deployment\n",
    "# This decorator registers the class as a Ray Serve deployment\n",
    "@serve.deployment(num_replicas=2) # num_replicas specifies the number of replicas for load balancing\n",
    "@serve.ingress(app) # This decorator allows the FastAPI app to be served by Ray Serve\n",
    "class MySentimentModel:\n",
    "    def __init__(self):\n",
    "        # Load a pre-trained sentiment analysis model\n",
    "        self.model = pipeline(\"sentiment-analysis\",\n",
    "                              model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "    # Define any necessary application logic or transformation logic\n",
    "    def application_logic(self, text):\n",
    "        \"\"\"        Apply any necessary application logic to the input text.\n",
    "        \"\"\"\n",
    "        # simple application logic: truncate text if it exceeds a certain length\n",
    "        if len(text) > 50:\n",
    "            return text[:50].lower()  # Truncate and convert to lowercase\n",
    "        else:\n",
    "            return text.lower()\n",
    "        \n",
    "    @app.get(\"/predict\") # Define an endpoint for predictions\n",
    "    def predict(self, text: str):\n",
    "        \"\"\"        Predict sentiment for the given text.\n",
    "        \"\"\"\n",
    "        # Define any necessary application logic or transformation logic\n",
    "        text = self.application_logic(text) # Apply any necessary application logic to the input text\n",
    "\n",
    "        # Use the model to make a prediction\n",
    "        result = self.model(text)\n",
    "        return {\"text\": text, \"sentiment\": result}\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 09:16:58,624\tINFO worker.py:1908 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(ProxyActor pid=56603)\u001b[0m INFO 2025-07-11 09:16:59,927 proxy 127.0.0.1 -- Proxy starting on node c2986ec5ee9361515a52f3c506843d5d247ffc6732250203b8a47f21 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=56603)\u001b[0m INFO 2025-07-11 09:16:59,946 proxy 127.0.0.1 -- Got updated endpoints: {}.\n",
      "INFO 2025-07-11 09:16:59,975 serve 18071 -- Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ServeController pid=56591)\u001b[0m INFO 2025-07-11 09:17:00,075 controller 56591 -- Deploying new version of Deployment(name='MySentimentModel', app='default') (initial target replicas: 2).\n",
      "\u001b[36m(ServeController pid=56591)\u001b[0m INFO 2025-07-11 09:17:00,178 controller 56591 -- Adding 2 replicas to Deployment(name='MySentimentModel', app='default').\n",
      "\u001b[36m(ProxyActor pid=56603)\u001b[0m INFO 2025-07-11 09:17:00,076 proxy 127.0.0.1 -- Got updated endpoints: {Deployment(name='MySentimentModel', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=56603)\u001b[0m INFO 2025-07-11 09:17:00,080 proxy 127.0.0.1 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x169597d10>.\n",
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=56595)\u001b[0m Device set to use mps:0\n",
      "INFO 2025-07-11 09:17:05,114 serve 18071 -- Application 'default' is ready at http://127.0.0.1:8000/.\n",
      "INFO 2025-07-11 09:17:05,120 serve 18071 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x337673dd0>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeploymentHandle(deployment='MySentimentModel')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=56599)\u001b[0m INFO 2025-07-11 09:20:39,996 default_MySentimentModel kcd1ofsq 1d920c68-1c59-4e2e-a64c-04f87d31bab7 -- GET /predict 200 128.0ms\n",
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=56595)\u001b[0m INFO 2025-07-11 09:21:11,350 default_MySentimentModel t4o4t2pu dda6dadc-f201-421d-9bbc-638d92b9f065 -- GET /predict 200 543.2ms\n",
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=56599)\u001b[0m INFO 2025-07-11 09:21:15,969 default_MySentimentModel kcd1ofsq e3611603-54d1-491a-8b05-1f693f413243 -- GET /predict 200 36.5ms\n",
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=56599)\u001b[0m INFO 2025-07-11 09:22:16,285 default_MySentimentModel kcd1ofsq 5be0e806-0bd5-4992-b15d-c59fd52e6195 -- GET /predict 200 422.7ms\n",
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=56599)\u001b[0m INFO 2025-07-11 09:22:58,953 default_MySentimentModel kcd1ofsq 8890c548-abf6-49b2-b923-b525fcc403da -- GET /predict 200 76.5ms\n",
      "\u001b[36m(ServeController pid=56591)\u001b[0m INFO 2025-07-11 09:23:20,248 controller 56591 -- Removing 2 replicas from Deployment(name='MySentimentModel', app='default').\n",
      "\u001b[36m(ServeController pid=56591)\u001b[0m INFO 2025-07-11 09:23:22,283 controller 56591 -- Replica(id='t4o4t2pu', deployment='MySentimentModel', app='default') is stopped.\n",
      "\u001b[36m(ServeController pid=56591)\u001b[0m INFO 2025-07-11 09:23:22,283 controller 56591 -- Replica(id='kcd1ofsq', deployment='MySentimentModel', app='default') is stopped.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "serve.run(MySentimentModel.bind()) # Bind the deployment to the Ray Serve runtime\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}