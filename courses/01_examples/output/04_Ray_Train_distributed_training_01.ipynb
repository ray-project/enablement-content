{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training with Ray Train, PyTorch and Hugging Face\n",
    "\u00a9 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udcbb **Launch Locally**: You can run this notebook locally.\n",
    "\n",
    "\ud83d\ude80 **Launch on Cloud**: Think about running this notebook on a Ray Cluster (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates how to perform distributed training of a BERT model for sequence classification using Ray Train, PyTorch, and Hugging Face libraries. The goal is to classify Yelp reviews into categories by leveraging the power of distributed computing, which allows you to train large models efficiently across multiple CPUs or GPUs.\n",
    "\n",
    "The notebook starts by importing all the necessary libraries, including PyTorch for deep learning, Hugging Face Transformers for model and tokenizer utilities, and Ray Train for distributed training. It then sets up the evaluation metric (accuracy) and defines a function to compute this metric during model evaluation.\n",
    "\n",
    "A key part of the notebook is the training function, which is executed by each worker in the distributed setup. This function handles loading the Yelp review dataset, tokenizing the text data, preparing data loaders for batching, and setting up the BERT model for training. The function is designed to automatically use the best available hardware, whether that's a CPU, GPU, or Apple Silicon's MPS.\n",
    "\n",
    "The main training function, `train_bert`, configures the distributed environment using Ray, sets up the training parameters, and launches the training process across multiple workers. This approach allows you to scale up your training easily, making it suitable for both local machines and cloud platforms. After training, Ray is properly shut down to free up resources.\n",
    "\n",
    "Overall, this notebook provides a practical introduction to distributed deep learning with modern Python tools, making it easier for machine learning engineers to train large models on big datasets efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<ol>\n",
    "    <li>Architecture Diagram\n",
    "    <li>Library Imports\n",
    "        <ul>\n",
    "            <li>Importing PyTorch, Hugging Face Transformers, Ray Train, and other dependencies\n",
    "        </ul>\n",
    "    <li>Metrics Setup\n",
    "        <ul>\n",
    "            <li>Defining accuracy as the evaluation metric\n",
    "            <li>Function to compute metrics during evaluation\n",
    "        </ul>\n",
    "    <li>Training Function Per Worker\n",
    "        <ul>\n",
    "            <li>Data loading and preprocessing (tokenization)\n",
    "            <li>Preparing data loaders for batching\n",
    "            <li>Model initialization (BERT for sequence classification)\n",
    "            <li>Device selection (CPU, GPU, or MPS)\n",
    "            <li>Training and evaluation loop\n",
    "        </ul>\n",
    "    <li>Main Training Function\n",
    "        <ul>\n",
    "            <li>Setting up distributed training configuration with Ray\n",
    "            <li>Scaling configuration for CPUs/GPUs\n",
    "            <li>Initializing and running the Ray TorchTrainer\n",
    "        </ul>\n",
    "    <li>Running the Training\n",
    "        <ul>\n",
    "            <li>Executing the main training function with a specified number of workers\n",
    "        </ul>\n",
    "    <li>Shutdown Ray Cluster\n",
    "    <li>Summary\n",
    "</ol>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}