{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04e Recommendation system pattern with Ray Train \n",
    "This notebook builds a **scalable matrix factorization recommendation system** using the **MovieLens 100K** dataset, fully distributed on an Anyscale cluster with **Ray Train V2** and **Ray Data**. For larger scale recommendation use-cases we additionally have an integration with TorchRec. An example can be found [here](https://github.com/ray-project/ray/tree/d84d0fd0e88f116302c3fa22ed80fbc3d358c4a3/release/train_tests/benchmark/recsys).\n",
    "\n",
    "### What you learn and take away  \n",
    "* How to use **Ray Data** to load, encode, and shard tabular datasets across many workers  \n",
    "* How to **stream training data** directly into PyTorch using `iter_torch_batches()`  \n",
    "* How to build a **custom training loop with validation and checkpointing** using `ray.train.report()`  \n",
    "* How to use **Ray Train V2's fault-tolerant trainer** to resume training from the latest checkpoint with no extra logic  \n",
    "* How to separate **training, evaluation, and inference** while keeping all code modular and distributed-ready  \n",
    "* How to run real-world recommendation workloads with **no changes to your model code**, using Ray\u2019s orchestration  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What problem are you solving? (matrix factorization for recommendations)\n",
    "\n",
    "Build a **collaborative filtering recommendation system** that predicts how much a user likes an item  \n",
    "based on **historical interaction data**\u2014in this case, user ratings from the MovieLens 100K dataset.\n",
    "\n",
    "Use **matrix factorization**, a classic yet scalable approach where you embed each user and item in a latent space.  \n",
    "The model learns to represent users and items as vectors and predicts ratings by computing their dot product.\n",
    "\n",
    "---\n",
    "\n",
    "### Input: user\u2013item\u2013rating triples\n",
    "\n",
    "Each row in the dataset represents a user\u2019s explicit rating of a movie:\n",
    "\n",
    "$$\n",
    "(u, {i}, r) \\in \\{\\text{users}\\} \\times \\{\\text{items}\\} \\times \\{1, 2, 3, 4, 5\\}\n",
    "$$\n",
    "\n",
    "Encode these using contiguous integer indices (`user_idx`, `item_idx`)  \n",
    "and normalize them for efficient embedding lookup and training.\n",
    "\n",
    "---\n",
    "\n",
    "### Model: embedding-based matrix factorization\n",
    "\n",
    "Learn an embedding vector for each user and each item:\n",
    "\n",
    "$$\n",
    "U_{u} \\in \\mathbb{R}^d, \\quad V_{i} \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "The predicted rating is the dot product of these vectors:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,{i}} = U_{u}^\\top V_{i}\n",
    "$$\n",
    "\n",
    "The embedding dimension $d$ controls model capacity.\n",
    "\n",
    "---\n",
    "\n",
    "### Training objective\n",
    "\n",
    "Minimize **Mean Squared Error (MSE)** between predicted and actual ratings:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{(u, {i}, r)}\\ \\big(\\hat{r}_{u,{i}} - r\\big)^2\n",
    "$$\n",
    "\n",
    "This encourages the model to assign higher scores to user\u2013item pairs that historically received high ratings.\n",
    "\n",
    "---\n",
    "\n",
    "### Inference: ranking items per user\n",
    "\n",
    "Once the model is trained, you can recommend items by computing predicted scores for a target user  \n",
    "against **all items in the catalog** (approximate methods can later be applied at scale):\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u, *} = U_{u}^\\top V^\\top\n",
    "$$\n",
    "\n",
    "Sort these scores and return the top-N items as personalized recommendations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to migrate this recommendation system workload to a distributed setup using Ray on Anyscale\n",
    "\n",
    "This tutorial **migrates a local matrix factorization pipeline for recommendation into a distributed, fault-tolerant training loop using Ray Train and Ray Data on Anyscale**.\n",
    "\n",
    "Approach the transition with the following steps:\n",
    "\n",
    "1. **Convert parquet files to sharded Ray Dataset**  \n",
    "   Load MovieLens 100K to parquet, encode the IDs to create a **multi-block Ray Dataset**. Each block is a training shard that Ray can distribute across workers.\n",
    "\n",
    "2. **Stream Torch data loaders**  \n",
    "   Instead of manually writing PyTorch `Dataset` logic, use `iter_torch_batches()` from **Ray Data** to stream batches directly into each worker. Ray handles all the parallelism and sharding behind the scenes.\n",
    "\n",
    "3. **Convert a single-node PyTorch process to a multi-GPU distributed training**  \n",
    "   Write a minimal `train_loop_per_worker` that runs on each Ray worker. Using `TorchTrainer` and `prepare_model()`, scale this loop across 8 GPU workers automatically, where each works on its own data shard.\n",
    "\n",
    "4. **Configure structured epoch logging and checkpoints**  \n",
    "   Each epoch logs `train_loss` and `val_loss` and report checkpoints with `ray.train.report(checkpoint=...)`. This enables **automatic recovery and metric tracking** without any additional code.\n",
    "\n",
    "5. **Declaratively configure tolerance, checkpointing and scaling**  \n",
    "   Configure fault tolerance, checkpointing, and scaling using `ScalingConfig`, `CheckpointConfig`, and `FailureConfig`. This lets Ray and Anyscale handle retries, recovery, and GPU orchestration.\n",
    "\n",
    "6. **Write lightweight Python functions for post- inferfence**  \n",
    "   After training, load the latest checkpoint and generate top-N recommendations for any user with a simple forward pass. No retraining, no re-initialization, just pure PyTorch inference.\n",
    "\n",
    "With just a few changes to your core code, scale a traditional recommendation pipeline across a Ray cluster with **distributed data loading, checkpointing, fault tolerance, and parallel training**, all fully managed by Anyscale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}