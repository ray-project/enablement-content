
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Introduction to Ray Serve LLM: Foundations of Large Language Model Serving &#8212; Course Notebooks</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom_hide.css?v=af9667c8" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'courses/llm_serving/00_intro_serve_llm/notebook';</script>
    <script src="../../../_static/custom_toggle.js?v=1235ef5b"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Course Notebooks</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Ray Enablement Content
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Intro to Ray</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_01.html">Introduction to Ray Core: Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_02.html">0. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_03.html">1. Creating Remote Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_04.html">2. Executing Remote Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_05.html">4. Putting It All Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_01.html">Introduction to Ray Core (Advancement): Object store, Tasks, Actors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_02.html">1. Object store</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_03.html">2. Chaining Tasks and Passing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_04.html">3. Task retries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_05.html">4. Task Runtime Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_06.html">5. Resource allocation and management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_07.html">6. Nested Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_08.html">7. Pattern: Pipeline data processing and waiting for results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_09.html">8. Ray Actors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_01.html">Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_02.html">1. Overview of the Ray AI Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_03.html">2. Quick end-to-end example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_01.html">Introduction to Ray Train + PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_02.html">1. When to use Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_03.html">2. Single GPU Training with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_04.html">3. Distributed Data Parallel Training with Ray Train and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01.html">Introduction to Ray Train: Ray Train + PyTorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02.html">1. When to use Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03.html">2. Single GPU Training with PyTorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04.html">3. Distributed Training with Ray Train and PyTorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05.html">4. Ray Train in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_01.html">Introduction to Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_02.html">1. Loading the data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_03.html">2. Starting out with vanilla PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_04.html">3. Hyperparameter tuning with Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_05.html">4. Ray Tune in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_01.html">Introduction to Ray Data: Industry Landscape</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_02.html">The Compute Layer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_03.html">The Orchestration Layer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_04.html">Distributed Computing Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_05.html">Data Processing with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_06.html">Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_01.html">Introduction to Ray Data: Ray Data + Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_02.html">0. What is Ray Data?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_03.html">2. Loading Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_04.html">3. Transforming Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_05.html">4. Writing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_06.html">5. Data Operations: Shuffling, Grouping and Aggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_07.html">6. When to use Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_01.html">Intro to Ray Data:  Ray Data + Unstructured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_02.html">1. When to Consider Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_03.html">2. How to work with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_04.html">3. Loading data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_05.html">3. Lazy execution mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_06.html">4. Transforming data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_07.html">5. Stateful transformations with Ray Actors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_08.html">6. Materializing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_09.html">7. Data Operations: grouping, aggregation, and shuffling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_10.html">8. Persisting data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_11.html">9. Ray Data in production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_01.html">Introduction to Ray Serve with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_02.html">1. When to Consider Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_03.html">2. Overview of Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_04.html">3. Implement an image classification service</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_05.html">4. Development workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../00_Developer_Intro_to_Ray/output/README_01.html">Introduction to Ray: Developer</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">01 Examples</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/01_Ray_Data_batch_inference_01.html">Batch Inference with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/01_Ray_Data_batch_inference_02.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/01_Ray_Data_batch_inference_03.html">Load a dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/01_Ray_Data_batch_inference_04.html">Batch Inference Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/01_Ray_Data_batch_inference_05.html">Create a batch data and call the model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/01_Ray_Data_batch_inference_06.html">Run inference on the entire dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/02_Ray_Data_data_processing_01.html">Data Processing with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/02_Ray_Data_data_processing_02.html">Library Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/02_Ray_Data_data_processing_03.html">Convert to Ray Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/02_Ray_Data_data_processing_04.html">Filter Ray Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/02_Ray_Data_data_processing_05.html">Join Two Ray Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/02_Ray_Data_data_processing_06.html">Preprocessing with a Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/03_Ray_Serve_online_serving_01.html">Online Model Serving with Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/03_Ray_Serve_online_serving_02.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/03_Ray_Serve_online_serving_03.html">FastAPI webservice and deploy a model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/03_Ray_Serve_online_serving_04.html">Simulate Client: Send test requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/04_Ray_Train_distributed_training_01.html">Distributed training with Ray Train, PyTorch and Hugging Face</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/04_Ray_Train_distributed_training_02.html">1. Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/04_Ray_Train_distributed_training_03.html">3. Metrics Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/04_Ray_Train_distributed_training_04.html">4. Training function per worker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/04_Ray_Train_distributed_training_05.html">5. Main Training Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../01_examples/output/04_Ray_Train_distributed_training_06.html">6. Start Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">02 Anyscale Admin</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01.html">Anyscale Administrator Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02.html">1. What is an Anyscale Cloud?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03.html">2. Cloud Deployment Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04.html">3. A Demonstrative Example of Resource Creation with AWS EC2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05.html">3.1 IAM Role Definition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06.html">4. Register Anyscale Cloud to Your Cloud Provider</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01.html">Deployment Options: Virtual Machines vs. Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02.html">2. Virtual Machines (VM) vs. Kubernetes (K8s)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03.html">3. (Optional) More Kubernetes Deployments Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01.html">Introduction: Deploy Anyscale Ray on AWS EC2 Instances</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02.html">1. Create Anyscale Resources with Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03.html">2. Register the Anyscale Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04.html">3. Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05.html">4. Cleanup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06.html">5. Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01.html">Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02.html">Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03.html">1. Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04.html">2. Create Anyscale Resources with Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05.html">3. Register the Anyscale Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06.html">4. Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07.html">5. Cleanup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01.html">Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02.html">1. Create Anyscale Resources with Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03.html">2. Install Kubernetes Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04.html">3. Register the Anyscale Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05.html">4. Install the Anyscale Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06.html">5. Verify the Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07.html">6. Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08.html">7. Clean up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09.html">8. Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01.html">Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02.html">Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03.html">1. Create Anyscale Resources with Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04.html">2. Attach Required IAM Policies to Your existing EKSâ€™s Node Role</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05.html">3. Install Kubernetes Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06.html">4. Register the Anyscale Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07.html">5. Install the Anyscale Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08.html">6. Verify the Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09.html">7. Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10.html">8. Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11.html">9. Clean up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12.html">10. Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01.html">Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02.html">Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03.html">1. Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04.html">2. Create Anyscale Resources with Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05.html">3. Troubleshooting GPU Availability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06.html">4. kubectl Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07.html">5. Install NGINX Ingress Controller</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08.html">6. (Optional) Upgrade Anyscale Dependencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09.html">7. Register the Anyscale Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10.html">8. Install the Anyscale Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11.html">8. Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12.html">9. Cleanup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">03 Observability</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../03_Observability/output/01_general_intro_and_setup_01.html">Observability Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../03_Observability/output/01_general_intro_and_setup_02.html">Observability Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../03_Observability/output/01_general_intro_and_setup_03.html">Setting Up Local Ray Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01.html">Ray and Anyscale Observability Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02.html">Ray Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03.html">Anyscale Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01.html">Ray and Anyscale Observability in Detail</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02.html">Data Pipeline Observability (Ray Data)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03.html">Web Application Observability (Ray Serve)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Gettingstarted</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_01_anyscale_intro_workspace_01.html">101 â€” Introduction to Anyscale Workspaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_02_anyscale_development_intro_01.html">101 â€“ Developing Application with Anyscale</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_03_anyscale_compute_runtime_intro_01.html">101 â€“ Compute Configs and Execution Environments in Anyscale</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_04_anyscale_storage_options_01.html">101 â€“ Storage Options in the Anyscale Platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_05_anyscale_logging_metrics_01.html">101 â€“ Debug and Monitor Your Anyscale Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_06_anyscale_intro_jobs_01.html">101 â€“ Introduction to Anyscale Jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_07_anyscale_intro_services_01.html">101 â€“  Introduction to Anyscale Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_08_anyscale_collaboration_01.html">101 â€“ Collaboration on Anyscale</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_09_anyscale_org_setup_01.html">101 - Anyscale Organization and Cloud Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_09_anyscale_org_setup_02.html">ðŸ“Œ Overview of Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_09_anyscale_org_setup_03.html">ðŸ§  Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_anyscale_intro_jobs_01.html">Content Used</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_anyscale_intro_jobs_02.html">Part 1. Creating and Submitting your first job</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_anyscale_intro_jobs_03.html">Part 2. Automation and Scheduling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_anyscale_intro_services_01.html">Sources</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../GettingStarted/output/101_anyscale_intro_services_02.html">Part 1: Starting your first Anyscale Service</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Anyscale 101</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../anyscale_101/output/101_anyscale_intro_jobs_01.html">Content Used</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../anyscale_101/output/101_anyscale_intro_jobs_02.html">Part 1. Creating and Submitting your first job</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../anyscale_101/output/101_anyscale_intro_jobs_03.html">Part 2. Automation and Scheduling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../anyscale_101/output/101_anyscale_intro_services_01.html">Sources</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../anyscale_101/output/101_anyscale_intro_services_02.html">Part 1: Starting your first Anyscale Service</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Llm Serving</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="output/notebook_01.html">Introduction to Ray Serve LLM: Foundations of Large Language Model Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/notebook_02.html">What is LLM Serving?</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/notebook_03.html">Key Concepts and Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/notebook_04.html">Challenges in LLM Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/notebook_05.html">Ray Serve LLM + Anyscale Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/notebook_06.html">Getting Started with Ray Serve LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/notebook_07.html">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_deploy_medium_llm/output/notebook_01.html">Deploy a Medium-Sized LLM with Ray Serve LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_deploy_medium_llm/output/notebook_02.html">Overview: Why Medium-Sized Models?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_deploy_medium_llm/output/notebook_03.html">Setting up Ray Serve LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_deploy_medium_llm/output/notebook_04.html">Local Deployment &amp; Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_deploy_medium_llm/output/notebook_05.html">Deploying to Anyscale Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_deploy_medium_llm/output/notebook_06.html">Advanced Topics: Monitoring &amp; Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_deploy_medium_llm/output/notebook_07.html">Summary &amp; Outlook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_advanced_llm_features/output/notebook_01.html">Advanced LLM Features with Ray Serve LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_advanced_llm_features/output/notebook_02.html">Overview: Advanced Features Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_advanced_llm_features/output/notebook_03.html">Example: Deploying LoRA Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_advanced_llm_features/output/notebook_04.html">Example: Getting Structured JSON Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_advanced_llm_features/output/notebook_05.html">Example: Setting up Tool Calling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_advanced_llm_features/output/notebook_06.html">How to Choose an LLM?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_advanced_llm_features/output/notebook_07.html">Conclusion: Next Steps</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Ray 101</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/1_AI_Libs_Intro_01.html">Introduction to the Ray AI Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/1_AI_Libs_Intro_02.html">1. Overview of the Ray AI Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/1_AI_Libs_Intro_03.html">2. End-to-end example: predicting taxi tips in New York</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/2_Intro_Train_01.html">Introduction to Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/2_Intro_Train_02.html">1. PyTorch introductory example (single GPU)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/2_Intro_Train_03.html">2. Distributed Data Parallel Training with Ray Train and PyTorch (multiple GPUs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/2_Intro_Train_04.html">3. Overview of the training loop in Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/2_Intro_Train_05.html">4. Migrating the model and dataset to Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/2_Intro_Train_06.html">5. Reporting checkpoints and metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/2_Intro_Train_07.html">6. Launching the distributed training job</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/2_Intro_Train_08.html">7. Accessing the training results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/2_Intro_Train_09.html">8. Ray Train in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/3_Intro_Tune_01.html">Intro to Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/3_Intro_Tune_02.html">1. Loading and visualizing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/3_Intro_Tune_03.html">2. Setting up a PyTorch model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/3_Intro_Tune_04.html">3. Introduction to Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/3_Intro_Tune_05.html">4. Diving deeper into Ray Tune concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/3_Intro_Tune_06.html">5. Hyperparameter tuning the PyTorch model using Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/4_Intro_Data_01.html">Intro to Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/4_Intro_Data_02.html">1. When to use Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/4_Intro_Data_03.html">2. Loading Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/4_Intro_Data_04.html">3. Transforming Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/4_Intro_Data_05.html">4. Data Operations: Grouping, Aggregation, and Shuffling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/4_Intro_Data_06.html">5. Persisting Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/5_Intro_Serve_01.html">Intro to Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/5_Intro_Serve_02.html">1. Overview of Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/5_Intro_Serve_03.html">2. Implement an Classifier service</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/5_Intro_Serve_04.html">3. Advanced features of Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/5_Intro_Serve_05.html">4. Ray Serve in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-101/output/5_Intro_Serve_06.html">Clean up</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Ray Train 201</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_01.html">ðŸ“š 01 Â· Introduction to Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_02.html">01 Â· Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_03.html">04 Â· Define ResNet-18 Model for MNIST</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_04.html">05 Â· Define the Ray Train Loop (DDP per-worker)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_05.html">06 Â· Define <code class="docutils literal notranslate"><span class="pre">train_loop_config</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_06.html">07 Â· Configure Scaling with <code class="docutils literal notranslate"><span class="pre">ScalingConfig</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_07.html">08 Â· Wrap the Model with <code class="docutils literal notranslate"><span class="pre">prepare_model()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_08.html">09 Â· Build the DataLoader with <code class="docutils literal notranslate"><span class="pre">prepare_data_loader()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_09.html">10 Â· Report Training Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_10.html">11 Â· Save Checkpoints and Report Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_11.html">14 Â· Create the <code class="docutils literal notranslate"><span class="pre">TorchTrainer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_12.html">16 Â· Inspect the Training Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_13.html">18 Â· Load a Checkpoint for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_14.html">ðŸ”„ 02 Â· Integrating Ray Train with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_15.html">01 Â· Define Training Loop with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_16.html">02 Â· Build DataLoader from Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_17.html">03 Â· Prepare Dataset for Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_18.html">05 Â· Define Image Transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_19.html">07 Â· Configure <code class="docutils literal notranslate"><span class="pre">TorchTrainer</span></code> with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_20.html">ðŸ›¡ï¸ 03 Â· Fault Tolerance in Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_21.html">01 Â· Modify Training Loop to Enable Checkpoint Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_22.html">02 Â· Save Full Checkpoint with Extra State</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_23.html">04 Â· Launch Fault-Tolerant Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_24.html">05 Â· Manual Restoration from Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_25.html">07 Â· Clean Up Cluster Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/01_02_03_intro_to_ray_train_26.html">ðŸŽ‰ Wrapping Up &amp; Next Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04a_vision_pattern_01.html">04a Computer-vision pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04a_vision_pattern_02.html">1. Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04a_vision_pattern_03.html">6. Custom <code class="docutils literal notranslate"><span class="pre">Food101Dataset</span></code> for Parquet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04a_vision_pattern_04.html">10. Helper: Ray-prepared DataLoaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04a_vision_pattern_05.html">11. <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04a_vision_pattern_06.html">12. Launch distributed training with <code class="docutils literal notranslate"><span class="pre">TorchTrainer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04a_vision_pattern_07.html">13. Plot training and validation loss curves</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04a_vision_pattern_08.html">14. Demonstrate fault-tolerant resumption</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04a_vision_pattern_09.html">15. Batch inference with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04b_tabular_workload_pattern_01.html">04b Tabular workload pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04b_tabular_workload_pattern_02.html">1. Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04b_tabular_workload_pattern_03.html">8. Define the Ray Train worker loop (Arrow-based, memory-efficient)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04b_tabular_workload_pattern_04.html">12. Confusion matrix visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04b_tabular_workload_pattern_05.html">15. Continue training from the latest checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04c_time_series_workload_pattern_01.html">04c Time-Series workload pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04c_time_series_workload_pattern_02.html">1. Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04c_time_series_workload_pattern_03.html">9. PositionalEncoding and Transformer model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04c_time_series_workload_pattern_04.html">10. Ray Train training loop (with teacher forcing)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04c_time_series_workload_pattern_05.html">13. Resume training from checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04c_time_series_workload_pattern_06.html">14. Inference helper â€” Ray Data batch predictor on GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d1_generative_cv_pattern_01.html">04-d1 Generative computer-vision pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d1_generative_cv_pattern_02.html">1. Imports and setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d1_generative_cv_pattern_03.html">8. Pixel diffusion LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d1_generative_cv_pattern_04.html">9. Ray Train <code class="docutils literal notranslate"><span class="pre">train_loop</span></code> (Lightning + Ray integration)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d1_generative_cv_pattern_05.html">12. Resume from latest checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d1_generative_cv_pattern_06.html">13. Reverse diffusion sampler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d2_policy_learning_pattern_01.html">04-d2 Diffusion-Policy Pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d2_policy_learning_pattern_02.html">1. Imports and setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d2_policy_learning_pattern_03.html">4. DiffusionPolicy LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d2_policy_learning_pattern_04.html">5. Distributed Train loop with checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04d2_policy_learning_pattern_05.html">8. Reverse diffusion helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04e_rec_sys_workload_pattern_01.html">04e Recommendation system pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04e_rec_sys_workload_pattern_02.html">1. Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04e_rec_sys_workload_pattern_03.html">7. Define matrix factorization model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04e_rec_sys_workload_pattern_04.html">8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04e_rec_sys_workload_pattern_05.html">11. Resume training from checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ray-train-201/output/04e_rec_sys_workload_pattern_06.html">12. Inference: recommend top-N items for a user</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/courses/llm_serving/00_intro_serve_llm/notebook.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction to Ray Serve LLM: Foundations of Large Language Model Serving</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-llm-serving">What is LLM Serving?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-llm-text-generation-process">The LLM Text Generation Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#two-phases-of-llm-inference">Two Phases of LLM Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prefill-phase">Prefill Phase</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decode-phase">Decode Phase</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts-and-optimizations">Key Concepts and Optimizations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-kv-caching">1. Key-Value (KV) Caching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-batching">2. Continuous Batching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parallelization-or-alternatives">3. Model parallelization or alternatives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-window-considerations">4. Context Window Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-llm-serving">Challenges in LLM Serving</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-management">1. Memory Management</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latency-requirements">2. Latency Requirements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scalability-demands">3. Scalability Demands</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-optimization">4. Cost Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-not-kubernetes">Why not Kubernetes ?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ray-serve-llm-anyscale-architecture">Ray Serve LLM + Anyscale Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ray-serve-for-orchestration">1. Ray Serve for Orchestration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-as-the-inference-engine">2. vLLM as the inference engine</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#anyscale-for-infrastructure">3. Anyscale for Infrastructure</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started-with-ray-serve-llm">Getting Started with Ray Serve LLM</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-configuration">Step 1: Configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-deployment">Step 2: Deployment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-querying">Step 3: Querying</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-shutdown">Step 4: Shutdown</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">Key Takeaways</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resources">Resources</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-ray-serve-llm-foundations-of-large-language-model-serving">
<h1>Introduction to Ray Serve LLM: Foundations of Large Language Model Serving<a class="headerlink" href="#introduction-to-ray-serve-llm-foundations-of-large-language-model-serving" title="Link to this heading">#</a></h1>
<p>Â© 2025, Anyscale. All Rights Reserved</p>
<p>ðŸ’» <strong>Launch Locally</strong>: You can run this notebook locally, but performance will be reduced.</p>
<p>ðŸš€ <strong>Launch on Cloud</strong>: A Ray Cluster with GPUs (Click <a class="reference external" href="http://console.anyscale.com/register">here</a> to easily start a Ray cluster on Anyscale) is recommended to run this notebook.</p>
<p>This module provides a comprehensive introduction to serving Large Language Models (LLMs) with Ray Serve LLM. Weâ€™ll explore the fundamentals of LLM serving, understand the challenges, and learn how Ray Serve LLM provides production-grade solutions for deploying LLMs at scale.</p>
<div class="alert alert-block alert-info">
<b> Here is the roadmap for this module:</b>
<ul>
    <li>What is LLM Serving?</li>
    <li>Key Concepts and Optimizations</li>
    <li>Challenges in LLM Serving</li>
    <li>Ray Serve LLM Architecture</li>
    <li>Getting Started with Ray Serve LLM</li>
    <li>Key Takeaways</li>
</ul>
</div>
<section id="what-is-llm-serving">
<h2>What is LLM Serving?<a class="headerlink" href="#what-is-llm-serving" title="Link to this heading">#</a></h2>
<p>Large Language Model (LLM) serving refers to the process of deploying trained language models to production environments where they can handle user requests and generate responses in real-time. This is fundamentally different from training models - serving focuses on making models available, scalable, and performant for end users.</p>
<section id="the-llm-text-generation-process">
<h3>The LLM Text Generation Process<a class="headerlink" href="#the-llm-text-generation-process" title="Link to this heading">#</a></h3>
<p>LLMs operate as <strong>next-token predictors</strong>. Hereâ€™s how they work:</p>
<ol class="arabic simple">
<li><p><strong>Tokenization</strong>: Input text is converted into tokens (words, subwords, or characters)</p></li>
<li><p><strong>Processing</strong>: The model processes these tokens to understand context</p></li>
<li><p><strong>Generation</strong>: The model generates output one token at a time</p></li>
<li><p><strong>Completion</strong>: Generation stops when reaching stopping criteria or maximum length</p></li>
</ol>
<img src="https://anyscale-materials.s3.us-west-2.amazonaws.com/public-images/ray-serve-llm/diagrams/LLM-text-generation.png" width="800">
</section>
<section id="two-phases-of-llm-inference">
<h3>Two Phases of LLM Inference<a class="headerlink" href="#two-phases-of-llm-inference" title="Link to this heading">#</a></h3>
<p>LLM inference operates through two distinct phases that determine performance characteristics:</p>
<section id="prefill-phase">
<h4>Prefill Phase<a class="headerlink" href="#prefill-phase" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The model encodes <strong>all input tokens simultaneously</strong></p></li>
<li><p>High efficiency through parallelized computations</p></li>
<li><p>Maximizes GPU utilization</p></li>
<li><p>Precomputes and caches key-value (KV) vectors as intermediate token representations</p></li>
</ul>
</section>
<section id="decode-phase">
<h4>Decode Phase<a class="headerlink" href="#decode-phase" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The model generates tokens <strong>sequentially</strong> using the key-value cache (KV cache)</p></li>
<li><p>Each token depends on all previous tokens</p></li>
<li><p>Limited by memory bandwidth rather than compute capacity</p></li>
<li><p>Underutilizes GPU resources compared to prefill phase</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><img src="https://cdn-uploads.huggingface.co/production/uploads/65263bfb3177c2a794997821/BGKtYLqM1X9o72oc9NW8Y.png" width="70%" loading="lazy"></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>prefill: parallel processing of prompt tokens, decode: sequential processing of single output tokens.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<section id="key-concepts-and-optimizations">
<h2>Key Concepts and Optimizations<a class="headerlink" href="#key-concepts-and-optimizations" title="Link to this heading">#</a></h2>
<p>These key concepts will help you design an LLM serving pipeline that meets your service level objectives (SLOs).</p>
<section id="key-value-kv-caching">
<h3>1. Key-Value (KV) Caching<a class="headerlink" href="#key-value-kv-caching" title="Link to this heading">#</a></h3>
<p>KV caching eliminates redundant computations during text generation:</p>
<p><strong>Without KV Cache</strong>:</p>
<ul class="simple">
<li><p>Recalculate keys and values for entire sequence each time</p></li>
<li><p>Extremely inefficient for long sequences</p></li>
</ul>
<p><strong>With KV Cache</strong>:</p>
<ul class="simple">
<li><p>Cache computed K and V values for all previous tokens</p></li>
<li><p>Only compute K and V for the new token</p></li>
<li><p>Reuse cached values for context</p></li>
</ul>
</section>
<section id="continuous-batching">
<h3>2. Continuous Batching<a class="headerlink" href="#continuous-batching" title="Link to this heading">#</a></h3>
<p>Continuous batching optimizes throughput by eliminating GPU idle time:</p>
<p><strong>Vanilla Static Batching</strong>:</p>
<ul class="simple">
<li><p>Wait for all requests in batch to complete</p></li>
<li><p>Creates idle time when requests finish at different rates</p></li>
<li><p>Underutilizes GPU resources</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><img src="https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png" width="70%" loading="lazy"></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Completing four sequences using static batching. On the first iteration (left), each sequence generates one token (blue) from the prompt tokens (yellow). After several iterations (right), the completed sequences each have different sizes because each emits their end-of-sequence-token (red) at different iterations. Even though sequence 3 finished after two iterations, static batching means that the GPU will be underutilized until the last sequence in the batch finishes generation (in this example, sequence 2 after six iterations).</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Continuous Batching</strong>:</p>
<ul class="simple">
<li><p>Immediately replace completed requests with new ones</p></li>
<li><p>Maintains constant GPU utilization</p></li>
<li><p>Increases concurrent user capacity</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><img src="https://images.ctfassets.net/xjan103pcp94/744TAv4dJIQqeHcEaz5lko/b823cc2d92bbb0d82eb252901e1dce6d/cb_03_diagram-continuous-batching.png" width="70%" loading="lazy"></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Completing seven sequences using continuous batching. Left shows the batch after a single iteration, right shows the batch after several iterations. Once a sequence emits an end-of-sequence token, we insert a new sequence in its place (i.e. sequences S5, S6, and S7). This achieves higher GPU utilization since the GPU does not wait for all sequences to complete before starting a new one.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="model-parallelization-or-alternatives">
<h3>3. Model parallelization or alternatives<a class="headerlink" href="#model-parallelization-or-alternatives" title="Link to this heading">#</a></h3>
<p>Large LLMs (&gt;70B) might provides more accurate answers but might not fit entirely on one GPU or one node. You can parallelize your model accross multiple GPUs or nodes to virtually increase your memory resources at the cost of some latency due to communication overhead.</p>
<p>You can also use alternative options such as quantization, distillation, or multi-LoRA adapters to</p>
</section>
<section id="context-window-considerations">
<h3>4. Context Window Considerations<a class="headerlink" href="#context-window-considerations" title="Link to this heading">#</a></h3>
<p>The context window defines the maximum tokens a model can process:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Context Length</p></th>
<th class="head"><p>Use Cases</p></th>
<th class="head"><p>Memory Impact</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>4K-8K tokens</strong></p></td>
<td><p>Q&amp;A, simple chat</p></td>
<td><p>Low KV cache requirements</p></td>
</tr>
<tr class="row-odd"><td><p><strong>32K-128K tokens</strong></p></td>
<td><p>Document analysis, summarization</p></td>
<td><p>Moderate memory usage</p></td>
</tr>
<tr class="row-even"><td><p><strong>128K+ tokens</strong></p></td>
<td><p>Multi-step agents, complex reasoning</p></td>
<td><p>High memory requirements</p></td>
</tr>
</tbody>
</table>
</div>
<p>A large context window might provide more accurate answers but also increase the memory pressure and how many requests can be processed concurrently.</p>
</section>
</section>
<section id="challenges-in-llm-serving">
<h2>Challenges in LLM Serving<a class="headerlink" href="#challenges-in-llm-serving" title="Link to this heading">#</a></h2>
<p>Serving LLMs in production presents several unique challenges that traditional model serving doesnâ€™t face. Letâ€™s explore these challenges and understand why they matter.</p>
<section id="memory-management">
<h3>1. Memory Management<a class="headerlink" href="#memory-management" title="Link to this heading">#</a></h3>
<p>Deploying LLMs is a <strong>memory-intensive</strong> task. A non-exhaustive list of memory constraints are:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Memory Impact</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Model Weights</strong></p></td>
<td><p>Model parameters</p></td>
<td><p>7B model â‰ˆ 14GB (FP16)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>KV Cache</strong></p></td>
<td><p>Token representations</p></td>
<td><p>Depends on context length</p></td>
</tr>
<tr class="row-even"><td><p><strong>Activations</strong></p></td>
<td><p>Temporary buffers</p></td>
<td><p>Varies with batch size</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Example</strong>: A 7B parameter model in FP16 precision requires approximately 14GB just for the model weights, not including the KV cache or activations.</p>
<img src="https://anyscale-materials.s3.us-west-2.amazonaws.com/public-images/ray-serve-llm/diagrams/gpu-memory.png" width="800">
<p>You can distribute your deployment on multiple GPUs or nodes. For example you could split the model accross multiple GPUs on a single node or accross multiple GPUs on multiple nodes.</p>
<p>See examples below for examples of different types of deployment:</p>
<ul class="simple">
<li><p>Single node, single GPU: <a class="reference external" href="https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/small-size-llm/README.html">Deploy a small-sized LLM</a></p></li>
<li><p>Single node, multiple GPU with tensor parallelism: <a class="reference external" href="https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/medium-size-llm/README.html">Deploy a medium-sized LLM</a></p></li>
<li><p>Multiple nodes, multiple GPU with tensor and pipeline parallelism: <a class="reference external" href="https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/large-size-llm/README.html">Deploy a large-sized LLM</a></p></li>
</ul>
</section>
<section id="latency-requirements">
<h3>2. Latency Requirements<a class="headerlink" href="#latency-requirements" title="Link to this heading">#</a></h3>
<p>Users expect <strong>fast, interactive responses</strong> from LLM applications:</p>
<ul class="simple">
<li><p><strong>Time to First Token (TTFT)</strong>: How long until the first token appears</p></li>
<li><p><strong>Time Per Output Token (TPOT)</strong>: How long between subsequent tokens</p></li>
<li><p><strong>Total Response Time</strong>: End-to-end latency</p></li>
</ul>
</section>
<section id="scalability-demands">
<h3>3. Scalability Demands<a class="headerlink" href="#scalability-demands" title="Link to this heading">#</a></h3>
<p>Production traffic is <strong>unpredictable and bursty</strong>:</p>
<ul class="simple">
<li><p>Traffic spikes during peak hours</p></li>
<li><p>Need to scale up quickly during high demand</p></li>
<li><p>Scale down to zero during idle periods to save costs</p></li>
</ul>
</section>
<section id="cost-optimization">
<h3>4. Cost Optimization<a class="headerlink" href="#cost-optimization" title="Link to this heading">#</a></h3>
<p>GPUs represent <strong>significant infrastructure costs</strong>:</p>
<ul class="simple">
<li><p>Maximize hardware utilization</p></li>
<li><p>Scale to zero during idle periods</p></li>
<li><p>Choose appropriate GPU types for your workload</p></li>
</ul>
</section>
<section id="why-not-kubernetes">
<h3>Why not Kubernetes ?<a class="headerlink" href="#why-not-kubernetes" title="Link to this heading">#</a></h3>
<p>You could either use Ray Serve or Kubernetes microservices to solve the challenges above. They are not mutually exclusive, as Ray Serve can run on Kubernetes. The differences are mostly about who does the orchestration and how much abstraction you want from the inference pipeline.</p>
<p><strong>Ray Serve LLM</strong></p>
<ul class="simple">
<li><p>Python-native orchestration (routing, batching, streaming).</p></li>
<li><p>Built-in autoscaling, backpressure, health checks or <a class="reference external" href="https://docs.ray.io/en/latest/serve/llm/prefix-aware-request-router.html">LLM-optimized routing</a>.</p></li>
<li><p>Actor-based sharding across nodes/GPUs.</p></li>
<li><p>Easy multi-model serving behind one endpoint.</p></li>
</ul>
<p><strong>Kubernetes</strong></p>
<ul class="simple">
<li><p>Pod = unit per node; multi-node model parallelism needs extra controllers/operators.</p></li>
<li><p>Batching/routing/backpressure are DIY (app or sidecars).</p></li>
<li><p>Strong platform features (networking, security, quotas), but inference control isnâ€™t built-in.</p></li>
</ul>
</section>
</section>
<section id="ray-serve-llm-anyscale-architecture">
<h2>Ray Serve LLM + Anyscale Architecture<a class="headerlink" href="#ray-serve-llm-anyscale-architecture" title="Link to this heading">#</a></h2>
<p>Here is a diagram of how Ray Serve LLM + Anyscale provides a production-grade solution to your LLM deployment:</p>
<img src="https://anyscale-materials.s3.us-west-2.amazonaws.com/public-images/ray-serve-llm/diagrams/anyscale-serve-vllm.png" width="800">
<p><strong>Notes:</strong></p>
<ul class="simple">
<li><p>The above shows only one replica per model, but Ray Serve can easily scale to deploying multiple replicas.</p></li>
</ul>
<p>Ray Serve LLM + Anyscale provides a production-grade solution through three integrated components:</p>
<section id="ray-serve-for-orchestration">
<h3>1. Ray Serve for Orchestration<a class="headerlink" href="#ray-serve-for-orchestration" title="Link to this heading">#</a></h3>
<p>Ray Serve handles the <strong>orchestration and scaling</strong> of your LLM deployment:</p>
<ul class="simple">
<li><p><strong>Automatic scaling</strong>: Adds/removes model replicas based on traffic</p></li>
<li><p><strong>Load balancing</strong>: Distributes requests across available replicas</p></li>
<li><p><strong>Unified multi-model deployment</strong>: Deploy and manage multiple models</p></li>
<li><p><strong>OpenAI-compatible API</strong>: Drop-in replacement for OpenAI clients</p></li>
</ul>
<p>Here is a diagram of how Ray Serve LLM interact with a clientâ€™s request</p>
<img src="https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/Ray+Serve+LLM.png" width="800">
</section>
<section id="vllm-as-the-inference-engine">
<h3>2. vLLM as the inference engine<a class="headerlink" href="#vllm-as-the-inference-engine" title="Link to this heading">#</a></h3>
<p>LLM inference is a non-trivial problem that requires tuning low-level hardware use and high-level algorithms. An <strong>inference engine</strong> abstracts this complexity and optimizes model execution. Ray Serve LLM natively integrates <strong>vLLM</strong> as its inference engine for several reasons:</p>
<ul class="simple">
<li><p><strong>Fast GPU computation</strong> with CUDA kernels specifically optimized for LLM inference.</p></li>
<li><p><strong>Continuous batching</strong>: Continuously schedule tokens to be processed to maximize GPU utilization.</p></li>
<li><p><strong>Smart memory use</strong>: Optimize memory usage with state-of-the-art algorithms like PagedAttention</p></li>
</ul>
<p>Ray Serve LLM gives you high flexibility on how to configure your vLLM engine (more on that later).</p>
</section>
<section id="anyscale-for-infrastructure">
<h3>3. Anyscale for Infrastructure<a class="headerlink" href="#anyscale-for-infrastructure" title="Link to this heading">#</a></h3>
<p>Anyscale provides <strong>managed infrastructure</strong> and enterprise features:</p>
<ul class="simple">
<li><p><strong>Managed infrastructure</strong>: Optimized Ray clusters in your cloud</p></li>
<li><p><strong>Cost optimization</strong>: Pay-as-you-go, scale-to-zero</p></li>
<li><p><strong>Enterprise security</strong>: VPC, SSO, audit logs</p></li>
<li><p><strong>Seamless scaling</strong>: Handle traffic spikes automatically</p></li>
</ul>
</section>
</section>
<section id="getting-started-with-ray-serve-llm">
<h2>Getting Started with Ray Serve LLM<a class="headerlink" href="#getting-started-with-ray-serve-llm" title="Link to this heading">#</a></h2>
<p>Now that we understand the fundamentals, letâ€™s see how to get started with Ray Serve LLM. The process involves three main steps:</p>
<ol class="arabic simple">
<li><p><strong>Configure</strong> your LLM deployment</p></li>
<li><p><strong>Deploy</strong> the service</p></li>
<li><p><strong>Query</strong> the deployed model</p></li>
<li><p><strong>Shutdown</strong> the deployment</p></li>
</ol>
<section id="step-1-configuration">
<h3>Step 1: Configuration<a class="headerlink" href="#step-1-configuration" title="Link to this heading">#</a></h3>
<p>Letâ€™s create a simple configuration:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#serve_llama.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.serve.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMConfig</span><span class="p">,</span> <span class="n">build_openai_app</span>

<span class="n">llm_config</span> <span class="o">=</span> <span class="n">LLMConfig</span><span class="p">(</span>
    <span class="c1"># Model loading configuration</span>
    <span class="n">model_loading_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;my-llama&quot;</span><span class="p">,</span> <span class="c1"># custom name for the model</span>
        <span class="n">model_source</span><span class="o">=</span><span class="s2">&quot;unsloth/Meta-Llama-3.1-8B-Instruct&quot;</span><span class="p">,</span> <span class="c1"># huggingface model repo</span>
    <span class="p">),</span>
    <span class="n">accelerator_type</span><span class="o">=</span><span class="s2">&quot;L4&quot;</span><span class="p">,</span> <span class="c1"># device to use (picked from your ray cluster)</span>
    <span class="c1">## Optional: configure Ray Serve autoscaling</span>
    <span class="n">deployment_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">autoscaling_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">min_replicas</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># keep at least 1 replica up to avoid cold starts</span>
            <span class="n">max_replicas</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># no more than 2 replicas to control cost</span>
        <span class="p">)</span>
    <span class="p">),</span>
    <span class="c1"># Configure your vLLM engine. Follow the same API as vLLM</span>
    <span class="c1"># https://docs.vllm.ai/en/stable/configuration/engine_args.html</span>
    <span class="n">engine_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">max_model_len</span><span class="o">=</span><span class="mi">8192</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">build_openai_app</span><span class="p">({</span><span class="s2">&quot;llm_configs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_config</span><span class="p">]})</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-deployment">
<h3>Step 2: Deployment<a class="headerlink" href="#step-2-deployment" title="Link to this heading">#</a></h3>
<p>Deployment can be done locally or on Anyscale Services:</p>
<p><strong>Local Deployment</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>serve<span class="w"> </span>run<span class="w"> </span>serve_llama:app<span class="w"> </span>--non-blocking
</pre></div>
</div>
</div>
</div>
<p><strong>Anyscale Services</strong>:</p>
<p>To deploy your LLM with Anyscale Service, configure your cloud and compute configuration and point to your LLM configuration:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># service.yaml</span>
<span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">deploy-llama-3-8b</span>
<span class="nt">image_uri</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">anyscale/ray-llm:2.49.0-py311-cu128</span><span class="w"> </span><span class="c1"># Anyscale Ray Serve LLM image. Use `containerfile: ./Dockerfile` to use a custom Dockerfile.</span>
<span class="nt">compute_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">auto_select_worker_config</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"> </span>
<span class="nt">working_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">.</span>
<span class="nt">cloud</span><span class="p">:</span>
<span class="nt">applications</span><span class="p">:</span>
<span class="w">  </span><span class="c1"># Point to your app in your Python module</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">import_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">serve_llama:app</span>
</pre></div>
</div>
<p>Deploy your service:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>!anyscale<span class="w"> </span>service<span class="w"> </span>deploy<span class="w"> </span>-f<span class="w"> </span>service.yaml
</pre></div>
</div>
</section>
<section id="step-3-querying">
<h3>Step 3: Querying<a class="headerlink" href="#step-3-querying" title="Link to this heading">#</a></h3>
<p>Once deployed, you can use the OpenAI Python client with <code class="docutils literal notranslate"><span class="pre">base_url</span></code> pointing to your Ray Serve endpoint.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">urllib.parse</span><span class="w"> </span><span class="kn">import</span> <span class="n">urljoin</span>

<span class="c1"># because deployed locally, we use localhost:8000 and a dummy placeholder API key</span>
<span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:8000&quot;</span>
<span class="n">token</span><span class="o">=</span><span class="s2">&quot;DUMMY_KEY&quot;</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span> <span class="n">urljoin</span><span class="p">(</span><span class="n">base_url</span><span class="p">,</span> <span class="s2">&quot;v1&quot;</span><span class="p">),</span> <span class="n">api_key</span><span class="o">=</span><span class="n">token</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;my-llama&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the capital of France?&quot;</span><span class="p">}</span>
    <span class="p">],</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Stream and print JSON</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span>
    <span class="k">if</span> <span class="n">data</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-4-shutdown">
<h3>Step 4: Shutdown<a class="headerlink" href="#step-4-shutdown" title="Link to this heading">#</a></h3>
<p>Shutdown a local deployment</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>serve<span class="w"> </span>shutdown<span class="w"> </span>-y
</pre></div>
</div>
</div>
</div>
<p>Terminate an Anyscale service:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>anyscale<span class="w"> </span>service<span class="w"> </span>terminate<span class="w"> </span>deploy-my-llama
</pre></div>
</div>
</section>
</section>
<section id="key-takeaways">
<h2>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading">#</a></h2>
<p>In this module, weâ€™ve covered the essential foundations of LLM serving with Ray Serve LLM:</p>
<ol class="arabic simple">
<li><p><strong>Understanding LLM Serving</strong>: How LLMs generate text through prefill and decode phases</p></li>
<li><p><strong>Key Optimizations</strong>: KV caching, paged attention, and continuous batching</p></li>
<li><p><strong>Challenges</strong>: Memory management, latency, scalability, and cost optimization</p></li>
<li><p><strong>Ray Serve LLM Architecture</strong>: Three-component solution with Ray Serve, vLLM, and Anyscale</p></li>
<li><p><strong>Getting Started</strong>: Simple configuration and deployment process</p></li>
</ol>
<section id="next-steps">
<h3>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">#</a></h3>
<p>In the next modules, weâ€™ll dive deeper into:</p>
<ul class="simple">
<li><p><strong>Hands-on deployment</strong> of a medium-sized LLMs,</p></li>
<li><p><strong>Advanced configurations</strong> and optimizations (tool calling, LoRA, structured outputsâ€¦)</p></li>
</ul>
</section>
<section id="resources">
<h3>Resources<a class="headerlink" href="#resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.anyscale.com/llm/serving">Ray Serve LLM with Anyscale Documentation</a></p></li>
<li><p><a class="reference external" href="https://console.anyscale.com/template-preview/deployment-serve-llm?utm_source=anyscale_docs&amp;amp;utm_medium=docs&amp;amp;utm_campaign=examples_page&amp;amp;utm_content=deployment-serve-llm?utm_source=anyscale&amp;amp;utm_medium=docs&amp;amp;utm_campaign=examples_page&amp;amp;utm_content=deployment-serve-llm">Deploy LLM templates</a></p></li>
<li><p><a class="reference external" href="https://docs.ray.io/en/latest/serve/llm/index.html">Ray Serve LLM Documentation</a></p></li>
<li><p><a class="reference external" href="https://docs.vllm.ai/">vLLM Documentation</a></p></li>
</ul>
<p>Ready to start serving LLMs with Ray? Letâ€™s move on to the next module!</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./courses/llm_serving/00_intro_serve_llm"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-llm-serving">What is LLM Serving?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-llm-text-generation-process">The LLM Text Generation Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#two-phases-of-llm-inference">Two Phases of LLM Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prefill-phase">Prefill Phase</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decode-phase">Decode Phase</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts-and-optimizations">Key Concepts and Optimizations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-kv-caching">1. Key-Value (KV) Caching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-batching">2. Continuous Batching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parallelization-or-alternatives">3. Model parallelization or alternatives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-window-considerations">4. Context Window Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-llm-serving">Challenges in LLM Serving</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-management">1. Memory Management</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latency-requirements">2. Latency Requirements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scalability-demands">3. Scalability Demands</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-optimization">4. Cost Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-not-kubernetes">Why not Kubernetes ?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ray-serve-llm-anyscale-architecture">Ray Serve LLM + Anyscale Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ray-serve-for-orchestration">1. Ray Serve for Orchestration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vllm-as-the-inference-engine">2. vLLM as the inference engine</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#anyscale-for-infrastructure">3. Anyscale for Infrastructure</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started-with-ray-serve-llm">Getting Started with Ray Serve LLM</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-configuration">Step 1: Configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-deployment">Step 2: Deployment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-querying">Step 3: Querying</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-shutdown">Step 4: Shutdown</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">Key Takeaways</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resources">Resources</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book community
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>