{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying to Anyscale Services\n",
    "\n",
    "For production deployment, we'll use Anyscale Services to deploy our Ray Serve app to a dedicated cluster. The great news is that **no code changes are needed** - we can use the exact same LLM configuration!\n",
    "\n",
    "### What is an Anyscale Service?\n",
    "\n",
    "An **Anyscale Service** is a managed deployment that provides:\n",
    "- **Dedicated Infrastructure**: Your own Ray cluster in the cloud\n",
    "- **Automatic Scaling**: Handles traffic spikes and load balancing\n",
    "- **Fault Tolerance**: Resilient against node failures and rolling updates\n",
    "- **Enterprise Features**: Security, monitoring, and compliance\n",
    "\n",
    "### Setting up the Configuration File\n",
    "\n",
    "Let's create the service configuration:\n",
    "```yaml\n",
    "# service.yaml\n",
    "name: deploy-llama-3-70b\n",
    "image_uri: anyscale/ray-llm:2.49.0-py311-cu128 # Anyscale Ray Serve LLM image. Use `containerfile: ./Dockerfile` to use a custom Dockerfile.\n",
    "compute_config:\n",
    "  auto_select_worker_config: true \n",
    "working_dir: .\n",
    "cloud:\n",
    "applications:\n",
    "  # Point to your app in your Python module\n",
    "  - import_path: serve_llama_3_1_70b:app\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching the Service\n",
    "\n",
    "Now let's deploy our service to Anyscale:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!anyscale service deploy -f service.yaml --env HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Inference on Anyscale\n",
    "\n",
    "Once deployed, you'll get an endpoint and authentication token. Let's see how to use them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://deploy-llama-3-70b-jgz99.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com/v1\",\n",
    "    api_key=\"2YKUt_IJZ8q8GWT5VPHVitzsHKsddoL6mSszJxzwe5A\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-llama-3.1-70b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me about Anyscale!\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutting Down the Service\n",
    "\n",
    "When you're done with your service:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!anyscale service terminate -n deploy-llama-3-70b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}