{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Ray Serve LLM\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. The main abstractions we'll work with are:\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **`LLMConfig`**: Configuration object that defines your model, hardware, and deployment settings\n",
    "2. **`build_openai_app`**: Public function that creates an OpenAI-compatible application from your configuration\n",
    "3. **Ray Serve**: The underlying orchestration layer that handles scaling and load balancing\n",
    "\n",
    "### Configuration for Medium-Sized Models\n",
    "\n",
    "For medium-sized models, we need to:\n",
    "- Set appropriate `accelerator_type` for the hardware\n",
    "- Configure **tensor parallelism** with `tensor_parallel_size` to match the number of GPUs\n",
    "\n",
    "Let's create our configuration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serve_llama_3_1_70b.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-llama-3.1-70b\",\n",
    "        # Or unsloth/Meta-Llama-3.1-70B-Instruct for an ungated model\n",
    "        model_source=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "    ),\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1,\n",
    "            max_replicas=4,\n",
    "        )\n",
    "    ),\n",
    "    accelerator_type=\"L40S\", # Or with similar VRAM like \"A100-40G\"\n",
    "    # Type `export HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>` in a terminal\n",
    "    runtime_env=dict(env_vars={\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}),\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len=32768, # See model's Hugging Face card for max context length\n",
    "        # Split weights among 8 GPUs in the node\n",
    "        tensor_parallel_size=8,\n",
    "    ),\n",
    "    log_engine_metrics=True,\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Breakdown\n",
    "\n",
    "Let's understand each part of our configuration:\n",
    "\n",
    "**Model Loading:**\n",
    "- `model_id`: Unique identifier for your model in the API\n",
    "- `model_source`: Hugging Face model path (gated model requires HF token)\n",
    "- `HF_TOKEN`: Hugging Face token for accessing gated models\n",
    "\n",
    "**Hardware Configuration:**\n",
    "- `accelerator_type`: GPU type (L40S, A100-40G, etc.)\n",
    "- `tensor_parallel_size`: Number of GPUs to split the model across\n",
    "\n",
    "**Deployment Settings:**\n",
    "- `autoscaling_config`: Min/max replicas for horizontal scaling\n",
    "\n",
    "**Monitoring**\n",
    "- `log_engine_metrics`: Display LLM-specific metrics (Time to First Toke, Time Per Output Token, Request Per Second...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}