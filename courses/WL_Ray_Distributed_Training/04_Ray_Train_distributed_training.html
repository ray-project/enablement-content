
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Distributed training with Ray Train, PyTorch and Hugging Face &#8212; Course Notebooks</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom_hide.css?v=af9667c8" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'courses/WL_Ray_Distributed_Training/04_Ray_Train_distributed_training';</script>
    <script src="../../_static/custom_toggle.js?v=1235ef5b"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Course Notebooks</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    Ray Enablement Content
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Intro to Ray</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_01.html">Introduction to Ray Core: Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_02.html">0. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_03.html">1. Creating Remote Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_04.html">2. Executing Remote Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_05.html">4. Putting It All Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_01.html">Introduction to Ray Core (Advancement): Object store, Tasks, Actors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_02.html">1. Object store</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_03.html">2. Chaining Tasks and Passing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_04.html">3. Task retries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_05.html">4. Task Runtime Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_06.html">5. Resource allocation and management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_07.html">6. Nested Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_08.html">7. Pattern: Pipeline data processing and waiting for results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_09.html">8. Ray Actors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_01.html">Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_02.html">1. Overview of the Ray AI Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_03.html">2. Quick end-to-end example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_01.html">Introduction to Ray Train + PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_02.html">1. When to use Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_03.html">2. Single GPU Training with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_04.html">3. Distributed Data Parallel Training with Ray Train and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01.html">Introduction to Ray Train: Ray Train + PyTorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02.html">1. When to use Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03.html">2. Single GPU Training with PyTorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04.html">3. Distributed Training with Ray Train and PyTorch Lightning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05.html">4. Ray Train in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_01.html">Introduction to Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_02.html">1. Loading the data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_03.html">2. Starting out with vanilla PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_04.html">3. Hyperparameter tuning with Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_05.html">4. Ray Tune in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_01.html">Introduction to Ray Data: Industry Landscape</a></li>

<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_02.html">The Compute Layer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_03.html">The Orchestration Layer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_04.html">Distributed Computing Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_05.html">Data Processing with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_06.html">Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_01.html">Introduction to Ray Data: Ray Data + Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_02.html">0. What is Ray Data?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_03.html">2. Loading Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_04.html">3. Transforming Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_05.html">4. Writing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_06.html">5. Data Operations: Shuffling, Grouping and Aggregation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_07.html">6. When to use Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_01.html">Intro to Ray Data:  Ray Data + Unstructured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_02.html">1. When to Consider Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_03.html">2. How to work with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_04.html">3. Loading data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_05.html">3. Lazy execution mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_06.html">4. Transforming data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_07.html">5. Stateful transformations with Ray Actors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_08.html">6. Materializing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_09.html">7. Data Operations: grouping, aggregation, and shuffling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_10.html">8. Persisting data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_11.html">9. Ray Data in production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_01.html">Introduction to Ray Serve with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_02.html">1. When to Consider Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_03.html">2. Overview of Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_04.html">3. Implement an image classification service</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_05.html">4. Development workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../00_Developer_Intro_to_Ray/output/README_01.html">Introduction to Ray: Developer</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">02 Anyscale Admin</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01.html">Anyscale Administrator Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02.html">1. What is an Anyscale Cloud?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03.html">2. Cloud Deployment Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04.html">3. A Demonstrative Example of Resource Creation with AWS EC2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05.html">3.1 IAM Role Definition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06.html">4. Register Anyscale Cloud to Your Cloud Provider</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01.html">Deployment Options: Virtual Machines vs. Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02.html">2. Virtual Machines (VM) vs. Kubernetes (K8s)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03.html">3. (Optional) More Kubernetes Deployments Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01.html">Introduction: Deploy Anyscale Ray on AWS EC2 Instances</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02.html">1. Create Anyscale Resources with Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03.html">2. Register the Anyscale Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04.html">3. Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05.html">4. Cleanup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06.html">5. Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01.html">Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02.html">Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03.html">1. Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04.html">2. Create Anyscale Resources with Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05.html">3. Register the Anyscale Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06.html">4. Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07.html">5. Cleanup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01.html">Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02.html">1. Create Anyscale Resources with Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03.html">2. Install Kubernetes Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04.html">3. Register the Anyscale Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05.html">4. Install the Anyscale Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06.html">5. Verify the Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07.html">6. Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08.html">7. Clean up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09.html">8. Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01.html">Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02.html">Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03.html">1. Create Anyscale Resources with Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04.html">2. Attach Required IAM Policies to Your existing EKS‚Äôs Node Role</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05.html">3. Install Kubernetes Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06.html">4. Register the Anyscale Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07.html">5. Install the Anyscale Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08.html">6. Verify the Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09.html">7. Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10.html">8. Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11.html">9. Clean up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12.html">10. Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01.html">Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02.html">Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03.html">1. Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04.html">2. Create Anyscale Resources with Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05.html">3. Troubleshooting GPU Availability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06.html">4. kubectl Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07.html">5. Install NGINX Ingress Controller</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08.html">6. (Optional) Upgrade Anyscale Dependencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09.html">7. Register the Anyscale Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10.html">8. Install the Anyscale Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11.html">8. Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12.html">9. Cleanup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">03 Observability</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../03_Observability/01_Intro_and_setup/output/01_general_intro_and_setup_01.html">Observability Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_Observability/01_Intro_and_setup/output/01_general_intro_and_setup_02.html">Observability Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_Observability/01_Intro_and_setup/output/01_general_intro_and_setup_03.html">Setting Up Local Ray Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01.html">Ray and Anyscale Observability Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02.html">Ray Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03.html">Anyscale Observability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01.html">Ray and Anyscale Observability in Detail</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02.html">Data Pipeline Observability (Ray Data)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03.html">Web Application Observability (Ray Serve)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Gettingstarted</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_01_anyscale_intro_workspace_01.html">101 ‚Äî Introduction to Anyscale Workspaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_02_anyscale_development_intro_01.html">101 ‚Äì Developing Application with Anyscale</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_03_anyscale_compute_runtime_intro_01.html">101 ‚Äì Compute Configs and Execution Environments in Anyscale</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_04_anyscale_storage_options_01.html">101 ‚Äì Storage Options in the Anyscale Platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_05_anyscale_logging_metrics_01.html">101 ‚Äì Debug and Monitor Your Anyscale Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_06_anyscale_intro_jobs_01.html">101 ‚Äì Introduction to Anyscale Jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_07_anyscale_intro_services_01.html">101 ‚Äì  Introduction to Anyscale Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_08_anyscale_collaboration_01.html">101 ‚Äì Collaboration on Anyscale</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_09_anyscale_org_setup_01.html">101 - Anyscale Organization and Cloud Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_09_anyscale_org_setup_02.html">üìå Overview of Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_09_anyscale_org_setup_03.html">üß† Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_anyscale_intro_jobs_01.html">Content Used</a></li>

<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_anyscale_intro_jobs_02.html">Part 1. Creating and Submitting your first job</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_anyscale_intro_jobs_03.html">Part 2. Automation and Scheduling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_anyscale_intro_services_01.html">Sources</a></li>

<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/output/101_anyscale_intro_services_02.html">Part 1: Starting your first Anyscale Service</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Ray Train Foundation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_01.html">üìö 01 ¬∑ Introduction to Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_02.html">01 ¬∑ Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_03.html">04 ¬∑ Define ResNet-18 Model for MNIST</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_04.html">05 ¬∑ Define the Ray Train Loop (DDP per-worker)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_05.html">06 ¬∑ Define <code class="docutils literal notranslate"><span class="pre">train_loop_config</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_06.html">07 ¬∑ Configure Scaling with <code class="docutils literal notranslate"><span class="pre">ScalingConfig</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_07.html">08 ¬∑ Wrap the Model with <code class="docutils literal notranslate"><span class="pre">prepare_model()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_08.html">09 ¬∑ Build the DataLoader with <code class="docutils literal notranslate"><span class="pre">prepare_data_loader()</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_09.html">10 ¬∑ Report Training Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_10.html">11 ¬∑ Save Checkpoints and Report Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_11.html">14 ¬∑ Create the <code class="docutils literal notranslate"><span class="pre">TorchTrainer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_12.html">16 ¬∑ Inspect the Training Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_13.html">18 ¬∑ Load a Checkpoint for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_14.html">üîÑ 02 ¬∑ Integrating Ray Train with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_15.html">01 ¬∑ Define Training Loop with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_16.html">02 ¬∑ Build DataLoader from Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_17.html">03 ¬∑ Prepare Dataset for Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_18.html">05 ¬∑ Define Image Transformation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_19.html">07 ¬∑ Configure <code class="docutils literal notranslate"><span class="pre">TorchTrainer</span></code> with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_20.html">üõ°Ô∏è 03 ¬∑ Fault Tolerance in Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_21.html">01 ¬∑ Modify Training Loop to Enable Checkpoint Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_22.html">02 ¬∑ Save Full Checkpoint with Extra State</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_23.html">04 ¬∑ Launch Fault-Tolerant Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_24.html">05 ¬∑ Manual Restoration from Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_25.html">07 ¬∑ Clean Up Cluster Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_26.html">üéâ Wrapping Up &amp; Next Steps</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wl Ray Data Processing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_01.html">Data Processing with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_02.html">Library Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_03.html">Convert to Ray Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_04.html">Filter Ray Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_05.html">Join Two Ray Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_06.html">Preprocessing with a Tokenizer</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wl Ray Distributed Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="output/04_Ray_Train_distributed_training_01.html">Distributed training with Ray Train, PyTorch and Hugging Face</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/04_Ray_Train_distributed_training_02.html">1. Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/04_Ray_Train_distributed_training_03.html">3. Metrics Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/04_Ray_Train_distributed_training_04.html">4. Training function per worker</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/04_Ray_Train_distributed_training_05.html">5. Main Training Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="output/04_Ray_Train_distributed_training_06.html">6. Start Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wl Ray Serve Online Serving</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_01.html">Online Model Serving with Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_02.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_03.html">FastAPI webservice and deploy a model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_04.html">Simulate Client: Send test requests</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wl Train Generative Cv</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Generative_CV/output/04d1_generative_cv_pattern_01.html">04-d1 Generative computer-vision pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Generative_CV/output/04d1_generative_cv_pattern_02.html">1. Imports and setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Generative_CV/output/04d1_generative_cv_pattern_03.html">8. Pixel diffusion LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Generative_CV/output/04d1_generative_cv_pattern_04.html">9. Ray Train <code class="docutils literal notranslate"><span class="pre">train_loop</span></code> (Lightning + Ray integration)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Generative_CV/output/04d1_generative_cv_pattern_05.html">12. Resume from latest checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Generative_CV/output/04d1_generative_cv_pattern_06.html">13. Reverse diffusion sampler</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wl Train Policy Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_01.html">04-d2 Diffusion-Policy Pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_02.html">1. Imports and setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_03.html">4. DiffusionPolicy LightningModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_04.html">5. Distributed Train loop with checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_05.html">8. Reverse diffusion helper</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wl Train Rec Sys</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_01.html">04e Recommendation system pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_02.html">1. Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_03.html">7. Define matrix factorization model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_04.html">8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_05.html">11. Resume training from checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_06.html">12. Inference: recommend top-N items for a user</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wl Train Tabular</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Tabular/output/04b_tabular_workload_pattern_01.html">04b Tabular workload pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Tabular/output/04b_tabular_workload_pattern_02.html">1. Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Tabular/output/04b_tabular_workload_pattern_03.html">8. Define the Ray Train worker loop (Arrow-based, memory-efficient)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Tabular/output/04b_tabular_workload_pattern_04.html">12. Confusion matrix visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Tabular/output/04b_tabular_workload_pattern_05.html">15. Continue training from the latest checkpoint</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wl Train Time Series</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Time_Series/output/04c_time_series_workload_pattern_01.html">04c Time-Series workload pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Time_Series/output/04c_time_series_workload_pattern_02.html">1. Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Time_Series/output/04c_time_series_workload_pattern_03.html">9. PositionalEncoding and Transformer model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Time_Series/output/04c_time_series_workload_pattern_04.html">10. Ray Train training loop (with teacher forcing)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Time_Series/output/04c_time_series_workload_pattern_05.html">13. Resume training from checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Time_Series/output/04c_time_series_workload_pattern_06.html">14. Inference helper ‚Äî Ray Data batch predictor on GPU</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Wl Train Vision Pattern</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Vision_Pattern/output/04a_vision_pattern_01.html">04a Computer-vision pattern with Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Vision_Pattern/output/04a_vision_pattern_02.html">1. Imports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Vision_Pattern/output/04a_vision_pattern_03.html">6. Custom <code class="docutils literal notranslate"><span class="pre">Food101Dataset</span></code> for Parquet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Vision_Pattern/output/04a_vision_pattern_04.html">10. Helper: Ray-prepared DataLoaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Vision_Pattern/output/04a_vision_pattern_05.html">11. <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Vision_Pattern/output/04a_vision_pattern_06.html">12. Launch distributed training with <code class="docutils literal notranslate"><span class="pre">TorchTrainer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Vision_Pattern/output/04a_vision_pattern_07.html">13. Plot training and validation loss curves</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Vision_Pattern/output/04a_vision_pattern_08.html">14. Demonstrate fault-tolerant resumption</a></li>
<li class="toctree-l1"><a class="reference internal" href="../WL_Train_Vision_Pattern/output/04a_vision_pattern_09.html">15. Batch inference with Ray Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Llm Serving</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/00_intro_serve_llm/output/notebook_01.html">Introduction to Ray Serve LLM: Foundations of Large Language Model Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/00_intro_serve_llm/output/notebook_02.html">What is LLM Serving?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/00_intro_serve_llm/output/notebook_03.html">Key Concepts and Optimizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/00_intro_serve_llm/output/notebook_04.html">Challenges in LLM Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/00_intro_serve_llm/output/notebook_05.html">Ray Serve LLM + Anyscale Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/00_intro_serve_llm/output/notebook_06.html">Getting Started with Ray Serve LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/00_intro_serve_llm/output/notebook_07.html">Key Takeaways</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/01_deploy_medium_llm/output/notebook_01.html">Deploy a Medium-Sized LLM with Ray Serve LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/01_deploy_medium_llm/output/notebook_02.html">Overview: Why Medium-Sized Models?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/01_deploy_medium_llm/output/notebook_03.html">Setting up Ray Serve LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/01_deploy_medium_llm/output/notebook_04.html">Local Deployment &amp; Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/01_deploy_medium_llm/output/notebook_05.html">Deploying to Anyscale Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/01_deploy_medium_llm/output/notebook_06.html">Advanced Topics: Monitoring &amp; Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/01_deploy_medium_llm/output/notebook_07.html">Summary &amp; Outlook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/02_advanced_llm_features/output/notebook_01.html">Advanced LLM Features with Ray Serve LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/02_advanced_llm_features/output/notebook_02.html">Overview: Advanced Features Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/02_advanced_llm_features/output/notebook_03.html">Example: Deploying LoRA Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/02_advanced_llm_features/output/notebook_04.html">Example: Getting Structured JSON Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/02_advanced_llm_features/output/notebook_05.html">Example: Setting up Tool Calling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/02_advanced_llm_features/output/notebook_06.html">How to Choose an LLM?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_serving/02_advanced_llm_features/output/notebook_07.html">Conclusion: Next Steps</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Ray 101</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/1_AI_Libs_Intro_01.html">Introduction to the Ray AI Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/1_AI_Libs_Intro_02.html">1. Overview of the Ray AI Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/1_AI_Libs_Intro_03.html">2. End-to-end example: predicting taxi tips in New York</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/2_Intro_Train_01.html">Introduction to Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/2_Intro_Train_02.html">1. PyTorch introductory example (single GPU)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/2_Intro_Train_03.html">2. Distributed Data Parallel Training with Ray Train and PyTorch (multiple GPUs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/2_Intro_Train_04.html">3. Overview of the training loop in Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/2_Intro_Train_05.html">4. Migrating the model and dataset to Ray Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/2_Intro_Train_06.html">5. Reporting checkpoints and metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/2_Intro_Train_07.html">6. Launching the distributed training job</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/2_Intro_Train_08.html">7. Accessing the training results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/2_Intro_Train_09.html">8. Ray Train in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/3_Intro_Tune_01.html">Intro to Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/3_Intro_Tune_02.html">1. Loading and visualizing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/3_Intro_Tune_03.html">2. Setting up a PyTorch model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/3_Intro_Tune_04.html">3. Introduction to Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/3_Intro_Tune_05.html">4. Diving deeper into Ray Tune concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/3_Intro_Tune_06.html">5. Hyperparameter tuning the PyTorch model using Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/4_Intro_Data_01.html">Intro to Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/4_Intro_Data_02.html">1. When to use Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/4_Intro_Data_03.html">2. Loading Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/4_Intro_Data_04.html">3. Transforming Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/4_Intro_Data_05.html">4. Data Operations: Grouping, Aggregation, and Shuffling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/4_Intro_Data_06.html">5. Persisting Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/5_Intro_Serve_01.html">Intro to Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/5_Intro_Serve_02.html">1. Overview of Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/5_Intro_Serve_03.html">2. Implement an Classifier service</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/5_Intro_Serve_04.html">3. Advanced features of Ray Serve</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/5_Intro_Serve_05.html">4. Ray Serve in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ray-101/output/5_Intro_Serve_06.html">Clean up</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Ray Data Batch Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../workloads/Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_01.html">Batch Inference with Ray Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workloads/Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_02.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workloads/Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_03.html">Load a dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workloads/Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_04.html">Batch Inference Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workloads/Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_05.html">Create a batch data and call the model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workloads/Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_06.html">Run inference on the entire dataset</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/courses/WL_Ray_Distributed_Training/04_Ray_Train_distributed_training.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Distributed training with Ray Train, PyTorch and Hugging Face</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">Outline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture">1. Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#library-imports">2. Library Imports</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-setup">3. Metrics Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-function-per-worker">4. Training function per worker</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizer">Tokenizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataloaders">Dataloaders</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-training-function">5. Main Training Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#start-training">6. Start Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shutdown-ray-cluster">7. Shutdown Ray Cluster</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">8. Summary</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="distributed-training-with-ray-train-pytorch-and-hugging-face">
<h1>Distributed training with Ray Train, PyTorch and Hugging Face<a class="headerlink" href="#distributed-training-with-ray-train-pytorch-and-hugging-face" title="Link to this heading">#</a></h1>
<p>¬© 2025, Anyscale. All Rights Reserved</p>
<p>üíª <strong>Launch Locally</strong>: You can run this notebook locally.</p>
<p>üöÄ <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click <a class="reference external" href="http://console.anyscale.com/register">here</a> to easily start a Ray cluster on Anyscale)</p>
<p>This notebook demonstrates how to perform distributed training of a BERT model for sequence classification using Ray Train, PyTorch, and Hugging Face libraries. The goal is to classify Yelp reviews into categories by leveraging the power of distributed computing, which allows you to train large models efficiently across multiple CPUs or GPUs.</p>
<p>The notebook starts by importing all the necessary libraries, including PyTorch for deep learning, Hugging Face Transformers for model and tokenizer utilities, and Ray Train for distributed training. It then sets up the evaluation metric (accuracy) and defines a function to compute this metric during model evaluation.</p>
<p>A key part of the notebook is the training function, which is executed by each worker in the distributed setup. This function handles loading the Yelp review dataset, tokenizing the text data, preparing data loaders for batching, and setting up the BERT model for training. The function is designed to automatically use the best available hardware, whether that‚Äôs a CPU, GPU, or Apple Silicon‚Äôs MPS.</p>
<p>The main training function, <code class="docutils literal notranslate"><span class="pre">train_bert</span></code>, configures the distributed environment using Ray, sets up the training parameters, and launches the training process across multiple workers. This approach allows you to scale up your training easily, making it suitable for both local machines and cloud platforms. After training, Ray is properly shut down to free up resources.</p>
<p>Overall, this notebook provides a practical introduction to distributed deep learning with modern Python tools, making it easier for machine learning engineers to train large models on big datasets efficiently.</p>
<section id="outline">
<h2>Outline<a class="headerlink" href="#outline" title="Link to this heading">#</a></h2>
<div class="alert alert-block alert-info">
<ol>
    <li>Architecture Diagram
    <li>Library Imports
        <ul>
            <li>Importing PyTorch, Hugging Face Transformers, Ray Train, and other dependencies
        </ul>
    <li>Metrics Setup
        <ul>
            <li>Defining accuracy as the evaluation metric
            <li>Function to compute metrics during evaluation
        </ul>
    <li>Training Function Per Worker
        <ul>
            <li>Data loading and preprocessing (tokenization)
            <li>Preparing data loaders for batching
            <li>Model initialization (BERT for sequence classification)
            <li>Device selection (CPU, GPU, or MPS)
            <li>Training and evaluation loop
        </ul>
    <li>Main Training Function
        <ul>
            <li>Setting up distributed training configuration with Ray
            <li>Scaling configuration for CPUs/GPUs
            <li>Initializing and running the Ray TorchTrainer
        </ul>
    <li>Running the Training
        <ul>
            <li>Executing the main training function with a specified number of workers
        </ul>
    <li>Shutdown Ray Cluster
    <li>Summary
</ol>
</div>
</section>
<section id="architecture">
<h2>1. Architecture<a class="headerlink" href="#architecture" title="Link to this heading">#</a></h2>
<p><img alt="Architecture Diagram" src="https://lz-public-demo.s3.us-east-1.amazonaws.com/anyscale101/01_examples/04_Ray_Train_architecture.svg?sanitize=true" /></p>
<section id="library-imports">
<h3>2. Library Imports<a class="headerlink" href="#library-imports" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span> <span class="c1"># For type hinting</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span> <span class="c1"># PyTorch for tensor operations</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span> <span class="c1"># PyTorch for deep learning</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span> <span class="c1"># DataLoader for batching and shuffling data</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span> <span class="c1"># To load datasets from Hugging Face</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span> <span class="c1"># Transformers library for model and tokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Trainer</span><span class="p">,</span> <span class="c1"># </span>
    <span class="n">TrainingArguments</span><span class="p">,</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="c1"># Tokenizer for Hugging Face models</span>
    <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="c1"># Model for sequence classification</span>
<span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">ray.train</span> <span class="c1"># Ray Train for distributed training</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.train</span><span class="w"> </span><span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.train.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">TorchTrainer</span> <span class="c1"># Trainer for PyTorch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.train.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">TorchConfig</span> <span class="c1"># Configuration for PyTorch training</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.runtime_env</span><span class="w"> </span><span class="kn">import</span> <span class="n">RuntimeEnv</span> <span class="c1"># Runtime environment for Ray tasks</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="metrics-setup">
<h2>3. Metrics Setup<a class="headerlink" href="#metrics-setup" title="Link to this heading">#</a></h2>
<p>We will use accuracy as our evaluation metric. The compute_metrics function will calculate the accuracy of our model‚Äôs predictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Metrics</span>
<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span> <span class="c1"># Load accuracy metric from Hugging Face evaluate library</span>

<span class="c1"># Function to compute metrics</span>
<span class="c1"># This function takes the evaluation predictions and computes the accuracy metric</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-function-per-worker">
<h2>4. Training function per worker<a class="headerlink" href="#training-function-per-worker" title="Link to this heading">#</a></h2>
<p>This function will be executed by each worker during training. It handles data loading, tokenization, model initialization, and the training loop. This will automatically select GPU, MPS (on Apple Silicon), or CPU.</p>
<section id="tokenizer">
<h3>Tokenizer<a class="headerlink" href="#tokenizer" title="Link to this heading">#</a></h3>
<p>Tokenizer function is used to convert text into input IDs and attention masks.</p>
<p>Padding and truncation are applied to ensure uniform input size. This is essential for training models that require fixed-size inputs. The function is applied to the dataset using the map method. The map method applies the function to each example in the dataset. The batched=True argument allows processing multiple examples at once, which is more efficient.</p>
<p>The resulting dataset will have the tokenized inputs ready for training. This is a crucial step in preparing the dataset for model training. It ensures that the text data is converted into a format that the model can understand.</p>
</section>
<section id="dataloaders">
<h3>Dataloaders<a class="headerlink" href="#dataloaders" title="Link to this heading">#</a></h3>
<p>Dataloaders are used to load the dataset in batches for training and evaluation. This is essential for efficient training, especially with large datasets. The DataLoader will shuffle the training data and collate it into batches
The collate_fn is set to transformers.default_data_collator, which handles padding and batching automatically. The batch_size is set to the batch size per worker, which is defined in the config. This allows each worker to process a subset of the data in parallel. This is crucial for distributed training, where each worker processes a portion of the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_func_per_worker</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">):</span>
    
    <span class="c1"># Datasets</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;yelp_review_full&quot;</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>
    
    <span class="c1"># Tokenization function</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;    </span>
<span class="sd">        This function will tokenize the text data in the dataset</span>
<span class="sd">        It uses the tokenizer to convert text into input IDs and attention masks</span>
<span class="sd">        Padding and truncation are applied to ensure uniform input size</span>
<span class="sd">        This is essential for training models that require fixed-size inputs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">lr</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;epochs&quot;</span><span class="p">]</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;batch_size_per_worker&quot;</span><span class="p">]</span>

    <span class="c1"># select a subset of the dataset for training and evaluation</span>
    <span class="c1"># In a real-world scenario, you would use the entire dataset</span>
    <span class="n">SMALL_SIZE</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1"># The map method applies the function to each example in the dataset</span>
    <span class="c1"># The batched=True argument allows processing multiple examples at once, which is more efficient</span>
    <span class="c1"># The resulting dataset will have the tokenized inputs ready for training</span>
    <span class="c1"># This is a crucial step in preparing the dataset for model training</span>
    <span class="c1"># It ensures that the text data is converted into a format that the model can understand</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">SMALL_SIZE</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">SMALL_SIZE</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Prepare dataloader for each worker</span>
    <span class="c1"># Dataloaders are used to load the dataset in batches for training and evaluation</span>
    <span class="c1"># The dataloaders dictionary will hold the training and evaluation dataloaders</span>
    <span class="c1"># This allows for easy access to the dataloaders during training and evaluation</span>
    <span class="c1"># The dataloaders will be used in the training loop to fetch batches of data for each worker</span>
    <span class="n">dataloaders</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">train_dataset</span><span class="p">,</span> 
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">collate_fn</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">default_data_collator</span><span class="p">,</span> 
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span>
    <span class="p">)</span>
    <span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">eval_dataset</span><span class="p">,</span> 
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">collate_fn</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">default_data_collator</span><span class="p">,</span> 
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span>
    <span class="p">)</span>

    <span class="c1"># Obtain GPU device automatically</span>
    <span class="c1"># device = ray.train.torch.get_device()</span>
    
    <span class="c1"># Alternatively, you can specify the device manually</span>
    <span class="c1"># Check if CUDA or MPS is available and set device accordingly</span>
    <span class="c1"># This is useful for running on different hardware configurations</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span><span class="p">)</span> <span class="c1"># For Apple Silicon Macs</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

    <span class="c1"># Prepare model and optimizer</span>
    <span class="c1"># Load a pre-trained BERT model for sequence classification</span>
    <span class="c1"># The model is initialized with the number of labels for classification</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="s2">&quot;bert-base-cased&quot;</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span>
    <span class="p">)</span>
    <span class="c1"># The model is moved to the selected device (GPU, MPS, or CPU)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># The optimizer is set to SGD with momentum</span>
    <span class="c1"># This is essential for training the model</span>
    <span class="c1"># The optimizer will update the model parameters during training</span>
    <span class="c1"># The learning rate and momentum are set based on the configuration</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

    <span class="c1"># Start training loops</span>
    <span class="c1"># The model will be trained for the specified number of epochs</span>
    <span class="c1"># The model will be trained using the training dataloader</span>
    <span class="c1"># The model will be evaluated using the evaluation dataloader</span>
    <span class="c1"># The training loop will iterate over the epochs and batches</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Each epoch has a training and validation phase</span>
        <span class="k">for</span> <span class="n">phase</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Set model to training mode</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Set model to evaluate mode</span>

            <span class="c1"># breakpoint()</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloaders</span><span class="p">[</span><span class="n">phase</span><span class="p">]:</span> <span class="c1"># Iterate over batches in the dataloader</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

                <span class="c1"># zero the parameter gradients</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

                <span class="c1"># forward pass</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">phase</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">):</span>
                    <span class="c1"># Get model outputs and calculate loss</span>
                    <span class="c1"># The model processes the input batch and returns outputs</span>
                    <span class="c1"># The outputs include the loss and logits</span>
                    <span class="c1"># The loss is calculated based on the model&#39;s predictions and the true labels</span>
                    <span class="c1"># The logits are the raw predictions from the model</span>
                    <span class="c1"># The loss is used to update the model parameters during training</span>
                    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>

                    <span class="c1"># backward + optimize only if in training phase</span>
                    <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">:</span>
                        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># Backpropagate the loss to compute gradients</span>
                        <span class="c1"># The optimizer updates the model parameters based on the computed gradients</span>
                        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;train epoch:[</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">]</span><span class="se">\t</span><span class="s2">loss:</span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="main-training-function">
<h2>5. Main Training Function<a class="headerlink" href="#main-training-function" title="Link to this heading">#</a></h2>
<p>The <em>train_bert</em> function sets up the distributed training environment using Ray and starts the training process. To enable training using GPU, we only need to make the following changes:</p>
<ul class="simple">
<li><p>Require an GPU for each worker in ScalingConfig</p></li>
<li><p>Set backend to ‚Äúnccl‚Äù in TorchConfig</p></li>
</ul>
<p>This function is designed to train a BERT model using Ray Train. It sets up the training configuration, scaling, and starts the Ray cluster. The function initializes the Ray Train environment, configures the trainer, and starts the training process.</p>
<ul class="simple">
<li><p>It is intended to be run in a distributed setting with multiple workers, allowing for efficient training of large models on large datasets by leveraging Ray‚Äôs distributed computing capabilities.</p></li>
<li><p>The function uses the Ray Train library to manage distributed training and the TorchTrainer for PyTorch models.</p></li>
<li><p>It supports both GPU and CPU training, making it flexible for different hardware configurations.</p></li>
<li><p>Additionally, it can be easily adapted for different models and datasets by changing the model and dataset loading parts.</p></li>
<li><p>This approach provides a scalable solution for training deep learning models in a distributed manner and can be used in various environments, including local machines and cloud platforms.</p></li>
<li><p>It is a powerful tool for researchers and developers working with large-scale machine learning tasks, enabling efficient training on large datasets and easy integration into existing machine learning workflows with minimal changes.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># function to train BERT model using Ray Train</span>
<span class="c1"># This function sets up the training configuration, scaling, and starts the Ray cluster.</span>
<span class="c1"># It initializes the Ray Train environment, configures the trainer, and starts the training process.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_bert</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># This is the total batch size across all workers</span>

    <span class="c1"># Define the training configuration</span>
    <span class="c1"># This configuration includes the learning rate, number of epochs, and batch size per worker</span>
    <span class="n">train_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span>  <span class="c1"># Learning rate</span>
        <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># Reduced for faster testing</span>
        <span class="s2">&quot;batch_size_per_worker&quot;</span><span class="p">:</span> <span class="n">global_batch_size</span> <span class="o">//</span> <span class="n">num_workers</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="c1"># Configure computation resources</span>
    <span class="c1"># if using CPUs or MPS</span>
    <span class="n">scaling_config</span> <span class="o">=</span> <span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">resources_per_worker</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,})</span>
    
    <span class="c1"># If using GPUs, you can specify resources_per_worker={&quot;CPU&quot;: 1, &quot;GPU&quot;: 1}</span>
    <span class="c1"># scaling_config = ScalingConfig(num_workers=num_workers, resources_per_worker={&quot;CPU&quot;: 1, &quot;GPU&quot;: 1})</span>
    <span class="c1"># Set backend to nccl in TorchConfig</span>
    <span class="c1"># torch_config = TorchConfig(backend = &quot;nccl&quot;)</span>
    
    <span class="c1"># start your ray cluster</span>
    <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span> 
    
    <span class="c1"># Initialize a Ray TorchTrainer</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
        <span class="n">train_loop_per_worker</span><span class="o">=</span><span class="n">train_func_per_worker</span><span class="p">,</span>
        <span class="n">train_loop_config</span><span class="o">=</span><span class="n">train_config</span><span class="p">,</span>
        <span class="c1"># torch_config=torch_config, # Uncomment if using nccl backend</span>
        <span class="n">scaling_config</span><span class="o">=</span><span class="n">scaling_config</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span> <span class="c1"># Start the training process</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># This will print the training result, which includes metrics like loss and accuracy</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="start-training">
<h2>6. Start Training<a class="headerlink" href="#start-training" title="Link to this heading">#</a></h2>
<p>Finally, we call the train_bert function to start the training process. You can adjust the number of workers to use.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run the training function with the specified number of workers</span>
<span class="c1"># You can adjust the number of workers based on your hardware configuration</span>
<span class="n">train_bert</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>== Status ==
Current time: 2025-07-11 10:16:11 (running for 00:00:00.11)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/16 CPUs, 0/0 GPUs
Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts
Number of trials: 1/1 (1 PENDING)


== Status ==
Current time: 2025-07-11 10:16:16 (running for 00:00:05.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/16 CPUs, 0/0 GPUs
Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>== Status ==
Current time: 2025-07-11 10:16:21 (running for 00:00:10.22)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/16 CPUs, 0/0 GPUs
Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Cyan">(RayTrainWorker pid=41599)</span> train epoch:[0]	loss:1.764641
== Status ==
Current time: 2025-07-11 10:16:26 (running for 00:00:15.28)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/16 CPUs, 0/0 GPUs
Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts
Number of trials: 1/1 (1 RUNNING)


<span class=" -Color -Color-Cyan">(RayTrainWorker pid=41598)</span> train epoch:[0]	loss:1.949393<span class=" -Color -Color-Green"> [repeated 27x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)</span>
== Status ==
Current time: 2025-07-11 10:16:31 (running for 00:00:20.35)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/16 CPUs, 0/0 GPUs
Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2025-07-11 10:16:36 (running for 00:00:25.46)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/16 CPUs, 0/0 GPUs
Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts
Number of trials: 1/1 (1 RUNNING)


<span class=" -Color -Color-Cyan">(RayTrainWorker pid=41599)</span> train epoch:[1]	loss:1.799808<span class=" -Color -Color-Green"> [repeated 23x across cluster]</span>
== Status ==
Current time: 2025-07-11 10:16:41 (running for 00:00:30.54)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/16 CPUs, 0/0 GPUs
Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts
Number of trials: 1/1 (1 RUNNING)


<span class=" -Color -Color-Cyan">(RayTrainWorker pid=41598)</span> train epoch:[1]	loss:1.422321<span class=" -Color -Color-Green"> [repeated 27x across cluster]</span>
== Status ==
Current time: 2025-07-11 10:16:46 (running for 00:00:35.63)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/16 CPUs, 0/0 GPUs
Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trial TorchTrainer_4dd7a_00000 completed. Last result: 
== Status ==
Current time: 2025-07-11 10:16:49 (running for 00:00:38.30)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/16 CPUs, 0/0 GPUs
Result logdir: /tmp/ray/session_2025-07-11_10-16-09_200164_18044/artifacts/2025-07-11_10-16-11/TorchTrainer_2025-07-11_10-16-11/driver_artifacts
Number of trials: 1/1 (1 TERMINATED)


Training result: Result(
  metrics={},
  path=&#39;/Users/maxpumperla/ray_results/TorchTrainer_2025-07-11_10-16-11/TorchTrainer_4dd7a_00000_0_2025-07-11_10-16-11&#39;,
  filesystem=&#39;local&#39;,
  checkpoint=None
)
</pre></div>
</div>
</div>
</div>
<section id="shutdown-ray-cluster">
<h3>7. Shutdown Ray Cluster<a class="headerlink" href="#shutdown-ray-cluster" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Shutdown Ray after training is complete</span>
<span class="n">ray</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="summary">
<h3>8. Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h3>
<p>This notebook demonstrates how to use Ray Train, PyTorch, and Hugging Face Transformers to perform distributed training of a BERT model for sequence classification on the Yelp review dataset. It covers data loading, tokenization, model setup, and distributed training configuration, allowing you to efficiently train large models across multiple CPUs or GPUs. The notebook is designed to be accessible for machine learning engineers who want to learn scalable deep learning workflows using modern Python tools.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./courses/WL_Ray_Distributed_Training"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outline">Outline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture">1. Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#library-imports">2. Library Imports</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-setup">3. Metrics Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-function-per-worker">4. Training function per worker</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizer">Tokenizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataloaders">Dataloaders</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-training-function">5. Main Training Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#start-training">6. Start Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shutdown-ray-cluster">7. Shutdown Ray Cluster</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">8. Summary</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book community
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>