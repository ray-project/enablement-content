{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Ray Serve LLM: Foundations of Large Language Model Serving\n",
        "\n",
        "Â© 2025, Anyscale. All Rights Reserved\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ’» **Launch Locally**: You can run this notebook locally, but performance will be reduced.\n",
        "\n",
        "ðŸš€ **Launch on Cloud**: A Ray Cluster with GPUs (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This module provides a comprehensive introduction to serving Large Language Models (LLMs) with Ray Serve LLM. We'll explore the fundamentals of LLM serving, understand the challenges, and learn how Ray Serve LLM provides production-grade solutions for deploying LLMs at scale.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b> Here is the roadmap for this module:</b>\n",
        "<ul>\n",
        "    <li>What is LLM Serving?</li>\n",
        "    <li>Key Concepts and Optimizations</li>\n",
        "    <li>Challenges in LLM Serving</li>\n",
        "    <li>Ray Serve LLM Architecture</li>\n",
        "    <li>Getting Started with Ray Serve LLM</li>\n",
        "    <li>Key Takeaways</li>\n",
        "</ul>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is LLM Serving?\n",
        "\n",
        "Large Language Model (LLM) serving refers to the process of deploying trained language models to production environments where they can handle user requests and generate responses in real-time. This is fundamentally different from training models - serving focuses on making models available, scalable, and performant for end users.\n",
        "\n",
        "### The LLM Text Generation Process\n",
        "\n",
        "LLMs operate as **next-token predictors**. Here's how they work:\n",
        "\n",
        "1. **Tokenization**: Input text is converted into tokens (words, subwords, or characters)\n",
        "2. **Processing**: The model processes these tokens to understand context\n",
        "3. **Generation**: The model generates output one token at a time\n",
        "4. **Completion**: Generation stops when reaching stopping criteria or maximum length\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/public-images/ray-serve-llm/diagrams/LLM-text-generation.png\" width=\"800\">\n",
        "\n",
        "### Two Phases of LLM Inference\n",
        "\n",
        "LLM inference operates through two distinct phases that determine performance characteristics:\n",
        "\n",
        "#### Prefill Phase\n",
        "- The model encodes **all input tokens simultaneously**\n",
        "- High efficiency through parallelized computations\n",
        "- Maximizes GPU utilization\n",
        "- Precomputes and caches key-value (KV) vectors as intermediate token representations\n",
        "\n",
        "#### Decode Phase\n",
        "- The model generates tokens **sequentially** using the key-value cache (KV cache)\n",
        "- Each token depends on all previous tokens\n",
        "- Limited by memory bandwidth rather than compute capacity\n",
        "- Underutilizes GPU resources compared to prefill phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|<img src=\"https://cdn-uploads.huggingface.co/production/uploads/65263bfb3177c2a794997821/BGKtYLqM1X9o72oc9NW8Y.png\" width=\"70%\" loading=\"lazy\">|\n",
        "|:--|\n",
        "|prefill: parallel processing of prompt tokens, decode: sequential processing of single output tokens.|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Concepts and Optimizations\n",
        "\n",
        "These key concepts will help you design an LLM serving pipeline that meets your service level objectives (SLOs).\n",
        "\n",
        "### 1. Key-Value (KV) Caching\n",
        "\n",
        "KV caching eliminates redundant computations during text generation:\n",
        "\n",
        "**Without KV Cache**:\n",
        "- Recalculate keys and values for entire sequence each time\n",
        "- Extremely inefficient for long sequences\n",
        "\n",
        "**With KV Cache**:\n",
        "- Cache computed K and V values for all previous tokens\n",
        "- Only compute K and V for the new token\n",
        "- Reuse cached values for context\n",
        "\n",
        "### 2. Continuous Batching\n",
        "\n",
        "Continuous batching optimizes throughput by eliminating GPU idle time:\n",
        "\n",
        "**Vanilla Static Batching**:\n",
        "- Wait for all requests in batch to complete\n",
        "- Creates idle time when requests finish at different rates\n",
        "- Underutilizes GPU resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|<img src=\"https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png\" width=\"70%\" loading=\"lazy\">|\n",
        "|:--|\n",
        "|Completing four sequences using static batching. On the first iteration (left), each sequence generates one token (blue) from the prompt tokens (yellow). After several iterations (right), the completed sequences each have different sizes because each emits their end-of-sequence-token (red) at different iterations. Even though sequence 3 finished after two iterations, static batching means that the GPU will be underutilized until the last sequence in the batch finishes generation (in this example, sequence 2 after six iterations).|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Continuous Batching**:\n",
        "- Immediately replace completed requests with new ones\n",
        "- Maintains constant GPU utilization\n",
        "- Increases concurrent user capacity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|<img src=\"https://images.ctfassets.net/xjan103pcp94/744TAv4dJIQqeHcEaz5lko/b823cc2d92bbb0d82eb252901e1dce6d/cb_03_diagram-continuous-batching.png\" width=\"70%\" loading=\"lazy\">|\n",
        "|:--|\n",
        "|Completing seven sequences using continuous batching. Left shows the batch after a single iteration, right shows the batch after several iterations. Once a sequence emits an end-of-sequence token, we insert a new sequence in its place (i.e. sequences S5, S6, and S7). This achieves higher GPU utilization since the GPU does not wait for all sequences to complete before starting a new one.|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Model parallelization or alternatives\n",
        "\n",
        "Large LLMs (>70B) might provides more accurate answers but might not fit entirely on one GPU or one node. You can parallelize your model accross multiple GPUs or nodes to virtually increase your memory resources at the cost of some latency due to communication overhead.\n",
        "\n",
        "You can also use alternative options such as quantization, distillation, or multi-LoRA adapters to\n",
        "\n",
        "### 4. Context Window Considerations\n",
        "\n",
        "The context window defines the maximum tokens a model can process:\n",
        "\n",
        "| Context Length | Use Cases | Memory Impact |\n",
        "|----------------|-----------|---------------|\n",
        "| **4K-8K tokens** | Q&A, simple chat | Low KV cache requirements |\n",
        "| **32K-128K tokens** | Document analysis, summarization | Moderate memory usage |\n",
        "| **128K+ tokens** | Multi-step agents, complex reasoning | High memory requirements |\n",
        "\n",
        "A large context window might provide more accurate answers but also increase the memory pressure and how many requests can be processed concurrently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenges in LLM Serving\n",
        "\n",
        "Serving LLMs in production presents several unique challenges that traditional model serving doesn't face. Let's explore these challenges and understand why they matter.\n",
        "\n",
        "### 1. Memory Management\n",
        "\n",
        "Deploying LLMs is a **memory-intensive** task. A non-exhaustive list of memory constraints are:\n",
        "| Component | Description | Memory Impact |\n",
        "|-----------|-------------|---------------|\n",
        "| **Model Weights** | Model parameters | 7B model â‰ˆ 14GB (FP16) |\n",
        "| **KV Cache** | Token representations | Depends on context length |\n",
        "| **Activations** | Temporary buffers | Varies with batch size |\n",
        "\n",
        "**Example**: A 7B parameter model in FP16 precision requires approximately 14GB just for the model weights, not including the KV cache or activations.\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/public-images/ray-serve-llm/diagrams/gpu-memory.png\" width=\"800\">\n",
        "\n",
        "You can distribute your deployment on multiple GPUs or nodes. For example you could split the model accross multiple GPUs on a single node or accross multiple GPUs on multiple nodes.  \n",
        "\n",
        "\n",
        "\n",
        "See examples below for examples of different types of deployment:\n",
        "- Single node, single GPU: [Deploy a small-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/small-size-llm/README.html)\n",
        "- Single node, multiple GPU with tensor parallelism: [Deploy a medium-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/medium-size-llm/README.html)\n",
        "- Multiple nodes, multiple GPU with tensor and pipeline parallelism: [Deploy a large-sized LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/large-size-llm/README.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Latency Requirements\n",
        "\n",
        "Users expect **fast, interactive responses** from LLM applications:\n",
        "\n",
        "- **Time to First Token (TTFT)**: How long until the first token appears\n",
        "- **Time Per Output Token (TPOT)**: How long between subsequent tokens\n",
        "- **Total Response Time**: End-to-end latency\n",
        "\n",
        "### 3. Scalability Demands\n",
        "\n",
        "Production traffic is **unpredictable and bursty**:\n",
        "- Traffic spikes during peak hours\n",
        "- Need to scale up quickly during high demand\n",
        "- Scale down to zero during idle periods to save costs\n",
        "\n",
        "### 4. Cost Optimization\n",
        "\n",
        "GPUs represent **significant infrastructure costs**:\n",
        "- Maximize hardware utilization\n",
        "- Scale to zero during idle periods\n",
        "- Choose appropriate GPU types for your workload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why not Kubernetes ?\n",
        "\n",
        "You could either use Ray Serve or Kubernetes microservices to solve the challenges above. They are not mutually exclusive, as Ray Serve can run on Kubernetes. The differences are mostly about who does the orchestration and how much abstraction you want from the inference pipeline.\n",
        "\n",
        "**Ray Serve LLM**\n",
        "\n",
        "* Python-native orchestration (routing, batching, streaming).\n",
        "* Built-in autoscaling, backpressure, health checks or [LLM-optimized routing](https://docs.ray.io/en/latest/serve/llm/prefix-aware-request-router.html).\n",
        "* Actor-based sharding across nodes/GPUs.\n",
        "* Easy multi-model serving behind one endpoint.\n",
        "\n",
        "**Kubernetes**\n",
        "\n",
        "* Pod = unit per node; multi-node model parallelism needs extra controllers/operators.\n",
        "* Batching/routing/backpressure are DIY (app or sidecars).\n",
        "* Strong platform features (networking, security, quotas), but inference control isnâ€™t built-in.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ray Serve LLM + Anyscale Architecture\n",
        "\n",
        "Here is a diagram of how Ray Serve LLM + Anyscale provides a production-grade solution to your LLM deployment:\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/public-images/ray-serve-llm/diagrams/anyscale-serve-vllm.png\" width=\"800\">\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "- The above shows only one replica per model, but Ray Serve can easily scale to deploying multiple replicas.\n",
        "\n",
        "Ray Serve LLM + Anyscale provides a production-grade solution through three integrated components:\n",
        "\n",
        "### 1. Ray Serve for Orchestration\n",
        "\n",
        "Ray Serve handles the **orchestration and scaling** of your LLM deployment:\n",
        "\n",
        "- **Automatic scaling**: Adds/removes model replicas based on traffic\n",
        "- **Load balancing**: Distributes requests across available replicas\n",
        "- **Unified multi-model deployment**: Deploy and manage multiple models\n",
        "- **OpenAI-compatible API**: Drop-in replacement for OpenAI clients\n",
        "\n",
        "Here is a diagram of how Ray Serve LLM interact with a client's request\n",
        "\n",
        "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-serve-deep-dive/Ray+Serve+LLM.png\" width=\"800\">\n",
        "\n",
        "### 2. vLLM as the inference engine\n",
        "\n",
        "LLM inference is a non-trivial problem that requires tuning low-level hardware use and high-level algorithms. An **inference engine** abstracts this complexity and optimizes model execution. Ray Serve LLM natively integrates **vLLM** as its inference engine for several reasons:\n",
        "\n",
        "- **Fast GPU computation** with CUDA kernels specifically optimized for LLM inference.\n",
        "- **Continuous batching**: Continuously schedule tokens to be processed to maximize GPU utilization.\n",
        "- **Smart memory use**: Optimize memory usage with state-of-the-art algorithms like PagedAttention\n",
        "\n",
        "Ray Serve LLM gives you high flexibility on how to configure your vLLM engine (more on that later).\n",
        "\n",
        "### 3. Anyscale for Infrastructure\n",
        "\n",
        "Anyscale provides **managed infrastructure** and enterprise features:\n",
        "\n",
        "- **Managed infrastructure**: Optimized Ray clusters in your cloud\n",
        "- **Cost optimization**: Pay-as-you-go, scale-to-zero\n",
        "- **Enterprise security**: VPC, SSO, audit logs\n",
        "- **Seamless scaling**: Handle traffic spikes automatically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting Started with Ray Serve LLM\n",
        "\n",
        "Now that we understand the fundamentals, let's see how to get started with Ray Serve LLM. The process involves three main steps:\n",
        "\n",
        "1. **Configure** your LLM deployment\n",
        "2. **Deploy** the service\n",
        "3. **Query** the deployed model\n",
        "4. **Shutdown** the deployment\n",
        "\n",
        "### Step 1: Configuration\n",
        "\n",
        "Let's create a simple configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#serve_llama.py\n",
        "from ray.serve.llm import LLMConfig, build_openai_app\n",
        "\n",
        "llm_config = LLMConfig(\n",
        "    # Model loading configuration\n",
        "    model_loading_config=dict(\n",
        "        model_id=\"my-llama\", # custom name for the model\n",
        "        model_source=\"unsloth/Meta-Llama-3.1-8B-Instruct\", # huggingface model repo\n",
        "    ),\n",
        "    accelerator_type=\"L4\", # device to use (picked from your ray cluster)\n",
        "    ## Optional: configure Ray Serve autoscaling\n",
        "    deployment_config=dict(\n",
        "        autoscaling_config=dict(\n",
        "            min_replicas=1, # keep at least 1 replica up to avoid cold starts\n",
        "            max_replicas=2, # no more than 2 replicas to control cost\n",
        "        )\n",
        "    ),\n",
        "    # Configure your vLLM engine. Follow the same API as vLLM\n",
        "    # https://docs.vllm.ai/en/stable/configuration/engine_args.html\n",
        "    engine_kwargs=dict(max_model_len=8192),\n",
        ")\n",
        "\n",
        "app = build_openai_app({\"llm_configs\": [llm_config]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Deployment\n",
        "\n",
        "Deployment can be done locally or on Anyscale Services:\n",
        "\n",
        "**Local Deployment**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve run serve_llama:app --non-blocking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Anyscale Services**:\n",
        "\n",
        "To deploy your LLM with Anyscale Service, configure your cloud and compute configuration and point to your LLM configuration:\n",
        "```yaml\n",
        "# service.yaml\n",
        "name: deploy-llama-3-8b\n",
        "image_uri: anyscale/ray-llm:2.49.0-py311-cu128 # Anyscale Ray Serve LLM image. Use `containerfile: ./Dockerfile` to use a custom Dockerfile.\n",
        "compute_config:\n",
        "  auto_select_worker_config: true \n",
        "working_dir: .\n",
        "cloud:\n",
        "applications:\n",
        "  # Point to your app in your Python module\n",
        "  - import_path: serve_llama:app\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy your service:\n",
        "```bash\n",
        "!anyscale service deploy -f service.yaml\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Querying\n",
        "\n",
        "Once deployed, you can use the OpenAI Python client with `base_url` pointing to your Ray Serve endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# because deployed locally, we use localhost:8000 and a dummy placeholder API key\n",
        "base_url = \"http://localhost:8000\"\n",
        "token=\"DUMMY_KEY\"\n",
        "client = OpenAI(base_url= urljoin(base_url, \"v1\"), api_key=token)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"my-llama\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "# Stream and print JSON\n",
        "for chunk in response:\n",
        "    data = chunk.choices[0].delta.content\n",
        "    if data:\n",
        "        print(data, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Shutdown\n",
        "\n",
        "Shutdown a local deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve shutdown -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Terminate an Anyscale service:\n",
        "```bash\n",
        "anyscale service terminate deploy-my-llama\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "In this module, we've covered the essential foundations of LLM serving with Ray Serve LLM:\n",
        "\n",
        "1. **Understanding LLM Serving**: How LLMs generate text through prefill and decode phases\n",
        "4. **Key Optimizations**: KV caching, paged attention, and continuous batching\n",
        "2. **Challenges**: Memory management, latency, scalability, and cost optimization\n",
        "3. **Ray Serve LLM Architecture**: Three-component solution with Ray Serve, vLLM, and Anyscale\n",
        "5. **Getting Started**: Simple configuration and deployment process\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "In the next modules, we'll dive deeper into:\n",
        "- **Hands-on deployment** of a medium-sized LLMs, \n",
        "- **Advanced configurations** and optimizations (tool calling, LoRA, structured outputs...)\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Ray Serve LLM with Anyscale Documentation](https://docs.anyscale.com/llm/serving)\n",
        "- [Deploy LLM templates](https://console.anyscale.com/template-preview/deployment-serve-llm?utm_source=anyscale_docs&utm_medium=docs&utm_campaign=examples_page&utm_content=deployment-serve-llm?utm_source=anyscale&utm_medium=docs&utm_campaign=examples_page&utm_content=deployment-serve-llm)\n",
        "- [Ray Serve LLM Documentation](https://docs.ray.io/en/latest/serve/llm/index.html)\n",
        "- [vLLM Documentation](https://docs.vllm.ai/)\n",
        "\n",
        "Ready to start serving LLMs with Ray? Let's move on to the next module!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
