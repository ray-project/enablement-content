01_intro:
  title: Intro
  description: Learn how to use Ray Train V2 to run distributed PyTorch data-parallel
    training on an Anyscale cluster by training a ResNet-18 model on MNIST across
    multiple GPUs. Youll also understand when to use Ray Train, the basics of DDP,
    and how to set up data, visualization, checkpointing, and metric reporting for
    scalable training runs.
  sources:
  - 01_intro_to_ray_train.ipynb
  lessons:
    00_lesson:
      title:  01 路 Introduction to Ray Train
      description: ''
    01_lesson:
      title: 01 路 Imports
      description: Learn how to set up the core imports and environment needed to
        run distributed data-parallel PyTorch training with Ray Train V2 on an Anyscale
        cluster. By the end, youll be ready to train a ResNet-18 on MNIST across
        multiple GPUs with built-in checkpointing and metrics reporting.
    02_lesson:
      title: 04 路 Define ResNet-18 Model for MNIST
      description: In this lesson, youll set up the MNIST workflow by importing required
        libraries, downloading the dataset, and visualizing sample digits. By the
        end, youll be ready to define and train a ResNet-18 model on 2828 grayscale
        MNIST images.
    03_lesson:
      title: 05 路 Define the Ray Train Loop (DDP per-worker)
      description: Learn how to wrap your PyTorch training logic into a Ray Train
        per-worker training function, defining what each worker executes in a distributed
        run. Youll also understand Ray Trains core concepts (training function,
        workers, scaling configuration, and Trainer) and how they map from a standalone
        loop to a scalable distributed setup.
    04_lesson:
      title: 06 路 Define `train_loop_config`
      description: Learn how to define `train_loop_config`, the configuration dictionary
        that packages hyperparameters (e.g., epochs, global batch size, gradient accumulation)
        and runtime settings to pass into Ray Trains per-worker `train_loop`. By
        the end, youll be able to centralize and control training behavior through
        a single config object used by `TorchTrainer`.
    05_lesson:
      title: 07 路 Configure Scaling with `ScalingConfig`
      description: Learn how to configure distributed training scale using Rays `ScalingConfig`,
        including how worker count and related settings affect your effective `global_batch_size`.
        Youll connect these scaling choices to your training loop so you can size
        and run a multi-worker job correctly.
    06_lesson:
      title: 08 路 Wrap the Model with `prepare_model()`
      description: Learn how to use Ray Trains `ScalingConfig` to control distributed
        training by specifying the number of parallel workers and the CPU/GPU resources
        assigned to each. By the end, youll be able to scale a training job across
        multiple workers with Ray handling synchronization automatically.
    07_lesson:
      title: 09 路 Build the DataLoader with `prepare_data_loader()`
      description: In this lesson, youll build the MNIST `DataLoader` and prepare
        it for Ray Train using `prepare_data_loader()`. Youll learn how Ray automatically
        handles device placement and distributed sharding so each worker receives
        the correct portion of the data.
    08_lesson:
      title: 10 路 Report Training Metrics
      description: Learn how to build an MNIST `DataLoader` thats ready for Ray Train
        by applying standard preprocessing (`ToTensor` and `Normalize`) inside a reusable
        helper function. Youll be able to prepare batched, normalized training data
        for distributed training (or keep it separate from Ray Data when needed).
    09_lesson:
      title: 11 路 Save Checkpoints and Report Metrics
      description: Learn how to report training metrics (e.g., loss and epoch) from
        Ray Train workers, including how to restrict logging to rank 0. Youll also
        save model checkpoints safely by writing checkpoint files only from the rank-0
        worker while attaching relevant metrics.
    10_lesson:
      title: 14 路 Create the `TorchTrainer`
      description: In this lesson, youll build a `TorchTrainer` that wraps your PyTorch
        DDP training loop with Ray Train, including metric reporting and checkpointing
        via `ray.train.report`. Youll also configure persistent output storage (and
        rank-0-only checkpoint writes) using `RunConfig` so results, logs, and checkpoints
        are saved reliably.
    11_lesson:
      title: 16 路 Inspect the Training Results
      description: Learn how to review the outputs of a Ray Train run after calling
        `trainer.fit()`, including the logs, metrics, and checkpoints collected in
        the returned `result`. Youll be able to confirm that the distributed workers
        launched correctly and interpret the training artifacts saved to your run
        directory.
    12_lesson:
      title: 18 路 Load a Checkpoint for Inference
      description: Learn how to inspect the `Result` object returned by `trainer.fit()`
        to access final metrics, checkpoints, and run history. Youll also view the
        full per-epoch metric history using `result.metrics_dataframe` for deeper
        analysis of training progress.
    13_lesson:
      title: Lesson 13
      description: In Lesson 13, youll learn how to reload a trained ResNet-18 checkpoint
        into a GPU-backed Ray actor and serve inference requests. Youll run CPU-side
        preprocessing, generate and visualize MNIST predictions, and then cleanly
        shut down the actor to release GPU resources.
