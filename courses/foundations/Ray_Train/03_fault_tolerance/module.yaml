03_fault_tolerance:
  title: Fault Tolerance
  description: Learn how to make Ray Train jobs resilient to worker or node failures
    by adding checkpoint loading/saving (model, optimizer, epoch) to your training
    loop. Youâ€™ll also configure automatic retries with `FailureConfig` and run training
    that can recover and resume after interruptions.
  sources:
  - 03_fault_tolerance_in_ray_train.ipynb
  lessons:
    00_lesson:
      title: ğŸ›¡ï¸ 03 Â· Fault Tolerance in Ray Train
      description: ''
    01_lesson:
      title: 01 Â· Modify Training Loop to Enable Checkpoint Loading
      description: Learn how to update a Ray Train training loop to load from checkpoints
        so jobs can resume after failures. Youâ€™ll implement checkpoint-aware recovery
        to enable fault-tolerant training with automatic retries or manual restoration.
    02_lesson:
      title: 02 Â· Save Full Checkpoint with Extra State
      description: Learn how to extend a Ray Train training loop to detect and load
        an existing checkpoint via `ray.train.get_checkpoint()` so training can resume
        after interruptions. Youâ€™ll restore key state (e.g., model and optimizer)
        to enable fault-tolerant continuation.
    03_lesson:
      title: 04 Â· Launch Fault-Tolerant Training
      description: Learn how to make a Ray Train training loop fault-tolerant by loading
        from checkpoints and saving full recovery state (model weights, optimizer
        state, and current epoch). Youâ€™ll also configure `FailureConfig` to enable
        automatic retries so training can resume after worker or node failures.
    04_lesson:
      title: 05 Â· Manual Restoration from Checkpoints
      description: Learn how to manually restore a Ray Train job from saved checkpoints
        after a failure or interruption. Youâ€™ll practice selecting the right checkpoint
        and restarting training so it resumes from the last saved state instead of
        starting over.
    05_lesson:
      title: 07 Â· Clean Up Cluster Storage
      description: Learn how to run a fault-tolerant Ray training job with checkpointing
        and automatic retries, then manually restore and resume training from the
        latest saved checkpoint by recreating a `TorchTrainer` with the same configuration.
        Youâ€™ll verify the resumed run continues where it left offâ€”or exits immediately
        if training already completed.
    06_lesson:
      title: ğŸ‰ Wrapping Up & Next Steps
      description: Learn how to clean up persistent cluster storage by deleting tutorial
        artifacts such as the downloaded MNIST dataset, training output directories,
        and the generated Parquet dataset. By the end, youâ€™ll safely remove these
        paths to free space and leave your environment tidy for next steps.
    07_lesson:
      title: Lesson 7
      description: 'In Lesson 7, youâ€™ll recap and consolidate what you built across
        the course: a production-style training workflow using Ray Train on Anyscale,
        extended with Ray Data. Youâ€™ll also review how you added fault tolerance and
        identify next steps for applying these patterns in your own projects.'
