Search.setIndex({"alltitles": {"0. Overview": [[2, "overview"], [17, "overview"], [18, "overview"], [175, "overview"], [177, null]], "0. What is Ray Data?": [[9, "what-is-ray-data"], [198, "what-is-ray-data"], [200, null]], "01 \u00b7 Define Training Loop with Ray Data": [[238, "define-training-loop-with-ray-data"], [240, null]], "01 \u00b7 Imports": [[224, "imports"], [226, null]], "01 \u00b7 Modify Training Loop to Enable Checkpoint Loading": [[245, "modify-training-loop-to-enable-checkpoint-loading"], [247, null]], "02 \u00b7 Build DataLoader from Ray Data": [[238, "build-dataloader-from-ray-data"], [241, null]], "02 \u00b7 Download MNIST Dataset": [[224, "download-mnist-dataset"], [226, "download-mnist-dataset"]], "02 \u00b7 Save Full Checkpoint with Extra State": [[245, "save-full-checkpoint-with-extra-state"], [248, null]], "03 \u00b7 Configure Automatic Retries with FailureConfig": [[245, "configure-automatic-retries-with-failureconfig"], [248, "configure-automatic-retries-with-failureconfig"]], "03 \u00b7 Prepare Dataset for Ray Data": [[238, "prepare-dataset-for-ray-data"], [242, null]], "03 \u00b7 Visualize Sample Digits": [[224, "visualize-sample-digits"], [226, "visualize-sample-digits"]], "04 \u00b7 Define ResNet-18 Model for MNIST": [[224, "define-resnet-18-model-for-mnist"], [227, null]], "04 \u00b7 Launch Fault-Tolerant Training": [[245, "launch-fault-tolerant-training"], [249, null]], "04 \u00b7 Load Dataset into Ray Data": [[238, "load-dataset-into-ray-data"], [242, "load-dataset-into-ray-data"]], "04-d1 Generative computer-vision pattern with Ray Train": [[317, null], [318, null]], "04-d2 Diffusion-Policy Pattern with Ray Train": [[324, null], [325, null]], "04a Computer-vision pattern with Ray Train": [[350, null], [351, null]], "04b Tabular workload pattern with Ray Train": [[337, null], [338, null]], "04c Time-Series workload pattern with Ray Train": [[343, null], [344, null]], "04e Recommendation system pattern with Ray Train": [[330, null], [331, null]], "05 \u00b7 Define Image Transformation": [[238, "define-image-transformation"], [243, null]], "05 \u00b7 Define the Ray Train Loop (DDP per-worker)": [[224, "define-the-ray-train-loop-ddp-per-worker"], [228, null]], "05 \u00b7 Manual Restoration from Checkpoints": [[245, "manual-restoration-from-checkpoints"], [250, null]], "06 \u00b7 Apply Transformations with Ray Data": [[238, "apply-transformations-with-ray-data"], [243, "apply-transformations-with-ray-data"]], "06 \u00b7 Define train_loop_config": [[224, "define-train-loop-config"], [229, null]], "06 \u00b7 Resume Training from the Last Checkpoint": [[245, "resume-training-from-the-last-checkpoint"], [250, "resume-training-from-the-last-checkpoint"]], "07 \u00b7 Clean Up Cluster Storage": [[245, "clean-up-cluster-storage"], [251, null]], "07 \u00b7 Configure Scaling with ScalingConfig": [[224, "configure-scaling-with-scalingconfig"], [230, null]], "07 \u00b7 Configure TorchTrainer with Ray Data": [[238, "configure-torchtrainer-with-ray-data"], [244, null]], "08 \u00b7 Launch Training with Ray Data": [[238, "launch-training-with-ray-data"], [244, "launch-training-with-ray-data"]], "08 \u00b7 Wrap the Model with prepare_model()": [[224, "wrap-the-model-with-prepare-model"], [231, null]], "09 \u00b7 Build the DataLoader with prepare_data_loader()": [[224, "build-the-dataloader-with-prepare-data-loader"], [232, null]], "1. Architecture": [[293, "architecture"], [296, null]], "1. Cloud Object Store": [[86, "cloud-object-store"], [87, "cloud-object-store"]], "1. Create Anyscale Resources with Terraform": [[28, "create-anyscale-resources-with-terraform"], [30, null], [43, "create-anyscale-resources-with-terraform"], [45, null], [53, "create-anyscale-resources-with-terraform"], [56, null]], "1. Creating Remote Functions": [[2, "creating-remote-functions"], [175, "creating-remote-functions"], [178, null]], "1. Dataset tuples": [[324, "dataset-tuples"], [325, "dataset-tuples"]], "1. Deploy to Kubernetes with Anyscale Operator": [[24, "deploy-to-kubernetes-with-anyscale-operator"], [25, "deploy-to-kubernetes-with-anyscale-operator"]], "1. How to Use Ray Data?": [[9, "how-to-use-ray-data"], [198, "how-to-use-ray-data"], [200, "how-to-use-ray-data"]], "1. Imports": [[330, "imports"], [332, null], [337, "imports"], [339, null], [343, "imports"], [345, null], [350, "imports"], [352, null]], "1. Imports and setup": [[317, "imports-and-setup"], [319, null], [324, "imports-and-setup"], [326, null]], "1. Installation": [[35, "installation"], [38, null], [66, "installation"], [69, null]], "1. Key-Value (KV) Caching": [[101, "key-value-kv-caching"], [102, "key-value-kv-caching"], [105, "key-value-kv-caching"]], "1. Loading and visualizing data": [[14, "loading-and-visualizing-data"]], "1. Loading the data": [[7, "loading-the-data"], [253, "loading-the-data"], [255, null]], "1. Memory Management": [[101, "memory-management"], [102, "memory-management"], [106, "memory-management"]], "1. Model Quality Benchmarks": [[118, "model-quality-benchmarks"], [124, "model-quality-benchmarks"]], "1. Object store": [[3, "object-store"], [181, "object-store"], [183, null]], "1. Overview of Ray Serve": [[16, "overview-of-ray-serve"]], "1. Overview of the Ray AI Libraries": [[4, "overview-of-the-ray-ai-libraries"], [12, "overview-of-the-ray-ai-libraries"], [171, "overview-of-the-ray-ai-libraries"], [173, null]], "1. PyTorch introductory example (single GPU)": [[13, "pytorch-introductory-example-single-gpu"]], "1. Ray Serve for Orchestration": [[101, "ray-serve-for-orchestration"], [102, "ray-serve-for-orchestration"], [107, "ray-serve-for-orchestration"]], "1. Reduce max_model_len": [[110, "reduce-max-model-len"], [116, "reduce-max-model-len"]], "1. Sign Up for Anyscale": [[100, "sign-up-for-anyscale"]], "1. Split Notebooks and Generate Navigation": [[0, "split-notebooks-and-generate-navigation"]], "1. User Profile Retrieval": [[168, "user-profile-retrieval"]], "1. What is an Anyscale Cloud?": [[17, "what-is-an-anyscale-cloud"], [19, null]], "1. When to Consider Ray Data": [[10, "when-to-consider-ray-data"], [206, "when-to-consider-ray-data"], [208, null]], "1. When to Consider Ray Serve": [[11, "when-to-consider-ray-serve"], [218, "when-to-consider-ray-serve"], [220, null]], "1. When to use Ray Data": [[15, "when-to-use-ray-data"]], "1. When to use Ray Train": [[5, "when-to-use-ray-train"], [6, "when-to-use-ray-train"], [259, "when-to-use-ray-train"], [261, null]], "1.1 Configure Google Cloud Authentication": [[35, "configure-google-cloud-authentication"], [38, "configure-google-cloud-authentication"]], "1.1. Configure Google Cloud Authentication": [[66, "configure-google-cloud-authentication"], [69, "configure-google-cloud-authentication"]], "1.1. Pattern: pass an object as a top-level argument": [[3, "pattern-pass-an-object-as-a-top-level-argument"], [181, "pattern-pass-an-object-as-a-top-level-argument"], [183, "pattern-pass-an-object-as-a-top-level-argument"]], "1.2 Enable Required APIs": [[35, "enable-required-apis"], [38, "enable-required-apis"]], "1.2: Enable Required APIs": [[66, "enable-required-apis"], [69, "enable-required-apis"]], "10 \u00b7 Report Training Metrics": [[224, "report-training-metrics"], [233, null]], "10. Clean up": [[324, "clean-up"], [329, "clean-up"]], "10. Conclusion": [[53, "conclusion"], [65, null]], "10. Helper: Ray-prepared DataLoaders": [[350, "helper-ray-prepared-dataloaders"], [354, null]], "10. Launch distributed Training with TorchTrainer": [[317, "launch-distributed-training-with-torchtrainer"], [321, "launch-distributed-training-with-torchtrainer"]], "10. Plot train and validation loss curves": [[330, "plot-train-and-validation-loss-curves"], [334, "plot-train-and-validation-loss-curves"]], "10. Ray Train training loop (with teacher forcing)": [[343, "ray-train-training-loop-with-teacher-forcing"], [347, null]], "10. Start distributed training": [[337, "start-distributed-training"], [340, "start-distributed-training"]], "101 - Anyscale Organization and Cloud Setup": [[96, null], [97, null]], "101 \u2013  Introduction to Anyscale Services": [[92, null], [93, null]], "101 \u2013 Collaboration on Anyscale": [[94, null], [95, null]], "101 \u2013 Compute Configs and Execution Environments in Anyscale": [[84, null], [85, null]], "101 \u2013 Debug and Monitor Your Anyscale Application": [[88, null], [89, null]], "101 \u2013 Developing Application with Anyscale": [[82, null], [83, null]], "101 \u2013 Introduction to Anyscale Jobs": [[90, null], [91, null]], "101 \u2013 Storage Options in the Anyscale Platform": [[86, null], [87, null]], "101 \u2014 Introduction to Anyscale Workspaces": [[80, null], [81, null]], "11 \u00b7 Save Checkpoints and Report Metrics": [[224, "save-checkpoints-and-report-metrics"], [234, null]], "11. Evaluate the trained model": [[337, "evaluate-the-trained-model"], [340, "evaluate-the-trained-model"]], "11. Launch training on 8 GPUs": [[343, "launch-training-on-8-gpus"], [347, "launch-training-on-8-gpus"]], "11. Plot loss curves": [[317, "plot-loss-curves"], [321, "plot-loss-curves"]], "11. Resume training from checkpoint": [[330, "resume-training-from-checkpoint"], [335, null]], "11. train_loop_per_worker": [[350, "train-loop-per-worker"], [355, null]], "12 \u00b7 Save Checkpoints on Rank-0 Only": [[224, "save-checkpoints-on-rank-0-only"], [234, "save-checkpoints-on-rank-0-only"]], "12. Confusion matrix visualization": [[337, "confusion-matrix-visualization"], [341, null]], "12. Inference: recommend top-N items for a user": [[330, "inference-recommend-top-n-items-for-a-user"], [336, null]], "12. Launch distributed training with TorchTrainer": [[350, "launch-distributed-training-with-torchtrainer"], [356, null]], "12. Plot training and validation loss": [[343, "plot-training-and-validation-loss"], [347, "plot-training-and-validation-loss"]], "12. Resume from latest checkpoint": [[317, "resume-from-latest-checkpoint"], [322, null]], "13 \u00b7 Configure Persistent Storage with RunConfig": [[224, "configure-persistent-storage-with-runconfig"], [234, "configure-persistent-storage-with-runconfig"]], "13. CPU batch inference with Ray Data": [[337, "cpu-batch-inference-with-ray-data"], [341, "cpu-batch-inference-with-ray-data"]], "13. Join top-N item IDs with movie titles": [[330, "join-top-n-item-ids-with-movie-titles"], [336, "join-top-n-item-ids-with-movie-titles"]], "13. Plot training and validation loss curves": [[350, "plot-training-and-validation-loss-curves"], [357, null]], "13. Resume training from checkpoint": [[343, "resume-training-from-checkpoint"], [348, null]], "13. Reverse diffusion sampler": [[317, "reverse-diffusion-sampler"], [323, null]], "14 \u00b7 Create the TorchTrainer": [[224, "create-the-torchtrainer"], [235, null]], "14. Clean up shared storage": [[330, "clean-up-shared-storage"], [336, "clean-up-shared-storage"]], "14. Demonstrate fault-tolerant resumption": [[350, "demonstrate-fault-tolerant-resumption"], [358, null]], "14. Feature-importance diagnostics": [[337, "feature-importance-diagnostics"], [341, "feature-importance-diagnostics"]], "14. Generate and display samples from the best checkpoint": [[317, "generate-and-display-samples-from-the-best-checkpoint"], [323, "generate-and-display-samples-from-the-best-checkpoint"]], "14. Inference helper \u2014 Ray Data batch predictor on GPU": [[343, "inference-helper-ray-data-batch-predictor-on-gpu"], [349, null]], "15 \u00b7 Launch Training with trainer.fit()": [[224, "launch-training-with-trainer-fit"], [235, "launch-training-with-trainer-fit"]], "15. Batch inference with Ray Data": [[350, "batch-inference-with-ray-data"], [359, null]], "15. Clean up shared storage": [[317, "clean-up-shared-storage"], [323, "clean-up-shared-storage"]], "15. Continue training from the latest checkpoint": [[337, "continue-training-from-the-latest-checkpoint"], [342, null]], "15. Run distributed inference and visualize results": [[343, "run-distributed-inference-and-visualize-results"], [349, "run-distributed-inference-and-visualize-results"]], "16 \u00b7 Inspect the Training Results": [[224, "inspect-the-training-results"], [236, null]], "16. Cleanup: remove all training artifacts": [[343, "cleanup-remove-all-training-artifacts"], [349, "cleanup-remove-all-training-artifacts"]], "16. Run and visualize Ray Data inference": [[350, "run-and-visualize-ray-data-inference"], [359, "run-and-visualize-ray-data-inference"]], "16. Verify post-training inference": [[337, "verify-post-training-inference"], [342, "verify-post-training-inference"]], "17 \u00b7 View Metrics as a DataFrame": [[224, "view-metrics-as-a-dataframe"], [236, "view-metrics-as-a-dataframe"]], "17. Clean up": [[337, "clean-up"], [342, "clean-up"], [350, "clean-up"], [359, "clean-up"]], "18 \u00b7 Load a Checkpoint for Inference": [[224, "load-a-checkpoint-for-inference"], [237, null]], "19 \u00b7 Run Inference and Visualize Predictions": [[224, "run-inference-and-visualize-predictions"], [237, "run-inference-and-visualize-predictions"]], "2. Attach Required IAM Policies to Your existing EKS\u2019s Node Role": [[53, "attach-required-iam-policies-to-your-existing-eks-s-node-role"], [57, null]], "2. Build the Book": [[0, "build-the-book"]], "2. Chaining Tasks and Passing Data": [[3, "chaining-tasks-and-passing-data"], [181, "chaining-tasks-and-passing-data"], [184, null]], "2. Clone the Repository (Optional)": [[100, "clone-the-repository-optional"]], "2. Cloud Deployment Types": [[17, "cloud-deployment-types"], [20, null]], "2. Continuous Batching": [[101, "continuous-batching"], [102, "continuous-batching"], [105, "continuous-batching"]], "2. Create Anyscale Resources with Terraform": [[35, "create-anyscale-resources-with-terraform"], [39, null], [66, "create-anyscale-resources-with-terraform"], [70, null]], "2. Distributed Data Parallel Training with Ray Train and PyTorch (multiple GPUs)": [[13, "distributed-data-parallel-training-with-ray-train-and-pytorch-multiple-gpus"]], "2. End-to-end example: predicting taxi tips in New York": [[12, "end-to-end-example-predicting-taxi-tips-in-new-york"]], "2. Executing Remote Functions": [[2, "executing-remote-functions"], [175, "executing-remote-functions"], [179, null]], "2. Generate a real pendulum dataset": [[324, "generate-a-real-pendulum-dataset"], [326, "generate-a-real-pendulum-dataset"]], "2. How to work with Ray Data": [[10, "how-to-work-with-ray-data"], [206, "how-to-work-with-ray-data"], [209, null]], "2. Implement an Classifier service": [[16, "implement-an-classifier-service"]], "2. Install Kubernetes Components": [[43, "install-kubernetes-components"], [46, null]], "2. Latency Requirements": [[101, "latency-requirements"], [102, "latency-requirements"], [106, "latency-requirements"]], "2. Library Imports": [[293, "library-imports"], [296, "library-imports"]], "2. Load 10 % of Food-101": [[317, "load-10-of-food-101"], [319, "load-10-of-food-101"], [350, "load-10-of-food-101"], [352, "load-10-of-food-101"]], "2. Load MovieLens 100K dataset": [[330, "load-movielens-100k-dataset"], [332, "load-movielens-100k-dataset"]], "2. Load NYC taxi passenger counts (30-min)": [[343, "load-nyc-taxi-passenger-counts-30-min"], [345, "load-nyc-taxi-passenger-counts-30-min"]], "2. Load the University of California, Irvine (UCI) Cover type dataset": [[337, "load-the-university-of-california-irvine-uci-cover-type-dataset"], [339, "load-the-university-of-california-irvine-uci-cover-type-dataset"]], "2. Loading Data": [[9, "loading-data"], [15, "loading-data"], [198, "loading-data"], [201, null]], "2. Overview of Ray Serve": [[11, "overview-of-ray-serve"], [218, "overview-of-ray-serve"], [221, null]], "2. Quick end-to-end example": [[4, "quick-end-to-end-example"], [171, "quick-end-to-end-example"], [174, null]], "2. Register the Anyscale Cloud": [[28, "register-the-anyscale-cloud"], [31, null]], "2. Setting up a PyTorch model": [[14, "setting-up-a-pytorch-model"]], "2. Shared File Storage": [[86, "shared-file-storage"], [87, "shared-file-storage"]], "2. Single GPU Training with PyTorch": [[5, "single-gpu-training-with-pytorch"]], "2. Single GPU Training with PyTorch Lightning": [[6, "single-gpu-training-with-pytorch-lightning"], [259, "single-gpu-training-with-pytorch-lightning"], [262, null]], "2. Starting out with vanilla PyTorch": [[7, "starting-out-with-vanilla-pytorch"], [253, "starting-out-with-vanilla-pytorch"], [256, null]], "2. Task and Domain Alignment": [[118, "task-and-domain-alignment"], [124, "task-and-domain-alignment"]], "2. Training objective": [[324, "training-objective"], [325, "training-objective"]], "2. Use Quantized Models": [[110, "use-quantized-models"], [116, "use-quantized-models"]], "2. User Registration": [[168, "user-registration"]], "2. Virtual Machines (VM) vs. Kubernetes (K8s)": [[24, "virtual-machines-vm-vs-kubernetes-k8s"], [26, null]], "2. vLLM as the inference engine": [[101, "vllm-as-the-inference-engine"], [102, "vllm-as-the-inference-engine"], [107, "vllm-as-the-inference-engine"]], "2.1 Control Layer: What Anyscale Manages or Needs Access To": [[24, "control-layer-what-anyscale-manages-or-needs-access-to"], [26, "control-layer-what-anyscale-manages-or-needs-access-to"]], "2.1 Create terraform.tfvars": [[35, "create-terraform-tfvars"], [39, "create-terraform-tfvars"]], "2.1 Install the Cluster Autoscaler": [[43, "install-the-cluster-autoscaler"], [46, "install-the-cluster-autoscaler"]], "2.1 Overview": [[6, "overview"], [259, "overview"], [262, "overview"]], "2.1 Vanilla XGboost code": [[4, "vanilla-xgboost-code"], [171, "vanilla-xgboost-code"], [174, "vanilla-xgboost-code"]], "2.1. Overview": [[5, "overview"]], "2.1: Create terraform.tfvars": [[66, "create-terraform-tfvars"], [70, "create-terraform-tfvars"]], "2.2 Data Layer: Storage, Object Stores, and External Dependencies": [[24, "data-layer-storage-object-stores-and-external-dependencies"], [26, "data-layer-storage-object-stores-and-external-dependencies"]], "2.2 Hyperparameter tuning with Ray Tune": [[4, "hyperparameter-tuning-with-ray-tune"], [171, "hyperparameter-tuning-with-ray-tune"], [174, "hyperparameter-tuning-with-ray-tune"]], "2.2 Install the AWS Load Balancer Controller": [[43, "install-the-aws-load-balancer-controller"], [46, "install-the-aws-load-balancer-controller"]], "2.2 Note on blocks": [[10, "note-on-blocks"], [206, "note-on-blocks"], [210, "note-on-blocks"]], "2.2 Run Terraform Commands": [[35, "run-terraform-commands"], [39, "run-terraform-commands"]], "2.2. Build model and load it on the GPU": [[5, "build-model-and-load-it-on-the-gpu"]], "2.2. Create a torch dataloader": [[6, "create-a-torch-dataloader"], [259, "create-a-torch-dataloader"], [262, "create-a-torch-dataloader"]], "2.2: Deploy Infrastructure": [[66, "deploy-infrastructure"], [70, "deploy-infrastructure"]], "2.3 Define a stable diffusion model": [[6, "define-a-stable-diffusion-model"], [259, "define-a-stable-diffusion-model"], [262, "define-a-stable-diffusion-model"]], "2.3 Install the Nginx Ingress Controller": [[43, "install-the-nginx-ingress-controller"], [46, "install-the-nginx-ingress-controller"]], "2.3 Workload Execution Layer: How Ray Runs on Each Backend": [[24, "workload-execution-layer-how-ray-runs-on-each-backend"], [26, "workload-execution-layer-how-ray-runs-on-each-backend"]], "2.3. Create Dataset and DataLoader": [[5, "create-dataset-and-dataloader"]], "2.3. Distributed training with Ray Train": [[4, "distributed-training-with-ray-train"], [171, "distributed-training-with-ray-train"], [174, "distributed-training-with-ray-train"]], "2.4 (Optional) Install the Nvidia Device Plugin": [[43, "optional-install-the-nvidia-device-plugin"], [46, "optional-install-the-nvidia-device-plugin"]], "2.4 Serving an ensemble model with Ray Serve": [[4, "serving-an-ensemble-model-with-ray-serve"], [171, "serving-an-ensemble-model-with-ray-serve"], [174, "serving-an-ensemble-model-with-ray-serve"]], "2.4 When to use which": [[24, "when-to-use-which"], [26, "when-to-use-which"]], "2.4. Create metrics and checkpointing": [[5, "create-metrics-and-checkpointing"]], "2.4. Define a PyTorch Lightning training loop": [[6, "define-a-pytorch-lightning-training-loop"], [259, "define-a-pytorch-lightning-training-loop"], [262, "define-a-pytorch-lightning-training-loop"]], "2.5 Batch inference with Ray Data": [[4, "batch-inference-with-ray-data"], [171, "batch-inference-with-ray-data"], [174, "batch-inference-with-ray-data"]], "2.5. Run the training loop": [[5, "run-the-training-loop"]], "2.6 Clean up": [[4, "clean-up"], [171, "clean-up"], [174, "clean-up"]], "2.6. Use checkpointed model to generate predictions": [[5, "use-checkpointed-model-to-generate-predictions"]], "20 \u00b7 Clean Up the Ray Actor": [[224, "clean-up-the-ray-actor"], [237, "clean-up-the-ray-actor"]], "3. (Optional) More Kubernetes Deployments Components": [[24, "optional-more-kubernetes-deployments-components"], [27, null]], "3. A Demonstrative Example of Resource Creation with AWS EC2": [[17, "a-demonstrative-example-of-resource-creation-with-aws-ec2"], [21, null]], "3. Advanced features of Ray Serve": [[16, "advanced-features-of-ray-serve"]], "3. Anyscale for Infrastructure": [[101, "anyscale-for-infrastructure"], [102, "anyscale-for-infrastructure"], [107, "anyscale-for-infrastructure"]], "3. Context Window Requirements": [[118, "context-window-requirements"], [124, "context-window-requirements"]], "3. Distributed Data Parallel Training with Ray Train and PyTorch": [[5, "distributed-data-parallel-training-with-ray-train-and-pytorch"]], "3. Distributed Training with Ray Train and PyTorch Lightning": [[6, "distributed-training-with-ray-train-and-pytorch-lightning"], [259, "distributed-training-with-ray-train-and-pytorch-lightning"], [263, null]], "3. Enable Pipeline Parallelism": [[110, "enable-pipeline-parallelism"], [116, "enable-pipeline-parallelism"]], "3. Getting Results": [[2, "getting-results"], [175, "getting-results"], [179, "getting-results"]], "3. Hyperparameter tuning with Ray Tune": [[7, "hyperparameter-tuning-with-ray-tune"], [253, "hyperparameter-tuning-with-ray-tune"], [257, null]], "3. Implement an image classification service": [[11, "implement-an-image-classification-service"], [218, "implement-an-image-classification-service"], [222, null]], "3. Install Kubernetes Components": [[53, "install-kubernetes-components"], [58, null]], "3. Install Ray and the Anyscale CLI (Recommended)": [[100, "install-ray-and-the-anyscale-cli-recommended"]], "3. Introduction to Ray Tune": [[14, "introduction-to-ray-tune"]], "3. Lazy execution mode": [[10, "lazy-execution-mode"], [206, "lazy-execution-mode"], [211, null]], "3. Loading data": [[10, "loading-data"], [206, "loading-data"], [210, null]], "3. Local Cluster Storage": [[86, "local-cluster-storage"], [87, "local-cluster-storage"]], "3. Metrics Setup": [[293, "metrics-setup"], [297, null]], "3. Model parallelization or alternatives": [[101, "model-parallelization-or-alternatives"], [102, "model-parallelization-or-alternatives"], [105, "model-parallelization-or-alternatives"]], "3. Normalize and split": [[324, "normalize-and-split"], [326, "normalize-and-split"]], "3. Overview of the training loop in Ray Train": [[13, "overview-of-the-training-loop-in-ray-train"]], "3. Point to Parquet dataset URI": [[330, "point-to-parquet-dataset-uri"], [332, "point-to-parquet-dataset-uri"]], "3. Register the Anyscale Cloud": [[35, "register-the-anyscale-cloud"], [40, null], [43, "register-the-anyscale-cloud"], [47, null]], "3. Resample to hourly, then normalize": [[343, "resample-to-hourly-then-normalize"], [345, "resample-to-hourly-then-normalize"]], "3. Resize and encode images": [[317, "resize-and-encode-images"], [319, "resize-and-encode-images"], [350, "resize-and-encode-images"], [352, "resize-and-encode-images"]], "3. Reverse diffusion (sampling)": [[324, "reverse-diffusion-sampling"], [325, "reverse-diffusion-sampling"]], "3. Running an experiment with Ray AI libraries": [[12, "running-an-experiment-with-ray-ai-libraries"]], "3. Scalability Demands": [[101, "scalability-demands"], [102, "scalability-demands"], [106, "scalability-demands"]], "3. Serve Locally for Testing": [[0, "serve-locally-for-testing"]], "3. Task retries": [[3, "task-retries"], [181, "task-retries"], [185, null]], "3. Test": [[28, "test"], [32, null]], "3. Transforming Data": [[9, "transforming-data"], [15, "transforming-data"], [198, "transforming-data"], [202, null]], "3. Troubleshooting GPU Availability": [[66, "troubleshooting-gpu-availability"], [71, null]], "3. Visualize class balance": [[337, "visualize-class-balance"], [339, "visualize-class-balance"]], "3.1 Distributed Data Parallel Training": [[6, "distributed-data-parallel-training"], [259, "distributed-data-parallel-training"], [263, "distributed-data-parallel-training"]], "3.1 IAM Role Definition": [[17, "iam-role-definition"], [22, null]], "3.1 Install the Cluster Autoscaler": [[53, "install-the-cluster-autoscaler"], [58, "install-the-cluster-autoscaler"]], "3.1. Overview of the training loop in Ray Train": [[5, "overview-of-the-training-loop-in-ray-train"]], "3.1.1\u202f\u202fAnyscale Control Plane Role (anyscale-iam-role-id)": [[17, "anyscale-control-plane-role-anyscale-iam-role-id"], [22, "anyscale-control-plane-role-anyscale-iam-role-id"]], "3.1.2\u202f\u202fInstance Role (instance-iam-role-id)": [[17, "instance-role-instance-iam-role-id"], [22, "instance-role-instance-iam-role-id"]], "3.10. Activity: Run the distributed training with more workers": [[5, "activity-run-the-distributed-training-with-more-workers"]], "3.2 Install the AWS Load Balancer Controller": [[53, "install-the-aws-load-balancer-controller"], [58, "install-the-aws-load-balancer-controller"]], "3.2 Ray Train Migration": [[6, "ray-train-migration"], [259, "ray-train-migration"], [263, "ray-train-migration"]], "3.2. Configure scale and GPUs": [[5, "configure-scale-and-gpus"]], "3.2.1. Note on Ray Train key concepts": [[5, "note-on-ray-train-key-concepts"]], "3.2\u202fVPC": [[17, "vpc"], [22, "vpc"]], "3.3 Install the Nginx Ingress Controller": [[53, "install-the-nginx-ingress-controller"], [58, "install-the-nginx-ingress-controller"]], "3.3 Subnets": [[17, "subnets"], [22, "subnets"]], "3.3. Configure scale and GPUs": [[6, "configure-scale-and-gpus"], [259, "configure-scale-and-gpus"], [263, "configure-scale-and-gpus"]], "3.3. Migrating the model to Ray Train": [[5, "migrating-the-model-to-ray-train"]], "3.3.1. Note on Ray Train key concepts": [[6, "note-on-ray-train-key-concepts"], [259, "note-on-ray-train-key-concepts"], [263, "note-on-ray-train-key-concepts"]], "3.4 (Optional) Install the Nvidia Device Plugin": [[53, "optional-install-the-nvidia-device-plugin"], [58, "optional-install-the-nvidia-device-plugin"]], "3.4 Create and fit a Ray Train TorchTrainer": [[6, "create-and-fit-a-ray-train-torchtrainer"], [259, "create-and-fit-a-ray-train-torchtrainer"], [263, "create-and-fit-a-ray-train-torchtrainer"]], "3.4. Migrating the dataset to Ray Train": [[5, "migrating-the-dataset-to-ray-train"]], "3.4\u202fSecurity Groups": [[17, "security-groups"], [22, "security-groups"]], "3.5. Access the training results": [[6, "access-the-training-results"], [259, "access-the-training-results"], [263, "access-the-training-results"]], "3.5. Reporting checkpoints and metrics": [[5, "reporting-checkpoints-and-metrics"]], "3.5.1. Note on the checkpoint lifecycle": [[5, "note-on-the-checkpoint-lifecycle"]], "3.5\u202fS3": [[17, "s3"], [22, "s3"]], "3.6. Configure remote storage": [[5, "configure-remote-storage"]], "3.6. Load the checkpointed model to generate predictions": [[6, "load-the-checkpointed-model-to-generate-predictions"], [259, "load-the-checkpointed-model-to-generate-predictions"], [263, "load-the-checkpointed-model-to-generate-predictions"]], "3.6\u202fEFS (Optional)": [[17, "efs-optional"], [22, "efs-optional"]], "3.7. Activity: Run the distributed training with more workers": [[6, "activity-run-the-distributed-training-with-more-workers"], [259, "activity-run-the-distributed-training-with-more-workers"], [263, "activity-run-the-distributed-training-with-more-workers"]], "3.7. Launching the distributed training job": [[5, "launching-the-distributed-training-job"]], "3.7\u202fMemoryDB (Optional)": [[17, "memorydb-optional"], [22, "memorydb-optional"]], "3.8 Summary": [[17, "summary"], [22, "summary"]], "3.8. Access the training results": [[5, "access-the-training-results"]], "3.9. Use checkpointed model to generate predictions": [[5, "id1"]], "4. Cleanup": [[28, "cleanup"], [33, null]], "4. Context Window Considerations": [[101, "context-window-considerations"], [102, "context-window-considerations"], [105, "context-window-considerations"]], "4. Cost Optimization": [[101, "cost-optimization"], [102, "cost-optimization"], [106, "cost-optimization"]], "4. Data Operations: Grouping, Aggregation, and Shuffling": [[15, "data-operations-grouping-aggregation-and-shuffling"]], "4. Development workflow": [[11, "development-workflow"], [218, "development-workflow"], [223, null]], "4. DiffusionPolicy LightningModule": [[324, "diffusionpolicy-lightningmodule"], [327, null]], "4. Diving deeper into Ray Tune concepts": [[14, "diving-deeper-into-ray-tune-concepts"]], "4. Examples Outlook: Deploying to Your Infrastructure": [[24, "examples-outlook-deploying-to-your-infrastructure"], [27, "examples-outlook-deploying-to-your-infrastructure"]], "4. Hardware and Cost Considerations": [[118, "hardware-and-cost-considerations"], [124, "hardware-and-cost-considerations"]], "4. Install the Anyscale Operator": [[43, "install-the-anyscale-operator"], [48, null]], "4. Local File Store": [[86, "local-file-store"], [87, "local-file-store"]], "4. Migrating the model and dataset to Ray Train": [[13, "migrating-the-model-and-dataset-to-ray-train"]], "4. Putting It All Together": [[2, "putting-it-all-together"], [175, "putting-it-all-together"], [180, null]], "4. Quick visual sanity-check": [[343, "quick-visual-sanity-check"], [345, "quick-visual-sanity-check"]], "4. Ray Serve in Production": [[16, "ray-serve-in-production"]], "4. Ray Train in Production": [[5, "ray-train-in-production"], [6, "ray-train-in-production"], [259, "ray-train-in-production"], [264, null]], "4. Ray Tune in Production": [[7, "ray-tune-in-production"], [253, "ray-tune-in-production"], [258, null]], "4. Register Anyscale Cloud to Your Cloud Provider": [[17, "register-anyscale-cloud-to-your-cloud-provider"], [23, null]], "4. Register the Anyscale Cloud": [[53, "register-the-anyscale-cloud"], [59, null]], "4. Scale with More Replicas": [[110, "scale-with-more-replicas"], [116, "scale-with-more-replicas"]], "4. Task Runtime Environments": [[3, "task-runtime-environments"], [181, "task-runtime-environments"], [186, null]], "4. Test": [[35, "test"], [41, null]], "4. Training function per worker": [[293, "training-function-per-worker"], [298, null]], "4. Transforming data": [[10, "transforming-data"], [206, "transforming-data"], [212, null]], "4. Visual sanity check": [[317, "visual-sanity-check"], [319, "visual-sanity-check"], [350, "visual-sanity-check"], [352, "visual-sanity-check"]], "4. Visualize dataset: ratings, users, and items": [[330, "visualize-dataset-ratings-users-and-items"], [332, "visualize-dataset-ratings-users-and-items"]], "4. Write train / validation Parquet files": [[337, "write-train-validation-parquet-files"], [339, "write-train-validation-parquet-files"]], "4. Writing Data": [[9, "writing-data"], [198, "writing-data"], [203, null]], "4. kubectl Configuration": [[66, "kubectl-configuration"], [72, null]], "4.1 On resource specification": [[10, "on-resource-specification"], [206, "on-resource-specification"], [212, "on-resource-specification"]], "4.1. Note about Ray ID Specification": [[2, "note-about-ray-id-specification"], [175, "note-about-ray-id-specification"], [180, "note-about-ray-id-specification"]], "4.1. Note about pip dependencies": [[3, "note-about-pip-dependencies"], [181, "note-about-pip-dependencies"], [186, "note-about-pip-dependencies"]], "4.2 On concurrency limiting": [[10, "on-concurrency-limiting"], [206, "on-concurrency-limiting"], [212, "on-concurrency-limiting"]], "4.2. Anti-pattern: Calling ray.get in a loop harms parallelism": [[2, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"], [175, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"], [180, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"]], "5. Cleanup": [[35, "cleanup"], [42, null]], "5. Conclusion": [[28, "conclusion"], [34, null]], "5. Create Ray Dataset from Parquet and encode IDs": [[330, "create-ray-dataset-from-parquet-and-encode-ids"], [332, "create-ray-dataset-from-parquet-and-encode-ids"]], "5. Data Operations: Shuffling, Grouping and Aggregation": [[9, "data-operations-shuffling-grouping-and-aggregation"], [198, "data-operations-shuffling-grouping-and-aggregation"], [204, null]], "5. Distributed Train loop with checkpointing": [[324, "distributed-train-loop-with-checkpointing"], [328, null]], "5. Hyperparameter tuning the PyTorch model using Ray Tune": [[14, "hyperparameter-tuning-the-pytorch-model-using-ray-tune"]], "5. Install NGINX Ingress Controller": [[66, "install-nginx-ingress-controller"], [73, null]], "5. Install the Anyscale Operator": [[53, "install-the-anyscale-operator"], [60, null]], "5. Load the train and validation splits as Ray Datasets": [[337, "load-the-train-and-validation-splits-as-ray-datasets"], [339, "load-the-train-and-validation-splits-as-ray-datasets"]], "5. Main Training Function": [[293, "main-training-function"], [299, null]], "5. Persist to Parquet": [[317, "persist-to-parquet"], [319, "persist-to-parquet"], [350, "persist-to-parquet"], [352, "persist-to-parquet"]], "5. Persisting Data": [[15, "persisting-data"]], "5. Reporting checkpoints and metrics": [[13, "reporting-checkpoints-and-metrics"]], "5. Resource allocation and management": [[3, "resource-allocation-and-management"], [181, "resource-allocation-and-management"], [187, null]], "5. Sliding-window dataset to Parquet": [[343, "sliding-window-dataset-to-parquet"], [345, "sliding-window-dataset-to-parquet"]], "5. Stateful transformations with Ray Actors": [[10, "stateful-transformations-with-ray-actors"], [206, "stateful-transformations-with-ray-actors"], [213, null]], "5. Upgrade Hardware": [[110, "upgrade-hardware"], [116, "upgrade-hardware"]], "5. Verify the Installation": [[43, "verify-the-installation"], [49, null]], "5.1 Resource specification for stateful transformations": [[10, "resource-specification-for-stateful-transformations"], [206, "resource-specification-for-stateful-transformations"], [213, "resource-specification-for-stateful-transformations"]], "5.1. Note on resources requests, available resources, configuring large clusters": [[3, "note-on-resources-requests-available-resources-configuring-large-clusters"], [181, "note-on-resources-requests-available-resources-configuring-large-clusters"], [187, "note-on-resources-requests-available-resources-configuring-large-clusters"]], "5.2 Note on autoscaling for stateful transformations": [[10, "note-on-autoscaling-for-stateful-transformations"], [206, "note-on-autoscaling-for-stateful-transformations"], [213, "note-on-autoscaling-for-stateful-transformations"]], "5.2. Fractional resources": [[3, "fractional-resources"], [181, "fractional-resources"], [187, "fractional-resources"]], "5.3. IO bound tasks and fractional resources": [[3, "io-bound-tasks-and-fractional-resources"], [181, "io-bound-tasks-and-fractional-resources"], [187, "io-bound-tasks-and-fractional-resources"]], "6. (Optional) Upgrade Anyscale Dependencies": [[66, "optional-upgrade-anyscale-dependencies"], [74, null]], "6. Custom Food101Dataset for Parquet": [[350, "custom-food101dataset-for-parquet"], [353, null]], "6. Inspect dataset sizes (optional)": [[337, "inspect-dataset-sizes-optional"], [339, "inspect-dataset-sizes-optional"]], "6. Launch Ray TorchTrainer": [[324, "launch-ray-torchtrainer"], [328, "launch-ray-torchtrainer"]], "6. Launching the distributed training job": [[13, "launching-the-distributed-training-job"]], "6. Load and decode with Ray Data": [[317, "load-and-decode-with-ray-data"], [319, "load-and-decode-with-ray-data"]], "6. Materializing data": [[10, "materializing-data"], [206, "materializing-data"], [214, null]], "6. Nested Tasks": [[3, "nested-tasks"], [181, "nested-tasks"], [188, null]], "6. PyTorch Dataset over Parquet": [[343, "pytorch-dataset-over-parquet"], [345, "pytorch-dataset-over-parquet"]], "6. Start Training": [[293, "start-training"], [300, null]], "6. Test": [[43, "test"], [50, null]], "6. Train/validation split using Ray Data": [[330, "train-validation-split-using-ray-data"], [332, "train-validation-split-using-ray-data"]], "6. Verify the Installation": [[53, "verify-the-installation"], [61, null]], "6. When to use Ray Data": [[9, "when-to-use-ray-data"], [198, "when-to-use-ray-data"], [205, null]], "7. Accessing the training results": [[13, "accessing-the-training-results"]], "7. Clean up": [[43, "clean-up"], [51, null]], "7. Data Operations: grouping, aggregation, and shuffling": [[10, "data-operations-grouping-aggregation-and-shuffling"], [206, "data-operations-grouping-aggregation-and-shuffling"], [215, null]], "7. Define matrix factorization model": [[330, "define-matrix-factorization-model"], [333, null]], "7. Image transform": [[350, "image-transform"], [353, "image-transform"]], "7. Inspect a mini-batch": [[337, "inspect-a-mini-batch"], [339, "inspect-a-mini-batch"]], "7. Inspect one random batch": [[343, "inspect-one-random-batch"], [345, "inspect-one-random-batch"]], "7. Pattern: Pipeline data processing and waiting for results": [[3, "pattern-pipeline-data-processing-and-waiting-for-results"], [181, "pattern-pipeline-data-processing-and-waiting-for-results"], [189, null]], "7. Plot train / val loss": [[324, "plot-train-val-loss"], [328, "plot-train-val-loss"]], "7. Ray Data in Production": [[9, "ray-data-in-production"], [198, "ray-data-in-production"], [205, "ray-data-in-production"]], "7. Register the Anyscale Cloud": [[66, "register-the-anyscale-cloud"], [75, null]], "7. Shuffle and Train/Val split": [[317, "shuffle-and-train-val-split"], [319, "shuffle-and-train-val-split"]], "7. Shutdown Ray Cluster": [[293, "shutdown-ray-cluster"], [300, "shutdown-ray-cluster"]], "7. Test": [[53, "test"], [62, null]], "7.1 Batch Processing Pattern": [[3, "batch-processing-pattern"], [181, "batch-processing-pattern"], [189, "batch-processing-pattern"]], "7.1. Custom batching using groupby.": [[10, "custom-batching-using-groupby"], [206, "custom-batching-using-groupby"], [215, "custom-batching-using-groupby"]], "7.2 Note on fetching too many objects at once with ray.get causes failure": [[3, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"], [181, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"], [189, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"]], "7.2. Aggregations": [[10, "aggregations"], [206, "aggregations"], [215, "aggregations"]], "7.3. Shuffling data": [[10, "shuffling-data"], [206, "shuffling-data"], [215, "shuffling-data"]], "7.3.1. File based shuffle on read": [[10, "file-based-shuffle-on-read"], [206, "file-based-shuffle-on-read"], [215, "file-based-shuffle-on-read"]], "7.3.2. Shuffling block order": [[10, "shuffling-block-order"], [206, "shuffling-block-order"], [215, "shuffling-block-order"]], "7.3.3. Shuffle all rows globally": [[10, "shuffle-all-rows-globally"], [206, "shuffle-all-rows-globally"], [215, "shuffle-all-rows-globally"]], "8. Conclusion": [[43, "conclusion"], [52, null]], "8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)": [[330, "define-ray-train-loop-with-validation-checkpointing-and-ray-managed-metrics"], [334, null]], "8. Define the Ray Train worker loop (Arrow-based, memory-efficient)": [[337, "define-the-ray-train-worker-loop-arrow-based-memory-efficient"], [340, null]], "8. Install the Anyscale Operator": [[66, "install-the-anyscale-operator"], [76, null]], "8. Persisting data": [[10, "persisting-data"], [206, "persisting-data"], [216, null]], "8. Pixel diffusion LightningModule": [[317, "pixel-diffusion-lightningmodule"], [320, null]], "8. Ray Actors": [[3, "ray-actors"], [181, "ray-actors"], [190, null]], "8. Ray Train in Production": [[13, "ray-train-in-production"]], "8. Ray-prepared DataLoader": [[343, "ray-prepared-dataloader"], [345, "ray-prepared-dataloader"]], "8. Reverse diffusion helper": [[324, "reverse-diffusion-helper"], [329, null]], "8. Summary": [[293, "summary"], [300, "summary"]], "8. Test": [[66, "test"], [77, null]], "8. Train/validation split": [[350, "train-validation-split"], [353, "train-validation-split"]], "8. Troubleshooting": [[53, "troubleshooting"], [63, null]], "8. Upcoming Features in Ray Data": [[9, "upcoming-features-in-ray-data"], [198, "upcoming-features-in-ray-data"], [205, "upcoming-features-in-ray-data"]], "9. Clean up": [[53, "clean-up"], [64, null]], "9. Cleanup": [[66, "cleanup"], [78, null]], "9. Configure XGBoost and build the Trainer": [[337, "configure-xgboost-and-build-the-trainer"], [340, "configure-xgboost-and-build-the-trainer"]], "9. Inspect a DataLoader batch": [[350, "inspect-a-dataloader-batch"], [353, "inspect-a-dataloader-batch"]], "9. Launch distributed training with Ray Train": [[330, "launch-distributed-training-with-ray-train"], [334, "launch-distributed-training-with-ray-train"]], "9. PositionalEncoding and Transformer model": [[343, "positionalencoding-and-transformer-model"], [346, null]], "9. Ray Data in production": [[10, "ray-data-in-production"], [206, "ray-data-in-production"], [217, null]], "9. Ray Train train_loop (Lightning + Ray integration)": [[317, "ray-train-train-loop-lightning-ray-integration"], [321, null]], "9. Sample an action from the trained policy": [[324, "sample-an-action-from-the-trained-policy"], [329, "sample-an-action-from-the-trained-policy"]], "API Endpoints": [[168, "api-endpoints"]], "Activity: Update the training loop to compute the area under the curve of ROC (AUROC)": [[13, "activity-update-the-training-loop-to-compute-the-area-under-the-curve-of-roc-auroc"]], "Adding New Notebooks or Courses": [[0, "adding-new-notebooks-or-courses"]], "Additional Setup (Optional)": [[155, "additional-setup-optional"], [158, "additional-setup-optional"]], "Additional dependencies": [[126, "additional-dependencies"], [127, "additional-dependencies"]], "Advanced LLM Features with Ray Serve LLM": [[118, null], [119, null]], "Advanced Topics: Monitoring & Optimization": [[110, "advanced-topics-monitoring-optimization"], [116, null]], "Aggregations": [[15, "aggregations"]], "Annotated experiment table": [[14, "annotated-experiment-table"]], "Anyscale 101 Learning Path": [[100, "anyscale-101-learning-path"]], "Anyscale Administrator Overview": [[17, null], [18, null]], "Anyscale For Admins": [[360, "anyscale-for-admins"]], "Anyscale Getting Started": [[360, "anyscale-getting-started"]], "Anyscale Observability": [[159, "anyscale-observability"], [162, null]], "Anyscale Projects": [[94, "anyscale-projects"], [95, "anyscale-projects"]], "Anyscale Ray Serve Observability": [[164, "anyscale-ray-serve-observability"], [167, "anyscale-ray-serve-observability"]], "Apache Arrow": [[8, "apache-arrow"], [191, "apache-arrow"], [192, "apache-arrow"]], "Application": [[147, "application"], [150, null]], "Applications": [[11, "applications"], [218, "applications"], [221, "applications"]], "Architecture": [[168, "architecture"], [267, "architecture"], [269, "architecture"], [280, null], [304, "architecture"], [307, "architecture"], [314, null]], "Architecture Overview": [[269, null], [304, null]], "Architecture### Import librariesIn addition to ray and serve, we also import FastAPI to create webservice and Hugging Face transformers to download ML models.# Import ray serve and FastAPI librariesimport rayfrom ray import servefrom fastapi import FastAPI# library for pre-trained modelsfrom transformers import pipeline": [[303, null]], "ArchitectureArchitecture Diagram": [[268, null]], "Available Endpoints": [[168, "available-endpoints"]], "Batch Inference Class": [[267, "batch-inference-class"], [273, "batch-inference-class"], [282, null]], "Batch Inference ClassMany machine learning models are optimized for processing a batch of inputs at once. When working with a large dataset, there could be many batches of data. Instead of loading machine learning models repeatedly to run each batch of data, you want to spin up a number of actor processes that are initialized once with your model and reused to process multiple batches. To implement this, you can use the map_batches API with a \u201cCallable\u201d class method that implements:- __init__: Initialize any expensive state.- __call__: Perform the stateful transformation.In this example, a lightweight sentence transformer model, all-MiniLM-L6-v2 is used to generate embeddings of text data.": [[272, null]], "Batch Inference with Ray Data": [[266, null], [266, "id1"], [267, null], [279, null]], "Batch Inference with Ray Data\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb Launch Locally: You can run this notebook locally.\ud83d\ude80 Launch on Cloud: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)This example shows how to do batch inference with Ray Data.Batch inference with Ray Data enables you to efficiently generate predictions from machine learning models on large datasets by processing multiple data points at once. Instead of running inference on one row at a time, which can be slow and resource-inefficient, batch inference leverages vectorized computation and parallelism to maximize throughput. This is especially useful when working with modern deep learning models, which are optimized for batch processing on CPUs, GPUs, or Apple Silicon devices.The typical workflow begins by loading your dataset\u2014such as a public dataset from Hugging Face\u2014into a Ray Dataset. Ray Data can automatically partition the data for parallel processing, or you can repartition it explicitly to control the number of data blocks. Once the data is loaded, you define a callable class (such as a text embedding model) that loads the machine learning model in its constructor and implements a __call__ method to process each batch. Ray Data\u2019s map_batches API is then used to apply this callable to each batch of data, with options to control concurrency and resource allocation (e.g., number of GPUs).This approach allows you to spin up multiple concurrent model instances, each processing different batches of data in parallel. The result is a significant speedup in inference time, especially for large datasets. After inference, you can materialize the results, inspect the output, and shut down the Ray cluster to free up resources. Batch inference with Ray Data is scalable, flexible, and integrates seamlessly with modern ML workflows, making it a powerful tool for production and research environments alike.": [[265, null]], "Batch Processing": [[8, "batch-processing"], [191, "batch-processing"], [195, "batch-processing"]], "Batch embeddings": [[128, "batch-embeddings"], [131, null]], "Batch inference": [[128, null], [129, null]], "Batching": [[137, "batching"], [141, null]], "Build and load our model on a single GPU": [[13, "build-and-load-our-model-on-a-single-gpu"]], "Building a FastAPI Web Service and Deploying a Model": [[306, null]], "CI/CD": [[147, "ci-cd"], [154, null]], "Challenges in LLM Serving": [[101, "challenges-in-llm-serving"], [102, "challenges-in-llm-serving"], [106, null]], "Challenges with JVM": [[8, "challenges-with-jvm"], [191, "challenges-with-jvm"], [195, "challenges-with-jvm"]], "Clean Up": [[88, "clean-up"], [89, "clean-up"], [90, "clean-up"], [91, "clean-up"], [92, "clean-up"], [93, "clean-up"]], "Clean up": [[16, "clean-up"]], "Cloning/Duplicating Resources": [[94, "cloning-duplicating-resources"], [95, "cloning-duplicating-resources"]], "Cloud": [[96, "cloud"], [98, "cloud"]], "Collaborating with Your Team": [[100, "collaborating-with-your-team"]], "Composing Deployments": [[16, "composing-deployments"]], "Compute by Function": [[8, "compute-by-function"], [191, "compute-by-function"], [193, "compute-by-function"]], "Conclusion: Next Steps": [[118, "conclusion-next-steps"], [125, null]], "Concurrency Optimization Strategies": [[110, "concurrency-optimization-strategies"], [116, "concurrency-optimization-strategies"]], "Configuration Breakdown": [[110, "configuration-breakdown"], [113, "configuration-breakdown"]], "Configuration for Medium-Sized Models": [[110, "configuration-for-medium-sized-models"], [113, "configuration-for-medium-sized-models"]], "Configure Ray Serve LLM with LoRA": [[118, "configure-ray-serve-llm-with-lora"], [121, "configure-ray-serve-llm-with-lora"]], "Configure persistent storage": [[13, "configure-persistent-storage"]], "Configure scale and GPUs": [[13, "configure-scale-and-gpus"]], "Configure the Worker Node(s)": [[84, "configure-the-worker-node-s"], [85, "configure-the-worker-node-s"]], "Convert the Hugging Face dataset to a Ray Dataset": [[271, "convert-the-hugging-face-dataset-to-a-ray-dataset"]], "Convert to Pandas DataFrame": [[285, "convert-to-pandas-dataframe"], [292, "convert-to-pandas-dataframe"]], "Convert to Ray Dataset": [[285, "convert-to-ray-dataset"], [289, null]], "Course Welcome and Overview": [[1, "course-welcome-and-overview"], [169, "course-welcome-and-overview"], [170, "course-welcome-and-overview"]], "Create a Second Dataset": [[285, "create-a-second-dataset"], [289, "create-a-second-dataset"]], "Create a batch data and call the model": [[267, "create-a-batch-data-and-call-the-model"], [275, "create-a-batch-data-and-call-the-model"], [283, null]], "Create a batch data and call the modelDefine a Ray Data map_batches function to embed text using the SentenceTransformer model. This function will be applied to each batch of data in the Ray Data dataset. It will take a batch of sentences, encode them into embeddings, and return the batch with the embeddings added.Showcasing two options of to do batch inference based on if the ray cluster has have GPU nodes or if it has just CPU nodes. The second option also works on a local ray cluster on an Apple Silicon Mac with MPS.# setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPSworker_device = \u201ccpu\u201d # or \u201ccuda\u201d if you have a nvidia gpu on worker nodes# batch_size should be set based on VRAM if worker_device == \u201ccuda\u201d: # if you have a nvidia gpu on worker nodes    # adjust batch_size based on the VRAM available on the GPU    ds = ds.map_batches(TextEmbedder, num_gpus=1, concurrency=2, batch_size=64) # 2 nodes with 1 GPU eachelse:    ds = ds.map_batches(TextEmbedder, concurrency=2, batch_size=64) # either cpu or mps (on a mac)": [[274, null]], "Create custom security group": [[17, "create-custom-security-group"], [22, "create-custom-security-group"]], "Creating Anyscale Resources": [[28, "creating-anyscale-resources"], [30, "creating-anyscale-resources"], [35, "creating-anyscale-resources"], [39, "creating-anyscale-resources"], [43, "creating-anyscale-resources"], [45, "creating-anyscale-resources"]], "Creating a Compute Config": [[84, "creating-a-compute-config"], [85, "creating-a-compute-config"]], "Creating a Container Image": [[84, "creating-a-container-image"], [85, "creating-a-container-image"]], "Creating a Data Batch and Calling the Model": [[275, null]], "Custom batching using groupby": [[15, "custom-batching-using-groupby"]], "Custom batching using groupby and aggregations": [[9, "custom-batching-using-groupby-and-aggregations"], [198, "custom-batching-using-groupby-and-aggregations"], [204, "custom-batching-using-groupby-and-aggregations"]], "Customization": [[0, "customization"]], "Customizing autoscaling": [[16, "customizing-autoscaling"]], "Data Engineering Compute": [[8, "data-engineering-compute"], [191, "data-engineering-compute"], [193, "data-engineering-compute"]], "Data Pipeline Observability (Ray Data)": [[164, "data-pipeline-observability-ray-data"], [166, null]], "Data Processing and ML examples with Ray": [[278, null], [286, null], [294, null], [312, null]], "Data Processing with Ray Data": [[8, "data-processing-with-ray-data"], [191, "data-processing-with-ray-data"], [196, null], [285, null], [287, null]], "Data flow": [[8, "data-flow"], [191, "data-flow"], [197, "data-flow"]], "Data ingestion": [[128, "data-ingestion"], [130, null]], "Data lakes": [[8, "data-lakes"], [191, "data-lakes"], [192, "data-lakes"]], "Data storage": [[128, "data-storage"], [133, null]], "Data warehouses": [[8, "data-warehouses"], [191, "data-warehouses"], [192, "data-warehouses"]], "Databases": [[8, "databases"], [191, "databases"], [192, "databases"]], "Dataloaders": [[293, "dataloaders"], [298, "dataloaders"]], "Dataset": [[9, "dataset"], [198, "dataset"], [201, "dataset"]], "Decode Phase": [[101, "decode-phase"], [102, "decode-phase"], [104, "decode-phase"]], "Default settings for Ray Tune": [[14, "default-settings-for-ray-tune"]], "Defining a data loader": [[13, "defining-a-data-loader"]], "Defining the Batch Inference Class": [[273, null]], "Deploy a Medium-Sized LLM with Ray Serve LLM": [[110, null], [111, null]], "Deploy the model": [[307, "deploy-the-model"], [309, "deploy-the-model"], [315, "deploy-the-model"]], "Deploy the modelserve.run(MySentimentModel.bind()) # Bind the deployment to the Ray Serve runtimeDeploymentHandle(deployment=\u2019MySentimentModel\u2019)": [[308, null]], "Deploying Applications with Services": [[100, "deploying-applications-with-services"]], "Deploying Our Model and Testing it": [[309, null]], "Deploying Pipelines with Jobs": [[100, "deploying-pipelines-with-jobs"]], "Deploying at scale": [[267, "deploying-at-scale"], [275, "deploying-at-scale"], [283, "deploying-at-scale"]], "Deploying at scale- The batch size for encoding can be adjusted based on the available memory and performance requirements.- The device parameter ensures that the model runs on the correct device (CPU, GPU, or MPS).- The concurrency parameter controls how many batches are processed in parallel. If there are 2 nodes with 1 GPU each or 1 node with 2 GPUs, then set concurrency = 2 and num_gpus=1.- map_batches() is a lazy function and not executed until needed (example, using take or show).Run inference on a batch of 128 rows. This will return a batch of 128 rows with the embeddings added to the caller\u2019s machine.# Run inference on a batch of 128 rows for testing.ds.take_batch(128)": [[274, "deploying-at-scale-the-batch-size-for-encoding-can-be-adjusted-based-on-the-available-memory-and-performance-requirements-the-device-parameter-ensures-that-the-model-runs-on-the-correct-device-cpu-gpu-or-mps-the-concurrency-parameter-controls-how-many-batches-are-processed-in-parallel-if-there-are-2-nodes-with-1-gpu-each-or-1-node-with-2-gpus-then-set-concurrency-2-and-num-gpus-1-map-batches-is-a-lazy-function-and-not-executed-until-needed-example-using-take-or-show-run-inference-on-a-batch-of-128-rows-this-will-return-a-batch-of-128-rows-with-the-embeddings-added-to-the-caller-s-machine-run-inference-on-a-batch-of-128-rows-for-testing-ds-take-batch-128"]], "Deploying to Anyscale Services": [[110, "deploying-to-anyscale-services"], [115, null]], "Deployment": [[168, "deployment"]], "Deployment Example Structure": [[79, "deployment-example-structure"]], "Deployment Options: Virtual Machines vs. Kubernetes": [[24, null], [25, null]], "Deployments": [[11, "deployments"], [16, "deployments"], [147, "deployments"], [149, null], [218, "deployments"], [221, "deployments"]], "Developing in Anyscale Workspaces": [[100, "developing-in-anyscale-workspaces"]], "Development": [[126, "development"], [127, "development"]], "Disabling Notebook Execution and Outputs": [[0, "disabling-notebook-execution-and-outputs"]], "Distributed Computing Frameworks": [[8, "distributed-computing-frameworks"], [191, "distributed-computing-frameworks"], [195, null]], "Distributed Data-Parallel Training with Ray Train": [[224, "distributed-data-parallel-training-with-ray-train"], [234, "distributed-data-parallel-training-with-ray-train"]], "Distributed training": [[137, null], [138, null]], "Distributed training with Ray Train, PyTorch and Hugging Face": [[293, null], [295, null]], "Distributed training with Ray Train, PyTorch and HuggingFace": [[278, "distributed-training-with-ray-train-pytorch-and-huggingface"], [286, "distributed-training-with-ray-train-pytorch-and-huggingface"], [294, "distributed-training-with-ray-train-pytorch-and-huggingface"], [312, "distributed-training-with-ray-train-pytorch-and-huggingface"]], "Diving deeper into Ray Tune concepts": [[7, "diving-deeper-into-ray-tune-concepts"], [253, "diving-deeper-into-ray-tune-concepts"], [257, "diving-deeper-into-ray-tune-concepts"]], "Dual-Subnet Architecture": [[17, "dual-subnet-architecture"], [22, "dual-subnet-architecture"]], "Enabling LLM Monitoring": [[110, "enabling-llm-monitoring"], [116, "enabling-llm-monitoring"]], "Environment state and action": [[324, "environment-state-and-action"], [325, "environment-state-and-action"]], "Evaluation": [[137, "evaluation"], [146, null]], "Example": [[159, "example"], [163, null]], "Example Workflow": [[0, "example-workflow"]], "Example: Car type description": [[118, "example-car-type-description"], [122, "example-car-type-description"]], "Example: Code Assistant LoRA": [[118, "example-code-assistant-lora"], [121, "example-code-assistant-lora"]], "Example: Deploying LoRA Adapters": [[118, "example-deploying-lora-adapters"], [121, null]], "Example: Getting Structured JSON Output": [[118, "example-getting-structured-json-output"], [122, null]], "Example: Setting up Tool Calling": [[118, "example-setting-up-tool-calling"], [123, null]], "Example: Weather Assistant with Tool Calling": [[118, "example-weather-assistant-with-tool-calling"], [123, "example-weather-assistant-with-tool-calling"]], "Execution mode": [[9, "execution-mode"], [15, "execution-mode"], [198, "execution-mode"], [202, "execution-mode"]], "Exercise": [[7, "exercise"], [14, "exercise"], [253, "exercise"], [257, "exercise"]], "Expected Output": [[118, "expected-output"], [122, "expected-output"]], "Exploring the Anyscale Log Viewer": [[88, "exploring-the-anyscale-log-viewer"], [89, "exploring-the-anyscale-log-viewer"]], "Exploring the Anyscale Metrics Tab": [[88, "exploring-the-anyscale-metrics-tab"], [89, "exploring-the-anyscale-metrics-tab"]], "Exploring the Ray Dashboard": [[88, "exploring-the-ray-dashboard"], [89, "exploring-the-ray-dashboard"]], "FastAPI webservice and deploy a model": [[306, "fastapi-webservice-and-deploy-a-model"], [307, "fastapi-webservice-and-deploy-a-model"], [315, null]], "FastAPI webservice and deploy a modelFastAPI is used to create a webservice \u2018app\u2019 to accept HTTP requests.MySentimentModel class loads the ML model and defines predict function for online inference. @serve.deployment decorator defines the Ray Serve deployment.@app.get() is used to create a GET \u2018/predict\u2019 route. Similarly, @app.post() can be used POST requests. See https://docs.ray.io/en/latest/serve/http-guide.html for more details.In this example, application_logic() function is used to define a sample transformation or business logic that can be applied before sending the input to the ML model for inference. See inline comments for further explanation.### Scaling deploymentnum_replicas parameter sets the number of instances of the deployment. FastAPI and RayServe automatically load balances to send requests to each instance. There are more options to set the accelerator_type to GPU and even use fractional GPUs. See configuration options here: https://docs.ray.io/en/latest/serve/configure-serve-deployment.html .": [[305, null]], "Features": [[0, "features"]], "File based shuffle on read": [[9, "file-based-shuffle-on-read"], [15, "file-based-shuffle-on-read"], [198, "file-based-shuffle-on-read"], [204, "file-based-shuffle-on-read"]], "Filter Ray Dataset": [[285, "filter-ray-dataset"], [290, null]], "Forward process: adding noise": [[317, "forward-process-adding-noise"], [318, "forward-process-adding-noise"]], "Foundations": [[360, "foundations"]], "General-Purpose Distributed Computing": [[8, "general-purpose-distributed-computing"], [191, "general-purpose-distributed-computing"], [195, "general-purpose-distributed-computing"]], "Get User Profile": [[168, "get-user-profile"]], "Getting Started": [[100, "getting-started"]], "Getting Started with Ray Serve LLM": [[101, "getting-started-with-ray-serve-llm"], [102, "getting-started-with-ray-serve-llm"], [108, null]], "Getting started": [[7, "getting-started"], [14, "getting-started"], [253, "getting-started"], [257, "getting-started"]], "How Navigation Works": [[0, "how-navigation-works"]], "How Other Sizes Differ": [[110, "how-other-sizes-differ"], [117, "how-other-sizes-differ"]], "How Resources are defined": [[17, "how-resources-are-defined"], [20, "how-resources-are-defined"]], "How to Choose an LLM?": [[118, "how-to-choose-an-llm"], [124, null]], "How to migrate this computer vision workload to a distributed setup using Ray on Anyscale": [[350, "how-to-migrate-this-computer-vision-workload-to-a-distributed-setup-using-ray-on-anyscale"], [351, "how-to-migrate-this-computer-vision-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this diffusion-policy workload to a distributed setup using Ray on Anyscale": [[317, "how-to-migrate-this-diffusion-policy-workload-to-a-distributed-setup-using-ray-on-anyscale"], [318, "how-to-migrate-this-diffusion-policy-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this recommendation system workload to a distributed setup using Ray on Anyscale": [[330, "how-to-migrate-this-recommendation-system-workload-to-a-distributed-setup-using-ray-on-anyscale"], [331, "how-to-migrate-this-recommendation-system-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this tabular workload to a distributed setup using Ray on Anyscale": [[337, "how-to-migrate-this-tabular-workload-to-a-distributed-setup-using-ray-on-anyscale"], [338, "how-to-migrate-this-tabular-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this time-series workload to a distributed multi-node setup using Ray on Anyscale": [[343, "how-to-migrate-this-time-series-workload-to-a-distributed-multi-node-setup-using-ray-on-anyscale"], [344, "how-to-migrate-this-time-series-workload-to-a-distributed-multi-node-setup-using-ray-on-anyscale"]], "How to scale this policy learning workload using Ray on Anyscale": [[324, "how-to-scale-this-policy-learning-workload-using-ray-on-anyscale"], [325, "how-to-scale-this-policy-learning-workload-using-ray-on-anyscale"]], "How we use Ray AI Libraries for this task": [[12, "how-we-use-ray-ai-libraries-for-this-task"]], "Hyperparameter tune the PyTorch model using Ray Tune": [[7, "hyperparameter-tune-the-pytorch-model-using-ray-tune"], [253, "hyperparameter-tune-the-pytorch-model-using-ray-tune"], [257, "hyperparameter-tune-the-pytorch-model-using-ray-tune"]], "Import Libraries": [[267, "import-libraries"], [269, "import-libraries"], [280, "import-libraries"]], "Import Librariesimport rayimport torchfrom typing import Dictimport numpy as npfrom sentence_transformers import SentenceTransformer # huggingface sentence transformersfrom datasets import load_dataset # huggingface datasets": [[268, "import-librariesimport-rayimport-torchfrom-typing-import-dictimport-numpy-as-npfrom-sentence-transformers-import-sentencetransformer-huggingface-sentence-transformersfrom-datasets-import-load-dataset-huggingface-datasets"]], "Import libraries": [[304, "import-libraries"], [307, "import-libraries"], [314, "import-libraries"]], "Import ray serve and FastAPI libraries": [[304, "import-ray-serve-and-fastapi-libraries"]], "Improving Concurrency": [[110, "improving-concurrency"], [116, "improving-concurrency"]], "In-memory data formats": [[8, "in-memory-data-formats"], [191, "in-memory-data-formats"], [192, "in-memory-data-formats"]], "Inference: ranking items per user": [[330, "inference-ranking-items-per-user"], [331, "inference-ranking-items-per-user"]], "Initialize Ray and Load a Dataset": [[285, "initialize-ray-and-load-a-dataset"], [288, "initialize-ray-and-load-a-dataset"]], "Input: Images as tensors": [[317, "input-images-as-tensors"], [318, "input-images-as-tensors"]], "Input: user\u2013item\u2013rating triples": [[330, "input-useritemrating-triples"], [331, "input-useritemrating-triples"]], "Inputs": [[350, "inputs"], [351, "inputs"]], "Inspecting the features of the NYC taxi dataset": [[12, "inspecting-the-features-of-the-nyc-taxi-dataset"]], "Installation": [[0, "installation"], [278, "installation"], [286, "installation"], [294, "installation"], [312, "installation"]], "Integrating with FastAPI": [[16, "integrating-with-fastapi"]], "Intro to Ray Data": [[15, null]], "Intro to Ray Data:  Ray Data + Unstructured Data": [[10, null], [206, null], [207, null]], "Intro to Ray Serve": [[16, null]], "Intro to Ray Tune": [[7, "intro-to-ray-tune"], [14, null], [253, "intro-to-ray-tune"], [257, "intro-to-ray-tune"]], "Introduction": [[82, "introduction"], [83, "introduction"], [84, "introduction"], [85, "introduction"], [86, "introduction"], [87, "introduction"], [88, "introduction"], [89, "introduction"]], "Introduction to Ray Core (Advancement): Object store, Tasks, Actors": [[3, null], [181, null], [182, null]], "Introduction to Ray Core: Getting Started": [[2, null], [175, null], [176, null]], "Introduction to Ray Data: Industry Landscape": [[8, null], [191, null], [192, null]], "Introduction to Ray Data: Ray Data + Structured Data": [[9, null], [198, null], [199, null]], "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving": [[101, null], [102, null], [103, null]], "Introduction to Ray Serve with PyTorch": [[11, null], [218, null], [219, null]], "Introduction to Ray Train": [[13, null]], "Introduction to Ray Train + PyTorch": [[5, null]], "Introduction to Ray Train: Ray Train + PyTorch Lightning": [[6, null], [259, null], [260, null]], "Introduction to Ray Tune": [[7, null], [253, null], [254, null]], "Introduction to Ray: Developer": [[1, null], [169, null], [170, null]], "Introduction to the Ray AI Libraries": [[12, null]], "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model": [[4, null], [171, null], [172, null]], "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster": [[43, null], [44, null]], "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster": [[66, null], [67, null]], "Introduction: Deploy Anyscale Ray on AWS EC2 Instances": [[28, null], [29, null]], "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster": [[53, null], [54, null]], "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)": [[35, null], [36, null]], "Introduction: Why Anyscale?": [[100, "introduction-why-anyscale"]], "Join Two Ray Datasets": [[285, "join-two-ray-datasets"], [291, null]], "Key Benefits": [[118, "key-benefits"], [118, "id1"], [118, "id3"], [121, "key-benefits"], [122, "key-benefits"], [123, "key-benefits"]], "Key Components": [[110, "key-components"], [113, "key-components"]], "Key Concepts and Optimizations": [[101, "key-concepts-and-optimizations"], [102, "key-concepts-and-optimizations"], [105, null]], "Key Functions": [[17, "key-functions"], [19, "key-functions"]], "Key Ray Serve Features": [[16, "key-ray-serve-features"]], "Key Takeaways": [[101, "key-takeaways"], [102, "key-takeaways"], [109, null], [110, "key-takeaways"], [117, "key-takeaways"], [118, "key-takeaways"], [125, "key-takeaways"]], "Key characteristics of Anyscale Services": [[92, "key-characteristics-of-anyscale-services"], [93, "key-characteristics-of-anyscale-services"]], "Key points": [[13, "key-points"]], "Labels": [[350, "labels"], [351, "labels"]], "Lakehouses": [[8, "lakehouses"], [191, "lakehouses"], [192, "lakehouses"]], "Last Updated 6/19": [[100, null]], "Launch Grafana": [[155, "launch-grafana"], [158, "launch-grafana"]], "Launching Ray Serve": [[110, "launching-ray-serve"], [114, "launching-ray-serve"]], "Launching a Anyscale Workspace": [[80, "launching-a-anyscale-workspace"], [81, "launching-a-anyscale-workspace"]], "Launching a Web Application using Ray Serve": [[164, "launching-a-web-application-using-ray-serve"], [167, "launching-a-web-application-using-ray-serve"]], "Launching a distributed training job with a TorchTrainer.": [[13, "launching-a-distributed-training-job-with-a-torchtrainer"]], "Launching the Service": [[110, "launching-the-service"], [115, "launching-the-service"]], "Learn More": [[118, "learn-more"], [118, "id2"], [118, "id4"], [121, "learn-more"], [122, "learn-more"], [123, "learn-more"]], "Learning Approach": [[118, "learning-approach"], [120, "learning-approach"]], "Learning Path Overview and Objectives": [[100, "learning-path-overview-and-objectives"]], "Library Imports": [[285, "library-imports"], [288, null]], "Llm Serving": [[360, "llm-serving"]], "Load a dataset": [[267, "load-a-dataset"], [271, "load-a-dataset"], [281, null]], "Load a datasetLoad a dataset from hugging face or local and convert into Ray Dataset. A Ray cluster automatically initialized on local or on Anyscale platform. You can also use ray.init() To explicitly create or connect to an existing Ray cluster.https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html#ray.init# load a Hugging Face datasethf_dataset = load_dataset(\u201ccardiffnlp/tweet_eval\u201d, \u201csentiment\u201d, split=\u201dtrain\u201d)# Convert the Hugging Face dataset to a Ray Datasetds = ray.data.from_huggingface(hf_dataset).repartition(2) # repartition to 2 blocks for parallel processing. Not necessary if already partitioned due to the size of the dataset.": [[270, null]], "Loading a Dataset": [[271, null]], "Loading and visualizing MNIST data": [[13, "loading-and-visualizing-mnist-data"]], "Local Deployment & Inference": [[110, "local-deployment-inference"], [114, null]], "Local Development": [[168, "local-development"]], "Local IDE (VSCode / Cursor)": [[82, "local-ide-vscode-cursor"], [83, "local-ide-vscode-cursor"]], "Logging Configuration": [[164, "logging-configuration"], [167, "logging-configuration"]], "Machine Learning and AI Compute": [[8, "machine-learning-and-ai-compute"], [191, "machine-learning-and-ai-compute"], [193, "machine-learning-and-ai-compute"]], "Managing Dependencies": [[1, "managing-dependencies"], [169, "managing-dependencies"], [170, "managing-dependencies"]], "Materializing Data": [[15, "materializing-data"]], "Model": [[137, "model"], [140, null]], "Model Recommendations by Use Case": [[118, "model-recommendations-by-use-case"], [124, "model-recommendations-by-use-case"]], "Model Selection Framework": [[118, "model-selection-framework"], [124, "model-selection-framework"]], "Model Size Comparison": [[110, "model-size-comparison"], [112, "model-size-comparison"]], "Model registry": [[137, "model-registry"], [142, null]], "Model: embedding-based matrix factorization": [[330, "model-embedding-based-matrix-factorization"], [331, "model-embedding-based-matrix-factorization"]], "Monitoring and Debugging": [[128, "monitoring-and-debugging"], [134, null]], "More Advanced Topics": [[118, "more-advanced-topics"], [125, "more-advanced-topics"]], "More about Datasets": [[15, "more-about-datasets"]], "Multi-Actor Ray Serve Tracing Example": [[168, null]], "Multi-modal AI pipeline": [[126, null], [127, null]], "Multimodal Ai Workloads": [[360, "multimodal-ai-workloads"]], "Next Steps": [[101, "next-steps"], [102, "next-steps"], [109, "next-steps"], [110, "next-steps"], [117, "next-steps"], [118, "next-steps"], [125, "next-steps"]], "No infrastructure headaches": [[126, "no-infrastructure-headaches"], [127, "no-infrastructure-headaches"]], "Note on the Checkpoint Lifecycle": [[224, "note-on-the-checkpoint-lifecycle"], [234, "note-on-the-checkpoint-lifecycle"]], "Note that this does not mutate the original Dataset.": [[277, "note-that-this-does-not-mutate-the-original-dataset"]], "Notebook": [[82, "notebook"], [83, "notebook"]], "Now launch a Ray worker node in the terminal:": [[155, "now-launch-a-ray-worker-node-in-the-terminal"], [158, "now-launch-a-ray-worker-node-in-the-terminal"]], "Observability": [[147, "observability"], [152, null], [360, "observability"]], "Observability Introduction": [[155, null], [156, null]], "Observability Overview": [[155, "observability-overview"], [157, null]], "On Ray Data vs Spark": [[8, "on-ray-data-vs-spark"], [191, "on-ray-data-vs-spark"], [196, "on-ray-data-vs-spark"]], "Online Model Serving with Ray Serve": [[302, null], [302, "id1"], [307, null], [313, null]], "Online Model Serving with Ray Serve\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb Launch Locally: You can run this notebook locally.\ud83d\ude80 Launch on Cloud: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)Model serving is the process of deploying machine learning models to production so that they can be accessed and used by applications or users. It involves creating an API or interface that allows users to send requests to the modeland receive predictions in response. There are several libraries and frameworks available for model serving, each with its own features and capabilities. In this notebook, we showcase Ray Serve and FastAPI to deploy a sentiment analysismachine learning (ML) model.": [[301, null]], "Online serving": [[147, null], [148, null]], "Option 1: Create a New VPC": [[17, "option-1-create-a-new-vpc"], [22, "option-1-create-a-new-vpc"]], "Option 2: Use Existing VPC": [[17, "option-2-use-existing-vpc"], [22, "option-2-use-existing-vpc"]], "Organization": [[96, "organization"], [98, "organization"]], "Our Example: Llama-3.1-70B": [[110, "our-example-llama-3-1-70b"], [112, "our-example-llama-3-1-70b"]], "Out of memory errors": [[267, "out-of-memory-errors"], [277, "out-of-memory-errors"], [284, "out-of-memory-errors"]], "Out of memory errorsGPU (or MPS or CPU) memory has to keep the machine learning model and the batch of data in memory during the inference. If the batch_size is too large, it can run out of memory and throw out of memory errors. In that case, reduce the batch_size.### Shutdown Ray cluster# avoids collisons with other notebooks running ray jobs on the same machineray.shutdown()": [[276, "out-of-memory-errorsgpu-or-mps-or-cpu-memory-has-to-keep-the-machine-learning-model-and-the-batch-of-data-in-memory-during-the-inference-if-the-batch-size-is-too-large-it-can-run-out-of-memory-and-throw-out-of-memory-errors-in-that-case-reduce-the-batch-size-shutdown-ray-cluster-avoids-collisons-with-other-notebooks-running-ray-jobs-on-the-same-machineray-shutdown"]], "Outline": [[266, "outline"], [267, "outline"], [279, "outline"], [293, "outline"], [295, "outline"], [302, "outline"], [307, "outline"], [313, "outline"]], "Outline    Architecture    Import Libraries    FastAPI service to accept HTTP requests and scaling with Ray Serve    Simulate Client: Send test requests    Shutdown the Serve app and the ray cluster</ul></div>": [[301, "outline-architecture-import-libraries-fastapi-service-to-accept-http-requests-and-scaling-with-ray-serve-simulate-client-send-test-requests-shutdown-the-serve-app-and-the-ray-cluster"]], "Outline of the notebook": [[285, "outline-of-the-notebook"], [287, "outline-of-the-notebook"]], "Outline<b>In this notebook, we go through a typical ML batch inference workflow:</b>    Architecture    Import Libraries    Load a public dataset from Hugging Face and move it into Ray Data object store.    Batch Inference Class        - Create a Ray actor class to load a ML model. In this example, we use SentenceTransformer library from Hugging Face to load a sentence embedding model.    Create batches of data to do inference.    Deploying at Scale    Inference on the entire dataset    Out of memory errors    Summary</ul></div>": [[265, "outlinein-this-notebook-we-go-through-a-typical-ml-batch-inference-workflow-architecture-import-libraries-load-a-public-dataset-from-hugging-face-and-move-it-into-ray-data-object-store-batch-inference-class-create-a-ray-actor-class-to-load-a-ml-model-in-this-example-we-use-sentencetransformer-library-from-hugging-face-to-load-a-sentence-embedding-model-create-batches-of-data-to-do-inference-deploying-at-scale-inference-on-the-entire-dataset-out-of-memory-errors-summary"]], "Outlook": [[17, "outlook"], [18, "outlook"]], "Outlook:  Ray Data in Production": [[15, "outlook-ray-data-in-production"]], "Overview": [[168, "overview"]], "Overview: Advanced Features Preview": [[118, "overview-advanced-features-preview"], [120, null]], "Overview: Why Medium-Sized Models?": [[110, "overview-why-medium-sized-models"], [112, null]], "Part 1. Creating and Submitting your first job": [[90, "part-1-creating-and-submitting-your-first-job"], [91, "part-1-creating-and-submitting-your-first-job"]], "Part 1: Starting your first Anyscale Service": [[92, "part-1-starting-your-first-anyscale-service"], [93, "part-1-starting-your-first-anyscale-service"]], "Part 2. Automation and Scheduling": [[90, "part-2-automation-and-scheduling"], [91, "part-2-automation-and-scheduling"]], "Practical Selection Process": [[118, "practical-selection-process"], [124, "practical-selection-process"]], "Prefill Phase": [[101, "prefill-phase"], [102, "prefill-phase"], [104, "prefill-phase"]], "Preprocess": [[137, "preprocess"], [139, null]], "Preprocessing with a Tokenizer": [[285, "preprocessing-with-a-tokenizer"], [292, null]], "Prerequisites": [[1, "prerequisites"], [28, "prerequisites"], [29, "prerequisites"], [35, "prerequisites"], [37, null], [43, "prerequisites"], [44, "prerequisites"], [53, "prerequisites"], [55, null], [66, "prerequisites"], [68, null], [79, "prerequisites"], [110, "prerequisites"], [114, "prerequisites"], [164, "prerequisites"], [165, "prerequisites"], [168, "prerequisites"], [169, "prerequisites"], [170, "prerequisites"]], "Prerequisites and Assumptions": [[155, "prerequisites-and-assumptions"], [158, "prerequisites-and-assumptions"]], "Production": [[126, "production"], [127, "production"]], "Production Job": [[137, "production-job"], [145, null]], "Production jobs": [[128, "production-jobs"], [135, null]], "Production services": [[147, "production-services"], [153, null]], "Projects": [[96, "projects"], [98, "projects"]], "Providers": [[43, "providers"], [44, "providers"]], "Publishing": [[0, "publishing"]], "Purpose": [[17, "purpose"], [19, "purpose"]], "Pytorch Lightning": [[360, "pytorch-lightning"]], "Query the deployed model": [[309, "query-the-deployed-model"]], "Ray Ai Libs": [[360, "ray-ai-libs"]], "Ray Core": [[360, "ray-core"]], "Ray Data": [[128, "ray-data"], [132, null], [360, "ray-data"]], "Ray Data Batch Inference": [[360, "ray-data-batch-inference"]], "Ray Data Logs": [[164, "ray-data-logs"], [166, "ray-data-logs"]], "Ray Data Metrics": [[164, "ray-data-metrics"], [166, "ray-data-metrics"]], "Ray Data Processing": [[360, "ray-data-processing"]], "Ray Distributed Training": [[360, "ray-distributed-training"]], "Ray Enablement Content": [[360, null]], "Ray Enablement Content: Jupyter Book Publishing": [[0, null]], "Ray Observability": [[159, "ray-observability"], [161, null]], "Ray Serve": [[8, "ray-serve"], [147, "ray-serve"], [151, null], [191, "ray-serve"], [197, null], [360, "ray-serve"]], "Ray Serve Alerts": [[164, "ray-serve-alerts"], [167, "ray-serve-alerts"]], "Ray Serve LLM + Anyscale Architecture": [[101, "ray-serve-llm-anyscale-architecture"], [102, "ray-serve-llm-anyscale-architecture"], [107, null]], "Ray Serve Logs": [[164, "ray-serve-logs"], [167, "ray-serve-logs"]], "Ray Serve Metrics": [[164, "ray-serve-metrics"], [167, "ray-serve-metrics"]], "Ray Serve Online Serving": [[360, "ray-serve-online-serving"]], "Ray Serve Tracing (Anyscale Only)": [[164, "ray-serve-tracing-anyscale-only"], [167, "ray-serve-tracing-anyscale-only"]], "Ray Serve vs Ray Data": [[8, "ray-serve-vs-ray-data"], [191, "ray-serve-vs-ray-data"], [197, "ray-serve-vs-ray-data"]], "Ray Train": [[137, "ray-train"], [144, null], [360, "ray-train"]], "Ray Tune": [[360, "ray-tune"]], "Ray Workloads Data Dashboard": [[164, "ray-workloads-data-dashboard"], [166, "ray-workloads-data-dashboard"]], "Ray and Anyscale Observability Introduction": [[159, null], [160, null]], "Ray and Anyscale Observability in Detail": [[164, null], [165, null]], "Recap": [[12, "recap"]], "Register New User": [[168, "register-new-user"]], "Related Examples": [[110, "related-examples"], [112, "related-examples"]], "Related Examples & Templates": [[110, "related-examples-templates"], [117, "related-examples-templates"]], "Replicas": [[11, "replicas"], [218, "replicas"], [221, "replicas"]], "Reporting metrics": [[13, "reporting-metrics"]], "Request Flow": [[168, "request-flow"]], "Requirements": [[43, "requirements"], [44, "requirements"]], "Resources": [[101, "resources"], [102, "resources"], [109, "resources"], [110, "resources"], [117, "resources"], [118, "resources"], [125, "resources"]], "Reverse diffusion: sampling new images": [[317, "reverse-diffusion-sampling-new-images"], [318, "reverse-diffusion-sampling-new-images"]], "Run a simple Data Pipeline": [[164, "run-a-simple-data-pipeline"], [166, "run-a-simple-data-pipeline"]], "Run inference on a batch of 128 rows for testing.": [[275, "run-inference-on-a-batch-of-128-rows-for-testing"]], "Run inference on the entire dataset": [[267, "run-inference-on-the-entire-dataset"], [277, "run-inference-on-the-entire-dataset"], [277, "id1"], [284, null]], "Run inference on the entire datasetExecute and materialize this dataset into object store memory. This operation will trigger execution of the lazy transformations performed on this dataset. The embedding model \u2018TextEmbedder\u2019 in map_batches() is called on the entire dataset.# Run inference on the entire dataset# Note that this does not mutate the original Dataset.materialized_ds = ds.materialize()# metadata after inferenceprint(\u2018** Original dataset:\u2019, ds)print(\u2018\\n** Materialized dataset:\u2019, materialized_ds)# Show a few rows of the materialized dataset with embeddingsmaterialized_ds.show(3)": [[276, null]], "Running Inference on Anyscale": [[110, "running-inference-on-anyscale"], [115, "running-inference-on-anyscale"]], "Running inference on the entire dataset": [[277, null]], "Running this notebook": [[12, "running-this-notebook"]], "Sample Requests": [[168, "sample-requests"]], "Saving a checkpoint in a local directory": [[13, "saving-a-checkpoint-in-a-local-directory"]], "Scaling deployment": [[306, "scaling-deployment"], [307, "scaling-deployment"], [315, "scaling-deployment"]], "Scheduling the training loop on a single GPU": [[13, "scheduling-the-training-loop-on-a-single-gpu"]], "Sending Requests": [[110, "sending-requests"], [114, "sending-requests"]], "Setting Up Local Ray Observability": [[155, "setting-up-local-ray-observability"], [158, null]], "Setting Up a Local Ray server using Jupyter Notebook": [[1, "setting-up-a-local-ray-server-using-jupyter-notebook"], [169, "setting-up-a-local-ray-server-using-jupyter-notebook"], [170, "setting-up-a-local-ray-server-using-jupyter-notebook"]], "Setting up Ray Serve LLM": [[110, "setting-up-ray-serve-llm"], [113, null]], "Setting up the Configuration File": [[110, "setting-up-the-configuration-file"], [115, "setting-up-the-configuration-file"]], "Setup and Installation": [[168, "setup-and-installation"]], "Show a few rows of the materialized dataset with embeddings": [[277, "show-a-few-rows-of-the-materialized-dataset-with-embeddings"]], "Shuffle all rows globally": [[9, "shuffle-all-rows-globally"], [198, "shuffle-all-rows-globally"], [204, "shuffle-all-rows-globally"]], "Shuffle rows globally": [[15, "shuffle-rows-globally"]], "Shuffling block order": [[9, "shuffling-block-order"], [15, "shuffling-block-order"], [198, "shuffling-block-order"], [204, "shuffling-block-order"]], "Shuffling data": [[9, "shuffling-data"], [15, "shuffling-data"], [198, "shuffling-data"], [204, "shuffling-data"]], "Shutdown Ray": [[285, "shutdown-ray"], [292, "shutdown-ray"]], "Shutdown Ray cluster": [[267, "shutdown-ray-cluster"], [277, "shutdown-ray-cluster"], [284, "shutdown-ray-cluster"]], "Shutdown and Summary": [[311, null]], "Shutdown the Ray Serve instances and Ray Cluster": [[307, "shutdown-the-ray-serve-instances-and-ray-cluster"], [311, "shutdown-the-ray-serve-instances-and-ray-cluster"], [316, "shutdown-the-ray-serve-instances-and-ray-cluster"]], "Shutdown the Ray Serve instances and Ray Cluster# stop ray serveserve.shutdown()  # Shutdown Ray Serve when done, ray cluster will still be runningray.shutdown()  # Shutdown Ray cluster": [[310, null]], "Shutting Down": [[110, "shutting-down"], [114, "shutting-down"]], "Shutting Down the Service": [[110, "shutting-down-the-service"], [115, "shutting-down-the-service"]], "Similar images": [[128, "similar-images"], [136, null]], "Simulate Client: Send test requests": [[307, "simulate-client-send-test-requests"], [309, "simulate-client-send-test-requests"], [316, null]], "Simulate Client: Send test requestsWe use requests library to send HTTP requests to the deployed model.Note: if you encounter any errors with serve not able to start, most likely it is due to previous instance of serve not being shutdown properly. Restart the notebook or see towards the end of notebook to see how to gracefully shutdown ray serve and the ray cluster.import requests # used to send HTTP requests to the deployed model# Query the deployed modelresponse = requests.get(\u201dhttp://localhost:8000/predict\u201d, params={\u201ctext\u201d: \u201cI love Ray Serve!\u201d})print(response.json())  # Should print the sentiment analysis result{\u2018text\u2019: \u2018i love ray serve!\u2019, \u2018sentiment\u2019: [{\u2018label\u2019: \u2018POSITIVE\u2019, \u2018score\u2019: 0.9998507499694824}]}": [[308, "simulate-client-send-test-requestswe-use-requests-library-to-send-http-requests-to-the-deployed-model-note-if-you-encounter-any-errors-with-serve-not-able-to-start-most-likely-it-is-due-to-previous-instance-of-serve-not-being-shutdown-properly-restart-the-notebook-or-see-towards-the-end-of-notebook-to-see-how-to-gracefully-shutdown-ray-serve-and-the-ray-cluster-import-requests-used-to-send-http-requests-to-the-deployed-model-query-the-deployed-modelresponse-requests-get-http-localhost-8000-predict-params-text-i-love-ray-serve-print-response-json-should-print-the-sentiment-analysis-result-text-i-love-ray-serve-sentiment-label-positive-score-0-9998507499694824"]], "Start by launching the Ray head node in the terminal:": [[155, "start-by-launching-the-ray-head-node-in-the-terminal"], [158, "start-by-launching-the-ray-head-node-in-the-terminal"]], "Stateful transformations with actors": [[15, "stateful-transformations-with-actors"]], "Step 1: Configuration": [[101, "step-1-configuration"], [102, "step-1-configuration"], [108, "step-1-configuration"]], "Step 1: Install Required Packages": [[155, "step-1-install-required-packages"], [158, "step-1-install-required-packages"]], "Step 2: Deployment": [[101, "step-2-deployment"], [102, "step-2-deployment"], [108, "step-2-deployment"]], "Step 2: Launch Prometheus": [[155, "step-2-launch-prometheus"], [158, "step-2-launch-prometheus"]], "Step 3: Launch Ray Cluster": [[155, "step-3-launch-ray-cluster"], [158, "step-3-launch-ray-cluster"]], "Step 3: Querying": [[101, "step-3-querying"], [102, "step-3-querying"], [108, "step-3-querying"]], "Step 4: Install and Launch Grafana": [[155, "step-4-install-and-launch-grafana"], [158, "step-4-install-and-launch-grafana"]], "Step 4: Shutdown": [[102, "step-4-shutdown"], [108, "step-4-shutdown"]], "Steps to run:": [[12, "steps-to-run"]], "Streaming Applications": [[8, "streaming-applications"], [191, "streaming-applications"], [195, "streaming-applications"]], "Structure of a data lake": [[8, "structure-of-a-data-lake"], [191, "structure-of-a-data-lake"], [192, "structure-of-a-data-lake"]], "Summary": [[267, "summary"], [277, "summary"], [284, "summary"], [285, "summary"], [292, "summary"], [307, "summary"], [311, "summary"], [316, "summary"]], "Summary & Outlook": [[110, "summary-outlook"], [117, null]], "SummaryIn this notebook, we deployed a sentiment analysis model from Hugging Face using Ray Serve and FastAPI. Using num_replicas we scaled the number of instances of the model. There are many more options to autoscale to increase the replicas when the traffic is high and downscale to zero when there is no traffic.": [[310, "summaryin-this-notebook-we-deployed-a-sentiment-analysis-model-from-hugging-face-using-ray-serve-and-fastapi-using-num-replicas-we-scaled-the-number-of-instances-of-the-model-there-are-many-more-options-to-autoscale-to-increase-the-replicas-when-the-traffic-is-high-and-downscale-to-zero-when-there-is-no-traffic"]], "SummaryThis notebook demonstrates how to perform efficient batch inference on large datasets using Ray Data. It walks through loading a public dataset from Hugging Face, converting it into a Ray Dataset, and defining a callable class to load and apply a machine learning model (SentenceTransformer) for embedding text. The notebook shows how to use Ray Data\u2019s map_batches API to process data in parallel batches, leveraging available CPUs or GPUs for high-throughput inference. It also covers best practices for scaling, handling memory constraints, and summarizes how Ray Data enables scalable, distributed batch inference for modern ML workflows.": [[276, "summarythis-notebook-demonstrates-how-to-perform-efficient-batch-inference-on-large-datasets-using-ray-data-it-walks-through-loading-a-public-dataset-from-hugging-face-converting-it-into-a-ray-dataset-and-defining-a-callable-class-to-load-and-apply-a-machine-learning-model-sentencetransformer-for-embedding-text-the-notebook-shows-how-to-use-ray-datas-map-batches-api-to-process-data-in-parallel-batches-leveraging-available-cpus-or-gpus-for-high-throughput-inference-it-also-covers-best-practices-for-scaling-handling-memory-constraints-and-summarizes-how-ray-data-enables-scalable-distributed-batch-inference-for-modern-ml-workflows"]], "Supported Infrastructure Types": [[17, "supported-infrastructure-types"], [20, "supported-infrastructure-types"]], "Testing the Container Image and Compute Config with an Anyscale Workflow": [[84, "testing-the-container-image-and-compute-config-with-an-anyscale-workflow"], [85, "testing-the-container-image-and-compute-config-with-an-anyscale-workflow"]], "The Compute Layer": [[8, "the-compute-layer"], [191, "the-compute-layer"], [193, null]], "The LLM Text Generation Process": [[101, "the-llm-text-generation-process"], [102, "the-llm-text-generation-process"], [104, "the-llm-text-generation-process"]], "The Orchestration Layer": [[8, "the-orchestration-layer"], [191, "the-orchestration-layer"], [194, null]], "The data layer": [[8, "the-data-layer"], [191, "the-data-layer"], [192, "the-data-layer"]], "The following instructions will walk you through running your first job. This notebook covers the following:": [[90, "the-following-instructions-will-walk-you-through-running-your-first-job-this-notebook-covers-the-following"], [91, "the-following-instructions-will-walk-you-through-running-your-first-job-this-notebook-covers-the-following"]], "This notebook covers the following:": [[92, "this-notebook-covers-the-following"], [93, "this-notebook-covers-the-following"]], "Tokenizer": [[293, "tokenizer"], [298, "tokenizer"]], "Trace Structure": [[168, "trace-structure"]], "Tracing Configuration": [[168, "tracing-configuration"]], "Train Generative Cv": [[360, "train-generative-cv"]], "Train Policy Learning": [[360, "train-policy-learning"]], "Train Rec Sys": [[360, "train-rec-sys"]], "Train Tabular": [[360, "train-tabular"]], "Train Time Series": [[360, "train-time-series"]], "Train Vision Pattern": [[360, "train-vision-pattern"]], "Training": [[137, "training"], [143, null]], "Training objective": [[317, "training-objective"], [318, "training-objective"], [330, "training-objective"], [331, "training-objective"]], "Two Phases of LLM Inference": [[101, "two-phases-of-llm-inference"], [102, "two-phases-of-llm-inference"], [104, "two-phases-of-llm-inference"]], "Usage": [[0, "usage"]], "Users and Roles": [[96, "users-and-roles"], [98, "users-and-roles"]], "Using LoRA Adapters": [[118, "using-lora-adapters"], [121, "using-lora-adapters"]], "Using Structured Output": [[118, "using-structured-output"], [122, "using-structured-output"]], "Using Tool Calling": [[118, "using-tool-calling"], [123, "using-tool-calling"]], "Using fractions of a GPU": [[16, "using-fractions-of-a-gpu"]], "VSCode": [[82, "vscode"], [83, "vscode"]], "Web Application Observability (Ray Serve)": [[164, "web-application-observability-ray-serve"], [167, null]], "Welcome to Anyscale Administration": [[79, null]], "What We Accomplished": [[110, "what-we-accomplished"], [117, "what-we-accomplished"], [118, "what-we-accomplished"], [125, "what-we-accomplished"]], "What We\u2019ll Cover": [[118, "what-we-ll-cover"], [120, "what-we-ll-cover"]], "What You\u2019ll Learn": [[79, "what-you-ll-learn"]], "What does the model learn?": [[350, "what-does-the-model-learn"], [351, "what-does-the-model-learn"]], "What is LLM Serving?": [[101, "what-is-llm-serving"], [102, "what-is-llm-serving"], [104, null]], "What is Ray Data ?": [[8, "what-is-ray-data"], [191, "what-is-ray-data"], [196, "what-is-ray-data"]], "What is Ray Serve ?": [[8, "what-is-ray-serve"], [191, "what-is-ray-serve"], [197, "what-is-ray-serve"]], "What is Ray Serve?": [[302, "what-is-ray-serve"], [307, "what-is-ray-serve"], [313, "what-is-ray-serve"]], "What is Ray Serve?Ray Serve is a scalable model serving library that allows you to deploy and manage machine learning models in production.With Ray Serve, you can easily create a scalable and distributed serving architecture thatcan handle high traffic and large workloads. It is built on top of Ray, a distributed computing frameworkthat allows you to run Python code in parallel across multiple machines. Ray Serve provides a simple API for deploying and managing models, as well as features like autoscaling,load balancing, and versioning.Ray Serve is designed to be easy to use and integrate with existing machine learning workflows.It supports a wide range of machine learning frameworks, including TensorFlow, PyTorch, and Scikit-learn.Ray Serve also provides a simple way to deploy models as REST APIs, using FastAPI,making it easy to integrate with web applications and other services.More information: https://docs.ray.io/en/latest/serve/index.html": [[301, "what-is-ray-serve-ray-serve-is-a-scalable-model-serving-library-that-allows-you-to-deploy-and-manage-machine-learning-models-in-production-with-ray-serve-you-can-easily-create-a-scalable-and-distributed-serving-architecture-thatcan-handle-high-traffic-and-large-workloads-it-is-built-on-top-of-ray-a-distributed-computing-frameworkthat-allows-you-to-run-python-code-in-parallel-across-multiple-machines-ray-serve-provides-a-simple-api-for-deploying-and-managing-models-as-well-as-features-like-autoscaling-load-balancing-and-versioning-ray-serve-is-designed-to-be-easy-to-use-and-integrate-with-existing-machine-learning-workflows-it-supports-a-wide-range-of-machine-learning-frameworks-including-tensorflow-pytorch-and-scikit-learn-ray-serve-also-provides-a-simple-way-to-deploy-models-as-rest-apis-using-fastapi-making-it-easy-to-integrate-with-web-applications-and-other-services-more-information-https-docs-ray-io-en-latest-serve-index-html"]], "What is an Anyscale Service?": [[110, "what-is-an-anyscale-service"], [115, "what-is-an-anyscale-service"]], "What problem are you solving? (Diffusion as image de-noising)": [[317, "what-problem-are-you-solving-diffusion-as-image-de-noising"], [318, "what-problem-are-you-solving-diffusion-as-image-de-noising"]], "What problem are you solving? (Forest cover classification with XGBoost)": [[337, "what-problem-are-you-solving-forest-cover-classification-with-xgboost"], [338, "what-problem-are-you-solving-forest-cover-classification-with-xgboost"]], "What problem are you solving? (Inverted Pendulum, Diffusion-Style)": [[324, "what-problem-are-you-solving-inverted-pendulum-diffusion-style"], [325, "what-problem-are-you-solving-inverted-pendulum-diffusion-style"]], "What problem are you solving? (NYC taxi demand forecasting with a Transformer)": [[343, "what-problem-are-you-solving-nyc-taxi-demand-forecasting-with-a-transformer"], [344, "what-problem-are-you-solving-nyc-taxi-demand-forecasting-with-a-transformer"]], "What problem are you solving? (image classification with Food-101-Lite)": [[350, "what-problem-are-you-solving-image-classification-with-food-101-lite"], [351, "what-problem-are-you-solving-image-classification-with-food-101-lite"]], "What problem are you solving? (matrix factorization for recommendations)": [[330, "what-problem-are-you-solving-matrix-factorization-for-recommendations"], [331, "what-problem-are-you-solving-matrix-factorization-for-recommendations"]], "What you learn and take away": [[317, "what-you-learn-and-take-away"], [318, "what-you-learn-and-take-away"], [324, "what-you-learn-and-take-away"], [325, "what-you-learn-and-take-away"], [330, "what-you-learn-and-take-away"], [331, "what-you-learn-and-take-away"], [337, "what-you-learn-and-take-away"], [338, "what-you-learn-and-take-away"], [343, "what-you-learn-and-take-away"], [344, "what-you-learn-and-take-away"], [350, "what-you-learn-and-take-away"], [351, "what-you-learn-and-take-away"]], "What you\u2019ll learn & take away": [[224, "what-youll-learn-take-away"], [225, "what-youll-learn-take-away"], [238, "what-youll-learn-take-away"], [239, "what-youll-learn-take-away"], [245, "what-youll-learn-take-away"], [246, "what-youll-learn-take-away"]], "What\u2019s Next": [[155, "what-s-next"], [158, "what-s-next"]], "What\u2019s XGBoost?": [[337, "what-s-xgboost"], [338, "what-s-xgboost"]], "What\u2019s a policy?": [[324, "what-s-a-policy"], [325, "what-s-a-policy"]], "What\u2019s a sequence-to-sequence Transformer?": [[343, "what-s-a-sequence-to-sequence-transformer"], [344, "what-s-a-sequence-to-sequence-transformer"]], "When to use Ray Core over Ray Data ?": [[8, "when-to-use-ray-core-over-ray-data"], [191, "when-to-use-ray-core-over-ray-data"], [196, "when-to-use-ray-core-over-ray-data"]], "When to use Ray Serve?": [[16, "when-to-use-ray-serve"]], "Where can you take this next?": [[317, "where-can-you-take-this-next"], [323, "where-can-you-take-this-next"], [324, "where-can-you-take-this-next"], [329, "where-can-you-take-this-next"], [330, "where-can-you-take-this-next"], [336, "where-can-you-take-this-next"], [337, "where-can-you-take-this-next"], [342, "where-can-you-take-this-next"], [343, "where-can-you-take-this-next"], [349, "where-can-you-take-this-next"], [350, "where-can-you-take-this-next"], [359, "where-can-you-take-this-next"]], "Why Choose Medium-Sized Models?": [[110, "why-choose-medium-sized-models"], [112, "why-choose-medium-sized-models"]], "Why Ray Data ?": [[8, "why-ray-data"], [191, "why-ray-data"], [196, "why-ray-data"]], "Why Ray Serve ?": [[8, "why-ray-serve"], [191, "why-ray-serve"], [197, "why-ray-serve"]], "Why Ray?": [[1, "why-ray"], [169, "why-ray"], [170, "why-ray"]], "Why Structured Output Matters": [[118, "why-structured-output-matters"], [122, "why-structured-output-matters"]], "Why These Features Matter": [[118, "why-these-features-matter"], [120, "why-these-features-matter"]], "Why Tool Calling Matters": [[118, "why-tool-calling-matters"], [123, "why-tool-calling-matters"]], "Why Use LoRA Adapters?": [[118, "why-use-lora-adapters"], [121, "why-use-lora-adapters"]], "Why not Kubernetes ?": [[101, "why-not-kubernetes"], [102, "why-not-kubernetes"], [106, "why-not-kubernetes"]], "Why not use just FastAPI or Flask?": [[302, "why-not-use-just-fastapi-or-flask"], [307, "why-not-use-just-fastapi-or-flask"], [313, "why-not-use-just-fastapi-or-flask"]], "Why not use just FastAPI or Flask?We could have simply used FastAPI or Flask to create a REST API for the model,but Ray Serve provides additional features like autoscaling and load balancing that make it a better choice for production deployments. Ray Serve also allows you to easilydeploy multiple models and manage their versions, which can be useful in a production environment where you may need to deploy multiple models or update existing ones.\u201d\u201d\u201d": [[301, "why-not-use-just-fastapi-or-flask-we-could-have-simply-used-fastapi-or-flask-to-create-a-rest-api-for-the-model-but-ray-serve-provides-additional-features-like-autoscaling-and-load-balancing-that-make-it-a-better-choice-for-production-deployments-ray-serve-also-allows-you-to-easilydeploy-multiple-models-and-manage-their-versions-which-can-be-useful-in-a-production-environment-where-you-may-need-to-deploy-multiple-models-or-update-existing-ones"]], "Why this works": [[317, "why-this-works"], [318, "why-this-works"]], "Workloads": [[96, "workloads"], [98, "workloads"], [360, "workloads"]], "Wrap up and next steps": [[317, "wrap-up-and-next-steps"], [323, "wrap-up-and-next-steps"], [324, "wrap-up-and-next-steps"], [329, "wrap-up-and-next-steps"], [330, "wrap-up-and-next-steps"], [336, "wrap-up-and-next-steps"], [337, "wrap-up-and-next-steps"], [342, "wrap-up-and-next-steps"], [343, "wrap-up-and-next-steps"], [349, "wrap-up-and-next-steps"], [350, "wrap-up-and-next-steps"], [359, "wrap-up-and-next-steps"]], "avoids collisons with other notebooks running ray jobs on the same machine": [[277, "avoids-collisons-with-other-notebooks-running-ray-jobs-on-the-same-machine"]], "batch_size should be set based on VRAM": [[275, "batch-size-should-be-set-based-on-vram"]], "library for pre-trained models": [[304, "library-for-pre-trained-models"]], "load a Hugging Face dataset": [[271, "load-a-hugging-face-dataset"]], "metadata after inference": [[277, "metadata-after-inference"]], "setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPS": [[275, "setting-manually-so-that-code-works-on-ray-clusters-with-both-cpu-or-gpu-workers-or-on-a-local-mac-with-mps"]], "stop ray serve": [[311, "stop-ray-serve"]], "\u25b6\ufe0f 3. Activate the Environment": [[1, "activate-the-environment"], [169, "activate-the-environment"], [170, "activate-the-environment"]], "\u2705 1. Install Conda": [[1, "install-conda"], [169, "install-conda"], [170, "install-conda"]], "\u2705 7. Verify Ray Installation with a Simple Example": [[1, "verify-ray-installation-with-a-simple-example"], [169, "verify-ray-installation-with-a-simple-example"], [170, "verify-ray-installation-with-a-simple-example"]], "\u2705 Module 01 \u00b7 Introduction to Ray Train": [[245, "module-01-introduction-to-ray-train"], [252, "module-01-introduction-to-ray-train"]], "\u2705 Module 02 \u00b7 Integrating Ray Train with Ray Data": [[245, "module-02-integrating-ray-train-with-ray-data"], [252, "module-02-integrating-ray-train-with-ray-data"]], "\u2705 Module 03 \u00b7 Fault Tolerance in Ray Train": [[245, "module-03-fault-tolerance-in-ray-train"], [252, "module-03-fault-tolerance-in-ray-train"]], "\ud83c\udf89 Wrapping Up & Next Steps": [[245, "wrapping-up-next-steps"], [252, null]], "\ud83d\udccb Notebook Compute Requirements Legend": [[1, "notebook-compute-requirements-legend"], [169, "notebook-compute-requirements-legend"], [170, "notebook-compute-requirements-legend"]], "\ud83d\udccc Overview of Structure": [[96, "overview-of-structure"], [98, null]], "\ud83d\udcda 01 \u00b7 Introduction to Ray Train": [[224, null], [225, null]], "\ud83d\udcda Next Tutorials in the Course": [[245, "next-tutorials-in-the-course"], [252, "next-tutorials-in-the-course"]], "\ud83d\udce6 4. Install UV and Dependencies": [[1, "install-uv-and-dependencies"], [169, "install-uv-and-dependencies"], [170, "install-uv-and-dependencies"]], "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data": [[238, null], [239, null]], "\ud83d\udd0e Integrating Ray Train with Ray Data": [[238, "id1"], [239, "id1"]], "\ud83d\udd0e When to use Ray Train": [[224, "when-to-use-ray-train"], [225, "when-to-use-ray-train"]], "\ud83d\udda5\ufe0f 5. (Optional but Recommended) Add Your Conda Environment to Jupyter": [[1, "optional-but-recommended-add-your-conda-environment-to-jupyter"], [169, "optional-but-recommended-add-your-conda-environment-to-jupyter"], [170, "optional-but-recommended-add-your-conda-environment-to-jupyter"]], "\ud83d\udda5\ufe0f How Distributed Data Parallel (DDP) Works": [[224, "how-distributed-data-parallel-ddp-works"], [225, "how-distributed-data-parallel-ddp-works"]], "\ud83d\ude80 6. Launch Jupyter Notebook": [[1, "launch-jupyter-notebook"], [169, "launch-jupyter-notebook"], [170, "launch-jupyter-notebook"]], "\ud83d\ude80 Where to go next": [[245, "where-to-go-next"], [252, "where-to-go-next"]], "\ud83d\udee0\ufe0f 2. Create a New Conda Environment": [[1, "create-a-new-conda-environment"], [169, "create-a-new-conda-environment"], [170, "create-a-new-conda-environment"]], "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train": [[245, null], [246, null]], "\ud83e\udde0 Summary": [[96, "summary"], [99, null]], "\ud83e\udde9 Miniforge Installation (It depends on your OS. In this case, we use ARM Macs)": [[1, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"], [169, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"], [170, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"]], "\ud83e\uddf9 8. Shut Down and Clean Up": [[1, "shut-down-and-clean-up"], [169, "shut-down-and-clean-up"], [170, "shut-down-and-clean-up"]]}, "docnames": ["README", "courses/deprecated/Developer_Intro_to_Ray/00_Introduction", "courses/deprecated/Developer_Intro_to_Ray/00a_Intro_Ray_Core_Basics", "courses/deprecated/Developer_Intro_to_Ray/00b_Intro_Ray_Core_Advancement", "courses/deprecated/Developer_Intro_to_Ray/01_Intro_Ray_AI_Libs_Overview", "courses/deprecated/Developer_Intro_to_Ray/02a_Intro_Ray_Train_with_PyTorch", "courses/deprecated/Developer_Intro_to_Ray/02b_Intro_Ray_Train_with_PyTorch_Lightning", "courses/deprecated/Developer_Intro_to_Ray/03_Intro_Ray_Tune", "courses/deprecated/Developer_Intro_to_Ray/04a_Intro_Ray_Data_Industry_Landscape", "courses/deprecated/Developer_Intro_to_Ray/04b_Intro_Ray_Data_Structured", "courses/deprecated/Developer_Intro_to_Ray/04c_Intro_Ray_Data_Unstructured", "courses/deprecated/Developer_Intro_to_Ray/05_Intro_Ray_Serve_PyTorch", "courses/deprecated/ray-101/1_AI_Libs_Intro", "courses/deprecated/ray-101/2_Intro_Train", "courses/deprecated/ray-101/3_Intro_Tune", "courses/deprecated/ray-101/4_Intro_Data", "courses/deprecated/ray-101/5_Intro_Serve", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/anyscale_administrator_overview", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/anyscale_vm_vs_k8s", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/deploy_to_ec2", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/deploy_to_GCE", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/deploy_to_a_new_EKS", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/deploy_to_an_existing_EKS", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/deploy_to_new_GKE", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12", "courses/foundations/Anyscale_For_Admins/README", "courses/foundations/Anyscale_Getting_Started/00_Intro_to_Workspaces/101_01_anyscale_intro_workspace", "courses/foundations/Anyscale_Getting_Started/00_Intro_to_Workspaces/output/101_01_anyscale_intro_workspace_01", "courses/foundations/Anyscale_Getting_Started/01_dev_intro/101_02_anyscale_development_intro", "courses/foundations/Anyscale_Getting_Started/01_dev_intro/output/101_02_anyscale_development_intro_01", "courses/foundations/Anyscale_Getting_Started/02_compute_runtime/101_03_anyscale_compute_runtime_intro", "courses/foundations/Anyscale_Getting_Started/02_compute_runtime/output/101_03_anyscale_compute_runtime_intro_01", "courses/foundations/Anyscale_Getting_Started/03_storage_options/101_04_anyscale_storage_options", "courses/foundations/Anyscale_Getting_Started/03_storage_options/output/101_04_anyscale_storage_options_01", "courses/foundations/Anyscale_Getting_Started/04_logging_metrics/101_05_anyscale_logging_metrics", "courses/foundations/Anyscale_Getting_Started/04_logging_metrics/output/101_05_anyscale_logging_metrics_01", "courses/foundations/Anyscale_Getting_Started/05_intro_jobs/101_06_anyscale_intro_jobs", "courses/foundations/Anyscale_Getting_Started/05_intro_jobs/output/101_06_anyscale_intro_jobs_01", "courses/foundations/Anyscale_Getting_Started/06_intro_services/101_07_anyscale_intro_services", "courses/foundations/Anyscale_Getting_Started/06_intro_services/output/101_07_anyscale_intro_services_01", "courses/foundations/Anyscale_Getting_Started/07_collaboration/101_08_anyscale_collaboration", "courses/foundations/Anyscale_Getting_Started/07_collaboration/output/101_08_anyscale_collaboration_01", "courses/foundations/Anyscale_Getting_Started/08_org_setup/101_09_anyscale_org_setup", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_01", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_02", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_03", "courses/foundations/Anyscale_Getting_Started/README", "courses/foundations/LLM_Serving/00_intro_serve_llm/README", "courses/foundations/LLM_Serving/00_intro_serve_llm/notebook", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_01", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_02", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_03", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_04", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_05", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_06", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_07", "courses/foundations/LLM_Serving/01_deploy_medium_llm/notebook", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_01", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_02", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_03", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_04", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_05", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_06", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_07", "courses/foundations/LLM_Serving/02_advanced_llm_features/notebook", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_01", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_02", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_03", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_04", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_05", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_06", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_07", "courses/foundations/Multimodal AI Workloads/00_overview/README", "courses/foundations/Multimodal AI Workloads/00_overview/output/README_01", "courses/foundations/Multimodal AI Workloads/01_batch_inference/01-Batch-Inference", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_01", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_02", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_03", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_04", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_05", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_06", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_07", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_08", "courses/foundations/Multimodal AI Workloads/02_distributed_training/02-Distributed-Training", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_01", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_02", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_03", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_04", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_05", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_06", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_07", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_08", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_09", "courses/foundations/Multimodal AI Workloads/03_online_serving/03-Online-Serving", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_01", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_02", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_03", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_04", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_05", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_06", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_07", "courses/foundations/Observability/01_Intro_and_setup/01_general_intro_and_setup", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_01", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_02", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_03", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/2_Ray_Anyscale_Observability_Overview", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/03_Ray_Anyscale_Observability_in_Details", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/tracing_example/README", "courses/foundations/Ray_AI_Libs/00_intro/README", "courses/foundations/Ray_AI_Libs/00_intro/output/README_01", "courses/foundations/Ray_AI_Libs/01_Overview/01_Intro_Ray_AI_Libs_Overview", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_01", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_02", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_03", "courses/foundations/Ray_Core/00_Basics/00_Intro_Ray_Core_Basics", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_01", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_02", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_03", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_04", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_05", "courses/foundations/Ray_Core/01_Advanced/00a_Intro_Ray_Core_Advancement", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_01", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_02", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_03", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_04", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_05", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_06", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_07", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_08", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_09", "courses/foundations/Ray_Data/00_Landscape/04a_Intro_Ray_Data_Industry_Landscape", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_01", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_02", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_03", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_04", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_05", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_06", "courses/foundations/Ray_Data/01_Structured/04b_Intro_Ray_Data_Structured", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_01", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_02", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_03", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_04", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_05", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_06", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_07", "courses/foundations/Ray_Data/02_Unstructured/04c_Intro_Ray_Data_Unstructured", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_01", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_02", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_03", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_04", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_05", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_06", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_07", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_08", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_09", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_10", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_11", "courses/foundations/Ray_Serve/00_Serve/05_Intro_Ray_Serve_PyTorch", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_01", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_02", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_03", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_04", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_05", "courses/foundations/Ray_Train/01_intro/01_intro_to_ray_train", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_01", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_02", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_03", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_04", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_05", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_06", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_07", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_08", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_09", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_10", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_11", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_12", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_13", "courses/foundations/Ray_Train/02_train_and_data/02_integrating_ray_train_with_ray_data", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_01", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_02", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_03", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_04", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_05", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_06", "courses/foundations/Ray_Train/03_fault_tolerance/03_fault_tolerance_in_ray_train", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_01", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_02", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_03", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_04", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_05", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_06", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_07", "courses/foundations/Ray_Tune/00_Tune/03_Intro_Ray_Tune", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_01", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_02", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_03", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_04", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_05", "courses/workloads/PyTorch_Lightning/00_workload/02b_Intro_Ray_Train_with_PyTorch_Lightning", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_Ray_Data_batch_inference", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/README", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_02", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_03", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_04", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_05", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_06", "courses/workloads/Ray_Data_Processing/00_workload/02_Ray_Data_data_processing", "courses/workloads/Ray_Data_Processing/00_workload/README", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_01", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_02", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_03", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_04", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_05", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_06", "courses/workloads/Ray_Distributed_Training/00_workload/04_Ray_Train_distributed_training", "courses/workloads/Ray_Distributed_Training/00_workload/README", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_01", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_02", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_03", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_04", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_05", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_06", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_Ray_Serve_online_serving", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/README", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_02", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_03", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_04", "courses/workloads/Train_Generative_CV/00_workload/04d1_generative_cv_pattern", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_01", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_02", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_03", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_04", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_05", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_06", "courses/workloads/Train_Policy_Learning/00_workload/04d2_policy_learning_pattern", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_01", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_02", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_03", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_04", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_05", "courses/workloads/Train_Rec_sys/00_workload/04e_rec_sys_workload_pattern", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_01", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_02", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_03", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_04", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_05", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_06", "courses/workloads/Train_Tabular/00_workload/04b_tabular_workload_pattern", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_01", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_02", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_03", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_04", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_05", "courses/workloads/Train_Time_Series/00_workload/04c_time_series_workload_pattern", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_01", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_02", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_03", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_04", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_05", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_06", "courses/workloads/Train_Vision_Pattern/00_workload/04a_vision_pattern", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_01", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_02", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_03", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_04", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_05", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_06", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_07", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_08", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_09", "index"], "envversion": {"sphinx": 62, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["README.md", "courses/deprecated/Developer_Intro_to_Ray/00_Introduction.ipynb", "courses/deprecated/Developer_Intro_to_Ray/00a_Intro_Ray_Core_Basics.ipynb", "courses/deprecated/Developer_Intro_to_Ray/00b_Intro_Ray_Core_Advancement.ipynb", "courses/deprecated/Developer_Intro_to_Ray/01_Intro_Ray_AI_Libs_Overview.ipynb", "courses/deprecated/Developer_Intro_to_Ray/02a_Intro_Ray_Train_with_PyTorch.ipynb", "courses/deprecated/Developer_Intro_to_Ray/02b_Intro_Ray_Train_with_PyTorch_Lightning.ipynb", "courses/deprecated/Developer_Intro_to_Ray/03_Intro_Ray_Tune.ipynb", "courses/deprecated/Developer_Intro_to_Ray/04a_Intro_Ray_Data_Industry_Landscape.ipynb", "courses/deprecated/Developer_Intro_to_Ray/04b_Intro_Ray_Data_Structured.ipynb", "courses/deprecated/Developer_Intro_to_Ray/04c_Intro_Ray_Data_Unstructured.ipynb", "courses/deprecated/Developer_Intro_to_Ray/05_Intro_Ray_Serve_PyTorch.ipynb", "courses/deprecated/ray-101/1_AI_Libs_Intro.ipynb", "courses/deprecated/ray-101/2_Intro_Train.ipynb", "courses/deprecated/ray-101/3_Intro_Tune.ipynb", "courses/deprecated/ray-101/4_Intro_Data.ipynb", "courses/deprecated/ray-101/5_Intro_Serve.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/anyscale_administrator_overview.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/anyscale_vm_vs_k8s.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/deploy_to_ec2.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/deploy_to_GCE.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/deploy_to_a_new_EKS.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/deploy_to_an_existing_EKS.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/deploy_to_new_GKE.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12.ipynb", "courses/foundations/Anyscale_For_Admins/README.md", "courses/foundations/Anyscale_Getting_Started/00_Intro_to_Workspaces/101_01_anyscale_intro_workspace.ipynb", "courses/foundations/Anyscale_Getting_Started/00_Intro_to_Workspaces/output/101_01_anyscale_intro_workspace_01.ipynb", "courses/foundations/Anyscale_Getting_Started/01_dev_intro/101_02_anyscale_development_intro.ipynb", "courses/foundations/Anyscale_Getting_Started/01_dev_intro/output/101_02_anyscale_development_intro_01.ipynb", "courses/foundations/Anyscale_Getting_Started/02_compute_runtime/101_03_anyscale_compute_runtime_intro.ipynb", "courses/foundations/Anyscale_Getting_Started/02_compute_runtime/output/101_03_anyscale_compute_runtime_intro_01.ipynb", "courses/foundations/Anyscale_Getting_Started/03_storage_options/101_04_anyscale_storage_options.ipynb", "courses/foundations/Anyscale_Getting_Started/03_storage_options/output/101_04_anyscale_storage_options_01.ipynb", "courses/foundations/Anyscale_Getting_Started/04_logging_metrics/101_05_anyscale_logging_metrics.ipynb", "courses/foundations/Anyscale_Getting_Started/04_logging_metrics/output/101_05_anyscale_logging_metrics_01.ipynb", "courses/foundations/Anyscale_Getting_Started/05_intro_jobs/101_06_anyscale_intro_jobs.ipynb", "courses/foundations/Anyscale_Getting_Started/05_intro_jobs/output/101_06_anyscale_intro_jobs_01.ipynb", "courses/foundations/Anyscale_Getting_Started/06_intro_services/101_07_anyscale_intro_services.ipynb", "courses/foundations/Anyscale_Getting_Started/06_intro_services/output/101_07_anyscale_intro_services_01.ipynb", "courses/foundations/Anyscale_Getting_Started/07_collaboration/101_08_anyscale_collaboration.ipynb", "courses/foundations/Anyscale_Getting_Started/07_collaboration/output/101_08_anyscale_collaboration_01.ipynb", "courses/foundations/Anyscale_Getting_Started/08_org_setup/101_09_anyscale_org_setup.ipynb", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_01.ipynb", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_02.ipynb", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_03.ipynb", "courses/foundations/Anyscale_Getting_Started/README.md", "courses/foundations/LLM_Serving/00_intro_serve_llm/README.md", "courses/foundations/LLM_Serving/00_intro_serve_llm/notebook.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_01.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_02.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_03.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_04.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_05.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_06.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_07.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/notebook.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_01.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_02.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_03.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_04.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_05.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_06.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_07.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/notebook.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_01.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_02.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_03.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_04.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_05.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_06.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_07.ipynb", "courses/foundations/Multimodal AI Workloads/00_overview/README.ipynb", "courses/foundations/Multimodal AI Workloads/00_overview/output/README_01.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/01-Batch-Inference.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_01.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_02.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_03.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_04.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_05.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_06.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_07.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_08.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/02-Distributed-Training.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_01.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_02.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_03.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_04.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_05.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_06.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_07.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_08.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_09.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/03-Online-Serving.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_01.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_02.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_03.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_04.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_05.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_06.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_07.ipynb", "courses/foundations/Observability/01_Intro_and_setup/01_general_intro_and_setup.ipynb", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_01.ipynb", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_02.ipynb", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_03.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/2_Ray_Anyscale_Observability_Overview.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/03_Ray_Anyscale_Observability_in_Details.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/tracing_example/README.md", "courses/foundations/Ray_AI_Libs/00_intro/README.ipynb", "courses/foundations/Ray_AI_Libs/00_intro/output/README_01.ipynb", "courses/foundations/Ray_AI_Libs/01_Overview/01_Intro_Ray_AI_Libs_Overview.ipynb", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_01.ipynb", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_02.ipynb", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_03.ipynb", "courses/foundations/Ray_Core/00_Basics/00_Intro_Ray_Core_Basics.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_01.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_02.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_03.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_04.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_05.ipynb", "courses/foundations/Ray_Core/01_Advanced/00a_Intro_Ray_Core_Advancement.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_01.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_02.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_03.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_04.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_05.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_06.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_07.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_08.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_09.ipynb", "courses/foundations/Ray_Data/00_Landscape/04a_Intro_Ray_Data_Industry_Landscape.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_01.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_02.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_03.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_04.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_05.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_06.ipynb", "courses/foundations/Ray_Data/01_Structured/04b_Intro_Ray_Data_Structured.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_01.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_02.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_03.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_04.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_05.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_06.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_07.ipynb", "courses/foundations/Ray_Data/02_Unstructured/04c_Intro_Ray_Data_Unstructured.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_01.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_02.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_03.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_04.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_05.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_06.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_07.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_08.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_09.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_10.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_11.ipynb", "courses/foundations/Ray_Serve/00_Serve/05_Intro_Ray_Serve_PyTorch.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_01.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_02.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_03.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_04.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_05.ipynb", "courses/foundations/Ray_Train/01_intro/01_intro_to_ray_train.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_01.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_02.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_03.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_04.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_05.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_06.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_07.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_08.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_09.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_10.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_11.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_12.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_13.ipynb", "courses/foundations/Ray_Train/02_train_and_data/02_integrating_ray_train_with_ray_data.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_01.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_02.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_03.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_04.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_05.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_06.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/03_fault_tolerance_in_ray_train.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_01.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_02.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_03.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_04.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_05.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_06.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_07.ipynb", "courses/foundations/Ray_Tune/00_Tune/03_Intro_Ray_Tune.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_01.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_02.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_03.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_04.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_05.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/02b_Intro_Ray_Train_with_PyTorch_Lightning.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_Ray_Data_batch_inference.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/README.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_02.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_03.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_04.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_05.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_06.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/02_Ray_Data_data_processing.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/README.md", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_01.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_02.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_03.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_04.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_05.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_06.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/04_Ray_Train_distributed_training.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/README.md", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_01.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_02.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_03.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_04.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_05.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_06.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_Ray_Serve_online_serving.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/README.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_02.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_03.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_04.ipynb", "courses/workloads/Train_Generative_CV/00_workload/04d1_generative_cv_pattern.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_01.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_02.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_03.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_04.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_05.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_06.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/04d2_policy_learning_pattern.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_01.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_02.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_03.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_04.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_05.ipynb", "courses/workloads/Train_Rec_sys/00_workload/04e_rec_sys_workload_pattern.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_01.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_02.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_03.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_04.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_05.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_06.ipynb", "courses/workloads/Train_Tabular/00_workload/04b_tabular_workload_pattern.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_01.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_02.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_03.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_04.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_05.ipynb", "courses/workloads/Train_Time_Series/00_workload/04c_time_series_workload_pattern.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_01.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_02.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_03.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_04.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_05.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_06.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/04a_vision_pattern.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_01.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_02.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_03.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_04.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_05.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_06.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_07.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_08.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_09.ipynb", "index.md"], "indexentries": {}, "objects": {}, "objnames": {}, "objtypes": {}, "terms": {"": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 28, 30, 31, 34, 35, 39, 40, 43, 45, 46, 47, 56, 58, 59, 66, 75, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 100, 101, 102, 104, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 136, 137, 138, 139, 140, 143, 146, 147, 148, 150, 151, 152, 153, 154, 159, 162, 163, 164, 166, 167, 169, 170, 171, 172, 174, 175, 180, 181, 183, 184, 185, 187, 189, 190, 191, 192, 196, 197, 198, 201, 202, 203, 204, 206, 208, 210, 211, 213, 215, 218, 220, 221, 222, 224, 225, 226, 227, 228, 232, 233, 234, 237, 238, 239, 241, 245, 252, 253, 255, 256, 257, 259, 261, 262, 263, 266, 267, 275, 277, 279, 281, 283, 284, 285, 288, 291, 292, 293, 295, 297, 298, 299, 317, 318, 319, 321, 323, 328, 329, 330, 331, 332, 333, 334, 335, 336, 339, 340, 341, 342, 345, 347, 349, 350, 351, 352, 353, 354, 355, 357, 359], "0": [1, 3, 4, 5, 6, 7, 10, 12, 13, 14, 15, 16, 22, 28, 29, 30, 35, 37, 40, 43, 44, 45, 46, 47, 53, 55, 56, 58, 59, 66, 68, 75, 80, 81, 84, 85, 88, 89, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 128, 130, 133, 136, 137, 139, 140, 141, 143, 144, 146, 147, 149, 150, 155, 158, 159, 163, 168, 169, 170, 171, 174, 181, 185, 187, 189, 190, 199, 203, 206, 211, 212, 213, 225, 226, 227, 228, 232, 233, 235, 237, 238, 242, 243, 245, 247, 248, 252, 253, 255, 256, 257, 259, 262, 263, 267, 281, 283, 285, 288, 289, 292, 293, 298, 300, 307, 309, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 328, 329, 330, 332, 334, 336, 337, 338, 339, 340, 343, 345, 346, 347, 349, 350, 351, 352, 353, 355, 359], "00": [12, 13, 14, 15, 86, 87, 137, 144, 146, 168, 267, 283, 293, 300, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352, 360], "000": [7, 14, 224, 226, 253, 255, 324, 326, 330, 332, 337, 339], "0000000000000": [28, 30], "00000000000000000": [28, 30], "0001": [6, 10, 206, 213, 259, 263], "00026041e": [137, 146], "0003573892": 14, "0003590581": 14, "0003788471": 14, "0003824231": 14, "0004189012": 14, "00043082e": [137, 146], "00046369e": [267, 284], "00054121e": [137, 146], "00087994e": [267, 284], "001": [137, 144], "00129196e": [267, 284], "00172612e": [137, 146], "00217544e": [267, 284], "00348413e": [137, 146], "00403815e": [267, 284], "00439209e": [137, 146], "0059": 12, "00592375e": [137, 146], "00596860e": [267, 284], "00612268e": [137, 146], "00641076e": [267, 284], "006742": 13, "00719017e": [267, 284], "00724374e": [267, 284], "00728178e": [267, 284], "00749106": [267, 283], "00753223": [267, 283], "00785351e": [137, 146], "007877049646500664": 14, "00787705": 14, "00813907e": [137, 146], "00816345e": [137, 146], "00844238": [267, 283], "00926834e": [267, 284], "0092816": [267, 283], "00958297e": [267, 284], "00974117e": [267, 284], "00982723e": [267, 284], "00994138e": [267, 284], "00_developer_intro_to_rai": [278, 286, 294, 312], "00_overview": 360, "00a": 360, "00z": 168, "01": [4, 6, 13, 126, 127, 137, 146, 171, 174, 259, 263, 267, 284, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352, 360], "01000227e": [267, 284], "01085338e": [137, 146], "01103884e": [267, 284], "01104600e": [267, 284], "01141734e": [267, 284], "01148352e": [267, 284], "01190887": [267, 283], "01222771": [267, 283], "01231135": [267, 283], "01273207e": [267, 284], "01295078e": [137, 146], "01347007e": [267, 284], "01351717e": [267, 284], "01387227e": [267, 284], "01402104e": [267, 284], "01455652": [267, 283], "01504247": [267, 283], "01505721e": [267, 284], "01544438e": [267, 284], "01616676e": [267, 284], "01646197e": [267, 284], "01848297e": [267, 284], "01875377e": [267, 284], "01878762e": [137, 146], "01880312e": [137, 146], "01893711e": [137, 146], "01946776e": [267, 284], "01948935e": [137, 146], "01951000e": [267, 284], "01958193e": [267, 284], "01_batch_infer": 360, "02": [4, 126, 127, 137, 146, 171, 174, 267, 284, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352, 360], "02095584": [267, 283], "02193573e": [137, 146], "02202111": [267, 283], "02216589e": [137, 146], "02220649e": [137, 146], "02316421e": [267, 284], "02334268e": [137, 146], "02338964e": [267, 284], "02352677e": [267, 284], "02359867e": [137, 146], "02369718e": [137, 146], "0242": 12, "02428794e": [137, 146], "02448604e": [267, 284], "02480531e": [267, 284], "02481507e": [267, 284], "02496293": [267, 283], "02508835e": [267, 284], "02556132": [267, 283], "02750473e": [267, 284], "02791084": [267, 283], "02834240e": [137, 146], "02842702e": [267, 284], "0299": [137, 141], "02993000e": [137, 146], "02_distributed_train": 360, "02_service_hello_world": [92, 93], "02b": 360, "02d": [330, 332], "03": [4, 12, 14, 126, 127, 137, 146, 171, 174, 267, 284, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352, 360], "03000000000000000": [28, 30], "03162946e": [267, 284], "03174406e": [137, 146], "03241184e": [267, 284], "03302041": [267, 283], "03319024e": [267, 284], "03347346e": [267, 284], "03351809e": [267, 284], "03357503": [267, 283], "03508779e": [137, 146], "03519934e": [137, 146], "0362": [137, 141], "03620186e": [267, 284], "03722222e": [267, 284], "0385": [137, 141], "03865302e": [137, 146], "03901269e": [267, 284], "03915609e": [267, 284], "03924675": [267, 283], "03927016e": [137, 146], "03974594e": [267, 284], "03_online_serv": 360, "03d": [317, 321, 324, 328], "04": [137, 146, 267, 284, 319, 327, 330, 332, 337, 339, 343, 345, 350, 352, 360], "04023737e": [267, 284], "04127836e": [267, 284], "04148921": [267, 283], "04188204e": [267, 284], "04267104e": [267, 284], "04279362e": [267, 284], "04313433e": [267, 284], "04401015e": [267, 284], "04419766e": [267, 284], "04514116e": [267, 284], "04530790e": [137, 146], "04760937e": [267, 284], "04781413e": [267, 284], "04831458": [267, 283], "04853529e": [137, 146], "04871886e": [267, 284], "04a": 360, "04b": 360, "04c": 360, "04d1": 360, "04d2": 360, "04e": 360, "05": [6, 9, 12, 13, 137, 140, 164, 166, 198, 201, 204, 259, 262, 293, 300, 317, 319, 324, 328, 330, 332, 337, 339, 343, 345, 350, 352, 360], "050227": [267, 283], "05029851e": [267, 284], "05031021e": [267, 284], "05048873e": [267, 284], "05110919e": [267, 284], "05117615e": [267, 284], "05286286": [267, 283], "05403318e": [267, 284], "05412337e": [267, 284], "0564279": 14, "05699580e": [267, 284], "0582891": 14, "05838008e": [267, 284], "05897461e": [267, 284], "05951829e": [267, 284], "05955295e": [267, 284], "05959303e": [137, 146], "05962829": [267, 283], "05964907e": [267, 284], "05_069833_2422": 13, "06": [1, 13, 86, 87, 169, 170, 317, 319, 324, 328, 330, 332, 343, 345, 350, 353, 360], "06012297e": [267, 284], "06039613e": [267, 284], "06096685": [267, 283], "06099542": [267, 283], "06128380e": [267, 284], "06155156e": [267, 284], "06164196e": [267, 284], "06194988": [267, 283], "06202352e": [267, 284], "06234232e": [267, 284], "06241195e": [267, 284], "06251295": [267, 283], "06282867e": [267, 284], "06317782e": [267, 284], "06359579e": [267, 284], "06367093324661255": 13, "063671": 13, "06383444e": [267, 284], "06388348e": [137, 146], "06465332e": [267, 284], "06585157e": [137, 146], "06659506e": [267, 284], "06678507e": [267, 284], "06857569e": [267, 284], "06872221": [267, 283], "06887527e": [267, 284], "07": [118, 123, 293, 300, 317, 319, 324, 328, 330, 333, 337, 339, 343, 345, 350, 353, 360], "07005756e": [267, 284], "07007243e": [137, 146], "07039157e": [267, 284], "07176238": [267, 283], "07179873e": [137, 146], "07271361e": [137, 146], "07316985e": [267, 284], "07334603e": [267, 284], "07348490e": [267, 284], "07383470982313156": [147, 150], "07393348e": [137, 146], "07420641e": [267, 284], "07442781e": [137, 146], "07510021": [267, 283], "07512747e": [267, 284], "07565679e": [267, 284], "07582638e": [267, 284], "07590961e": [267, 284], "07614997e": [267, 284], "07706316e": [137, 146], "07735191e": [267, 284], "07769895e": [267, 284], "07796153e": [267, 284], "07813133e": [267, 284], "07829855": [267, 283], "07924390e": [137, 146], "07964912e": [137, 146], "08": [110, 116, 137, 144, 168, 267, 284, 317, 320, 324, 329, 330, 334, 343, 345, 350, 353, 360], "08080895": [267, 283], "08113792": [267, 283], "08117312e": [267, 284], "08142687": [267, 283], "08161136e": [267, 284], "08262652e": [137, 146], "08306534e": [267, 284], "08318681e": [267, 284], "08351701e": [137, 146], "08386130e": [267, 284], "08393911e": [267, 284], "08408488e": [137, 146], "08411038e": [137, 146], "08423311e": [267, 284], "08562492e": [267, 284], "08593434e": [267, 284], "08646134e": [137, 146], "08834168e": [267, 284], "08835796e": [137, 146], "08847059e": [267, 284], "08849846e": [267, 284], "08855490e": [267, 284], "08866049e": [267, 284], "08888834e": [267, 284], "08900222e": [267, 284], "08926150e": [267, 284], "08956832e": [137, 146], "08967713e": [267, 284], "09": [13, 14, 317, 321, 324, 329, 330, 334, 337, 340, 343, 346, 350, 353, 360], "09058516e": [267, 284], "09124994e": [137, 146], "09158831e": [267, 284], "09305708e": [267, 284], "09318195e": [267, 284], "09376505e": [267, 284], "09415853e": [137, 146], "09640113e": [267, 284], "09668531e": [267, 284], "09694359e": [267, 284], "09729558e": [267, 284], "09752440e": [137, 146], "09788750e": [267, 284], "09830695e": [137, 146], "09841380e": [267, 284], "09894407e": [137, 146], "09954223e": [267, 284], "09_200164_18044": [293, 300], "0a000000000000000": [28, 30], "0aa72cef9b8921af5": [128, 136, 137, 146], "0b5c2c9a5a27cfba2": [128, 133], "0bd7bde3f2c914b3": [28, 30], "0e941ed71ef3480e": [128, 136], "0f8bb12ddf9a451e9": [28, 30, 43, 45, 53, 56], "0ffe5abae6e899f5a": [128, 136, 137, 146], "0m": [13, 14, 16, 128, 129, 133, 136, 137, 138, 139, 143, 146, 147, 148, 150, 293, 300], "0x72c6d85fc9d0": 16, "0x72c6d85fcf90": 16, "0x72c6d85fd3d0": 16, "0x72c6d85fd550": 16, "0x72c6d85fe590": 16, "0x72c6d85ff250": 16, "0x72c6d85ff3d0": 16, "0x72c6d8608750": 16, "0x72c6d8609050": 16, "0x72c6d860b6d0": 16, "0x72c6d860b7d0": 16, "0x72c6d8610490": 16, "0x72c6d8610a50": 16, "0x72c6d8611310": 16, "0x72c6d8611ad0": 16, "0x72c6d8611b90": 16, "0x72c6d8612050": 16, "0x72c6d8613690": 16, "0x72c6d8620a90": 16, "0x72c6d8620e10": 16, "0x72c6d86218d0": 16, "0x72c6d8621b90": 16, "0x72c6d8622ad0": 16, "0x72c6d8623590": 16, "0x72c6d86281d0": 16, "0x72c6d8628710": 16, "0x72c6d862a1d0": 16, "0x72c6d862ac50": 16, "0x72c6d862b790": 16, "0x72c6d862b7d0": 16, "0x72c6d862c690": 16, "0x72c6d872db50": 16, "0x72c6d8747390": 16, "0x72c6d87500d0": 16, "0x72c6d8752290": 16, "0x72c6d8752e50": 16, "0x72c6d8757dd0": 16, "0x72c6d87793d0": 16, "0x72c6d8779b90": 16, "0x72c6d877a010": 16, "0x72c6d877a6d0": 16, "0x72c6d877b010": 16, "0x72c6d877bc10": 16, "0x72c6d8785e50": 16, "0x72c6d8785fd0": 16, "0x72c6d8786a50": 16, "0x72c6d8787c90": 16, "0x72c6d8794350": 16, "0x72c6d8795110": 16, "0x72c6d8796b50": 16, "0x72c6d8797150": 16, "0x72c6d8797ed0": 16, "0x72c6d87a0690": 16, "0x72c6d87a14d0": 16, "0x72c6d87a1b50": 16, "0x72c6d87a1f50": 16, "0x72c6d87a2c10": 16, "0x72c6d87a3d50": 16, "0x72c6d87a3ed0": 16, "0x72c6d87a8690": 16, "0x72c6d87a96d0": 16, "0x72c6d87a9cd0": 16, "0x72c6d87aa0d0": 16, "0x72c6d87aa4d0": 16, "0x72c6d87aad50": 16, "0x72c6d87ab690": 16, "0x72c6d87ac4d0": 16, "0x72c6d87adb90": 16, "0x72c6d87ae4d0": 16, "0x72c6d87ae710": 16, "0x72c6d87c0110": 16, "0x72c6d87c1110": 16, "0x72c6d87c1250": 16, "0x72c6d87c18d0": 16, "0x72c6d87c2350": 16, "0x72c6d87c3a50": 16, "0x72c6d87d0690": 16, "0x72c6d87d0d90": 16, "0x72c6d87d1c50": 16, "0x72c6d87d1cd0": 16, "0x72c6d87d3190": 16, "0x72c6d87d3fd0": 16, "0x72c6d87d8250": 16, "0x72c6d87d8e90": 16, "0x72c6d87d96d0": 16, "0x72c6d87da1d0": 16, "0x72c6d87e4810": 16, "0x72c6d87e4f90": 16, "0x72c6d87e62d0": 16, "0x72c6d87e64d0": 16, "0x72c6e01cbd50": 16, "0x72c6e034a510": 16, "0x72c6e034b950": 16, "0x72c6e0351e10": 16, "0x72c6e0353410": 16, "0x72c6e035ca50": 16, "0x72c6e035d5d0": 16, "0x72c6e03660d0": 16, "0x72c72000ddd0": 16, "0x72c73032a850": 16, "0xxxxxxxx": [28, 30], "0xxxxxxxxx": [28, 30], "0xxxxxxxxxx": [28, 30], "1": [29, 37, 42, 44, 50, 55, 62, 68, 73, 78, 88, 89, 113, 114, 115, 117, 121, 122, 123, 128, 130, 131, 133, 136, 137, 138, 139, 140, 143, 144, 146, 147, 149, 150, 153, 159, 163, 164, 165, 167, 179, 184, 188, 199, 219, 222, 224, 225, 226, 227, 230, 231, 235, 236, 237, 238, 243, 244, 245, 247, 248, 250, 254, 256, 257, 267, 275, 281, 283, 284, 285, 288, 290, 291, 292, 297, 299, 300, 318, 320, 321, 323, 327, 328, 329, 331, 333, 334, 336, 338, 340, 341, 342, 344, 346, 347, 349, 351, 353, 355, 356, 359], "10": [1, 3, 4, 7, 10, 11, 12, 13, 14, 15, 16, 17, 22, 35, 39, 43, 44, 45, 55, 56, 66, 68, 70, 86, 87, 88, 89, 128, 133, 136, 137, 146, 159, 163, 164, 166, 168, 169, 170, 171, 174, 181, 183, 185, 189, 190, 206, 208, 212, 218, 222, 227, 253, 255, 257, 267, 270, 271, 281, 283, 285, 291, 292, 293, 300, 318, 322, 323, 326, 328, 332, 336, 338, 339, 342, 345, 349, 351, 360], "100": [3, 7, 10, 14, 15, 16, 88, 89, 118, 121, 168, 181, 190, 206, 212, 213, 224, 231, 253, 257, 285, 289, 293, 298, 324, 329, 330, 332], "1000": [9, 10, 15, 88, 89, 159, 163, 198, 201, 206, 210, 317, 320, 324, 326, 327], "10000": [343, 346], "100000000000": [6, 259, 262], "100k": [331, 336], "100th": [285, 289], "101": [17, 22, 318, 323, 355, 359, 360], "1010": [285, 292], "10127169e": [137, 146], "10129036e": [267, 284], "10140711e": [137, 146], "10182568e": [137, 146], "101_01_anyscale_intro_workspac": 100, "101_02_anyscale_development_intro": 100, "101_03_anycale_compute_runtime_intro": 100, "101_04_anyscale_storage_opt": 100, "101_05_anyscale_logging_and_metr": 100, "101_06_anyscale_intro_job": 100, "101_07_anyscale_intro_servic": 100, "101_08_anyscale_collaboration_intro": 100, "101_09_anyscale_org_setup": 100, "102": [17, 22], "1024": [3, 6, 9, 10, 35, 39, 181, 183, 198, 201, 202, 206, 212, 259, 262, 343, 346], "10279503e": [267, 284], "10307": 14, "104": [128, 133, 136, 137, 139], "10526211e": [267, 284], "10536157": [267, 283], "10537948e": [267, 284], "105m": [164, 167, 168], "10628068e": [137, 146], "10776436e": [267, 284], "10807291e": [267, 284], "10863163e": [267, 284], "10879738e": [267, 284], "108934": 13, "10893423855304718": 13, "10937572e": [137, 146], "10954670e": [267, 284], "10956261e": [267, 284], "10_000": [6, 259, 262, 263, 324, 326], "10am": [267, 283], "10m": [330, 336], "11": [1, 12, 13, 14, 137, 141, 144, 169, 170, 267, 283, 293, 300, 360], "11016287e": [267, 284], "11058047e": [267, 284], "11085677e": [267, 284], "110m": [164, 167, 168], "112": [128, 136, 137, 143, 146], "11493243e": [267, 284], "11518174e": [137, 146], "11712754e": [267, 284], "11721872": [267, 283], "11745796e": [267, 284], "11773282e": [137, 146], "11788076e": [267, 284], "118": [128, 130], "11807573e": [137, 146], "11824808e": [137, 146], "11897744e": [267, 284], "119": [128, 130], "11912012e": [137, 146], "11_10": [293, 300], "11th": [267, 283], "12": [1, 4, 12, 13, 43, 46, 53, 58, 66, 73, 86, 87, 128, 136, 137, 143, 144, 169, 170, 171, 174, 278, 286, 294, 312, 332, 345, 360], "120": [128, 130, 136, 137, 146], "12014441e": [267, 284], "12090182e": [137, 146], "12174596e": [267, 284], "12183236e": [267, 284], "12196040e": [137, 146], "12234001e": [267, 284], "1228194534778595": [147, 150], "123": [128, 130, 168], "123456": [35, 38], "12422097e": [137, 146], "12468980e": [267, 284], "12480514e": [267, 284], "125": [128, 130], "12500": [285, 288, 291, 292], "12501": [285, 292], "12502": [285, 292], "12503": [285, 292], "12504": [285, 292], "12587933e": [267, 284], "12685782e": [267, 284], "127": [1, 128, 130, 147, 150, 155, 158, 169, 170], "128": [5, 7, 13, 14, 128, 130, 224, 229, 253, 256, 257, 267, 283, 324, 327, 343, 347, 349], "12821269e": [267, 284], "12832280e": [267, 284], "12841654e": [267, 284], "128k": [101, 102, 105, 118, 124], "12907687e": [267, 284], "12912727e": [267, 284], "12939501e": [267, 284], "12959743e": [137, 146], "129887": 13, "12m47": [128, 136], "12m52": [128, 136], "12th": [267, 283], "12x": 14, "12xlarg": [128, 133, 136, 137, 139, 143, 146, 159, 163], "13": [12, 14, 43, 46, 53, 58, 128, 136, 137, 146, 339, 360], "13000": [285, 291, 292], "13086134e": [267, 284], "13093004e": [137, 146], "13095595e": [267, 284], "13100": [285, 291, 292], "13238472e": [267, 284], "13421358e": [137, 146], "13470632e": [137, 146], "13547181e": [267, 284], "13586960e": [267, 284], "13600": [285, 291, 292], "13700": [285, 291, 292], "13730657e": [137, 146], "138": [13, 128, 136, 137, 146], "13803817e": [267, 284], "13828215e": [267, 284], "13841531e": [267, 284], "13845997e": [137, 146], "13898420e": [137, 146], "13922286e": [137, 146], "13934547e": [137, 146], "13b": [110, 112, 117, 118, 124], "13m0": [137, 146], "13m35": [147, 150], "13m37": [128, 136], "13m5": [137, 146], "14": [110, 112, 267, 283, 293, 300, 319, 324, 326, 332, 339, 345, 352], "140": [110, 112], "14019522e": [267, 284], "14075851e": [137, 146], "140gb": [110, 112, 114], "14105418e": [137, 146], "14159100e": [267, 284], "14268738e": [267, 284], "144": [128, 130], "14403330e": [137, 146], "14443852e": [267, 284], "14489758e": [137, 146], "14533243e": [267, 284], "14597031e": [137, 146], "146": [128, 130], "14656349e": [267, 284], "14693624e": [137, 146], "14703774e": [267, 284], "14777484e": [267, 284], "14787792e": [267, 284], "14892137e": [267, 284], "149": [128, 130], "14971709e": [267, 284], "14gb": [101, 102, 106], "14th": [267, 283], "15": [3, 5, 12, 13, 28, 29, 35, 39, 43, 44, 45, 53, 55, 56, 66, 70, 118, 123, 155, 158, 181, 189, 267, 281, 283, 293, 300, 341], "150": [128, 130, 267, 281, 283], "15048778e": [137, 146], "15072963e": [267, 284], "150m": 168, "151": [128, 130], "15157820e": [267, 284], "15213782e": [137, 146], "15247765e": [267, 284], "153": [128, 130], "15391724e": [267, 284], "15394783": [267, 283], "15416703e": [137, 146], "15428728e": [267, 284], "15451038e": [267, 284], "15464866e": [137, 146], "155": [9, 198, 201], "15531293e": [267, 284], "15545475e": [137, 146], "15553670e": [267, 284], "15556864e": [267, 284], "15585802e": [267, 284], "155m": 168, "156": 12, "15658525e": [267, 284], "15747452e": [267, 284], "15786707e": [267, 284], "158": [128, 130], "15815112e": [137, 146], "15844142e": [267, 284], "15874708e": [267, 284], "15949178e": [137, 146], "15_08": 12, "15_15": 12, "15m32": [128, 136], "15x": 13, "16": [12, 13, 14, 110, 116, 229, 267, 283, 293, 300, 317, 319, 338, 340, 352, 353], "160": [6, 110, 112, 128, 130, 136, 137, 143, 259, 262], "161": [128, 130], "16129310e": [267, 284], "16136428e": [137, 146], "16142738e": [267, 284], "162": [128, 130], "16249922e": [267, 284], "16273381e": [267, 284], "16315296e": [267, 284], "163491": 12, "163492": 12, "16403189e": [137, 146], "16499318e": [137, 146], "16707636e": [267, 284], "168": [128, 136, 137, 146, 343, 344, 345], "16848189e": [267, 284], "1693310400": 168, "16946062e": [267, 284], "16959971e": [137, 146], "16970104e": [267, 284], "16m0": [137, 146], "16m2": [128, 136], "16m5": [137, 146], "16m52": [128, 136], "16m7": [128, 136], "16th": [267, 281, 283], "16xlarg": [159, 163], "17": [12, 17, 22, 43, 46, 47, 53, 58, 59, 66, 75, 86, 87, 110, 116, 267, 283, 285, 288], "17139973e": [137, 146], "17145060e": [267, 284], "172": [17, 22], "17217854e": [137, 146], "17218718e": [267, 284], "17278847e": [267, 284], "17298350e": [137, 146], "17306670e": [267, 284], "1732276209": 13, "1732276227": 13, "17342269e": [267, 284], "1737": [137, 141], "17392734e": [137, 146], "17458829e": [267, 284], "17499933e": [137, 146], "17503238e": [267, 284], "17549804e": [267, 284], "175m": [164, 167, 168], "17605399e": [267, 284], "17623755e": [267, 284], "17677706e": [267, 284], "17716263e": [267, 284], "17771959e": [137, 146], "1783": [137, 141], "17952654e": [267, 284], "17967087e": [137, 146], "17982033e": [267, 284], "17th": [267, 283], "17x": [128, 130], "18": [13, 35, 40, 43, 47, 53, 59, 66, 75, 86, 87, 110, 116, 225, 226, 231, 267, 283, 350, 351], "18025970e": [267, 284], "180m": [164, 167, 168], "18165373e": [267, 284], "18200418e": [267, 284], "18251920e": [137, 146], "18252768e": [267, 284], "18264270e": [267, 284], "1833": [137, 141], "18500029e": [267, 284], "18641007e": [137, 146], "18707759e": [267, 284], "1879": [1, 169, 170], "1881": [137, 141], "18899436e": [267, 284], "18926680e": [267, 284], "18932852e": [267, 284], "18th": [267, 283], "19": [12, 14, 86, 87, 110, 116, 267, 283, 317, 319, 343, 345, 350, 352], "19056532e": [137, 146], "19172417e": [267, 284], "19192507e": [267, 284], "19229007e": [137, 146], "19254841e": [267, 284], "19263542e": [137, 146], "19275388e": [267, 284], "192gb": [128, 133, 136, 137, 139, 143, 146], "19408388e": [267, 284], "19601890e": [267, 284], "19630387e": [267, 284], "19634366e": [137, 146], "1967": [285, 288], "19670653e": [267, 284], "19684739e": [267, 284], "19716156e": [267, 284], "19767813e": [267, 284], "19797611e": [267, 284], "19835320e": [267, 284], "19861914e": [267, 284], "19884178e": [267, 284], "199": [128, 136, 137, 146], "19939610e": [137, 146], "1995": [267, 283], "19986786e": [267, 284], "19m52": [128, 136], "1_000_000": [3, 181, 189], "1d": [324, 327], "1e": [5, 6, 7, 13, 14, 137, 140, 143, 224, 228, 238, 240, 245, 247, 253, 256, 257, 259, 262, 293, 299, 324, 327, 330, 334, 343, 347, 350, 356], "1f": [343, 345], "1gb": [9, 198, 201], "1h34m20": 13, "1m": [330, 336], "1m10": 16, "1m15": [137, 139], "1mb": [159, 163], "1pb": [159, 163], "1st": [3, 181, 184, 267, 281, 283], "1xt4": [12, 13, 14, 128, 136, 137, 146], "2": [29, 30, 42, 44, 45, 55, 56, 78, 80, 81, 92, 93, 112, 115, 117, 121, 122, 123, 128, 130, 136, 137, 139, 143, 144, 146, 147, 150, 153, 159, 163, 164, 165, 167, 185, 188, 199, 203, 219, 222, 224, 225, 227, 229, 237, 238, 241, 254, 255, 257, 267, 271, 275, 281, 283, 284, 285, 290, 291, 292, 299, 300, 305, 306, 307, 315, 318, 323, 331, 334, 336, 340, 346, 347, 349, 351, 354, 355, 359, 360], "20": [1, 7, 12, 13, 14, 17, 22, 86, 87, 110, 116, 137, 143, 144, 169, 170, 253, 255, 257, 293, 300, 317, 319, 324, 326, 330, 332, 334, 337, 339, 343, 347], "200": [267, 281, 283, 285, 289], "20022464e": [137, 146], "20059741e": [137, 146], "20093006e": [267, 284], "200m": 168, "20103974e": [267, 284], "20113872e": [267, 284], "2012": [17, 22, 267, 283], "20136715e": [137, 146], "2014": [343, 344, 345], "2015": [267, 283], "20152864e": [267, 284], "2017": [267, 281, 283], "20175812e": [267, 284], "2021": [4, 12, 171, 174], "2023": [9, 198, 205], "2024": [9, 10, 12, 13, 14, 15, 168, 198, 205, 206, 217], "20241398e": [137, 146], "2025": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 69, 86, 87, 100, 101, 102, 103, 110, 111, 118, 119, 123, 137, 144, 155, 156, 159, 160, 164, 165, 169, 170, 171, 172, 175, 176, 181, 182, 191, 192, 198, 199, 206, 207, 253, 254, 259, 260, 266, 267, 279, 285, 287, 293, 295, 300, 302, 307, 313], "20302368700504303": 14, "20312032e": [267, 284], "20320046e": [137, 146], "20370263e": [267, 284], "20407227e": [267, 284], "20495850e": [137, 146], "20567256e": [267, 284], "20587808e": [267, 284], "205m": 168, "20716389e": [267, 284], "20745975e": [137, 146], "2076": [137, 141], "20779848e": [137, 146], "20950916e": [137, 146], "20m42": [128, 136], "20m47": [128, 136], "20m52": [128, 136], "20yr": [267, 283], "21": [13, 17, 22, 293, 300], "210m": 168, "21142501e": [137, 146], "21206143e": [267, 284], "213": [128, 130], "21334969e": [267, 284], "21351314e": [137, 146], "21377383e": [267, 284], "21380231e": [267, 284], "214": [128, 130], "21405767e": [267, 284], "2147483648": [3, 181, 187], "21503563e": [267, 284], "21508265e": [267, 284], "21584712e": [267, 284], "216": [128, 130], "21600205e": [267, 284], "21627375e": [137, 146], "21637220e": [267, 284], "217": [128, 130], "21715315e": [137, 146], "21745682e": [267, 284], "21776196e": [267, 284], "21796946e": [267, 284], "219": [128, 130], "21971425e": [267, 284], "21988237e": [137, 146], "21m32": [128, 136], "21m37": [128, 136], "21st": [267, 283], "22": [13, 137, 144, 267, 283, 293, 300], "220": [128, 130], "22013104e": [137, 146], "22061956e": [137, 146], "221": [128, 130], "22192677e": [267, 284], "222": [14, 128, 130], "22204229e": [137, 146], "22234142e": [267, 284], "22277103e": [267, 284], "223": [128, 130], "22332841e": [267, 284], "224": [317, 319, 323, 350, 351, 352, 353], "22424790e": [137, 146], "22458421e": [267, 284], "225": [128, 130, 350, 353], "22552105e": [267, 284], "22592783e": [137, 146], "227": [128, 130], "228": [128, 130], "22804798e": [267, 284], "22867932e": [267, 284], "22891478e": [267, 284], "229": [350, 353], "22918031e": [267, 284], "2292557954788208": [147, 150], "22967193e": [267, 284], "2298": [137, 141], "22_06": 13, "22_11": 13, "22nd": [267, 283], "23": [12, 267, 283, 317, 319, 350, 352], "230": [128, 130], "23042464e": [137, 146], "23053056e": [267, 284], "23069037e": [267, 284], "23083012e": [267, 284], "23107583e": [267, 284], "23113478e": [267, 284], "23149785e": [137, 146], "232": [128, 130], "23204021e": [267, 284], "233": [128, 130], "23302741e": [267, 284], "23314434e": [267, 284], "234": [128, 130], "23478852e": [267, 284], "235": [128, 130], "235m": [164, 167, 168], "23639209e": [267, 284], "23694149e": [267, 284], "23694418e": [267, 284], "237": [128, 130], "23702966e": [267, 284], "23758684e": [137, 146], "23793101e": [267, 284], "23815991e": [267, 284], "239": [128, 130], "23926058e": [137, 146], "23960292e": [267, 284], "23972368e": [137, 146], "23982316e": [267, 284], "23x": [293, 300], "24": [3, 12, 17, 22, 181, 187, 343, 344, 345], "24085984e": [267, 284], "240m": [164, 167, 168], "24192730e": [267, 284], "24219281e": [267, 284], "24348718e": [137, 146], "24473451e": [267, 284], "245m": [164, 167, 168], "24615501e": [267, 284], "24660867e": [137, 146], "24661736e": [137, 146], "24854468e": [267, 284], "24885803e": [267, 284], "24984046e": [267, 284], "24xlarg": [159, 163], "25": [12, 13, 14, 43, 44, 53, 55, 66, 68, 137, 139, 293, 300], "250": [285, 289], "25000": [285, 288, 289, 290], "25093237e": [137, 146], "25101686e": [137, 146], "25175510e": [267, 284], "25248020e": [267, 284], "25258800e": [267, 284], "25268223e": [267, 284], "25294994e": [267, 284], "25365251e": [137, 146], "25387060e": [267, 284], "25401214e": [267, 284], "255": [7, 14, 15, 253, 255, 317, 319], "25536823e": [137, 146], "25569642e": [137, 146], "256": [6, 13, 137, 140, 143, 144, 159, 163, 259, 262, 263, 317, 319, 350, 352], "25669474e": [267, 284], "25707304e": [267, 284], "25723777e": [267, 284], "25795130e": [267, 284], "25806283e": [267, 284], "25825119e": [137, 146], "25883007e": [137, 146], "25912409e": [267, 284], "25934454e": [267, 284], "25_924022_2383": 12, "25m22": [128, 136], "25m27": [128, 136], "25th": [267, 283], "26": [13, 14, 110, 112, 293, 300], "26000811e": [267, 284], "26009388e": [267, 284], "26060665e": [137, 146], "26151049e": [137, 146], "26156314e": [267, 284], "26208720e": [267, 284], "26232028e": [137, 146], "26244992e": [267, 284], "26258478e": [267, 284], "26309279e": [267, 284], "26314560e": [267, 284], "26320729e": [267, 284], "26351237e": [267, 284], "26371822e": [267, 284], "264131": 14, "26429361e": [267, 284], "26502474e": [267, 284], "26629007e": [137, 146], "26750994e": [137, 146], "26772380e": [137, 146], "26816351e": [267, 284], "26885992e": [137, 146], "26948217e": [267, 284], "26990714e": [267, 284], "27": [12, 13, 128, 133, 136, 267, 281, 283], "27106623e": [267, 284], "27159020e": [267, 284], "27211": 14, "27212": 14, "27213278e": [267, 284], "27219909e": [267, 284], "27320850e": [137, 146], "27394149e": [267, 284], "27448589e": [137, 146], "27486494e": [267, 284], "27514724e": [267, 284], "27598614e": [267, 284], "27724269e": [137, 146], "27818017e": [267, 284], "2784426808357239": 13, "278443": 13, "27902825e": [267, 284], "27904241e": [267, 284], "27935463e": [267, 284], "27x": [293, 300], "28": [10, 11, 13, 14, 15, 16, 164, 167, 206, 212, 218, 222, 224, 226, 293, 300], "28050": 14, "28076579e": [267, 284], "28086493e": [267, 284], "28125295e": [267, 284], "28131025e": [267, 284], "28160176e": [267, 284], "28278339e": [137, 146], "28330866e": [267, 284], "28378880e": [137, 146], "28415197e": [267, 284], "28529142e": [267, 284], "28545947e": [267, 284], "28572544e": [267, 284], "28628640e": [267, 284], "28712449e": [267, 284], "28749549e": [267, 284], "28796948e": [267, 284], "28858958e": [267, 284], "28869668e": [267, 284], "28947487e": [267, 284], "28947702e": [267, 284], "28m22": [128, 136], "28m27": [128, 136], "28th": [267, 283], "28x28": [7, 14, 253, 255], "29": [12, 13, 14, 118, 123], "29297644e": [267, 284], "29473785e": [267, 284], "29535252e": [267, 284], "29538319e": [137, 146], "29549801e": [137, 146], "29641497e": [137, 146], "29715446e": [267, 284], "29792884e": [267, 284], "29807210e": [267, 284], "29825398e": [267, 284], "29964035e": [267, 284], "29_09": 14, "29t10": 168, "2a": [43, 45, 53, 56, 128, 133, 136, 137, 139, 143, 146], "2b": [43, 45, 53, 56], "2cpu": [43, 50, 53, 62], "2d": [7, 14, 253, 255, 330, 336], "2e": [317, 320], "2f": [3, 88, 89, 137, 146, 181, 183, 330, 336, 337, 341], "2m12": [128, 133], "2m17": [128, 133], "2m57": [128, 133], "2nd": [3, 181, 184, 267, 283], "2xlarg": [128, 136, 137, 139, 146], "2ykut_ijz8q8gwt5vphvitzshksddol6msszjxzwe5a": [110, 115], "3": [31, 42, 44, 51, 55, 64, 68, 78, 84, 85, 113, 114, 115, 120, 121, 122, 125, 128, 130, 137, 139, 140, 141, 143, 144, 146, 147, 150, 153, 159, 163, 164, 167, 168, 180, 189, 199, 212, 213, 219, 224, 225, 226, 227, 237, 238, 240, 245, 247, 248, 254, 256, 264, 267, 277, 278, 283, 284, 285, 286, 288, 292, 294, 299, 300, 312, 318, 320, 323, 327, 328, 329, 331, 334, 336, 340, 347, 349, 351, 356], "30": [13, 118, 123, 137, 139, 267, 283, 293, 300, 330, 332, 344, 349], "30062854e": [137, 146], "30101690e": [137, 146], "30219358e": [137, 146], "30242959e": [137, 146], "30407149e": [137, 146], "30422497e": [137, 146], "30472469e": [137, 146], "30478994e": [267, 284], "30517557e": [267, 284], "30540405e": [267, 284], "30551": 13, "30557770e": [267, 284], "30565623e": [267, 284], "30582720e": [267, 284], "30603": 13, "30618355e": [267, 284], "30638674e": [267, 284], "30642965e": [137, 146], "3069": [137, 141], "30711952e": [137, 146], "30715715e": [267, 284], "30746688e": [267, 284], "30834863e": [267, 284], "308870": 13, "30887049436569214": 13, "30980236e": [267, 284], "30981060e": [267, 284], "30999158e": [267, 284], "30min": [343, 345], "30th": [267, 283], "31": [1, 12, 13, 14, 128, 136, 137, 146, 169, 170, 293, 300], "31001920e": [137, 146], "31019164e": [267, 284], "31141504e": [267, 284], "31161788e": [137, 146], "31172134e": [267, 284], "31209707e": [267, 284], "31232068e": [137, 146], "31235719e": [137, 146], "31236637e": [137, 146], "31332219e": [137, 146], "31449399e": [267, 284], "31453001e": [137, 146], "31499174e": [267, 284], "31506741e": [137, 146], "31562141e": [267, 284], "31659403e": [267, 284], "31720269e": [137, 146], "31913936e": [267, 284], "31973000e": [267, 284], "31st": [267, 283], "32": [3, 6, 10, 110, 116, 118, 121, 128, 133, 137, 144, 181, 183, 190, 206, 212, 259, 262, 317, 320, 321, 324, 328, 337, 340], "320": [6, 259, 262], "32145682e": [267, 284], "32192443e": [267, 284], "32214013e": [137, 146], "32244647e": [267, 284], "32266051e": [267, 284], "32296453e": [267, 284], "32373542e": [267, 284], "32400897e": [137, 146], "32401919e": [267, 284], "32454751e": [267, 284], "32564947e": [267, 284], "326001912355423": 14, "32635012e": [267, 284], "32654747e": [267, 284], "3266499161421599": 14, "32665": 14, "32722983e": [267, 284], "32756231e": [267, 284], "32768": [110, 113, 116, 118, 123], "32768166e": [267, 284], "32868871e": [137, 146], "32875013e": [137, 146], "32888782e": [137, 146], "32890965e": [267, 284], "32902160e": [267, 284], "32918817e": [267, 284], "3297": [137, 141], "32b": [118, 123, 124], "32gb": [14, 128, 136, 137, 139, 146], "32k": [101, 102, 105, 118, 124], "32m": [13, 14, 293, 300], "33": [267, 281, 283, 284], "33019698e": [137, 146], "33023707e": [267, 284], "33163792e": [267, 284], "33167297e": [267, 284], "33217961e": [137, 146], "33281359e": [267, 284], "33300245e": [267, 284], "33315668e": [267, 284], "3336": [137, 141], "33374238e": [137, 146], "33572224e": [267, 284], "33688403e": [267, 284], "33728483e": [267, 284], "33760041e": [267, 284], "33768886e": [267, 284], "33827804e": [267, 284], "33832851e": [267, 284], "33951919e": [267, 284], "34": [13, 14, 15, 128, 136, 137, 141, 267, 284], "34206163e": [267, 284], "34323069e": [267, 284], "3434": [137, 141], "34348310e": [267, 284], "34402555e": [137, 146], "34404474e": [137, 146], "34449054e": [267, 284], "34450802e": [137, 146], "34561165e": [267, 284], "34613437e": [267, 284], "34668782e": [267, 284], "34740751e": [267, 284], "34842591e": [267, 284], "34856862e": [137, 146], "34950879e": [137, 146], "34999743e": [267, 284], "35": [13, 14, 267, 284, 293, 300], "35016027e": [267, 284], "35024523e": [267, 284], "35026570e": [267, 284], "35150540e": [137, 146], "35173336e": [137, 146], "35185423e": [267, 284], "35189386e": [267, 284], "35268092e": [267, 284], "35481167e": [267, 284], "35504": [137, 144], "35658276e": [137, 146], "35665376e": [267, 284], "35665385e": [267, 284], "35833579e": [267, 284], "35872006e": [137, 146], "35873899e": [267, 284], "35882646e": [137, 146], "35890651e": [267, 284], "36": [14, 110, 116, 137, 140, 144, 293, 300], "36022663e": [137, 146], "36150215e": [267, 284], "36222595e": [137, 146], "36240765e": [267, 284], "365191": 13, "36595646e": [267, 284], "36629641e": [137, 146], "36664629e": [137, 146], "36710434e": [267, 284], "36786141e": [267, 284], "36829392e": [267, 284], "36855166e": [267, 284], "36857450e": [267, 284], "36868265e": [267, 284], "36873224e": [267, 284], "3688": [137, 141], "36969042e": [137, 146], "36m": [13, 14, 16, 128, 133, 136, 137, 139, 143, 146, 147, 150, 293, 300], "37": [13, 14, 110, 116], "37044120e": [137, 146], "37080820e": [267, 284], "37142141e": [267, 284], "37153175e": [267, 284], "37196398e": [267, 284], "37221038e": [137, 146], "37391533e": [267, 284], "37459409e": [137, 146], "37605290e": [267, 284], "37751494e": [267, 284], "37764162e": [267, 284], "37784477e": [267, 284], "37850042e": [267, 284], "37893021e": [137, 146], "37964366e": [267, 284], "37986809e": [137, 146], "38": [293, 300], "38109175e": [267, 284], "38115362e": [267, 284], "38248950e": [137, 146], "38281021e": [267, 284], "384": [110, 116, 267, 283, 284], "38448": 14, "384480": 14, "38451725e": [267, 284], "38496622e": [267, 284], "38554320e": [267, 284], "38563488e": [267, 284], "38644290e": [137, 146], "38687068e": [267, 284], "38798335e": [267, 284], "38858718e": [267, 284], "38910706e": [267, 284], "38927615e": [137, 146], "38930988e": [137, 146], "39053045e": [267, 284], "39066431e": [267, 284], "39178723e": [137, 146], "39178753e": [137, 146], "39268537e": [267, 284], "39287962e": [267, 284], "39421923e": [267, 284], "39532143e": [267, 284], "39561000e": [267, 284], "39590132e": [267, 284], "39660065e": [267, 284], "39745891e": [267, 284], "39747020e": [267, 284], "39796034e": [267, 284], "39855982e": [267, 284], "3989": [137, 141], "39904258e": [137, 146], "39905545e": [267, 284], "39945886e": [137, 146], "3b": [118, 122], "3d": [324, 327], "3f": [337, 340, 341, 342], "3m10": [137, 139], "3m40": [137, 143], "3m45": [137, 143], "3rd": [267, 283], "3x": [147, 151], "3x3": [224, 226, 237], "4": [12, 42, 73, 78, 88, 89, 111, 112, 113, 114, 121, 122, 123, 128, 129, 130, 131, 133, 137, 138, 139, 140, 143, 146, 147, 148, 153, 164, 167, 168, 172, 185, 199, 213, 219, 224, 225, 254, 257, 260, 267, 283, 284, 285, 292, 320, 321, 328, 331, 334, 336, 340, 347, 349, 353], "40": [13, 168, 267, 283, 285, 288], "4000": [3, 181, 187], "40002957e": [137, 146], "40064341e": [267, 284], "40084913e": [267, 284], "40099749e": [137, 146], "400b": [110, 112, 117, 118, 124], "40222309e": [267, 284], "40240113e": [267, 284], "40254933e": [267, 284], "403": [13, 14], "40336857e": [267, 284], "40409318e": [267, 284], "40434751e": [137, 146], "40456108e": [267, 284], "40510444e": [267, 284], "40537590e": [267, 284], "40543866e": [137, 146], "406": [350, 353], "40600796e": [267, 284], "40880044e": [267, 284], "40937243e": [267, 284], "40g": [110, 113, 114], "41": [1, 169, 170, 293, 300], "41017914e": [137, 146], "41169238e": [267, 284], "41174936e": [137, 146], "41280317e": [267, 284], "41516277e": [267, 284], "41520910e": [267, 284], "41526775e": [267, 284], "41544282e": [137, 146], "41575071e": [267, 284], "41593361e": [137, 146], "41598": [293, 300], "41599": [293, 300], "415m": 168, "41709536e": [267, 284], "41786465e": [267, 284], "41920993e": [267, 284], "41922843e": [267, 284], "41933542e": [267, 284], "41968962e": [267, 284], "41985670e": [267, 284], "42": [4, 171, 174, 330, 332, 337, 339, 350, 353], "42026821e": [137, 146], "42042309e": [137, 146], "42074719e": [137, 146], "420m": 168, "42177847e": [267, 284], "4219": [137, 141], "422321": [293, 300], "42241838e": [267, 284], "42471355e": [267, 284], "42482564e": [267, 284], "42522454e": [137, 146], "42548003e": [267, 284], "42604055e": [267, 284], "42662169e": [267, 284], "42837034e": [267, 284], "42856956e": [267, 284], "42857780e": [267, 284], "42904809e": [267, 284], "42934144e": [137, 146], "42943636e": [267, 284], "43057": 12, "43168001e": [267, 284], "43248991e": [267, 284], "43266308e": [137, 146], "43267226e": [267, 284], "43328887e": [267, 284], "43380117e": [137, 146], "43496174e": [137, 146], "43732": 13, "43747482e": [267, 284], "43779554e": [267, 284], "43806068e": [267, 284], "43821533e": [267, 284], "43902507e": [267, 284], "43916206e": [267, 284], "43940079e": [267, 284], "43996522e": [267, 284], "44": [12, 14], "44072895e": [267, 284], "44087312e": [267, 284], "44139796e": [137, 146], "44193405e": [137, 146], "44237953e": [267, 284], "44269560e": [267, 284], "44299744e": [267, 284], "443": [17, 22], "44326049e": [267, 284], "44500265e": [267, 284], "44628939e": [267, 284], "44769201e": [267, 284], "44773570": 13, "44875658e": [267, 284], "44877301e": [267, 284], "44888324e": [267, 284], "44945610e": [267, 284], "45": [13, 28, 31, 267, 283], "45010501e": [137, 146], "45237213e": [267, 284], "45387161e": [267, 284], "45463008e": [267, 284], "45558545e": [267, 284], "45565206e": [267, 284], "45596695e": [267, 284], "456": [168, 350, 353], "45615": [267, 281, 284], "45630976e": [267, 284], "45667797e": [267, 284], "45728597e": [137, 146], "45732313e": [137, 146], "45758486e": [137, 146], "45803327e": [267, 284], "45804777e": [267, 284], "45845979e": [267, 284], "45928478e": [267, 284], "45977615e": [267, 284], "45981687e": [267, 284], "46": [43, 46, 53, 58, 293, 300], "46038486e": [267, 284], "46128924e": [267, 284], "46165411e": [267, 284], "46190545e": [267, 284], "46210968e": [267, 284], "46212946e": [267, 284], "46281177e": [267, 284], "46350823e": [267, 284], "46354072e": [267, 284], "46371266e": [267, 284], "46490431e": [267, 284], "46582437e": [137, 146], "46654081e": [267, 284], "46689427e": [137, 146], "46736982e": [267, 284], "46787928e": [267, 284], "46938870e": [267, 284], "46950224e": [137, 146], "46954274e": [267, 284], "46988708e": [137, 146], "47110615e": [267, 284], "47215438e": [137, 146], "47293536e": [267, 284], "47399181e": [267, 284], "47439016e": [267, 284], "47477984e": [267, 284], "47517592e": [137, 146], "475m": 168, "47602344e": [267, 284], "47635157e": [267, 284], "47678024e": [267, 284], "47783903e": [267, 284], "47834991e": [267, 284], "47863048e": [137, 146], "47896763e": [267, 284], "47925606e": [267, 284], "47997355e": [267, 284], "48": [13, 110, 116, 159, 163, 343, 344, 345], "48037392e": [137, 146], "48048115e": [267, 284], "48062438e": [137, 146], "480m": 168, "48107335e": [137, 146], "48195577e": [137, 146], "48344308e": [137, 146], "48345065e": [137, 146], "485": [350, 353], "485m": 168, "48663167e": [267, 284], "48783788e": [137, 146], "48813944e": [267, 284], "48855758e": [267, 284], "48856053e": [267, 284], "48863769e": [137, 146], "48883224e": [137, 146], "48921371e": [137, 146], "48cpu": [128, 133, 136, 137, 139, 143, 146], "49": [13, 101, 102, 108, 110, 115, 293, 300], "49026504e": [267, 284], "49106100e": [267, 284], "49187856e": [267, 284], "49242997e": [137, 146], "49249397e": [267, 284], "49257514e": [267, 284], "49290550e": [137, 146], "49307863e": [267, 284], "49331436e": [267, 284], "49361154e": [267, 284], "49398082e": [137, 146], "49425897e": [267, 284], "49499828e": [137, 146], "49631714e": [267, 284], "49642932e": [267, 284], "49676067e": [137, 146], "49779003e": [267, 284], "49808554e": [267, 284], "49876371e": [267, 284], "49959813e": [267, 284], "4f": [330, 334], "4k": [101, 102, 105, 118, 124], "4m30": [137, 143], "4m37": 14, "4th": [267, 283], "4xt4": [128, 133, 136, 137, 139, 143, 146], "5": [2, 7, 12, 16, 29, 37, 44, 55, 68, 118, 121, 122, 124, 128, 136, 137, 144, 146, 147, 153, 164, 167, 168, 175, 180, 184, 185, 190, 199, 212, 224, 228, 232, 237, 238, 243, 253, 255, 256, 257, 267, 283, 284, 285, 291, 292, 298, 321, 323, 326, 331, 334, 347, 349, 356, 357, 359], "50": [10, 13, 15, 28, 30, 110, 116, 206, 210, 285, 291, 305, 306, 307, 308, 309, 315, 316, 317, 323, 324, 329, 337, 340, 342], "500": [7, 159, 163, 253, 257, 317, 319, 350, 352, 353], "50048220e": [137, 146], "50099332e": [267, 284], "50164117e": [267, 284], "50424745e": [267, 284], "50576949e": [267, 284], "50653918e": [267, 284], "50681949e": [137, 146], "50758056e": [137, 146], "50785267e": [267, 284], "50843680e": [137, 146], "50856599e": [137, 146], "50892793e": [267, 284], "50946071e": [267, 284], "50_per_index": [10, 15, 16, 206, 210, 212, 215], "51002133e": [267, 284], "51042950e": [267, 284], "51119480e": [267, 284], "51127565e": [137, 146], "512": [13, 128, 136, 137, 140, 143, 144, 238, 244, 245, 248, 250, 330, 334], "51281480e": [267, 284], "51315774e": [267, 284], "51320172e": [267, 284], "51369202e": [137, 146], "51379186e": [137, 146], "51383309e": [267, 284], "51503164e": [267, 284], "51518283e": [267, 284], "51617597e": [267, 284], "51706925e": [137, 146], "51739728e": [267, 284], "51786283e": [267, 284], "51865722e": [267, 284], "52": [128, 130], "52094781e": [137, 146], "52096841e": [267, 284], "52135583e": [267, 284], "52154651e": [267, 284], "522000": [137, 144], "52212310e": [137, 146], "52294970e": [137, 146], "52319247e": [137, 146], "52324782e": [267, 284], "525325868955": [17, 22], "52539432e": [267, 284], "52647242e": [267, 284], "52814901e": [137, 146], "53": [13, 128, 130], "53022516e": [137, 146], "53134540e": [137, 146], "53193595e": [267, 284], "53348368e": [137, 146], "53377999e": [267, 284], "53381238e": [267, 284], "53421593e": [137, 146], "53534813e": [267, 284], "53647423e": [267, 284], "53650725e": [137, 146], "53716086e": [267, 284], "53754605e": [267, 284], "53796408e": [137, 146], "53836450e": [137, 146], "53959617e": [267, 284], "53998228e": [267, 284], "54": [12, 128, 130, 147, 151, 293, 300, 337, 338, 339], "5404948": 12, "54087022e": [137, 146], "54091814e": [137, 146], "54113623e": [267, 284], "54230055e": [267, 284], "54291149e": [267, 284], "54304842e": [267, 284], "54419112e": [137, 146], "54450669e": [267, 284], "54470068e": [137, 146], "54546804e": [267, 284], "54573391e": [267, 284], "54621774e": [267, 284], "54645248e": [267, 284], "54685497e": [267, 284], "5492": [137, 141], "55": 13, "55025972e": [267, 284], "55034113e": [137, 146], "55071461e": [137, 146], "55099240e": [267, 284], "550_000": [6, 259, 263], "5529555": 12, "5552995": 12, "55531311e": [137, 146], "55601753e": [267, 284], "55613178e": [267, 284], "55635225e": [267, 284], "55699139e": [267, 284], "5588189": 12, "55886114e": [137, 146], "55900936e": [267, 284], "55923614e": [267, 284], "55929470e": [137, 146], "56": [128, 133], "56011016e": [267, 284], "56031644e": [137, 146], "56041365e": [267, 284], "56115811e": [267, 284], "56188577e": [267, 284], "56204057e": [137, 146], "56274483e": [267, 284], "56360346e": [137, 146], "56389399e": [267, 284], "56662523e": [267, 284], "56688479e": [267, 284], "56718080e": [267, 284], "56892097e": [137, 146], "56896329e": [267, 284], "56896693e": [267, 284], "56920916e": [267, 284], "57": [110, 116, 128, 130], "57008413e": [267, 284], "57041806e": [137, 146], "57156193e": [137, 146], "57156566e": [267, 284], "57160601e": [267, 284], "57175567e": [267, 284], "57203451e": [137, 146], "57226282e": [267, 284], "57393692e": [267, 284], "57440994e": [267, 284], "57462114e": [137, 146], "57502690e": [267, 284], "57504702e": [137, 146], "57536250e": [267, 284], "57737350e": [267, 284], "57877821e": [267, 284], "57970206e": [267, 284], "58": 12, "580": [159, 163, 337, 338, 339], "58003160e": [267, 284], "58086956e": [267, 284], "580k": [337, 339], "580x580x3": [159, 163], "58155808e": [267, 284], "58189368e": [267, 284], "58204031e": [137, 146], "58261052e": [267, 284], "58301930e": [267, 284], "58354291e": [137, 146], "58395988e": [137, 146], "58452298e": [267, 284], "58531007e": [267, 284], "58614498e": [267, 284], "58722073e": [137, 146], "58806413e": [267, 284], "58818898e": [267, 284], "58824509e": [267, 284], "58904049e": [267, 284], "58911937e": [137, 146], "59": [12, 118, 123, 128, 130], "59060508e": [267, 284], "59105931e": [267, 284], "59115836e": [137, 146], "59260547e": [267, 284], "59284808e": [137, 146], "59323025e": [137, 146], "593301": [137, 144], "59349391e": [267, 284], "59405363e": [137, 146], "59438765e": [267, 284], "59552705e": [267, 284], "59555081e": [267, 284], "59640062e": [137, 146], "59683321e": [137, 146], "59728657e": [267, 284], "59733031e": [267, 284], "59756446e": [137, 146], "59785998e": [137, 146], "59831755e": [267, 284], "59940967e": [137, 146], "59980466e": [267, 284], "5x": [147, 151], "6": [14, 15, 16, 137, 146, 155, 158, 164, 167, 168, 199, 267, 283, 284, 326, 338, 347, 352], "60": [7, 14, 128, 130, 136, 137, 146, 164, 167, 224, 226, 253, 255], "60005671e": [137, 146], "60166726e": [137, 146], "60254669e": [267, 284], "60417205": 12, "60533416e": [137, 146], "60535234e": [137, 146], "60535276e": [137, 146], "60559933e": [267, 284], "60586494e": [137, 146], "60700288e": [267, 284], "60769555e": [267, 284], "60900429e": [267, 284], "60926636e": [267, 284], "60929009e": [267, 284], "61124960e": [267, 284], "61173157e": [267, 284], "61207989e": [137, 146], "61210120e": [267, 284], "61377022e": [267, 284], "61495838e": [267, 284], "61530429e": [137, 146], "61626707e": [267, 284], "61644816e": [267, 284], "61783993e": [267, 284], "61808310e": [267, 284], "62014505e": [267, 284], "62131321e": [267, 284], "62141061e": [137, 146], "62198240e": [137, 146], "62209135e": [267, 284], "62317707e": [267, 284], "62408248e": [267, 284], "62470581e": [267, 284], "62640566e": [137, 146], "62676229e": [267, 284], "62701386e": [137, 146], "62736428e": [137, 146], "62778306e": [137, 146], "62782574e": [267, 284], "6288": [137, 141], "6290559": 12, "629055917263031": 12, "62961054e": [137, 146], "63": [293, 300], "63008086e": [267, 284], "63077107e": [267, 284], "63363028e": [267, 284], "63391770e": [267, 284], "63410094e": [267, 284], "63487554e": [137, 146], "63496330e": [267, 284], "63529071e": [267, 284], "63559180e": [267, 284], "63560931e": [137, 146], "63716504e": [267, 284], "63769671e": [267, 284], "6379": [155, 158], "63797104e": [137, 146], "63952804e": [137, 146], "64": [5, 6, 7, 13, 14, 128, 131, 137, 139, 146, 224, 227, 253, 256, 257, 259, 262, 267, 275, 283, 330, 333, 334, 343, 346, 350, 356, 359], "640": [6, 259, 262], "64019221e": [137, 146], "64042781e": [267, 284], "641551": 13, "64302550e": [137, 146], "64337176e": [137, 146], "64353992e": [267, 284], "64574068e": [267, 284], "64641732e": [137, 146], "64772916e": [137, 146], "64824829e": [267, 284], "64865339e": [137, 146], "64927173e": [267, 284], "65101083e": [267, 284], "65246567e": [137, 146], "65288996e": [267, 284], "65297012e": [267, 284], "65307403e": [137, 146], "65330970e": [137, 146], "65584633e": [137, 146], "65683129e": [267, 284], "65831": 14, "65908886e": [267, 284], "66008": 12, "66033891e": [267, 284], "66034375e": [267, 284], "66039094e": [137, 146], "660569": 12, "662196": 12, "66246349e": [267, 284], "662499": 12, "66315883e": [137, 146], "66424632e": [137, 146], "66462481e": [137, 146], "66510823e": [267, 284], "66672035e": [267, 284], "66712171e": [267, 284], "66808441e": [267, 284], "66871315e": [267, 284], "66888866e": [267, 284], "66935217e": [137, 146], "67037442e": [137, 146], "67044029e": [137, 146], "67074795e": [267, 284], "67096058e": [267, 284], "67168414e": [267, 284], "67199332e": [137, 146], "67200039e": [267, 284], "67249476e": [267, 284], "67394597e": [267, 284], "67415068e": [137, 146], "67530221e": [267, 284], "67625724e": [267, 284], "67702921e": [267, 284], "67820978e": [137, 146], "67874378e": [137, 146], "68044616e": [267, 284], "682": [330, 332], "68261507e": [267, 284], "68279577e": [267, 284], "68410710e": [267, 284], "68456510e": [137, 146], "68468618e": [137, 146], "68614852e": [137, 146], "68623075e": [267, 284], "68863142e": [267, 284], "68928802e": [267, 284], "68990344e": [267, 284], "69025257e": [137, 146], "69066399e": [137, 146], "69076526e": [137, 146], "69132429e": [137, 146], "69230062e": [267, 284], "69240460e": [137, 146], "69288528e": [137, 146], "69341841e": [137, 146], "69485952e": [267, 284], "69494259e": [137, 146], "69655108e": [267, 284], "69689246e": [267, 284], "69690510e": [267, 284], "69710606e": [137, 146], "69849765e": [267, 284], "69882979e": [267, 284], "69915810e": [137, 146], "69974936e": [267, 284], "6_000108_000000": [6, 259, 262], "6_2024": 14, "6f": [88, 89, 293, 298], "6m52": [128, 133], "6th": [267, 283], "6x": 14, "7": [7, 12, 14, 15, 137, 141, 146, 164, 167, 168, 199, 224, 227, 253, 256, 257, 267, 283, 284, 321, 334, 340, 347, 352], "70": [110, 112], "70009223e": [137, 146], "70078446e": [267, 284], "70130879e": [267, 284], "70160577e": [267, 284], "70176884e": [267, 284], "70194232e": [137, 146], "70207208e": [267, 284], "70480572e": [267, 284], "70483862e": [267, 284], "70502144e": [267, 284], "70606668e": [267, 284], "70623609e": [267, 284], "70640138e": [267, 284], "70672083e": [137, 146], "70758316e": [267, 284], "70819569e": [137, 146], "70849383e": [267, 284], "70991046e": [267, 284], "70b": [101, 102, 105, 113, 114, 115, 116, 117, 118, 124], "71": 12, "71107422e": [267, 284], "71122622e": [137, 146], "71239424e": [137, 146], "71276686e": [267, 284], "71327189e": [267, 284], "71361454e": [267, 284], "71367359e": [267, 284], "71599907e": [267, 284], "71614686e": [267, 284], "71635705e": [137, 146], "71677093e": [267, 284], "71802861e": [267, 284], "71831726e": [267, 284], "72043703e": [267, 284], "72060728e": [267, 284], "72087287e": [267, 284], "72130489e": [137, 146], "72232352e": [267, 284], "72453296e": [137, 146], "72505680e": [267, 284], "72557199e": [137, 146], "72602186e": [267, 284], "72664893e": [267, 284], "72761095e": [137, 146], "72793153e": [267, 284], "72884393e": [137, 146], "72918749e": [267, 284], "72939146e": [137, 146], "72964428e": [267, 284], "73005784e": [267, 284], "73056117e": [267, 284], "73160497e": [267, 284], "73187131e": [137, 146], "73239648e": [137, 146], "73338448e": [267, 284], "73339425e": [267, 284], "73386657e": [267, 284], "73405361e": [137, 146], "73471044e": [267, 284], "73475703e": [267, 284], "73528048e": [267, 284], "73541475e": [267, 284], "73574477e": [137, 146], "73729736e": [137, 146], "73842654e": [267, 284], "73909385e": [267, 284], "73954112e": [267, 284], "73962507e": [267, 284], "74006203e": [267, 284], "74086124e": [137, 146], "74092592e": [267, 284], "74106744e": [267, 284], "74198601e": [267, 284], "74435990e": [267, 284], "74475431e": [137, 146], "74639919e": [267, 284], "74725649e": [267, 284], "74946982e": [137, 146], "75": [110, 116, 350, 359], "75087003e": [267, 284], "75092961e": [267, 284], "75342406e": [267, 284], "75359203e": [267, 284], "75361482e": [267, 284], "75378935e": [267, 284], "75653571e": [267, 284], "75755769e": [267, 284], "75817132e": [137, 146], "75831634e": [137, 146], "75859511e": [137, 146], "75899668e": [267, 284], "76054996e": [137, 146], "76055871e": [267, 284], "76067518e": [267, 284], "76138058e": [267, 284], "76263633e": [267, 284], "76323075e": [267, 284], "76346910e": [267, 284], "76389585e": [267, 284], "76404205e": [267, 284], "76421027e": [137, 146], "76428038e": [137, 146], "76448804e": [267, 284], "76463524e": [267, 284], "764641": [293, 300], "76478487e": [137, 146], "76568236e": [267, 284], "76666151e": [137, 146], "76702512e": [267, 284], "76708573e": [137, 146], "76715726e": [267, 284], "76780378e": [267, 284], "76795331e": [267, 284], "768": [110, 116], "76873404e": [137, 146], "76875392e": [267, 284], "76900255e": [267, 284], "76936167e": [267, 284], "76972622e": [267, 284], "77": [6, 259, 262], "77095975e": [267, 284], "7721": 14, "7722": 14, "7725": 14, "77374096e": [267, 284], "774": [1, 169, 170], "77459675e": [267, 284], "77555919e": [267, 284], "77573276e": [137, 146], "77590072e": [137, 146], "77606642e": [267, 284], "77690476e": [137, 146], "77768275e": [137, 146], "77784879e": [267, 284], "77830246e": [267, 284], "77848917e": [137, 146], "77938917e": [137, 146], "77998394e": [137, 146], "78": [128, 130], "78041089e": [267, 284], "78072055e": [267, 284], "78197911e": [267, 284], "78284839e": [267, 284], "78317158e": [137, 146], "7834368348121643": 13, "783437": 13, "78439620e": [267, 284], "78455579e": [137, 146], "78474542e": [267, 284], "78646958e": [137, 146], "78715137e": [267, 284], "78778227e": [267, 284], "78793629e": [267, 284], "78836194e": [267, 284], "78844824e": [137, 146], "78927866e": [267, 284], "79095644e": [137, 146], "79144512e": [267, 284], "79200619e": [137, 146], "79223316e": [267, 284], "79288665e": [267, 284], "79409343e": [267, 284], "79455946e": [267, 284], "79587227e": [137, 146], "79640555e": [137, 146], "79642552e": [137, 146], "79656491e": [267, 284], "79822421e": [137, 146], "79875319e": [267, 284], "79880509e": [267, 284], "79887915e": [137, 146], "79888725e": [267, 284], "79926023e": [137, 146], "799808": [293, 300], "79x": [110, 116], "7am": [267, 283], "7b": [101, 102, 106, 110, 112, 117, 118, 124], "7th": [267, 281, 283, 284], "7x": [5, 6, 13, 259, 264], "8": [6, 12, 15, 110, 111, 112, 113, 114, 116, 117, 118, 124, 128, 133, 136, 137, 139, 143, 144, 146, 147, 153, 168, 183, 187, 199, 204, 224, 226, 229, 230, 237, 259, 262, 263, 267, 283, 284, 285, 291, 299, 318, 319, 321, 325, 326, 328, 331, 332, 344, 346, 351, 352, 356, 357], "80": [110, 116, 128, 130, 317, 319, 324, 326, 330, 332, 337, 339], "800": [110, 112], "8000": [0, 4, 11, 12, 16, 101, 102, 108, 110, 114, 118, 121, 122, 123, 147, 150, 164, 167, 171, 174, 218, 222, 307, 309, 316], "80058852e": [267, 284], "80134752e": [267, 284], "80346647e": [267, 284], "80356482e": [267, 284], "80584863e": [267, 284], "80600139e": [267, 284], "80705532e": [137, 146], "80770779e": [267, 284], "80772168e": [267, 284], "80778313e": [267, 284], "80779386e": [267, 284], "8080": [137, 144, 155, 158], "80813038e": [137, 146], "80910456e": [137, 146], "80974126e": [137, 146], "80b": [110, 112, 118, 124], "81101489e": [267, 284], "81141561e": [137, 146], "81150190e": [267, 284], "81153491e": [267, 284], "81209888e": [267, 284], "81231391e": [137, 146], "81385726e": [267, 284], "81385851e": [137, 146], "81416702e": [137, 146], "8147535920143127": 13, "814754": 13, "81557357e": [137, 146], "81605020e": [267, 284], "81611199e": [267, 284], "81677291e": [267, 284], "81750199e": [267, 284], "81875241e": [137, 146], "8192": [101, 102, 108, 118, 121, 122], "81969379e": [267, 284], "81974494e": [267, 284], "81992236e": [267, 284], "82": [128, 130], "82090014e": [137, 146], "82237032e": [267, 284], "82248098e": [267, 284], "82329589e": [137, 146], "82377563e": [267, 284], "82394725e": [137, 146], "82457250e": [137, 146], "82634446e": [267, 284], "8265": [1, 155, 158, 169, 170], "82662451e": [137, 146], "82670507e": [267, 284], "82704625e": [267, 284], "82709087e": [267, 284], "82714777e": [267, 284], "82736081e": [267, 284], "82819171e": [267, 284], "82860744e": [137, 146], "82876396e": [267, 284], "82900697e": [267, 284], "82929088e": [267, 284], "82934299e": [267, 284], "82964134e": [137, 146], "82985464e": [267, 284], "83": [13, 128, 130], "83045536e": [267, 284], "83105981e": [137, 146], "83186028e": [267, 284], "83414386e": [267, 284], "83462293e": [267, 284], "83483610e": [267, 284], "83486040e": [267, 284], "83672294e": [137, 146], "837": [110, 116], "83878148e": [137, 146], "83987024e": [267, 284], "84": [137, 146], "84016372e": [267, 284], "84025085e": [267, 284], "84072098e": [267, 284], "84077434e": [267, 284], "84100178e": [267, 284], "84181255e": [137, 146], "84200376e": [267, 284], "84204574e": [267, 284], "84257627e": [267, 284], "84342591e": [267, 284], "84438897e": [267, 284], "84440348e": [137, 146], "84560393e": [267, 284], "84609653e": [267, 284], "84649059e": [267, 284], "84714490e": [137, 146], "84724203e": [267, 284], "84787306e": [267, 284], "84826868e": [267, 284], "84889567e": [137, 146], "85": [118, 121, 128, 130], "85011083e": [267, 284], "85030222e": [267, 284], "85076341e": [137, 146], "85247302e": [137, 146], "85358205e": [137, 146], "85489166e": [137, 146], "85491417e": [267, 284], "85511327e": [137, 146], "85630648e": [267, 284], "85643661e": [137, 146], "85674404e": [267, 284], "85712914e": [267, 284], "85737485e": [267, 284], "8588": [137, 144], "858816514880031760": [137, 144], "86": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 118, 123], "86025219e": [267, 284], "86067504e": [267, 284], "86104554e": [137, 146], "86110076e": [267, 284], "86166140e": [267, 284], "86302094e": [267, 284], "86359452e": [267, 284], "86366066e": [137, 146], "86417520e": [137, 146], "86425143e": [137, 146], "86543170e": [267, 284], "86662281e": [137, 146], "86763499e": [267, 284], "86765429e": [137, 146], "87054926e": [137, 146], "87068461e": [267, 284], "87136489e": [267, 284], "87145798e": [267, 284], "87155861e": [267, 284], "87222108e": [267, 284], "87272584e": [267, 284], "87305135e": [267, 284], "87395632e": [267, 284], "87472761e": [137, 146], "87498319e": [137, 146], "87503409e": [267, 284], "87604594e": [137, 146], "87622940e": [267, 284], "87672335e": [137, 146], "87687856e": [267, 284], "87722724e": [267, 284], "87785244e": [137, 146], "87823978e": [267, 284], "87826851e": [267, 284], "87948608e": [137, 146], "88": [128, 130], "88060308e": [267, 284], "88125470e": [267, 284], "88136353e": [267, 284], "88193573e": [267, 284], "88231084e": [267, 284], "88282757e": [267, 284], "88321698e": [137, 146], "88339034e": [267, 284], "88483773e": [267, 284], "88547347e": [267, 284], "88624009e": [267, 284], "88648832e": [137, 146], "88702995e": [137, 146], "88834351e": [267, 284], "88834363e": [137, 146], "88852262e": [267, 284], "88920692e": [137, 146], "89050034e": [267, 284], "89084566e": [137, 146], "89146360e": [267, 284], "89203757e": [137, 146], "89227986e": [267, 284], "89326443e": [267, 284], "89382686e": [137, 146], "89384127e": [267, 284], "89393270e": [267, 284], "89414667e": [267, 284], "89481646e": [137, 146], "89499091e": [137, 146], "895000": [137, 144], "89564747e": [267, 284], "89572328e": [137, 146], "89613281e": [267, 284], "89739510e": [267, 284], "89740060e": [267, 284], "89891556e": [267, 284], "89910729e": [267, 284], "89954895e": [137, 146], "8b": [101, 102, 108, 118, 121, 124], "8cpu": [14, 128, 136, 137, 139, 146], "8gb": [43, 50, 53, 62], "8k": [101, 102, 105, 118, 124], "8m20": [137, 146], "8m25": [137, 146], "8m30": [137, 146], "8th": [267, 281, 283], "8xl40": [110, 116], "9": [3, 7, 12, 14, 15, 28, 29, 35, 37, 43, 44, 46, 47, 55, 58, 59, 68, 75, 128, 136, 137, 146, 147, 153, 159, 163, 168, 181, 190, 224, 226, 227, 238, 242, 253, 255, 256, 267, 284, 293, 298, 319, 323, 345, 351, 352], "90": [43, 44, 118, 122, 159, 163, 267, 283], "90151963e": [267, 284], "90165711e": [267, 284], "90290013e": [267, 284], "90306157e": [137, 146], "90392175e": [267, 284], "90476887e": [267, 284], "90511677e": [267, 284], "90528610e": [267, 284], "90639007e": [137, 146], "90640": 14, "90656148e": [267, 284], "90659517e": [267, 284], "90668219e": [267, 284], "90754700e": [137, 146], "90833239e": [267, 284], "91": [13, 128, 130], "91010717e": [267, 284], "91011913e": [267, 284], "91103810e": [137, 146], "91166097e": [137, 146], "91205712e": [267, 284], "91232206e": [137, 146], "91410846e": [137, 146], "91437262e": [267, 284], "91462375e": [267, 284], "91497517e": [137, 146], "91614306e": [137, 146], "91614705e": [267, 284], "91802080e": [267, 284], "91829026e": [137, 146], "91938744e": [267, 284], "92105675e": [137, 146], "92106171e": [267, 284], "92230538e": [267, 284], "92250460e": [267, 284], "92267558e": [267, 284], "92389667e": [137, 146], "92452052e": [267, 284], "92482489e": [137, 146], "92608377e": [267, 284], "92633970e": [267, 284], "92644155e": [137, 146], "92704949e": [267, 284], "92810488e": [137, 146], "92848936e": [267, 284], "92968845e": [267, 284], "92msuccessfulli": [128, 129, 137, 138, 147, 148], "92mview": [128, 129, 137, 138, 147, 148], "931": [137, 144], "93108886e": [267, 284], "93218452e": [267, 284], "93256009e": [267, 284], "93267390e": [267, 284], "93274853e": [267, 284], "93358018e": [267, 284], "93396618e": [267, 284], "93402106e": [137, 146], "93505753e": [267, 284], "93599756e": [267, 284], "93615": 12, "93653901e": [267, 284], "93655512e": [267, 284], "93691164e": [137, 146], "93784940e": [137, 146], "93872128e": [267, 284], "93877090e": [267, 284], "93991698e": [267, 284], "94008095e": [267, 284], "94048835e": [267, 284], "94161111e": [137, 146], "94202918e": [267, 284], "942508": 13, "9425080418586731": 13, "943": [330, 332], "94304550e": [137, 146], "94449548e": [267, 284], "94474315e": [267, 284], "94507216e": [267, 284], "94511395e": [267, 284], "94528699e": [137, 146], "94559568e": [267, 284], "9478": [126, 127], "94806529e": [267, 284], "94919703e": [267, 284], "94921815e": [267, 284], "94926137e": [267, 284], "949393": [293, 300], "94980036e": [267, 284], "94988877e": [137, 146], "94998607e": [137, 146], "94999364e": [137, 146], "95": [128, 130], "950654": 13, "9506543874740601": 13, "95128240e": [267, 284], "95235109e": [137, 146], "95309842e": [267, 284], "95409912e": [267, 284], "95493992e": [267, 284], "95530374e": [267, 284], "95612733e": [267, 284], "95642184e": [267, 284], "95678936e": [267, 284], "95717700e": [267, 284], "95755831e": [267, 284], "95777296e": [267, 284], "95807119e": [267, 284], "9590334296226501": [307, 308, 309, 316], "959243851260": [28, 30], "96": 15, "96016713e": [267, 284], "96020412e": [137, 146], "96078059e": [267, 284], "96112816e": [267, 284], "96223371e": [267, 284], "96230914e": [267, 284], "96423475e": [267, 284], "96519734e": [267, 284], "96568063e": [267, 284], "96603352e": [137, 146], "96672040e": [267, 284], "96707787e": [267, 284], "96853566e": [137, 146], "96858853e": [137, 146], "96917129e": [267, 284], "96927994e": [137, 146], "96963590e": [137, 146], "97122377e": [267, 284], "97128675e": [267, 284], "97161290e": [267, 284], "97195402e": [137, 146], "97206768e": [137, 146], "97234564e": [267, 284], "97312343e": [267, 284], "97332066e": [137, 146], "97360516e": [137, 146], "97468323e": [267, 284], "97478577e": [267, 284], "97603959e": [267, 284], "97606084e": [267, 284], "97651729e": [267, 284], "97699012e": [267, 284], "97720259e": [137, 146], "97741865e": [267, 284], "97746691e": [137, 146], "97773625e": [267, 284], "97781667e": [137, 146], "97899076e": [267, 284], "97899440e": [267, 284], "97910169e": [267, 284], "97926176e": [267, 284], "97952076e": [267, 284], "97973699e": [267, 284], "98": [15, 137, 146], "98070179e": [267, 284], "98125350e": [137, 146], "98221380e": [267, 284], "98229402e": [137, 146], "98234466e": [267, 284], "98250581e": [267, 284], "98300147e": [137, 146], "98440845e": [267, 284], "98496": 14, "98530062e": [267, 284], "98561847e": [137, 146], "98645657e": [267, 284], "98712543e": [267, 284], "988": 15, "98850322e": [137, 146], "99006638e": [267, 284], "99074054e": [267, 284], "99094741e": [267, 284], "99195677e": [137, 146], "99340957e": [267, 284], "99445761e": [267, 284], "99487356e": [267, 284], "99530393e": [267, 284], "99537045e": [267, 284], "99769610e": [137, 146], "999": [324, 326], "99955577e": [267, 284], "9996353387832642": [307, 308, 309, 316], "9998507499694824": [307, 309, 316], "9m10": [137, 146], "9m15": [137, 146], "A": [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 24, 26, 35, 37, 41, 53, 62, 82, 83, 84, 85, 96, 98, 101, 102, 103, 105, 106, 110, 111, 112, 118, 119, 121, 124, 128, 129, 137, 138, 139, 147, 148, 168, 171, 172, 174, 175, 178, 181, 183, 187, 190, 198, 199, 201, 206, 207, 210, 218, 219, 221, 224, 227, 234, 236, 253, 254, 257, 259, 260, 263, 267, 271, 272, 273, 281, 282, 283, 285, 291, 292, 293, 295, 317, 320, 323, 324, 325, 327, 329, 330, 332, 333, 334, 336, 337, 339, 343, 344, 347], "AND": [92, 93], "And": [128, 129, 137, 138, 147, 148, 267, 283, 285, 288], "As": [1, 15, 82, 83, 90, 91, 118, 121, 128, 130, 159, 163, 169, 170, 285, 288, 292, 343, 349], "At": [2, 3, 5, 13, 17, 22, 164, 167, 175, 177, 181, 185, 267, 283, 285, 292, 317, 318, 324, 325, 350, 351], "Be": [267, 283], "But": [7, 14, 128, 130, 137, 139, 146, 253, 256, 267, 283, 285, 288, 291, 292], "By": [3, 8, 10, 90, 91, 100, 164, 167, 181, 187, 191, 195, 206, 210, 212, 224, 225, 226, 234, 337, 340, 343, 344, 350, 351, 352, 353, 357, 359], "For": [1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 17, 18, 20, 24, 25, 26, 27, 43, 47, 53, 59, 66, 69, 75, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 106, 110, 113, 114, 115, 118, 121, 122, 123, 124, 155, 158, 164, 166, 167, 169, 170, 171, 174, 175, 177, 181, 186, 188, 191, 197, 198, 202, 203, 204, 205, 206, 213, 215, 218, 223, 224, 234, 238, 239, 267, 278, 283, 285, 286, 287, 292, 293, 294, 296, 298, 312, 324, 325, 330, 331, 337, 341, 350, 351], "IN": [164, 167], "IT": [267, 283], "If": [1, 2, 3, 4, 5, 7, 8, 9, 14, 28, 29, 35, 42, 43, 44, 50, 53, 55, 62, 63, 66, 68, 71, 78, 82, 83, 84, 85, 86, 87, 88, 89, 100, 118, 121, 128, 129, 137, 138, 147, 148, 169, 170, 171, 172, 175, 179, 181, 186, 187, 191, 193, 196, 197, 198, 203, 224, 230, 233, 245, 247, 249, 250, 253, 257, 267, 275, 277, 283, 284, 285, 288, 290, 291, 292, 293, 299, 317, 323, 330, 334, 336, 343, 347, 350, 351, 357, 358], "In": [3, 4, 5, 9, 10, 11, 15, 24, 27, 28, 30, 43, 45, 53, 56, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 96, 98, 100, 101, 102, 109, 110, 112, 118, 120, 121, 159, 161, 171, 174, 181, 184, 187, 188, 196, 197, 198, 204, 206, 211, 212, 213, 215, 218, 223, 224, 225, 226, 229, 234, 236, 238, 239, 242, 245, 246, 252, 266, 267, 273, 277, 279, 281, 282, 283, 284, 285, 288, 289, 290, 291, 292, 293, 298, 302, 304, 306, 307, 311, 313, 314, 315, 316, 317, 323, 324, 325, 329, 330, 336, 337, 338, 340, 343, 344, 347, 350, 351], "It": [0, 4, 5, 6, 7, 8, 9, 10, 15, 16, 17, 19, 22, 28, 29, 43, 44, 47, 53, 54, 59, 66, 67, 75, 82, 83, 84, 85, 96, 97, 110, 112, 118, 122, 137, 138, 164, 167, 171, 174, 176, 191, 196, 198, 200, 201, 206, 209, 212, 213, 224, 228, 229, 232, 253, 257, 259, 262, 267, 275, 277, 283, 284, 285, 288, 292, 293, 295, 298, 299, 300, 302, 307, 313, 324, 327, 330, 333, 337, 338, 343, 346], "Its": [8, 191, 196, 285, 292], "NOT": [267, 283], "No": [0, 9, 24, 26, 110, 117, 128, 130, 155, 158, 198, 203, 238, 240, 267, 283, 317, 323, 324, 325, 329, 330, 331, 350, 357, 359], "Not": [3, 5, 6, 17, 18, 181, 190, 245, 251, 259, 260, 262, 267, 271, 281, 283, 350, 359], "OR": [35, 38, 66, 69], "Of": [285, 291], "On": [13, 17, 20, 24, 27, 66, 69, 84, 85, 94, 95, 101, 102, 105, 110, 116, 330, 334, 350, 351], "One": [82, 83, 96, 98, 118, 121, 285, 288, 292, 350, 356], "Or": [0, 86, 87, 101, 110, 113, 118, 121], "Such": [285, 292], "THE": [267, 283, 285, 291], "TO": [267, 283], "That": [3, 100, 181, 183, 267, 283, 285, 291, 292], "The": [0, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 17, 20, 22, 23, 24, 25, 26, 27, 28, 31, 35, 39, 40, 43, 46, 47, 53, 58, 59, 63, 66, 69, 75, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 99, 100, 105, 106, 107, 108, 110, 113, 115, 116, 118, 121, 122, 123, 126, 127, 128, 130, 131, 134, 147, 152, 155, 157, 158, 159, 163, 164, 166, 167, 168, 171, 173, 174, 175, 178, 179, 181, 184, 190, 195, 197, 198, 201, 202, 204, 206, 211, 212, 214, 224, 225, 227, 229, 230, 234, 235, 236, 237, 238, 239, 240, 242, 243, 245, 247, 250, 253, 255, 257, 258, 259, 263, 266, 267, 275, 277, 279, 283, 284, 285, 288, 291, 292, 293, 295, 297, 298, 299, 300, 317, 318, 324, 325, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 343, 346, 349, 350, 351, 359], "Their": [267, 283], "Then": [0, 1, 8, 11, 28, 31, 35, 40, 43, 47, 53, 57, 59, 66, 75, 169, 170, 191, 196, 218, 222, 267, 281, 283, 284, 285, 291, 330, 336], "There": [9, 10, 15, 90, 91, 198, 201, 203, 204, 206, 215, 267, 283, 285, 291, 292, 302, 306, 307, 311, 313, 315, 316], "These": [0, 3, 4, 8, 10, 15, 17, 22, 35, 39, 80, 81, 84, 85, 88, 89, 101, 102, 105, 110, 112, 155, 157, 171, 173, 181, 183, 187, 191, 195, 206, 211, 224, 226, 235, 278, 285, 286, 292, 294, 312, 330, 332], "To": [1, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 22, 25, 28, 30, 43, 45, 53, 56, 82, 83, 86, 87, 88, 89, 100, 101, 102, 108, 126, 127, 128, 133, 137, 139, 147, 150, 155, 157, 158, 164, 166, 167, 169, 170, 171, 174, 181, 184, 188, 191, 194, 198, 202, 204, 206, 211, 212, 213, 214, 215, 218, 223, 224, 234, 236, 238, 243, 245, 247, 248, 253, 257, 267, 271, 273, 281, 282, 283, 293, 296, 299, 330, 336, 350, 353, 358], "With": [1, 5, 16, 96, 99, 101, 102, 105, 118, 124, 159, 163, 169, 170, 224, 225, 237, 238, 239, 244, 245, 246, 249, 252, 285, 287, 292, 302, 307, 313, 330, 331, 337, 338, 350, 351], "_": [3, 7, 14, 16, 88, 89, 118, 121, 137, 143, 181, 185, 253, 257, 267, 283, 317, 318, 320, 323, 324, 325, 326, 330, 331, 343, 344, 350, 351, 359], "_2": [317, 318, 324, 325], "__call__": [4, 10, 11, 12, 15, 16, 128, 131, 137, 146, 164, 167, 171, 174, 206, 213, 218, 222, 266, 267, 273, 279, 282, 337, 341, 343, 349, 350, 359], "__file__": [86, 87], "__getitem__": [6, 259, 262, 343, 345, 350, 353], "__index_level_0__": [337, 340], "__init__": [3, 4, 6, 10, 11, 12, 15, 16, 86, 87, 128, 131, 137, 139, 140, 146, 147, 149, 150, 171, 174, 181, 190, 206, 213, 218, 222, 224, 237, 259, 262, 267, 273, 282, 305, 306, 307, 315, 317, 320, 324, 327, 330, 333, 337, 341, 343, 345, 346, 349, 350, 353, 359], "__len__": [6, 259, 262, 343, 345, 350, 353], "__main__": [88, 89, 159, 163], "__name__": [88, 89, 159, 163], "_arrow_table_from_shard": [337, 340], "_build": 0, "_class_nam": [6, 259, 262], "_config": 0, "_diffusers_vers": [6, 259, 262], "_dmat_from_arrow": [337, 340], "_k": [324, 325], "_model": [4, 12, 171, 174], "_sample_timestep": [6, 259, 262], "_shared_step": [317, 320, 324, 327], "_static": 0, "_toc": 0, "a10": [350, 351], "a100": [110, 113, 114, 116, 350, 351], "a10g": [324, 325, 328, 329], "a_random_job_nam": [35, 41, 53, 62], "abandon": [285, 292], "abil": [3, 5, 6, 80, 81, 84, 85, 159, 161, 181, 188, 224, 225, 259, 261, 285, 288], "abl": [5, 8, 9, 28, 29, 43, 44, 53, 55, 63, 66, 68, 155, 158, 191, 196, 198, 204, 307, 309, 316], "abortmultipartupload": [17, 22], "about": [5, 7, 12, 13, 14, 16, 24, 26, 28, 34, 84, 85, 86, 87, 88, 89, 101, 102, 106, 110, 115, 118, 121, 123, 126, 127, 159, 161, 163, 176, 177, 182, 184, 187, 188, 189, 224, 226, 228, 236, 253, 257, 266, 267, 279, 283, 285, 287, 288, 291, 292, 293, 295, 302, 307, 313, 350, 351], "abov": [3, 5, 6, 101, 102, 106, 107, 118, 121, 128, 129, 131, 137, 138, 147, 148, 155, 158, 159, 163, 164, 167, 181, 187, 224, 225, 226, 227, 234, 236, 259, 263, 267, 283, 285, 292, 343, 347], "absenc": [164, 167, 267, 283, 337, 342], "absent": [285, 292], "absolut": [5, 343, 345], "abspath": [128, 129, 137, 138, 147, 148], "abstract": [4, 8, 15, 17, 19, 100, 101, 102, 106, 107, 110, 113, 126, 127, 171, 173, 191, 192, 324, 325, 337, 338, 343, 349], "absurd": [285, 292], "academ": [118, 124, 343, 344], "academi": [285, 292], "acc": [3, 13, 181, 190, 337, 340], "acc_metr": [350, 355], "acceler": [1, 2, 3, 6, 8, 10, 11, 28, 34, 66, 71, 84, 85, 100, 126, 127, 128, 132, 169, 170, 175, 177, 181, 187, 191, 192, 206, 213, 218, 220, 259, 262, 263, 317, 321, 324, 328], "accelerator_shap": [12, 13, 14], "accelerator_typ": [10, 12, 13, 14, 101, 102, 108, 110, 113, 116, 118, 121, 122, 123, 128, 131, 137, 139, 143, 146, 147, 149, 206, 213, 306, 307, 315], "accept": [1, 2, 7, 10, 11, 14, 15, 169, 170, 175, 180, 206, 212, 213, 218, 220, 224, 227, 237, 253, 257, 302, 306, 307, 313, 315, 343, 346], "access": [3, 8, 17, 20, 21, 22, 27, 53, 63, 82, 83, 86, 87, 88, 89, 94, 95, 96, 97, 98, 100, 110, 111, 113, 114, 118, 119, 121, 122, 123, 126, 127, 128, 135, 147, 151, 155, 158, 159, 163, 164, 167, 168, 181, 183, 190, 191, 192, 224, 229, 234, 236, 238, 244, 293, 298, 300, 302, 307, 313, 337, 339, 350, 351, 352], "accident": [337, 340], "acclaim": [285, 292], "accomplish": [245, 252], "accord": [2, 9, 10, 15, 84, 85, 175, 180, 198, 204, 206, 215, 224, 235, 238, 244], "accordingli": [118, 124, 293, 298, 324, 329], "account": [3, 8, 17, 21, 22, 24, 26, 28, 29, 30, 35, 36, 39, 43, 44, 45, 53, 55, 56, 66, 68, 70, 78, 100, 126, 127, 181, 190, 191, 196, 285, 288], "account_id": [17, 22], "accross": [101, 102, 105, 106], "accumul": [5, 13, 224, 228], "accur": [101, 102, 105, 343, 344], "accuraci": [7, 10, 13, 14, 15, 110, 112, 118, 124, 137, 146, 206, 215, 253, 256, 293, 295, 297, 299, 337, 338, 340, 341, 342, 350, 355, 359], "accuracy_scor": [337, 339, 340], "achiev": [3, 5, 6, 9, 10, 11, 15, 101, 102, 105, 137, 146, 181, 184, 189, 198, 204, 206, 213, 215, 218, 220, 224, 225, 259, 261], "acid": [8, 191, 192], "acl": [96, 98], "across": [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 17, 19, 22, 24, 26, 79, 84, 85, 86, 87, 88, 89, 96, 99, 101, 102, 106, 107, 110, 113, 116, 117, 118, 121, 124, 126, 127, 128, 130, 131, 134, 137, 146, 159, 161, 163, 168, 169, 170, 171, 174, 175, 177, 181, 183, 186, 188, 191, 192, 195, 198, 204, 205, 206, 208, 210, 214, 215, 217, 224, 225, 226, 227, 228, 229, 231, 234, 235, 236, 238, 239, 242, 243, 244, 245, 252, 259, 263, 285, 289, 292, 293, 295, 299, 300, 302, 307, 313, 317, 318, 319, 323, 324, 325, 326, 329, 330, 331, 332, 334, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 349, 350, 351, 355, 357, 359], "act": [17, 22, 224, 229, 285, 292], "act_dim": [324, 327, 329], "act_fn": [6, 259, 262], "action": [16, 17, 22, 118, 121, 123, 159, 163, 285, 291, 292, 326, 327], "action_spac": [324, 326], "activ": [0, 2, 3, 7, 9, 10, 14, 80, 81, 90, 91, 101, 102, 106, 168, 175, 180, 181, 190, 198, 204, 206, 212, 253, 257, 330, 332], "actor": [5, 6, 8, 9, 11, 13, 16, 101, 102, 106, 128, 132, 159, 161, 183, 187, 191, 196, 198, 201, 207, 218, 220, 221, 227, 245, 252, 259, 263, 266, 267, 273, 279, 282, 337, 341, 342, 343, 344, 349, 350, 359], "actorpoolmapoper": [10, 206, 214], "actorpoolstrategi": [337, 339, 341, 342], "actress": [285, 288], "actual": [2, 7, 10, 14, 15, 28, 30, 35, 38, 43, 45, 48, 53, 56, 60, 66, 69, 72, 76, 118, 123, 128, 130, 175, 179, 206, 213, 224, 234, 253, 257, 267, 283, 285, 292, 324, 326, 330, 331], "ad": [6, 16, 17, 22, 28, 30, 34, 84, 85, 94, 95, 100, 128, 129, 137, 138, 146, 147, 148, 238, 239, 245, 252, 259, 263, 267, 275, 283, 324, 325, 337, 340, 341], "adam": [5, 7, 13, 14, 137, 143, 224, 226, 228, 238, 240, 245, 247, 253, 254, 256, 257, 317, 320, 324, 327, 330, 334, 343, 347, 350, 355], "adamw": [6, 259, 262], "adapt": [101, 102, 105, 119, 120, 125, 224, 226, 231, 293, 299, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "adapter_config": [118, 121], "adapter_model": [118, 121], "adapter_nam": [118, 121], "adaptiveavgpool2d": 13, "add": [2, 3, 5, 6, 8, 9, 10, 11, 13, 16, 17, 22, 35, 39, 43, 46, 48, 51, 53, 58, 60, 63, 64, 66, 69, 73, 76, 80, 81, 82, 83, 84, 85, 92, 93, 101, 102, 107, 118, 121, 123, 126, 127, 128, 130, 133, 136, 137, 139, 143, 146, 147, 153, 159, 163, 164, 166, 175, 178, 179, 180, 181, 183, 184, 186, 190, 191, 192, 198, 202, 203, 206, 212, 218, 222, 224, 226, 227, 229, 232, 237, 245, 252, 259, 262, 263, 278, 285, 286, 289, 294, 312, 317, 318, 323, 324, 326, 329, 330, 336, 337, 342, 343, 347, 349, 350, 359], "add_class": [128, 130, 137, 139, 146], "add_label": [10, 15, 206, 212, 215], "add_nois": [6, 259, 262], "add_ref": [3, 181, 188], "add_subplot": [5, 224, 226, 237], "addit": [5, 9, 17, 20, 80, 81, 82, 83, 84, 85, 88, 89, 100, 118, 121, 125, 159, 162, 198, 202, 224, 231, 245, 246, 247, 302, 304, 307, 313, 314, 330, 331, 336, 337, 342, 350, 359], "addition": [9, 10, 16, 17, 22, 28, 34, 126, 127, 159, 163, 198, 204, 206, 212, 293, 299, 330, 331], "addr": [317, 321], "address": [2, 8, 17, 22, 28, 30, 43, 45, 53, 56, 126, 127, 137, 138, 155, 158, 175, 177, 191, 192, 224, 235, 267, 283], "adebayor": [267, 283], "adher": [285, 292], "adjust": [6, 9, 118, 124, 198, 203, 224, 227, 259, 262, 267, 275, 283, 285, 291, 293, 300, 337, 340, 350, 359], "adjust_total_amount": [9, 198, 202], "adjusted_data": [9, 198, 203, 205], "adjusted_data_rai": [9, 198, 203, 205], "adjusted_total_amount": [9, 164, 166, 198, 202], "admin": [17, 18, 22, 96, 98, 126, 127], "administr": [285, 292, 360], "admittedli": [285, 288], "adopt": [9, 198, 201], "adv": [267, 281, 283], "advanc": [6, 7, 8, 14, 24, 26, 27, 28, 34, 84, 85, 101, 102, 109, 111, 117, 121, 164, 167, 191, 192, 196, 253, 256, 259, 262, 350, 359, 360], "advantag": [8, 110, 112, 191, 196], "adventureland": [267, 283], "adversari": [317, 318], "affect": [11, 218, 221], "affin": [13, 137, 140], "afford": [110, 112, 267, 283], "after": [1, 2, 3, 4, 5, 6, 11, 13, 16, 43, 47, 50, 53, 57, 59, 62, 66, 75, 82, 83, 84, 85, 88, 89, 101, 102, 105, 128, 129, 137, 138, 146, 147, 148, 159, 163, 164, 167, 168, 169, 170, 171, 174, 175, 180, 181, 183, 189, 218, 223, 224, 226, 228, 234, 237, 259, 262, 266, 267, 279, 283, 284, 285, 291, 293, 295, 300, 317, 318, 323, 324, 325, 330, 331, 332, 334, 336, 337, 339, 342, 343, 345, 347, 348, 350, 351, 355, 357], "afterward": [350, 351], "again": [3, 4, 9, 28, 33, 84, 85, 90, 91, 128, 130, 171, 172, 181, 187, 198, 202, 285, 292, 317, 322, 330, 335, 337, 338, 350, 358], "against": [110, 115, 118, 121, 245, 249, 267, 281, 283, 285, 291, 292, 324, 329, 330, 331, 334, 336, 343, 349, 350, 357], "agent": [101, 102, 105, 118, 124, 324, 325, 329], "aggreg": [8, 13, 137, 139, 146, 191, 195, 199, 207, 211, 224, 225, 317, 320, 321, 337, 341, 342, 350, 355], "aggregate_metr": [137, 146], "aggress": [10, 164, 167, 206, 212], "agil": [324, 325], "agnost": [147, 151, 343, 345], "ago": [285, 288], "agre": [285, 291], "aguero": [267, 283], "ahead": [110, 117, 118, 125, 285, 291], "ai": [1, 3, 9, 10, 11, 15, 16, 86, 87, 90, 91, 94, 95, 100, 101, 102, 108, 110, 117, 118, 120, 121, 123, 169, 170, 181, 190, 198, 205, 206, 210, 212, 213, 215, 218, 222], "air": 12, "airflow": [147, 154], "aj": [267, 281, 283], "ak": [17, 20], "ako": [267, 283], "akz9ul28": [147, 153], "alaska": [285, 291], "alb": [24, 27], "alberta": [285, 291], "album": [267, 283], "alciato": [267, 281, 283], "ald": [267, 283], "alert": [92, 93, 128, 135, 147, 153, 165, 245, 252], "algorithm": [7, 12, 13, 14, 24, 27, 100, 101, 102, 107, 253, 257, 293, 300, 337, 338], "alic": 168, "align": [8, 191, 195, 224, 226, 317, 321, 350, 352, 355], "alik": [266, 267, 279], "all": [0, 1, 3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 17, 20, 22, 24, 26, 28, 29, 35, 36, 38, 43, 44, 49, 50, 51, 53, 54, 61, 62, 64, 66, 67, 69, 71, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 96, 98, 100, 101, 102, 103, 104, 105, 110, 111, 118, 119, 121, 122, 125, 126, 127, 128, 129, 130, 134, 135, 136, 137, 138, 140, 146, 147, 148, 154, 155, 156, 158, 159, 160, 163, 164, 165, 168, 169, 170, 171, 172, 173, 176, 181, 182, 183, 188, 189, 191, 192, 199, 201, 205, 207, 212, 224, 225, 226, 228, 229, 233, 234, 236, 238, 244, 245, 248, 249, 250, 253, 254, 257, 259, 260, 263, 266, 267, 273, 278, 279, 282, 283, 285, 286, 287, 288, 291, 292, 293, 294, 295, 299, 302, 307, 312, 313, 317, 321, 324, 326, 329, 330, 331, 332, 334, 336, 337, 339, 340, 341, 342, 344, 345, 350, 351, 354, 355, 357, 359], "all_fil": [86, 87], "all_results_at_onc": [3, 181, 189], "alleg": [285, 288], "allegori": [285, 292], "alli": [285, 291], "allobjectact": [17, 22], "alloc": [7, 8, 10, 14, 17, 22, 24, 25, 28, 30, 43, 45, 53, 56, 182, 191, 197, 206, 208, 212, 224, 230, 237, 253, 257, 266, 267, 279, 337, 340, 343, 344], "allow": [3, 4, 5, 6, 8, 15, 16, 17, 22, 53, 63, 80, 81, 84, 85, 88, 89, 94, 95, 96, 98, 110, 116, 118, 120, 121, 128, 135, 137, 144, 147, 151, 164, 167, 171, 173, 174, 181, 183, 187, 191, 197, 224, 225, 230, 231, 234, 238, 244, 245, 248, 250, 259, 261, 266, 267, 279, 283, 285, 292, 293, 295, 298, 299, 300, 302, 305, 306, 307, 313, 315, 317, 321, 330, 335, 337, 340, 343, 347, 349, 350, 351], "allowedhead": [17, 22, 53, 63], "allowedmethod": [17, 22, 53, 63], "allowedorigin": [17, 22, 53, 63], "allreduc": [224, 225], "alltoallapi": [10, 206, 215], "allus": [285, 292], "almost": [285, 292, 337, 339], "alon": [9, 10, 15, 198, 204, 206, 215], "along": [84, 85, 224, 233, 267, 283, 350, 351], "alongsid": [245, 246, 252, 337, 339, 350, 359], "alonso": [267, 283], "alphas_cumprod": [6, 259, 262], "alreadi": [1, 9, 10, 11, 15, 43, 46, 53, 58, 82, 83, 86, 87, 100, 159, 163, 169, 170, 198, 201, 206, 215, 218, 220, 224, 226, 228, 237, 245, 250, 267, 271, 281, 283, 330, 332, 336, 337, 341, 343, 345, 350, 358], "also": [3, 5, 9, 10, 11, 13, 15, 16, 28, 29, 35, 37, 43, 44, 50, 53, 55, 62, 66, 68, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 96, 97, 101, 102, 105, 126, 127, 128, 130, 132, 133, 135, 137, 144, 147, 154, 159, 163, 164, 165, 167, 181, 184, 198, 205, 206, 215, 218, 222, 224, 226, 236, 245, 248, 267, 271, 275, 277, 281, 283, 284, 285, 287, 288, 291, 292, 302, 304, 307, 313, 314, 317, 321, 330, 332, 333, 334, 337, 339, 343, 347], "altern": [1, 28, 29, 43, 44, 53, 55, 66, 68, 126, 127, 169, 170, 293, 298, 337, 342], "alwai": [0, 3, 118, 121, 122, 128, 133, 159, 162, 181, 187, 337, 342, 343, 345], "am": [285, 288], "amaz": [118, 125, 267, 283, 285, 291], "amazon": [8, 17, 20, 28, 29, 43, 44, 53, 55, 86, 87, 88, 89, 191, 192, 267, 283], "amazonaw": [13, 14, 17, 22, 128, 136, 147, 150, 153], "amazonelasticfilesystemclientreadwriteaccess": [53, 57, 63], "ambush": [285, 291], "america": [285, 288], "american": [285, 288, 291, 292], "ami": [24, 26, 267, 283], "amnt": [267, 283], "among": [9, 10, 15, 110, 113, 198, 204, 206, 215, 285, 291, 292], "amount": [3, 4, 5, 6, 8, 9, 10, 12, 86, 87, 171, 174, 181, 190, 191, 192, 198, 201, 206, 212, 224, 225, 259, 261, 343, 347, 349, 350, 351], "amp": [267, 283, 317, 323, 350, 359], "amus": [285, 292], "an": [0, 2, 5, 8, 9, 10, 13, 15, 18, 21, 22, 23, 24, 27, 28, 29, 35, 41, 43, 44, 52, 55, 65, 82, 83, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 105, 107, 108, 113, 114, 116, 119, 121, 126, 127, 128, 130, 131, 134, 135, 137, 139, 147, 148, 151, 155, 158, 159, 160, 163, 164, 166, 167, 173, 175, 177, 179, 182, 184, 185, 186, 190, 191, 192, 196, 197, 198, 199, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 219, 221, 223, 224, 225, 226, 267, 271, 272, 273, 275, 278, 281, 282, 283, 285, 286, 288, 290, 291, 292, 293, 294, 299, 302, 305, 306, 307, 312, 313, 315, 317, 318, 323, 325, 330, 331, 337, 338, 340, 341, 342, 343, 344, 350, 351, 353, 360], "anaconda3": [137, 144], "anal": [285, 292], "analys": [285, 292], "analysi": [8, 79, 88, 89, 101, 102, 105, 118, 121, 124, 191, 195, 285, 292, 302, 305, 306, 307, 309, 311, 313, 315, 316], "analyt": [8, 191, 192, 193, 195], "analyz": [8, 118, 121, 155, 157, 191, 192, 224, 236], "anatom": [285, 288], "anatomi": [267, 283], "angel": [285, 291, 292], "angelbr": [285, 292], "anger": [267, 283], "anggrek": [267, 283], "angl": [324, 325], "angular": [324, 325], "ani": [1, 2, 3, 4, 7, 9, 10, 11, 14, 15, 16, 28, 30, 35, 39, 42, 43, 45, 53, 56, 66, 78, 80, 81, 82, 83, 94, 95, 100, 118, 121, 126, 127, 128, 132, 133, 137, 138, 147, 154, 155, 158, 159, 163, 164, 166, 169, 170, 171, 174, 175, 177, 178, 181, 187, 198, 201, 206, 213, 216, 218, 219, 222, 224, 229, 238, 243, 245, 251, 253, 254, 256, 257, 267, 273, 282, 285, 287, 288, 291, 292, 305, 306, 307, 309, 315, 316, 317, 318, 321, 324, 329, 330, 331, 334, 336, 337, 338, 339, 340, 342, 343, 349, 350, 351, 359], "anniversari": [267, 283], "annot": [7, 24, 27, 253, 257, 337, 341], "anon": [6, 259, 262], "anonym": [12, 164, 166], "anoth": [9, 10, 24, 27, 84, 85, 90, 91, 128, 129, 137, 138, 146, 147, 148, 198, 202, 206, 211, 267, 281, 283, 285, 289, 291, 292, 317, 323], "answer": [101, 102, 105, 267, 283, 285, 288], "ant": [267, 283], "anthoni": [285, 291, 292], "anti": [3, 176, 181, 184, 189, 267, 283, 285, 292], "antiwoman": [285, 292], "anymor": [317, 323, 330, 336], "anyon": [267, 283, 285, 288], "anyscal": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 20, 21, 27, 32, 33, 34, 37, 38, 41, 42, 46, 49, 50, 51, 52, 55, 57, 58, 61, 62, 63, 64, 65, 68, 69, 72, 77, 78, 98, 99, 103, 108, 109, 111, 112, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 133, 134, 135, 137, 138, 142, 145, 147, 148, 151, 153, 154, 155, 156, 157, 158, 163, 166, 168, 169, 170, 171, 172, 174, 175, 176, 181, 182, 191, 192, 198, 199, 201, 204, 206, 207, 210, 212, 213, 215, 218, 219, 222, 224, 225, 226, 245, 252, 253, 254, 259, 260, 264, 266, 267, 271, 279, 281, 285, 287, 293, 295, 302, 307, 313, 323, 329, 336, 342, 349, 356], "anyscale_artifact_storag": [6, 86, 87, 159, 163, 259, 262], "anyscale_cloud_id": [35, 39], "anyscale_cloud_nam": [17, 23, 28, 29, 30, 31, 32, 33, 35, 36, 39, 40, 41, 42, 43, 44, 45, 47, 50, 51, 53, 54, 56, 59, 62, 64, 66, 67, 69, 70, 75, 77, 78], "anyscale_cloud_storage_bucket": [86, 87], "anyscale_cloud_storage_bucket_region": [86, 87], "anyscale_iam_rol": [53, 57], "anyscale_iam_role_arn": [17, 23], "anyscale_iam_s3_policy_arn": [53, 57], "anyscale_registration_command": [28, 30, 35, 39, 43, 45, 53, 56, 66, 70], "anyscale_s3_bucket_nam": [28, 30, 33, 43, 45, 51, 53, 56, 64], "anyscale_security_group": [17, 22], "anyscale_vpc": [17, 22], "anyscale_vpc_nam": [17, 22], "anyscalerai": [43, 51, 53, 64], "anyscaleuserdata": [110, 115, 147, 153], "anyth": [24, 27, 86, 87, 224, 229, 285, 288, 292, 350, 358], "anywher": [3, 90, 91, 128, 135, 181, 183], "apach": 195, "aperitif": [267, 283], "api": [3, 8, 9, 10, 11, 13, 15, 16, 24, 25, 26, 27, 37, 68, 78, 90, 91, 92, 93, 96, 98, 100, 101, 102, 107, 108, 110, 113, 118, 120, 122, 123, 125, 128, 135, 137, 145, 147, 150, 151, 153, 181, 190, 191, 194, 195, 196, 197, 198, 200, 205, 206, 212, 213, 218, 220, 266, 267, 271, 273, 277, 279, 281, 282, 284, 285, 290, 302, 307, 313, 317, 321, 330, 332, 337, 342, 343, 349, 350, 351, 359], "api_kei": [101, 102, 108, 110, 114, 115, 118, 121, 122, 123], "apigatewai": [164, 167, 168], "app": [2, 4, 11, 16, 53, 63, 88, 89, 92, 93, 101, 102, 106, 108, 110, 113, 114, 115, 116, 118, 121, 122, 123, 147, 150, 153, 164, 167, 168, 171, 174, 175, 177, 218, 223, 302, 306, 307, 313, 315, 317, 323], "app1": [11, 164, 167, 218, 223], "app_build": [11, 218, 223], "appapp": 305, "apparatu": [324, 329], "appear": [101, 102, 106, 267, 283, 285, 288, 292], "append": [2, 3, 10, 86, 87, 118, 123, 128, 129, 137, 138, 146, 147, 148, 175, 180, 181, 183, 189, 206, 212, 317, 319, 324, 326, 330, 334, 343, 345, 350, 352, 359], "appl": [1, 169, 170, 266, 267, 275, 279, 283, 293, 295, 298], "appli": [0, 3, 4, 5, 6, 9, 10, 11, 12, 15, 16, 28, 30, 31, 35, 39, 40, 42, 43, 45, 47, 53, 56, 59, 66, 70, 75, 78, 128, 129, 130, 131, 132, 137, 139, 146, 171, 174, 181, 190, 198, 200, 201, 202, 204, 206, 209, 213, 215, 218, 222, 224, 225, 226, 228, 232, 235, 237, 259, 262, 266, 267, 275, 277, 279, 283, 284, 285, 287, 290, 292, 293, 298, 306, 307, 315, 317, 319, 324, 325, 326, 330, 331, 332, 343, 347, 350, 351, 353], "applic": [1, 3, 4, 9, 12, 15, 16, 24, 27, 28, 34, 35, 38, 66, 69, 80, 81, 84, 85, 90, 91, 92, 93, 94, 95, 101, 102, 106, 108, 110, 112, 113, 115, 117, 118, 120, 122, 123, 125, 126, 127, 149, 151, 152, 153, 154, 155, 157, 159, 161, 162, 165, 168, 169, 170, 171, 173, 181, 185, 192, 196, 197, 198, 200, 219, 222, 223, 302, 305, 306, 307, 308, 309, 313, 315, 316, 330, 336], "application_log": [306, 307, 315], "approach": [3, 8, 79, 128, 130, 137, 146, 181, 184, 191, 197, 266, 267, 279, 285, 287, 292, 293, 295, 299, 317, 318, 330, 331, 332, 337, 339, 350, 351], "appropri": [43, 50, 53, 62, 101, 102, 106, 110, 113, 126, 127, 128, 135], "approv": [28, 30, 33, 35, 39, 42, 43, 45, 51, 53, 64, 66, 70, 78], "approx": [3, 181, 183, 324, 325], "approxim": [101, 102, 106, 330, 331, 332], "april": [267, 283], "ar": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 22, 24, 26, 27, 28, 29, 30, 33, 35, 38, 39, 43, 44, 45, 49, 50, 51, 53, 55, 56, 57, 61, 62, 63, 64, 66, 68, 69, 71, 78, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 96, 98, 101, 102, 106, 110, 112, 113, 115, 116, 117, 118, 121, 122, 123, 125, 128, 132, 147, 151, 153, 154, 155, 158, 159, 163, 164, 165, 166, 167, 168, 169, 170, 175, 177, 179, 180, 181, 182, 183, 187, 189, 190, 191, 192, 194, 195, 196, 197, 198, 201, 202, 203, 204, 205, 206, 210, 211, 212, 213, 215, 218, 220, 221, 224, 225, 226, 227, 228, 229, 231, 232, 234, 235, 236, 237, 238, 240, 243, 245, 246, 250, 253, 257, 258, 259, 263, 264, 266, 267, 273, 275, 278, 279, 282, 283, 285, 286, 288, 291, 292, 293, 294, 298, 302, 306, 307, 308, 309, 311, 312, 313, 315, 316, 319, 323, 326, 329, 334, 336, 342, 349, 352, 355, 359], "arang": [343, 346, 349], "arbitrari": [2, 118, 122, 175, 177], "architectur": [5, 6, 11, 13, 16, 21, 24, 26, 79, 103, 109, 218, 221, 224, 227, 259, 262, 263, 266, 268, 279, 295, 302, 313, 317, 323, 330, 333, 343, 344, 346, 350, 359], "archuleta": [267, 283], "area": [9, 118, 125, 198, 204], "aree": 101, "aren": [53, 63, 343, 347], "arena": [118, 124], "arg": [118, 122, 137, 140, 146, 343, 346, 350, 359], "argmax": [5, 10, 11, 13, 15, 16, 137, 140, 143, 206, 213, 218, 222, 224, 237, 293, 297, 337, 340, 341, 350, 355, 359], "args_fp": [137, 140, 146], "argu": [285, 292], "arguabl": [285, 288], "argument": [2, 5, 6, 7, 10, 11, 14, 15, 16, 90, 91, 118, 121, 123, 175, 180, 182, 184, 187, 206, 213, 218, 222, 224, 229, 231, 253, 257, 259, 263, 267, 283, 285, 292, 293, 298], "arm": [285, 291], "arm64": [1, 169, 170], "arn": [17, 22, 28, 30, 43, 45, 53, 56, 57], "around": [5, 6, 92, 93, 118, 121, 128, 135, 224, 227, 259, 263, 285, 288, 291, 292, 350, 359], "arr": [224, 237, 317, 319], "arrai": [7, 9, 10, 11, 12, 14, 16, 118, 122, 128, 130, 136, 137, 146, 198, 202, 206, 212, 218, 222, 224, 226, 237, 238, 242, 243, 253, 255, 257, 267, 283, 284], "arrang": [3, 181, 188], "array_equ": [3, 181, 183], "array_split": [330, 332], "arriv": [8, 191, 195, 324, 329], "arrow_ref": [337, 340], "arsen": [267, 283], "art": [101, 102, 107, 285, 291], "articl": 16, "artifact": [5, 12, 13, 86, 87, 126, 127, 128, 133, 137, 138, 143, 144, 147, 154, 224, 226, 234, 235, 245, 251, 293, 300, 337, 342, 350, 359], "artifact_dir": [147, 150, 337, 342], "artifact_storag": [86, 87], "artifact_storage_path": [6, 259, 262], "artifact_uri": [137, 144, 146, 147, 150], "artifacts_dir": [137, 146, 147, 149, 150], "artifici": [324, 325], "artist": [285, 288], "as_directori": [5, 6, 13, 224, 237, 245, 247, 259, 263, 317, 321, 323, 324, 328, 329, 330, 334, 336, 337, 340, 343, 347, 349, 350, 355, 359], "as_fram": [337, 339], "as_index": [330, 334, 343, 347, 350, 357], "as_tensor": [6, 137, 141, 224, 237, 259, 262], "asarrai": [317, 319], "asc": [137, 144, 147, 150], "asgi": 168, "ask": [3, 181, 190, 285, 288, 317, 321, 350, 356], "aspen": [337, 338], "assert": [3, 10, 15, 16, 181, 189, 206, 212, 317, 323, 324, 329, 337, 339], "assess": [343, 347, 349, 350, 357], "assign": [9, 10, 15, 66, 73, 94, 95, 198, 203, 206, 210, 224, 230, 235, 238, 241, 330, 331, 337, 340, 350, 351], "assist": [124, 267, 283], "associ": [8, 82, 83, 159, 161, 191, 197, 224, 236, 330, 336], "assum": [3, 17, 22, 24, 26, 155, 158, 181, 183, 224, 226, 337, 341], "assumerol": [17, 22], "astral": [1, 169, 170], "astyp": [6, 11, 218, 222, 259, 262, 324, 326, 330, 332, 337, 341, 343, 349, 350, 359], "async": [4, 11, 12, 16, 147, 150, 164, 167, 171, 174, 218, 222], "asynchron": [7, 253, 256], "asyncio": [4, 8, 171, 172, 174, 191, 197], "athen": [267, 283], "atom": [8, 147, 154, 191, 192, 267, 283], "attach": [24, 26, 27, 63, 224, 234, 237, 245, 248, 330, 334, 343, 347, 350, 355], "attempt": [10, 128, 133, 136, 137, 139, 143, 146, 206, 211, 212, 245, 247, 343, 347], "attend": [285, 292], "attent": [101, 102, 109, 285, 288, 293, 298, 343, 344, 349], "attention_head_dim": [6, 259, 262], "attribut": [317, 321, 337, 342], "audienc": [285, 291, 292, 330, 336], "audio": [267, 283], "audit": [101, 102, 107], "augment": [238, 239, 245, 252, 350, 359], "august": [267, 283], "auschwitz": [285, 292], "auth": [35, 38, 66, 69, 72, 343, 345], "authent": [17, 22, 24, 27, 36, 37, 68, 82, 83, 86, 87, 110, 114, 115], "author": [110, 114, 118, 121, 147, 153, 168], "auto": [6, 16, 24, 26, 28, 30, 33, 35, 39, 42, 43, 45, 51, 53, 64, 66, 70, 78, 92, 93, 118, 123, 137, 138, 147, 153, 164, 166, 259, 262, 263, 317, 321, 324, 325, 328, 330, 334, 350, 351, 359], "auto_select_worker_config": [101, 102, 108, 110, 115], "autocal": [24, 26], "autocomplet": [118, 124], "autodiscoveri": [43, 46, 53, 58], "autograd": [224, 237], "autom": [24, 25, 26, 82, 83, 118, 123, 125, 337, 338, 343, 349], "automat": [0, 3, 5, 6, 9, 10, 13, 16, 17, 22, 24, 26, 27, 35, 39, 82, 83, 90, 91, 92, 93, 101, 102, 107, 110, 115, 126, 127, 137, 144, 147, 152, 181, 185, 198, 201, 206, 208, 213, 224, 225, 226, 227, 228, 229, 231, 232, 234, 236, 238, 240, 241, 242, 244, 246, 249, 252, 259, 261, 262, 266, 267, 271, 272, 273, 279, 281, 282, 285, 288, 293, 295, 298, 306, 307, 315, 317, 318, 321, 323, 324, 325, 326, 328, 329, 330, 331, 334, 335, 336, 337, 338, 340, 342, 343, 344, 347, 348, 349, 350, 351, 354, 355, 356, 357, 358, 359], "automodelforsequenceclassif": [293, 296, 298], "autoreload": [128, 129, 137, 138, 147, 148], "autosc": [8, 11, 24, 26, 28, 34, 35, 42, 84, 85, 88, 89, 92, 93, 101, 102, 106, 108, 128, 132, 134, 147, 151, 191, 197, 208, 218, 221, 224, 225, 302, 307, 313], "autoscal": [3, 5, 6, 10, 13, 14, 16, 24, 26, 27, 28, 34, 49, 50, 51, 52, 61, 62, 63, 64, 65, 80, 81, 84, 85, 126, 127, 128, 133, 136, 137, 139, 143, 146, 147, 148, 150, 151, 159, 161, 181, 187, 206, 213, 259, 261, 307, 311, 316], "autoscaling_config": [16, 101, 102, 108, 110, 113, 116, 118, 123], "autotoken": [293, 296, 298], "autotun": [7, 253, 258], "auxiliari": [324, 329], "avail": [1, 5, 6, 8, 9, 10, 17, 22, 28, 30, 43, 45, 53, 56, 82, 83, 90, 91, 92, 93, 101, 102, 104, 107, 110, 114, 118, 121, 124, 126, 127, 128, 129, 137, 138, 147, 148, 151, 155, 158, 159, 162, 163, 164, 167, 169, 170, 182, 183, 184, 188, 189, 191, 197, 198, 204, 206, 212, 213, 224, 226, 227, 229, 234, 238, 244, 245, 247, 259, 263, 267, 275, 277, 283, 284, 285, 290, 293, 295, 298, 302, 307, 313, 317, 323, 324, 328, 330, 332, 334, 343, 348, 349, 350, 355, 359], "available_resourc": [3, 181, 187], "availi": [80, 81, 92, 93], "avalanch": [285, 291], "averag": [5, 6, 9, 198, 204, 224, 225, 228, 259, 263, 285, 288, 330, 334, 337, 341, 350, 355], "avg_loss": 5, "avg_train_loss": [330, 334, 343, 347], "avg_val_loss": [330, 334, 343, 347], "avgpool": 13, "avoid": [2, 3, 5, 6, 8, 9, 10, 11, 88, 89, 90, 91, 92, 93, 101, 102, 108, 137, 139, 147, 151, 164, 167, 175, 180, 181, 188, 191, 195, 198, 202, 203, 206, 208, 212, 218, 220, 224, 225, 234, 237, 245, 248, 251, 259, 261, 262, 267, 284, 285, 288, 291, 324, 325, 329, 330, 332, 337, 339, 340, 341, 343, 345, 350, 359], "aw": [8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 30, 33, 34, 45, 48, 50, 51, 52, 55, 56, 57, 60, 63, 64, 65, 66, 76, 79, 86, 87, 94, 95, 96, 98, 118, 121, 191, 192, 198, 201, 206, 210, 213, 218, 222, 285, 292], "awai": [10, 82, 83, 100, 126, 127, 206, 211, 267, 283, 285, 288, 292, 349], "await": [4, 11, 12, 16, 147, 150, 171, 174, 218, 222], "awak": [285, 292], "awar": [147, 151, 324, 325, 330, 336, 343, 345], "award": [267, 283, 285, 292], "aws_region": [17, 23, 28, 30, 43, 45, 46, 48, 53, 56, 58, 60, 118, 121], "aws_role_nam": [35, 39], "awsregion": [43, 46, 53, 58], "ax": [7, 13, 14, 224, 226, 253, 255, 317, 319, 323, 350, 352], "axi": [5, 7, 10, 11, 13, 14, 15, 16, 206, 211, 213, 218, 222, 224, 226, 237, 253, 255, 293, 297, 317, 319, 323, 337, 340, 341, 343, 349, 350, 352, 359], "axvlin": [343, 349], "aydin": [118, 121], "azur": [17, 20, 224, 234, 245, 252, 350, 359], "b": [2, 3, 11, 175, 178, 180, 181, 184, 187, 218, 222, 224, 237, 267, 283, 317, 319, 320, 330, 336, 343, 346, 347, 349, 350, 359], "babi": [267, 281, 283], "back": [3, 4, 12, 16, 92, 93, 164, 167, 171, 174, 181, 190, 238, 241, 242, 267, 281, 283, 284, 285, 291, 292, 317, 323, 324, 325, 329, 330, 332, 337, 340, 341, 350, 359], "backbon": [224, 227, 317, 323], "backdrop": [285, 292], "backend": [110, 116, 118, 121, 137, 144, 159, 162, 163, 164, 165, 293, 298, 299, 350, 359], "background": [2, 175, 179, 285, 292], "backpressur": [8, 101, 102, 106, 164, 166, 191, 196, 197], "backpropag": [293, 298], "backward": [5, 6, 7, 13, 14, 137, 143, 224, 225, 228, 238, 240, 245, 247, 253, 256, 257, 259, 263, 285, 292, 293, 298, 317, 318, 330, 334, 343, 347, 350, 355], "bad": [267, 283, 285, 292], "bai": [285, 292], "bake": [3, 181, 186], "balanc": [3, 11, 17, 22, 24, 27, 28, 34, 51, 52, 64, 65, 92, 93, 101, 102, 107, 110, 112, 113, 115, 118, 124, 181, 190, 218, 220, 302, 306, 307, 313, 315, 324, 325], "bale": [267, 283], "ball": [267, 283], "band": [267, 283], "bandwidth": [101, 102, 104], "bank": [285, 292], "bar": [128, 136, 137, 146, 147, 154, 330, 332, 337, 339, 350, 352], "barca": [267, 283], "barcelona": [118, 121, 267, 283], "barh": [337, 341], "barr": [285, 292], "barrier": [317, 321, 324, 328], "base": [1, 4, 5, 6, 7, 8, 14, 16, 24, 25, 26, 27, 43, 44, 53, 54, 66, 67, 79, 80, 81, 84, 85, 88, 89, 92, 93, 96, 98, 101, 102, 106, 107, 118, 120, 121, 124, 125, 128, 129, 131, 136, 137, 138, 139, 140, 143, 147, 148, 150, 151, 154, 159, 163, 164, 167, 169, 170, 171, 174, 191, 192, 194, 195, 197, 203, 209, 210, 213, 224, 225, 234, 237, 253, 257, 259, 261, 262, 263, 267, 283, 285, 287, 291, 293, 298, 300, 305, 306, 307, 315, 317, 318, 319, 324, 325, 326, 336, 338, 339, 341, 343, 349, 350, 351, 357, 359], "base_dir": [350, 359], "base_model_id": [118, 121], "base_s3_path": [118, 121], "base_url": [92, 93, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123], "baselin": [7, 14, 224, 226, 253, 256, 324, 329, 330, 333], "basemodel": [4, 118, 122, 171, 172, 174], "bash": [1, 128, 129, 135, 137, 138, 145, 147, 148, 155, 158, 169, 170], "bash_profil": [1, 169, 170], "basic": [2, 5, 7, 10, 11, 13, 14, 15, 17, 21, 24, 27, 79, 90, 91, 100, 118, 119, 120, 125, 155, 158, 164, 165, 175, 176, 206, 215, 218, 223, 253, 254, 256, 257, 343, 345, 360], "basicblock": 13, "basicvariantgener": [7, 14, 253, 257], "basset": [137, 144, 146], "basset_10005": [137, 146], "bastion": [17, 22], "batch": [5, 6, 7, 11, 12, 13, 14, 16, 88, 89, 90, 91, 96, 98, 106, 107, 109, 126, 127, 130, 135, 136, 138, 139, 140, 143, 146, 147, 151, 164, 166, 172, 182, 187, 196, 197, 202, 205, 208, 211, 212, 213, 218, 220, 222, 224, 225, 228, 229, 232, 234, 237, 238, 239, 240, 241, 244, 245, 247, 252, 253, 255, 259, 262, 263, 277, 278, 284, 286, 293, 294, 295, 296, 298, 299, 312, 317, 319, 320, 324, 326, 327, 328, 329, 330, 331, 332, 334, 338, 340, 342, 344, 351], "batch_df": [317, 319], "batch_first": [343, 346], "batch_format": [9, 198, 202, 317, 319, 324, 326, 330, 332, 337, 339, 341, 342, 343, 349, 350, 359], "batch_idx": [6, 259, 262, 317, 320, 324, 327], "batch_metr": [137, 146], "batch_norm": [137, 140], "batch_pr": [10, 15, 206, 213], "batch_siz": [3, 5, 6, 7, 9, 10, 13, 14, 15, 16, 128, 131, 137, 139, 141, 143, 144, 146, 181, 189, 198, 202, 206, 211, 212, 213, 224, 228, 232, 238, 240, 241, 245, 247, 253, 255, 256, 257, 259, 262, 263, 267, 277, 283, 284, 293, 298, 317, 321, 324, 328, 330, 334, 337, 339, 343, 345, 347, 349, 350, 353, 354, 355, 356, 359], "batch_size_per_work": [6, 259, 263, 293, 298, 299], "batchnorm1d": [137, 140], "batchnorm2d": 13, "bathtub": [285, 292], "bathtuby": [285, 292], "batman": [267, 283], "batteri": [12, 147, 151], "battl": [267, 281, 283, 284, 285, 291, 292], "battleship": [285, 292], "bayesian": [7, 14, 253, 257], "bbc": [267, 283], "bc": [324, 329], "bd1": [267, 283], "beach": [118, 121], "bearer": [147, 153, 168], "bearer_token": [147, 153], "beast": [267, 283], "beauti": [267, 283, 285, 291, 292, 307, 308, 309, 316], "bebr": [285, 292], "becaus": [3, 7, 9, 14, 24, 26, 28, 33, 43, 51, 53, 63, 64, 82, 83, 101, 102, 105, 108, 128, 130, 137, 139, 159, 163, 181, 185, 198, 203, 253, 257, 285, 288, 291, 292, 317, 319, 321, 322, 337, 342, 350, 352, 357], "becom": [3, 8, 9, 128, 129, 137, 138, 147, 148, 159, 163, 181, 183, 189, 191, 197, 198, 201, 224, 229, 237, 267, 283, 317, 318], "bee": [267, 281, 283], "been": [8, 82, 83, 84, 85, 94, 95, 191, 196, 285, 291, 292, 337, 340], "befor": [3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 69, 78, 80, 81, 100, 101, 102, 105, 128, 130, 132, 137, 139, 164, 165, 171, 172, 174, 181, 189, 198, 204, 206, 215, 224, 226, 234, 238, 239, 240, 243, 244, 245, 247, 248, 250, 253, 256, 257, 259, 263, 285, 288, 306, 307, 315, 317, 319, 330, 332, 337, 339, 343, 345, 350, 351, 352, 353], "beforehand": [82, 83, 118, 121], "begin": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 82, 83, 100, 118, 124, 164, 165, 266, 267, 279, 285, 292, 330, 334], "beginn": [278, 286, 294, 312], "behalf": [155, 157], "behavior": [3, 84, 85, 118, 120, 121, 155, 157, 159, 161, 164, 167, 181, 184, 245, 246, 324, 329, 330, 332, 350, 352, 355, 357], "behaviour": [317, 319, 337, 341], "behind": [24, 27, 101, 102, 106, 285, 291, 292, 330, 331], "being": [5, 8, 13, 16, 84, 85, 88, 89, 191, 196, 285, 288, 291, 292, 307, 309, 316, 317, 319, 350, 352], "believ": [285, 291], "belong": [350, 351], "below": [2, 3, 5, 7, 8, 14, 24, 26, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 100, 101, 102, 106, 110, 116, 164, 166, 175, 180, 181, 185, 190, 191, 192, 224, 228, 236, 253, 257, 317, 323, 337, 342, 350, 359], "ben": [267, 281, 283, 284, 285, 291], "benchmark": [122, 337, 342, 350, 351], "benefit": [1, 3, 4, 7, 10, 12, 14, 17, 22, 125, 169, 170, 171, 173, 181, 184, 206, 212, 253, 256], "bergman": [285, 288], "berni": [267, 283], "bert": [278, 286, 293, 294, 295, 298, 299, 300, 312], "besok": [267, 283], "best": [4, 5, 7, 9, 10, 12, 14, 15, 17, 20, 35, 39, 84, 85, 101, 118, 121, 124, 128, 130, 159, 162, 171, 174, 198, 204, 206, 215, 224, 225, 234, 236, 238, 242, 245, 252, 253, 257, 267, 277, 283, 284, 285, 291, 292, 293, 295, 324, 329, 330, 336, 337, 340, 342, 343, 347, 349, 350, 351, 357, 359], "best_ckpt": [317, 321, 323, 324, 328, 329, 337, 340, 341, 342, 343, 347, 350, 356], "best_ckpt_path": [343, 349, 350, 359], "best_result": [7, 14, 253, 257], "best_run": [137, 144, 146, 147, 150], "best_val_loss": [137, 143], "better": [3, 7, 8, 10, 14, 88, 89, 92, 93, 118, 120, 126, 127, 181, 187, 191, 195, 206, 212, 245, 252, 253, 256, 267, 283, 285, 288, 290, 291, 302, 307, 313, 317, 323, 324, 329, 330, 336, 337, 341], "between": [3, 7, 8, 9, 10, 11, 14, 15, 17, 18, 19, 22, 24, 26, 53, 63, 79, 86, 87, 101, 102, 106, 110, 112, 118, 120, 121, 159, 162, 163, 181, 184, 191, 192, 194, 195, 198, 201, 204, 206, 208, 212, 215, 218, 220, 224, 229, 253, 255, 257, 285, 288, 289, 330, 331, 337, 339, 343, 345], "beyonc": [267, 283], "beyond": [94, 95, 118, 119, 123, 267, 283, 285, 291], "bf16": [6, 259, 262, 263], "bfloat16": [350, 359], "bia": [5, 7, 13, 14, 137, 140, 224, 227, 253, 256, 257, 337, 339], "bias": [245, 252, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "bidder": [285, 291], "bieber": [267, 283], "big": [8, 191, 192, 196, 285, 291, 293, 295, 317, 318, 324, 325, 330, 331], "bigger": [267, 283], "bigl": [350, 351], "bigqueri": [8, 191, 192], "bigr": [350, 351], "bill": [35, 37, 66, 68, 96, 98], "billion": [110, 112, 159, 163], "bin": [0, 66, 69, 330, 332], "binari": [12, 128, 130, 337, 338, 342], "bind": [4, 8, 11, 12, 16, 92, 93, 147, 150, 164, 167, 171, 174, 191, 192, 218, 222, 307, 309, 315], "birthdai": [267, 283], "bit": [3, 181, 189, 285, 291, 292], "bitten": [285, 291], "bjork": [285, 292], "bjp": [267, 283], "black": [7, 14, 253, 255, 267, 283, 285, 292, 330, 332], "blair": [267, 283], "blank": [80, 81], "bless": [267, 283], "blind": [285, 292], "blob": [224, 234, 330, 332, 350, 359], "block": [2, 3, 5, 7, 11, 13, 16, 43, 51, 53, 64, 101, 102, 108, 110, 114, 116, 118, 121, 122, 123, 128, 130, 164, 166, 167, 175, 179, 180, 181, 182, 188, 189, 201, 202, 203, 208, 211, 212, 213, 218, 221, 222, 223, 224, 235, 238, 239, 240, 253, 256, 266, 267, 271, 279, 281, 285, 290, 317, 319, 330, 331, 332, 337, 340], "block_out_channel": [6, 259, 262], "blockbust": [285, 292], "blog": [5, 6, 7, 13, 253, 258, 259, 264], "blow": [267, 283], "blue": [101, 102, 105, 267, 281, 283, 317, 318, 350, 351], "bn1": 13, "bn2": 13, "board": [285, 292], "boat": [285, 291], "bob": [267, 283], "bodi": [11, 16, 218, 222, 285, 288, 292], "boi": [285, 288, 292], "boilerpl": [224, 231, 234, 317, 318, 324, 325, 337, 338, 350, 359], "bomb": [285, 292], "bon": [267, 283], "book": [267, 281, 283, 284, 285, 291], "bookkeep": [350, 354], "bool": [350, 359], "boolean": 12, "boost": [4, 12, 171, 174, 337, 338, 339, 340, 342], "booster": [4, 12, 171, 174, 337, 340, 341, 342], "boot": [267, 283], "booth": [267, 283], "bootstrap": [24, 26], "border": [285, 291], "border_colli": [128, 130, 137, 144, 147, 150], "border_collie_1055": [128, 130], "bore": [285, 291], "both": [0, 8, 10, 17, 22, 24, 26, 53, 57, 96, 98, 100, 110, 111, 118, 122, 128, 130, 155, 157, 159, 161, 164, 167, 191, 192, 197, 206, 208, 224, 234, 236, 238, 239, 242, 245, 246, 250, 267, 283, 293, 295, 299, 317, 319, 337, 339, 340], "boto3": [86, 87, 118, 121], "bottleneck": [137, 144, 147, 151, 155, 157, 159, 163, 337, 339], "bottom": [3, 94, 95, 181, 189], "bound": [17, 22, 147, 151, 182], "boundari": [8, 191, 195], "bouquet": [267, 283], "bout": [267, 281, 283, 284], "box": [5, 88, 89, 94, 95, 224, 225], "br": [285, 288, 291, 292], "brain": [285, 291], "branch": 0, "brand": [118, 122], "braun": [267, 283], "break": [6, 7, 14, 88, 89, 253, 255, 259, 262, 285, 292, 350, 353], "breakdown": [8, 128, 134, 191, 192], "breakneck": [285, 292], "breakpoint": [293, 298], "breed": [126, 127], "breez": [285, 288], "brennan": [285, 291], "brew": [4, 28, 29, 43, 44, 53, 55, 66, 68, 155, 158, 171, 172], "bridg": [8, 15, 16, 126, 127, 191, 192, 224, 229, 285, 292], "brief": [118, 121], "bring": [4, 171, 173, 224, 235, 267, 281, 283, 285, 291, 292, 337, 339], "brit": [267, 283], "british": [267, 283, 285, 292], "broadcast": [224, 225], "broader": [8, 191, 196], "brock": [267, 283], "brought": [285, 291], "brown": [267, 283, 285, 288], "brows": 168, "browser": [0, 1, 35, 38, 82, 83, 169, 170], "bryant": [267, 283], "bst": [4, 171, 174], "bubbl": [267, 283], "bucket": [17, 21, 22, 23, 28, 30, 33, 34, 35, 36, 39, 42, 43, 45, 51, 53, 56, 63, 64, 66, 70, 78, 86, 87, 118, 121, 128, 130, 133], "bucket_nam": [35, 39, 118, 121], "budget": [118, 124], "buf": [317, 319, 350, 352], "buffalo": [267, 283], "buffer": [8, 101, 102, 106, 191, 192, 238, 241], "bug": [155, 157], "bui": [267, 283, 285, 291], "build": [1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 16, 17, 21, 80, 81, 84, 85, 100, 110, 117, 118, 120, 123, 125, 147, 151, 169, 170, 171, 173, 174, 175, 180, 181, 182, 191, 192, 195, 197, 198, 200, 206, 208, 211, 218, 219, 221, 222, 223, 226, 228, 231, 240, 242, 259, 262, 285, 292, 317, 318, 319, 324, 325, 330, 331, 332, 336, 338, 339, 343, 345, 349, 350, 353, 355, 359], "build_app": [11, 218, 223], "build_data_load": [7, 14, 253, 255, 256], "build_data_loader_ray_train": [5, 13, 224, 228, 232], "build_data_loader_ray_train_ray_data": [238, 240, 241, 245, 247], "build_data_loader_torch": [5, 13], "build_dataload": [343, 345, 347, 350, 354, 355], "build_inference_dataset": [350, 359], "build_openai_app": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123], "build_resnet18": [5, 13, 224, 227, 231, 237], "builder": [11, 218, 223], "built": [0, 2, 4, 5, 6, 8, 9, 12, 16, 24, 27, 28, 34, 35, 39, 82, 83, 84, 85, 88, 89, 101, 102, 106, 128, 135, 137, 144, 145, 147, 153, 154, 155, 158, 159, 161, 168, 171, 173, 175, 177, 191, 192, 198, 200, 224, 225, 227, 238, 240, 259, 261, 263, 302, 307, 313, 324, 325, 328, 330, 335, 337, 339, 340, 341, 342, 343, 349, 350, 351, 355, 359], "bulk": [90, 91], "bundl": [84, 85], "bunni": [285, 288], "burden": 100, "burrito": [350, 351], "bursti": [80, 81, 84, 85, 101, 102, 106], "busi": [8, 11, 118, 123, 147, 149, 151, 191, 192, 218, 221, 306, 307, 315], "button": [82, 83, 84, 85, 92, 93, 94, 95, 128, 136, 137, 146, 147, 154], "bx1": [317, 320], "bx3xhxw": [317, 320], "bxauk": [147, 153], "bypass": [3, 181, 184], "bystand": [285, 292], "byte": [10, 164, 166, 206, 212, 317, 319, 323, 350, 352], "bytesio": [128, 136, 317, 319, 350, 352, 353, 359], "byth": [86, 87], "c": [1, 3, 11, 84, 85, 86, 87, 169, 170, 181, 187, 218, 222, 224, 237, 267, 283, 285, 291, 330, 334, 337, 339, 340, 343, 347, 350, 357, 359], "cab": [4, 9, 12, 164, 166, 171, 174, 198, 201, 204], "cabin": [285, 292], "cabl": [285, 288], "cach": [3, 15, 17, 21, 84, 85, 104, 106, 109, 110, 116, 181, 183, 245, 252, 317, 319, 324, 326, 330, 332, 336, 337, 339, 343, 345, 350, 352, 353, 359], "cactu": [267, 281, 283], "caesar": [350, 351], "cahse": [267, 283], "calcul": [5, 9, 13, 88, 89, 137, 146, 198, 202, 224, 228, 293, 297, 298], "call": [3, 4, 5, 6, 7, 9, 10, 13, 14, 15, 92, 93, 102, 109, 119, 120, 125, 164, 167, 168, 171, 174, 176, 179, 181, 184, 189, 190, 198, 204, 205, 206, 211, 212, 215, 224, 226, 228, 231, 234, 235, 238, 240, 244, 245, 247, 248, 249, 250, 253, 257, 259, 263, 277, 284, 285, 292, 293, 300, 317, 319, 321, 322, 330, 334, 337, 339, 342, 350, 352, 356], "call_id": [118, 123], "callabl": [10, 11, 15, 118, 123, 128, 131, 206, 213, 218, 223, 266, 267, 273, 277, 279, 282, 284], "callback": [6, 259, 263, 317, 320, 321, 324, 328, 329, 337, 340, 342], "caller": [267, 275, 283], "came": [267, 283], "camera": [285, 291, 292], "campu": [267, 283], "can": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 22, 24, 26, 27, 28, 29, 30, 32, 33, 34, 35, 39, 41, 42, 43, 44, 45, 50, 51, 52, 53, 55, 56, 57, 62, 63, 64, 65, 66, 68, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 114, 115, 116, 118, 119, 120, 121, 123, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 139, 142, 144, 145, 146, 147, 149, 152, 153, 154, 155, 157, 158, 159, 161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 201, 202, 203, 204, 206, 207, 211, 212, 213, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 229, 230, 233, 235, 236, 237, 238, 239, 242, 243, 245, 246, 247, 248, 250, 252, 253, 254, 256, 257, 259, 261, 262, 263, 266, 267, 271, 273, 275, 277, 279, 281, 282, 283, 284, 285, 287, 288, 290, 291, 292, 293, 295, 298, 299, 300, 302, 306, 307, 313, 315, 318, 319, 320, 321, 325, 327, 331, 332, 338, 339, 344, 345, 348, 351, 352, 353], "canadian": [285, 291], "canari": [147, 153], "cancel": [285, 291], "candid": [118, 121, 317, 321, 324, 328, 329], "cannon": [285, 292], "cannot": [1, 3, 28, 31, 33, 35, 40, 43, 47, 53, 59, 66, 75, 84, 85, 169, 170, 181, 183, 285, 288], "canopi": [337, 342], "cant": [285, 292], "canva": [5, 6, 13, 259, 264], "capabl": [8, 11, 24, 27, 82, 83, 86, 87, 110, 112, 117, 118, 119, 120, 123, 124, 125, 126, 127, 159, 160, 162, 168, 191, 192, 196, 218, 221, 293, 299, 302, 307, 313], "capac": [28, 30, 43, 45, 53, 56, 84, 85, 101, 102, 104, 105, 126, 127, 330, 331], "capit": [101, 102, 108, 118, 121], "captain": [285, 291], "caption_lat": [6, 259, 262], "captur": [94, 95, 147, 152, 164, 167, 317, 321, 330, 334, 343, 344, 349], "car_typ": [118, 122], "card": [9, 110, 113, 198, 201], "cardescript": [118, 122], "cardiffnlp": [267, 271, 281], "care": [224, 234, 285, 291, 292, 343, 345], "carli": [267, 283], "carriag": [285, 292], "carrow": [267, 283], "cartograph": [337, 338], "cartpol": [324, 329], "cartyp": [118, 122], "case": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 24, 26, 28, 30, 43, 45, 53, 56, 84, 85, 88, 89, 101, 102, 105, 110, 112, 117, 120, 121, 125, 147, 151, 159, 162, 171, 173, 174, 175, 177, 181, 184, 190, 191, 195, 198, 204, 205, 206, 213, 215, 218, 223, 245, 248, 253, 257, 258, 259, 263, 264, 267, 277, 283, 284, 285, 292, 293, 298, 330, 331, 337, 338, 343, 347], "cash": [9, 198, 201], "castl": [285, 291], "casual": [267, 283], "cat": [317, 320, 324, 327, 343, 347], "catalog": [330, 331], "catch": [267, 283], "categor": [8, 88, 89, 191, 193, 195], "categori": [10, 24, 26, 206, 211, 293, 295, 350, 351], "cattl": [285, 291], "cattleman": [285, 291], "caus": [5, 6, 28, 30, 43, 45, 53, 56, 159, 163, 182, 224, 225, 259, 261], "cd": [0, 11, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 90, 91, 92, 93, 100, 118, 125, 126, 127, 159, 163, 164, 167, 218, 223, 350, 359], "cdot": [317, 318, 324, 325], "ceil_mod": 13, "cell": [4, 5, 6, 7, 9, 10, 11, 13, 82, 83, 84, 85, 86, 87, 90, 91, 128, 129, 137, 138, 147, 148, 171, 174, 198, 205, 206, 217, 218, 223, 224, 226, 237, 253, 258, 259, 260, 264, 330, 332, 343, 345], "celsiu": [3, 118, 123, 181, 190], "cena": [267, 281, 283], "center": [224, 232, 267, 283, 285, 288, 317, 319, 350, 351, 352], "center_input_sampl": [6, 259, 262], "centercrop": [317, 319, 350, 352], "central": [118, 121, 159, 162, 224, 234], "centric": [8, 191, 195], "ceph": [8, 191, 192], "cerebr": [285, 291], "cert": [24, 27], "certain": [5, 6, 7, 9, 14, 198, 202, 253, 257, 259, 260, 285, 288, 305, 306, 307, 315], "certif": [24, 27], "chain": [15, 147, 154, 182], "chair": [267, 283], "chalk": [285, 292], "challeng": [5, 6, 10, 11, 100, 103, 109, 126, 127, 206, 208, 218, 220, 224, 225, 238, 239, 259, 261], "chanc": [3, 181, 185, 267, 283], "chang": [3, 5, 6, 7, 8, 10, 11, 14, 28, 30, 35, 39, 43, 45, 53, 56, 63, 86, 87, 88, 89, 92, 93, 110, 115, 117, 118, 121, 128, 129, 137, 138, 143, 147, 148, 159, 161, 164, 167, 181, 183, 191, 192, 206, 216, 218, 220, 223, 224, 225, 253, 257, 259, 261, 263, 267, 283, 285, 291, 293, 299, 317, 318, 321, 323, 324, 325, 330, 331, 337, 338, 343, 349, 350, 351, 358, 359], "channel": [10, 13, 15, 16, 159, 163, 164, 167, 206, 212, 224, 226, 227, 237, 285, 292, 317, 318, 319, 320, 343, 349, 350, 351], "chao": [285, 292], "chap": [28, 29, 43, 44, 53, 55], "charact": [101, 102, 104, 285, 292, 307, 308, 309, 316], "characterist": [8, 9, 11, 15, 101, 102, 104, 191, 192, 198, 201, 218, 221], "charg": [9, 198, 201], "charli": [267, 283], "charm": [267, 283, 285, 291], "chart": [43, 46, 48, 53, 58, 60, 66, 76, 337, 339], "chase": [267, 281, 283], "chat": [101, 102, 105, 108, 110, 114, 115, 118, 121, 122, 123, 124], "chatbot": [118, 123, 124], "cheap": [267, 283], "cheaper": [8, 191, 194], "cheapli": [285, 288], "cheat": [285, 291], "check": [3, 5, 6, 9, 10, 11, 13, 28, 32, 35, 41, 43, 49, 50, 53, 61, 62, 63, 66, 71, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 101, 102, 106, 118, 123, 164, 166, 168, 181, 187, 198, 201, 203, 206, 212, 213, 218, 222, 224, 226, 234, 235, 245, 247, 259, 263, 267, 283, 293, 298, 321, 324, 328, 330, 332, 337, 339, 342], "check_cal": [317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "check_val_every_n_epoch": [317, 321, 324, 328], "checkout": [88, 89, 92, 93], "checkpoint": [9, 10, 12, 14, 86, 87, 126, 127, 128, 130, 132, 137, 142, 143, 144, 198, 205, 206, 208, 225, 226, 227, 228, 235, 236, 238, 240, 244, 246, 249, 251, 252, 261, 262, 293, 300, 318, 319, 321, 325, 326, 329, 331, 332, 336, 338, 340, 344, 345, 347, 349, 350, 351, 352, 355, 356, 357, 358, 359], "checkpoint_": [350, 359], "checkpoint_000000": 13, "checkpoint_000001": 13, "checkpoint_at_end": [343, 347], "checkpoint_config": [317, 321, 324, 328, 330, 334, 337, 340, 343, 347, 350, 356], "checkpoint_dir": [350, 359], "checkpoint_dir_nam": 13, "checkpoint_frequ": [317, 321, 324, 328, 337, 340, 350, 356], "checkpoint_nam": [337, 340], "checkpoint_path": [5, 6, 13, 259, 263, 343, 349, 350, 359], "checkpoint_root": [350, 359], "checkpoint_score_": [343, 347], "checkpoint_score_attribut": [317, 321, 324, 328, 337, 340, 343, 347, 350, 356], "checkpoint_score_ord": [317, 321, 324, 328, 337, 340, 343, 347, 350, 356], "checkpointconfig": [317, 318, 319, 321, 324, 325, 326, 328, 330, 331, 332, 334, 337, 338, 339, 340, 343, 345, 347, 350, 352, 356], "cheekbon": [267, 283], "chelsea": [267, 283], "cheri": [267, 283], "chill": [267, 283], "chip": [1, 169, 170], "chloe": [285, 288], "chmod": [155, 158], "choic": [8, 9, 24, 26, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 126, 127, 191, 197, 198, 205, 302, 307, 313, 343, 344], "choos": [1, 10, 15, 17, 18, 20, 53, 63, 66, 73, 80, 81, 82, 83, 84, 85, 101, 102, 106, 119, 125, 126, 127, 169, 170, 206, 214, 285, 292, 330, 336], "choreo": [267, 283], "chose": [5, 13], "chown": [155, 158], "chri": [267, 283], "chrisbrown": [267, 283], "christian": [267, 283], "chromadb": [8, 191, 192], "chronolog": [159, 161], "chuck": [267, 283], "chunk": [8, 10, 15, 88, 89, 101, 102, 108, 110, 114, 115, 118, 121, 128, 130, 191, 195, 206, 215], "church": [267, 283], "churn": [343, 349], "chw": [317, 319], "ci": [90, 91, 118, 125, 126, 127, 245, 252, 350, 359], "ciara": [267, 283], "cidr": [17, 22], "cidr_block": [17, 22], "cif": [267, 283], "cifar": [238, 244, 245, 248, 250], "cifar10": [238, 242, 245, 251], "cinema": [285, 288, 291, 292], "cinemat": [285, 292], "cinematograph": [285, 292], "cinematographi": [285, 291, 292], "cineworld": [267, 283], "citi": [4, 9, 12, 118, 123, 171, 174, 198, 201, 267, 283, 285, 292, 343, 344], "citizenship": [267, 283], "ckpt": [5, 6, 13, 259, 263, 317, 318, 321, 323, 324, 325, 328, 329, 330, 334, 337, 340, 341, 343, 347, 349, 350, 355], "ckpt_dir": [5, 6, 13, 224, 237, 245, 247, 259, 263, 317, 323, 324, 329, 330, 334, 336, 343, 347, 349, 350, 355, 359], "ckpt_file": [317, 323, 324, 329], "ckpt_out": [330, 334, 343, 347, 350, 355], "ckpt_path": [6, 259, 263, 317, 321, 324, 328], "ckpt_root": [317, 321, 324, 328], "cl": [137, 140, 146], "claim": [8, 191, 196, 285, 288, 291], "clamp": [317, 323], "clarifi": [96, 97], "class": [3, 4, 5, 6, 7, 10, 11, 12, 14, 15, 16, 118, 122, 128, 130, 131, 137, 139, 140, 146, 147, 149, 150, 164, 167, 171, 174, 181, 190, 206, 210, 213, 218, 222, 224, 227, 235, 237, 238, 242, 253, 255, 257, 259, 262, 263, 266, 277, 279, 283, 284, 306, 307, 315, 317, 318, 319, 320, 323, 324, 327, 330, 333, 338, 341, 343, 345, 346, 349, 350, 351, 352, 353, 359], "class_nam": [3, 181, 190], "class_to_label": [137, 139, 143, 144, 146], "classic": [324, 325, 329, 330, 331, 337, 338], "classif": [4, 12, 15, 126, 127, 171, 174, 219, 224, 226, 227, 228, 293, 295, 296, 298, 300, 355], "classifi": [7, 11, 14, 126, 127, 137, 139, 147, 149, 150, 218, 222, 253, 256, 293, 295, 350, 351], "classificationmodel": [137, 140, 143, 146], "classmat": [285, 288], "classmethod": [137, 140, 146], "classpredictor": [147, 149, 150], "claud": [118, 124], "cld": [86, 87, 110, 115, 147, 153], "cld_g54aiirwj1s8t9ktgzikqur41k": [86, 87], "cld_kvedzwag2qa8i5bjxuevf5i7": [128, 129, 137, 138, 147, 148], "cldrsrc_12345abcdefgh67890ijklmnop": [43, 47, 48, 53, 59, 60, 66, 75, 76], "clean": [0, 11, 28, 33, 35, 42, 66, 78, 128, 133, 137, 139, 142, 218, 221, 234, 285, 292, 318, 320, 321, 332, 343, 347, 349, 351, 352, 355, 358], "cleaner": [224, 226], "cleanli": [224, 234, 330, 332], "cleanup": [4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 24, 26, 79, 90, 91, 171, 174, 198, 205, 206, 217, 218, 223, 224, 226, 245, 251, 253, 258, 259, 264, 317, 323, 324, 329, 330, 336, 337, 342, 350, 359], "clear": [0, 96, 99, 285, 291, 324, 329, 337, 342, 343, 349, 350, 351, 359], "clearli": [164, 165, 285, 292], "cli": [17, 18, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 74, 82, 83, 86, 87, 88, 89, 90, 91, 126, 127, 128, 135, 147, 154, 164, 167, 168], "click": [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 17, 22, 28, 29, 43, 44, 53, 55, 63, 66, 68, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 101, 102, 103, 110, 111, 116, 118, 119, 128, 129, 137, 138, 147, 148, 155, 158, 164, 166, 167, 171, 172, 175, 176, 180, 181, 182, 190, 198, 199, 206, 207, 212, 218, 219, 253, 254, 257, 259, 260, 263, 266, 267, 279, 285, 287, 293, 295, 302, 307, 313], "client": [86, 87, 101, 102, 107, 108, 110, 114, 115, 118, 121, 122, 123, 302, 313], "cliff": [267, 283], "clip": [128, 131, 136, 137, 139, 147, 150], "clipboard": [92, 93], "clipmodel": [128, 131, 137, 139, 147, 148, 149], "clipprocessor": [128, 131, 137, 139, 147, 148, 149], "clitori": [285, 288], "clock": [285, 292], "clog": [285, 292], "clone": [0, 92, 93, 324, 329], "close": [224, 234, 267, 283, 285, 292], "cloud": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 18, 21, 22, 24, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 41, 42, 44, 45, 48, 49, 50, 51, 52, 54, 56, 60, 61, 62, 64, 65, 67, 68, 70, 76, 77, 78, 79, 80, 81, 84, 85, 94, 95, 99, 100, 101, 102, 103, 107, 108, 110, 111, 115, 118, 119, 121, 126, 127, 128, 129, 130, 133, 137, 139, 147, 153, 159, 163, 169, 170, 171, 172, 175, 176, 181, 182, 198, 199, 206, 207, 218, 219, 224, 225, 234, 245, 252, 253, 254, 259, 260, 266, 267, 279, 285, 287, 293, 295, 299, 302, 307, 313, 317, 323, 330, 336, 337, 338, 350, 351], "cloud_deployment_id": [43, 48, 53, 60, 66, 76], "cloud_nam": [43, 50, 53, 62], "clouddeploymentid": [43, 45, 48, 53, 56, 60, 66, 76], "cloudflar": [8, 191, 192], "cloudform": [17, 22], "cloudfound": [17, 22, 35, 39], "cloudprovid": [43, 45, 48, 53, 56, 60, 66, 76], "cloudresourcemanag": [35, 38, 66, 69], "cloudwatch": [17, 22], "club": [267, 281, 283, 285, 291], "cluster": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 35, 41, 42, 45, 49, 50, 51, 52, 55, 56, 57, 61, 62, 63, 64, 65, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 94, 95, 101, 102, 103, 107, 108, 110, 111, 115, 118, 119, 126, 127, 128, 129, 130, 132, 133, 134, 136, 137, 138, 139, 143, 146, 147, 148, 150, 157, 159, 161, 162, 164, 165, 169, 170, 171, 172, 174, 175, 176, 178, 179, 182, 183, 184, 186, 188, 190, 191, 195, 198, 199, 203, 205, 206, 207, 208, 210, 211, 212, 213, 214, 217, 218, 219, 221, 224, 225, 226, 227, 230, 232, 234, 235, 236, 237, 238, 239, 242, 243, 244, 248, 252, 253, 254, 259, 260, 261, 266, 271, 279, 281, 283, 285, 287, 295, 299, 302, 309, 313, 317, 318, 319, 323, 324, 325, 326, 329, 330, 331, 332, 334, 336, 337, 338, 339, 342, 343, 344, 350, 351, 356, 359], "cluster_id": [88, 89], "cluster_storag": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 86, 87, 128, 133, 137, 139, 142, 144, 147, 150, 164, 166, 171, 174, 198, 203, 206, 213, 218, 222, 224, 226, 227, 232, 234, 236, 238, 242, 245, 251, 259, 263, 264, 317, 319, 321, 323, 324, 328, 329, 330, 332, 334, 336, 337, 339, 340, 342, 343, 344, 345, 350, 351, 352, 353, 355, 356, 359], "clusternam": [43, 46, 53, 58], "clusteronc": [28, 32], "cm": [337, 341], "cm_norm": [337, 341], "cmap": [5, 7, 10, 13, 14, 16, 206, 211, 224, 226, 237, 253, 255, 337, 341], "cnn": [224, 226, 317, 320, 323, 350, 351], "co": [118, 121, 324, 325, 326, 329, 343, 346], "coach": [267, 283], "coars": [11, 218, 221], "cocki": [285, 291], "code": [0, 1, 2, 3, 5, 6, 7, 9, 11, 13, 14, 16, 17, 18, 79, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 110, 115, 117, 120, 124, 126, 127, 137, 143, 159, 163, 164, 167, 169, 170, 172, 175, 177, 181, 187, 190, 198, 201, 218, 222, 223, 224, 225, 228, 229, 230, 231, 232, 233, 238, 239, 245, 252, 253, 257, 259, 261, 263, 267, 283, 302, 307, 313, 317, 318, 324, 325, 330, 331, 336, 337, 338, 339, 343, 344, 345, 349, 350, 351, 359], "coder": [118, 124], "cogent": [285, 288], "coher": [285, 291, 292], "coi": [285, 291], "col": [5, 6, 224, 226, 237, 259, 262, 330, 334, 343, 347, 350, 357], "colbert": [267, 283], "cold": [101, 102, 108], "collabor": [96, 98, 99, 330, 331, 333, 336, 360], "collat": [6, 259, 262, 293, 298], "collate_fn": [137, 141, 143, 146, 147, 148, 149, 293, 298], "colleagu": [267, 283], "collect": [2, 3, 9, 10, 16, 17, 19, 155, 157, 158, 175, 177, 180, 181, 183, 198, 201, 206, 210, 215, 224, 226, 233, 235, 237, 267, 283, 324, 328, 329, 337, 340, 341, 350, 355, 360], "collector": [224, 237], "colli": [147, 150], "collison": [267, 284], "colour": [285, 292], "column": [4, 6, 9, 10, 82, 83, 128, 131, 137, 139, 146, 171, 174, 198, 201, 204, 206, 211, 224, 236, 238, 242, 259, 262, 267, 272, 273, 282, 285, 289, 292, 317, 319, 321, 323, 324, 328, 330, 332, 334, 336, 337, 339, 340, 343, 345, 347, 349, 350, 352, 353, 357, 359], "column_nam": [337, 340], "column_stack": [337, 340], "columnar": [8, 191, 192, 238, 242, 317, 319, 337, 339, 343, 345, 350, 352], "com": [0, 1, 13, 14, 17, 22, 24, 27, 28, 29, 35, 37, 38, 39, 43, 44, 53, 55, 63, 66, 68, 69, 70, 88, 89, 92, 93, 100, 110, 115, 126, 127, 128, 129, 136, 137, 138, 147, 148, 150, 153, 168, 169, 170, 343, 345], "combin": [3, 8, 181, 187, 191, 192, 224, 234, 285, 291, 292, 317, 318], "combur": [267, 283], "comcast": [267, 283], "come": [3, 24, 26, 84, 85, 88, 89, 100, 126, 127, 128, 134, 181, 186, 267, 283, 285, 288, 291, 292, 337, 342, 343, 349, 350, 359], "comedi": [285, 292], "comfort": [350, 359], "command": [17, 20, 23, 28, 30, 31, 40, 42, 43, 45, 46, 47, 51, 53, 56, 58, 59, 64, 66, 69, 75, 78, 82, 83, 88, 89, 90, 91, 92, 93, 128, 129, 135, 137, 138, 147, 148, 153, 155, 158, 159, 163, 164, 167, 324, 329], "commend": [285, 288], "comment": [92, 93, 306, 307, 315], "commerci": [8, 191, 192, 194], "commiss": [4, 9, 12, 171, 174, 198, 201], "commit": [317, 319, 350, 352], "common": [2, 3, 4, 9, 10, 53, 63, 66, 71, 118, 125, 155, 157, 164, 167, 171, 173, 175, 177, 181, 190, 198, 201, 203, 206, 210, 213, 267, 283, 285, 289, 291, 292], "common_prefix": [35, 39], "commonli": [8, 10, 191, 192, 195, 206, 209, 330, 333], "commun": [3, 8, 9, 10, 15, 17, 22, 101, 102, 105, 110, 116, 117, 118, 125, 181, 190, 191, 196, 198, 204, 206, 215, 343, 345], "compact": [11, 147, 151, 153, 218, 220, 317, 319, 323, 350, 351, 352], "compani": [24, 27, 96, 97], "compar": [7, 8, 14, 15, 84, 85, 101, 102, 104, 110, 112, 159, 160, 191, 195, 253, 257, 317, 323, 324, 329, 343, 347, 350, 357, 359], "comparison": [285, 292], "compat": [101, 102, 107, 110, 113, 118, 121, 164, 165, 238, 241, 350, 351], "compet": [8, 191, 196], "competit": [267, 283], "compil": [1, 137, 144, 169, 170, 278, 286, 294, 312], "complet": [1, 3, 4, 5, 6, 13, 24, 26, 28, 30, 43, 50, 53, 62, 82, 83, 88, 89, 90, 91, 100, 101, 102, 104, 105, 108, 110, 111, 114, 115, 118, 121, 122, 123, 124, 125, 159, 163, 168, 169, 170, 171, 174, 181, 188, 224, 225, 235, 245, 247, 248, 250, 252, 259, 261, 285, 292, 293, 300, 317, 321, 322, 323, 324, 328, 329, 350, 357], "complex": [4, 8, 11, 16, 24, 26, 100, 101, 102, 105, 107, 110, 112, 117, 118, 120, 124, 126, 127, 171, 174, 191, 192, 195, 196, 197, 218, 220, 317, 318], "compli": [6, 259, 263], "complianc": [17, 20, 110, 115, 118, 125], "compliant": [17, 22], "compon": [2, 6, 7, 8, 11, 14, 17, 20, 22, 25, 52, 65, 66, 72, 84, 85, 88, 89, 101, 102, 106, 107, 109, 128, 134, 137, 143, 146, 159, 161, 162, 168, 175, 177, 191, 192, 218, 220, 224, 226, 253, 257, 259, 263, 343, 345, 350, 352], "compos": [5, 6, 7, 10, 11, 13, 14, 15, 147, 149, 206, 207, 212, 218, 220, 221, 224, 226, 232, 238, 243, 253, 254, 255, 257, 259, 262, 317, 319, 350, 352, 353, 359], "composit": [8, 147, 149, 191, 197, 285, 292], "comprehens": [1, 9, 10, 15, 17, 21, 66, 67, 79, 101, 102, 103, 110, 116, 117, 118, 120, 121, 122, 123, 125, 168, 169, 170, 198, 201, 203, 206, 208, 216], "compress": [337, 339, 343, 349], "comput": [2, 3, 4, 5, 6, 9, 10, 11, 12, 15, 16, 17, 20, 22, 24, 26, 27, 28, 34, 38, 39, 42, 43, 45, 50, 53, 56, 62, 66, 69, 70, 71, 78, 79, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 96, 98, 99, 100, 101, 102, 104, 105, 107, 108, 126, 127, 128, 135, 136, 137, 139, 143, 145, 147, 149, 151, 153, 171, 174, 175, 177, 179, 180, 181, 182, 183, 187, 190, 192, 194, 196, 198, 204, 205, 206, 208, 212, 214, 215, 217, 218, 220, 224, 225, 227, 228, 238, 240, 259, 261, 263, 266, 267, 279, 285, 289, 293, 295, 297, 298, 299, 302, 307, 313, 319, 330, 331, 334, 336, 337, 340, 341, 342, 355, 359, 360], "computation": [7, 14, 253, 256], "compute_accuraci": [10, 15, 206, 215], "compute_config": [43, 50, 53, 62, 101, 102, 108, 110, 115, 159, 163], "compute_metr": [293, 297], "compute_nodes_service_account_email": [35, 39, 66, 70], "compute_tip_percentag": [9, 198, 202], "computeconfig": [43, 50, 53, 62], "con": [267, 283], "conc": [35, 41], "concat_t": [337, 340], "concept": [8, 10, 24, 26, 79, 103, 147, 154, 164, 165, 191, 194, 206, 207, 224, 227], "conceptu": 5, "concern": [2, 11, 175, 177, 218, 221, 285, 291], "concert": [267, 283], "concis": [118, 121], "conclud": [137, 146], "conclus": [119, 285, 287], "concomit": [267, 283], "concret": [17, 18], "concurr": [3, 4, 8, 9, 11, 12, 15, 16, 101, 102, 105, 117, 128, 130, 131, 137, 139, 146, 171, 174, 181, 187, 191, 197, 198, 205, 213, 218, 221, 224, 228, 266, 267, 275, 279, 283, 285, 290, 343, 349, 350, 359], "concurrency_limit": [10, 206, 212], "concuss": [267, 281, 283, 284], "conda": [155, 158], "condit": [3, 6, 17, 22, 118, 121, 181, 183, 259, 262, 285, 287, 317, 323], "conductor": [285, 292], "confid": [245, 246, 317, 323, 324, 329, 330, 336, 337, 342], "config": [4, 5, 6, 7, 11, 12, 13, 14, 16, 35, 38, 66, 69, 126, 127, 128, 135, 137, 143, 144, 145, 147, 153, 155, 158, 164, 167, 171, 174, 218, 223, 224, 227, 228, 229, 230, 235, 238, 240, 245, 247, 250, 253, 257, 259, 263, 293, 298, 317, 321, 324, 325, 328, 330, 334, 337, 340, 343, 347, 350, 352, 355], "configur": [7, 10, 11, 14, 16, 17, 19, 20, 21, 22, 24, 25, 27, 28, 29, 30, 34, 37, 39, 43, 44, 45, 50, 53, 55, 56, 62, 68, 70, 71, 80, 81, 88, 89, 90, 91, 92, 93, 107, 109, 111, 117, 120, 125, 128, 135, 137, 143, 145, 147, 149, 153, 155, 158, 159, 163, 182, 206, 208, 212, 218, 221, 223, 225, 226, 227, 229, 246, 250, 252, 253, 257, 262, 293, 295, 296, 298, 299, 300, 306, 307, 315, 317, 321, 324, 328, 330, 331, 334, 335, 338, 343, 344, 347, 350, 351, 355, 359], "configure_optim": [6, 259, 262, 317, 320, 324, 327], "confirm": [92, 93, 118, 121, 224, 226, 235, 245, 250, 317, 319, 321, 330, 332, 334, 337, 339, 342, 343, 345, 350, 352], "conflict": [224, 237], "confluent": [8, 191, 195], "confus": [10, 137, 146, 206, 212, 350, 359], "confusion_matrix": [337, 339, 341], "congratul": [110, 117, 118, 125], "congress": [267, 283], "conjur": [285, 292], "connect": [3, 5, 6, 10, 17, 22, 24, 27, 28, 34, 43, 46, 53, 58, 66, 72, 80, 81, 82, 83, 86, 87, 96, 98, 128, 129, 132, 137, 138, 147, 151, 181, 184, 206, 209, 224, 225, 238, 244, 259, 261, 267, 271, 281, 285, 292, 337, 341], "connector": [9, 10, 198, 205, 206, 209], "consecut": [267, 281, 283], "consid": [2, 3, 5, 6, 13, 110, 117, 118, 124, 164, 165, 175, 180, 181, 183, 185, 186, 187, 189, 219, 259, 263, 285, 288, 290, 291, 317, 319], "consider": [24, 26, 100], "consist": [7, 8, 14, 15, 79, 80, 81, 84, 85, 118, 120, 122, 125, 191, 192, 224, 225, 226, 227, 238, 239, 253, 255, 278, 286, 294, 312, 324, 329, 337, 340, 342, 343, 345, 346], "consol": [28, 31, 32, 35, 40, 41, 43, 47, 50, 53, 59, 62, 63, 66, 75, 77, 82, 83, 90, 91, 92, 93, 118, 125, 126, 127, 128, 129, 137, 138, 147, 148, 153, 155, 157, 164, 165, 168, 330, 334], "consolid": [147, 153], "conspicu": [267, 283], "constant": [6, 101, 102, 105, 259, 262, 285, 292, 343, 349], "constraint": [6, 101, 102, 106, 118, 124, 259, 262, 267, 277, 284, 285, 292], "construct": [3, 6, 181, 190, 224, 231, 232, 259, 262, 324, 325, 343, 347, 350, 353, 354], "constructor": [3, 10, 11, 15, 16, 181, 190, 206, 213, 218, 222, 266, 267, 279], "consum": [7, 9, 10, 14, 15, 28, 30, 43, 45, 53, 56, 198, 201, 203, 206, 209, 211, 238, 239, 244, 245, 252, 253, 257, 343, 345], "consumptionapi": [10, 206, 211, 215], "contain": [0, 3, 4, 5, 6, 10, 12, 13, 15, 24, 26, 27, 43, 51, 53, 64, 66, 69, 72, 80, 81, 82, 83, 86, 87, 92, 93, 118, 123, 147, 149, 171, 174, 181, 184, 206, 210, 224, 227, 236, 245, 250, 259, 263, 285, 292, 330, 332, 337, 339, 340, 343, 345, 349, 350, 351, 357, 360], "container": [11, 79, 218, 220], "containerfil": [101, 102, 108, 110, 115, 126, 127, 128, 135, 137, 145, 147, 153], "content": [10, 86, 87, 100, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 128, 136, 147, 153, 168, 206, 212, 330, 336, 350, 359], "context": [104, 106, 110, 113, 116, 159, 161, 163, 317, 321, 324, 328, 330, 336, 343, 344, 350, 351], "contextu": [155, 157, 159, 162, 163], "contigu": [10, 206, 210, 330, 331, 332], "continu": [5, 8, 13, 80, 81, 86, 87, 107, 109, 118, 124, 137, 144, 159, 163, 191, 197, 245, 247, 248, 249, 250, 267, 283, 285, 292, 317, 319, 324, 329, 330, 335, 350, 352, 355, 357, 358], "contrast": [8, 191, 195, 196, 285, 291, 292], "control": [4, 8, 9, 16, 19, 20, 21, 25, 27, 51, 52, 64, 65, 84, 85, 94, 95, 96, 98, 100, 101, 102, 106, 108, 118, 121, 159, 163, 164, 167, 171, 174, 191, 192, 198, 202, 224, 228, 238, 241, 266, 267, 275, 279, 283, 324, 325, 329, 330, 331, 337, 340, 350, 351], "controversi": [285, 288], "conv1": [5, 7, 13, 14, 224, 227, 253, 256, 257], "conv2": 13, "conv2d": [5, 7, 13, 14, 224, 227, 253, 256, 257, 317, 320], "convei": [285, 292], "conveni": [1, 4, 5, 128, 130, 169, 170, 171, 173, 224, 226], "convent": [224, 225, 285, 292], "converg": [317, 321, 323, 324, 328, 330, 334, 343, 347, 350, 357], "convers": [118, 121, 123, 124, 238, 243], "convert": [3, 6, 9, 13, 101, 102, 104, 128, 131, 136, 137, 139, 147, 149, 181, 190, 198, 202, 224, 232, 237, 238, 241, 242, 243, 259, 262, 267, 277, 281, 284, 287, 293, 298, 305, 306, 307, 315, 317, 319, 321, 330, 331, 332, 337, 338, 340, 343, 345, 350, 352, 353, 359], "convert_to_label": [137, 139], "convnext": [350, 359], "convolut": [224, 227, 317, 318], "cool": [285, 292], "coordin": [17, 22, 317, 318, 324, 325, 337, 338, 340, 350, 351, 359], "copi": [3, 5, 8, 82, 83, 84, 85, 90, 91, 92, 93, 164, 166, 181, 183, 191, 192, 224, 225, 232, 267, 283, 330, 332, 334, 337, 340, 343, 347, 349, 350, 357, 359], "cor": [17, 22, 53, 63], "cordern": [128, 129, 137, 138, 147, 148], "core": [4, 6, 7, 10, 11, 12, 14, 88, 89, 126, 127, 171, 173, 177, 187, 206, 208, 218, 220, 224, 226, 253, 256, 257, 259, 262, 267, 271, 281, 317, 318, 319, 321, 323, 324, 329, 330, 331, 334, 337, 340, 350, 351, 352], "corner": [82, 83, 267, 283], "correct": [5, 8, 13, 159, 163, 191, 197, 224, 227, 228, 231, 232, 234, 238, 240, 245, 247, 248, 267, 275, 283, 330, 335, 337, 338, 341, 342, 343, 345, 350, 351, 354, 355], "correct_squar": [3, 181, 185], "correct_square_mod": [3, 181, 185], "correctli": [1, 53, 63, 96, 97, 137, 139, 169, 170, 224, 226, 235, 237, 245, 250, 317, 319, 330, 332, 334, 337, 339, 343, 345, 350, 352, 353], "correl": [7, 14, 253, 257], "correspond": [2, 8, 13, 175, 179, 191, 197, 224, 236, 330, 333], "corrupt": [317, 318, 319], "cosin": [128, 136], "cost": [2, 5, 6, 8, 13, 84, 85, 88, 89, 90, 91, 92, 93, 105, 107, 108, 109, 110, 112, 117, 121, 126, 127, 159, 163, 175, 177, 191, 192, 194, 196, 259, 264, 324, 329], "costum": [267, 281, 283], "could": [3, 9, 101, 102, 106, 128, 135, 137, 139, 145, 147, 153, 181, 185, 189, 198, 204, 267, 273, 282, 283, 285, 291, 292, 302, 307, 313], "couldn": [267, 283], "count": [2, 9, 10, 15, 128, 130, 155, 157, 175, 177, 198, 204, 205, 206, 215, 285, 292, 317, 319, 324, 326, 330, 332, 334, 337, 339, 341, 344], "countri": [118, 123, 267, 283, 285, 288, 330, 336], "countrymen": [285, 288], "coup": [118, 122], "coupl": [285, 292], "cours": [17, 20, 79, 118, 125, 128, 135, 155, 157, 158, 164, 165, 285, 291, 292, 360], "cover": [17, 20, 80, 81, 82, 83, 84, 85, 88, 89, 100, 101, 102, 109, 110, 111, 117, 125, 126, 127, 155, 156, 157, 267, 277, 278, 284, 285, 286, 292, 293, 294, 300, 312, 341, 342], "cover_typ": [337, 339], "covtyp": [337, 339, 340, 342], "covtype_xgb_cpu": [337, 340], "coward": [285, 292], "cp": [10, 11, 15, 16, 206, 213, 218, 222], "cpu": [3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 80, 81, 84, 85, 88, 89, 128, 129, 131, 132, 133, 136, 137, 139, 140, 143, 144, 146, 147, 149, 155, 157, 158, 159, 161, 171, 174, 181, 187, 188, 190, 198, 203, 205, 206, 208, 210, 212, 213, 218, 222, 223, 224, 227, 230, 231, 232, 234, 235, 237, 238, 239, 244, 245, 252, 253, 256, 257, 259, 262, 263, 266, 267, 272, 273, 277, 279, 282, 283, 284, 285, 287, 293, 295, 298, 299, 300, 317, 323, 324, 325, 329, 330, 334, 336, 338, 339, 340, 342, 343, 347, 349, 350, 355, 359], "cpus_per_work": [337, 340], "craft": [285, 291, 292], "crappi": [285, 288], "crash": [8, 147, 153, 191, 195, 245, 249, 267, 281, 283, 284, 337, 338, 350, 358], "crazi": [267, 281, 283, 285, 292], "cream": [267, 283], "creat": [3, 4, 7, 8, 9, 10, 12, 13, 14, 15, 16, 19, 20, 21, 23, 24, 26, 27, 31, 32, 34, 36, 37, 40, 41, 42, 46, 47, 48, 50, 52, 57, 58, 59, 60, 62, 65, 68, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 96, 98, 101, 102, 105, 108, 110, 113, 114, 115, 117, 118, 121, 122, 123, 125, 126, 127, 128, 129, 135, 137, 138, 139, 142, 147, 148, 149, 155, 158, 159, 163, 171, 173, 174, 176, 180, 181, 183, 190, 191, 193, 194, 198, 200, 202, 206, 209, 226, 234, 237, 238, 241, 242, 245, 250, 252, 253, 257, 266, 271, 272, 273, 278, 279, 281, 282, 286, 287, 294, 302, 304, 306, 307, 312, 313, 314, 315, 317, 319, 331, 337, 342, 343, 344, 350, 351, 353, 354, 359], "create_us": 168, "creation": [3, 79, 181, 183], "cred": [267, 281, 283], "credenti": [28, 29, 35, 38, 43, 44, 53, 55, 66, 69, 72], "credit": [9, 198, 201, 285, 292], "creepi": [285, 292], "creepybr": [285, 292], "creighton": [267, 283], "crime": [285, 292], "criteria": [10, 11, 101, 102, 104, 206, 208, 218, 220], "criterion": [5, 7, 14, 224, 228, 238, 240, 245, 247, 253, 256, 257, 350, 355], "critic": [8, 86, 87, 191, 197, 267, 283], "crop": [317, 319, 350, 351, 352], "cross": [17, 21, 22, 24, 26, 53, 63, 224, 226, 343, 345, 350, 351], "cross_attention_dim": [6, 259, 262], "crossattndownblock2d": [6, 259, 262], "crossattnupblock2d": [6, 259, 262], "crossentropyloss": [5, 7, 13, 14, 137, 143, 224, 226, 228, 238, 240, 245, 247, 253, 254, 256, 257, 350, 355], "crouch": [285, 292], "crow": [285, 291], "crucial": [118, 124, 137, 146, 285, 287, 293, 298], "crush": [267, 283], "cry": [285, 288], "css": 0, "csv": [5, 8, 10, 13, 86, 87, 191, 192, 206, 210, 224, 226, 238, 239, 330, 332, 336, 337, 342, 343, 344, 345, 350, 357, 359], "csv_path": [343, 345], "ctor": [350, 359], "ctrl": [1, 169, 170], "cu128": [101, 102, 108, 110, 115, 159, 163], "cub": [267, 283], "cuda": [5, 7, 10, 13, 14, 15, 16, 101, 102, 107, 128, 131, 137, 139, 146, 147, 149, 150, 206, 213, 224, 228, 230, 231, 234, 235, 237, 238, 240, 253, 256, 267, 272, 273, 275, 282, 283, 293, 298, 317, 323, 324, 329, 343, 349, 350, 359], "cultur": [285, 288], "cumprod": [6, 259, 262], "cumul": [137, 143, 330, 336], "cup": [285, 292], "cure": [285, 292], "curiou": [245, 252, 285, 288], "curl": [1, 147, 153, 168, 169, 170], "current": [3, 5, 8, 9, 12, 13, 14, 16, 43, 46, 53, 58, 66, 69, 92, 93, 118, 123, 164, 166, 181, 187, 191, 196, 198, 204, 224, 233, 234, 238, 241, 245, 248, 293, 300, 317, 321, 324, 325, 328, 330, 332], "current_training_step": [6, 259, 262], "cursor": [126, 127], "curti": [267, 281, 283, 284], "curv": [224, 226, 236, 320, 343, 347, 351, 352], "custom": [3, 4, 6, 11, 12, 20, 21, 24, 26, 27, 28, 29, 35, 36, 39, 43, 44, 53, 54, 66, 67, 80, 81, 84, 85, 88, 89, 92, 93, 96, 98, 101, 102, 108, 110, 115, 117, 118, 121, 125, 137, 144, 159, 161, 164, 167, 171, 173, 181, 186, 187, 212, 218, 220, 223, 224, 227, 259, 262, 285, 287, 288, 292, 317, 318, 330, 331, 334, 337, 338, 342, 351, 359], "custom_hid": 0, "custom_light": 0, "custom_nam": [118, 121], "customer_ingress_cidr_rang": [17, 22], "cut": [5, 6, 13, 259, 264, 343, 345], "cv": [118, 121], "cv_job_match": [118, 121], "cybersecur": [118, 121], "d": [9, 10, 15, 16, 128, 130, 131, 137, 139, 143, 146, 147, 153, 159, 163, 164, 166, 168, 198, 201, 202, 204, 206, 210, 211, 212, 267, 270, 271, 275, 277, 281, 283, 284, 285, 291, 292, 317, 319, 321, 324, 326, 328, 329, 330, 331, 336, 337, 340, 341, 343, 345, 349, 350, 352, 359], "d3a9a7d0": [35, 39, 66, 70], "d89d0_00000": 13, "d_": [5, 13], "d_model": [343, 346, 347, 349], "da": [267, 283], "dag": [8, 128, 130, 147, 154, 191, 194, 197], "dai": [118, 123, 267, 283, 285, 292], "daili": [337, 342], "dalla": [267, 283], "damn": [267, 283], "dancer": [285, 292], "daniel": [267, 283, 285, 291], "dark": [168, 285, 291, 292], "darwin": [267, 283], "dash": [285, 292], "dashboard": [1, 5, 6, 8, 16, 24, 27, 82, 83, 92, 93, 110, 116, 117, 128, 134, 137, 144, 147, 152, 155, 157, 158, 159, 161, 162, 163, 165, 167, 169, 170, 191, 193, 195, 224, 225, 245, 252, 259, 261, 324, 329], "dask": [8, 191, 195, 196], "data": [2, 11, 12, 16, 17, 21, 22, 43, 51, 53, 64, 82, 83, 84, 85, 86, 87, 100, 102, 108, 118, 122, 123, 124, 125, 126, 127, 131, 134, 136, 137, 139, 141, 143, 144, 146, 147, 150, 155, 157, 159, 163, 165, 168, 175, 177, 182, 183, 194, 195, 211, 213, 218, 220, 221, 226, 228, 230, 232, 247, 251, 254, 257, 258, 260, 262, 264, 271, 273, 277, 281, 282, 284, 288, 289, 292, 293, 295, 296, 298, 300, 318, 321, 323, 324, 325, 326, 328, 329, 331, 334, 336, 338, 339, 340, 342, 344, 345, 347, 351, 352, 353], "data_dir": [343, 345, 347, 349], "data_load": [5, 6, 7, 13, 14, 224, 228, 238, 240, 241, 245, 247, 253, 255, 256, 257, 259, 262], "data_path": [9, 198, 201, 204], "data_url": [330, 332], "databas": [10, 118, 120, 123, 128, 133, 168, 206, 209], "databaseservic": [164, 167, 168], "databrick": [8, 10, 191, 192, 206, 210], "datadog": [155, 157], "datafram": [4, 8, 9, 12, 171, 174, 191, 195, 198, 202, 225, 226, 238, 242, 287, 317, 319, 321, 330, 332, 334, 336, 337, 341, 342, 343, 345, 350, 359], "dataload": [7, 13, 14, 226, 228, 234, 239, 240, 243, 253, 254, 255, 257, 260, 263, 296, 317, 319, 324, 326, 347, 351, 352, 355], "dataset": [4, 6, 7, 8, 10, 14, 16, 86, 87, 126, 127, 128, 130, 131, 132, 136, 137, 138, 139, 143, 146, 147, 150, 153, 164, 166, 171, 174, 191, 192, 193, 194, 200, 202, 204, 205, 206, 208, 209, 210, 211, 212, 214, 215, 216, 225, 227, 229, 232, 234, 237, 239, 240, 241, 243, 244, 245, 248, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 262, 263, 266, 269, 273, 275, 279, 280, 282, 283, 287, 292, 293, 295, 296, 298, 299, 300, 317, 318, 319, 321, 323, 328, 329, 331, 334, 336, 338, 340, 342, 344, 347, 349, 350, 351, 352, 353, 354, 359], "dataset_": [164, 166], "dataset_iter": [238, 241], "dataset_uri": [330, 332], "datasourc": [9, 10, 15, 198, 203, 206, 210], "date": [13, 118, 123, 267, 283], "datetim": [5, 13, 224, 226, 343, 345], "david": [168, 267, 281, 283], "dawson": [285, 291], "day_of_week": 12, "db": 168, "ddim": [317, 323], "ddp": [5, 6, 137, 144, 226, 227, 230, 231, 234, 235, 245, 248, 252, 259, 263, 317, 321, 330, 332, 333, 336, 343, 344, 347, 349, 350, 355, 359], "ddpmschedul": [6, 259, 260, 262], "ddpstrategi": [6, 259, 262], "de": [3, 181, 183, 267, 283, 320, 323, 324, 325, 329, 343, 349], "deactiv": [1, 169, 170], "dead": [285, 292], "deadlock": [3, 181, 188], "deal": [8, 191, 197], "dear": [267, 283], "debat": [267, 283], "debug": [8, 11, 82, 83, 94, 95, 100, 118, 125, 126, 127, 137, 144, 155, 157, 158, 159, 161, 163, 164, 165, 167, 191, 195, 218, 223, 224, 226, 233, 237, 317, 318], "debugg": [126, 127], "debut": [267, 283], "decemb": [267, 283], "decid": [2, 7, 14, 15, 118, 123, 175, 177, 224, 227, 235, 253, 257, 267, 283, 285, 292, 324, 325, 343, 347], "decis": [24, 26, 337, 338, 339], "declar": [224, 230, 317, 318, 324, 325, 330, 331, 337, 338, 350, 351], "decod": [109, 318, 323, 343, 344, 345, 346, 347, 349], "decode_and_norm": [317, 319], "decoder_input": [343, 346, 347], "decor": [2, 3, 10, 11, 16, 175, 178, 181, 187, 190, 206, 211, 215, 218, 222, 306, 307, 315], "decoupl": [11, 218, 223, 245, 252], "decreas": [128, 130, 224, 236, 343, 347, 350, 357], "dedic": [7, 14, 90, 91, 92, 93, 110, 115, 118, 124, 147, 153, 155, 158, 224, 237, 253, 257, 285, 292], "dedupl": [14, 293, 300], "deep": [9, 110, 116, 198, 201, 224, 226, 266, 267, 279, 285, 292, 293, 295, 296, 299, 300, 330, 332, 336, 350, 352], "deeper": [8, 101, 102, 109, 118, 120, 125, 191, 193], "deepli": [159, 163], "deepseek": [118, 124], "deepspe": [5, 6, 13, 137, 144, 245, 252, 259, 264], "def": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 82, 83, 88, 89, 90, 91, 118, 123, 128, 130, 131, 136, 137, 139, 140, 141, 143, 146, 147, 149, 150, 159, 163, 164, 166, 167, 169, 170, 171, 174, 175, 178, 180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 198, 202, 206, 212, 213, 215, 218, 222, 224, 227, 228, 231, 232, 233, 234, 237, 238, 240, 241, 243, 245, 247, 248, 253, 255, 256, 257, 259, 262, 263, 267, 272, 273, 282, 285, 292, 293, 297, 298, 299, 305, 306, 307, 315, 317, 319, 320, 321, 323, 324, 326, 327, 328, 329, 330, 332, 333, 334, 337, 340, 341, 343, 345, 346, 347, 349, 350, 353, 354, 355, 359], "default": [0, 3, 5, 6, 7, 9, 10, 11, 12, 28, 30, 35, 38, 39, 43, 45, 53, 56, 66, 69, 80, 81, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 118, 123, 126, 127, 128, 129, 131, 133, 135, 137, 138, 139, 145, 147, 148, 153, 159, 163, 164, 167, 181, 185, 187, 198, 201, 202, 206, 210, 212, 218, 223, 224, 227, 245, 247, 253, 257, 259, 262, 285, 290, 293, 300, 317, 319, 321, 337, 339, 343, 344, 350, 359], "default_cluster_storag": [164, 166], "default_data_col": [293, 298], "default_root_dir": [6, 259, 262, 317, 321, 324, 328], "default_tracing_servic": 168, "defens": [267, 283, 343, 347], "defin": [2, 3, 4, 5, 7, 10, 11, 12, 14, 19, 43, 50, 53, 62, 66, 69, 80, 81, 84, 85, 90, 91, 92, 93, 101, 102, 105, 110, 113, 118, 122, 124, 128, 135, 137, 139, 140, 143, 144, 145, 147, 150, 153, 159, 161, 171, 174, 175, 180, 181, 185, 187, 190, 206, 212, 218, 222, 223, 225, 230, 231, 232, 234, 237, 239, 244, 253, 257, 266, 267, 275, 277, 279, 283, 284, 285, 292, 293, 295, 298, 299, 306, 307, 315, 317, 319, 321, 332, 338, 341, 342, 343, 344, 345, 349, 350, 353, 354, 355, 359], "definit": [84, 85, 118, 122, 123, 324, 326], "degre": [9, 10, 15, 198, 204, 206, 215], "del": [224, 237], "delai": [16, 285, 291], "deleg": [4, 171, 174], "delet": [5, 17, 22, 28, 30, 33, 35, 42, 43, 45, 51, 53, 56, 64, 66, 78, 86, 87, 224, 226, 245, 251, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "delete_object": [86, 87], "deleteobject": [17, 22], "delhi": [267, 283], "deliv": [159, 163, 285, 291], "deloy": [92, 93], "delta": [8, 101, 102, 108, 110, 114, 115, 118, 121, 191, 192, 337, 342], "demand": [1, 16, 24, 25, 27, 80, 81, 84, 85, 92, 93, 128, 132, 133, 136, 137, 139, 143, 146, 169, 170, 317, 323, 345], "demo": [17, 20, 224, 228, 317, 323, 324, 329, 330, 336], "demonstr": [6, 20, 86, 87, 110, 111, 118, 120, 126, 127, 159, 163, 164, 165, 166, 168, 224, 237, 259, 260, 267, 277, 284, 285, 289, 291, 292, 293, 295, 300, 317, 319, 322, 324, 328, 330, 332, 335, 336, 343, 348, 351, 352], "denizen": [285, 288], "denni": [267, 283], "deped": [84, 85], "depend": [0, 8, 10, 17, 22, 28, 30, 43, 45, 53, 56, 73, 80, 81, 82, 83, 84, 85, 100, 101, 102, 104, 106, 128, 129, 135, 137, 138, 145, 147, 148, 153, 168, 182, 188, 191, 194, 195, 196, 206, 212, 238, 239, 278, 286, 293, 294, 295, 312, 317, 319, 324, 326, 330, 332, 337, 339, 343, 344, 345, 350, 352], "depict": [285, 292], "deploi": [4, 11, 12, 16, 17, 19, 20, 21, 22, 30, 39, 45, 56, 79, 84, 85, 92, 93, 101, 102, 103, 104, 106, 107, 108, 109, 112, 114, 117, 119, 120, 122, 123, 125, 126, 127, 147, 148, 153, 164, 167, 168, 171, 174, 218, 219, 223, 266, 279, 302, 311, 313, 316, 324, 329, 337, 342, 343, 349, 360], "deploy": [0, 4, 8, 12, 18, 19, 22, 23, 26, 28, 30, 35, 36, 38, 39, 43, 45, 47, 48, 53, 56, 59, 60, 63, 66, 69, 70, 75, 76, 92, 93, 94, 95, 106, 107, 109, 111, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 150, 151, 152, 153, 159, 162, 164, 167, 171, 174, 191, 197, 222, 223, 302, 309, 313, 330, 336, 343, 349, 350, 359], "deployment_config": [101, 102, 108, 110, 113, 116, 118, 123], "deploymenthandl": [147, 150, 307, 309, 315], "deploymentrespons": 16, "depress": [285, 288], "depth": 13, "deriv": [330, 334], "derp": [28, 30, 43, 45, 53, 56], "desc": [5, 350, 352], "descent": [285, 292], "describ": [28, 30, 43, 45, 53, 56, 57, 285, 292, 324, 325], "descript": [1, 9, 17, 21, 22, 101, 102, 106, 121, 123, 147, 150, 169, 170, 198, 201, 324, 326], "deseri": [2, 8, 175, 177, 191, 195], "design": [1, 2, 8, 9, 10, 16, 79, 101, 102, 105, 118, 121, 125, 137, 144, 169, 170, 175, 177, 191, 192, 195, 196, 198, 200, 203, 206, 211, 285, 292, 293, 295, 299, 300, 302, 307, 313, 337, 340, 343, 344, 350, 355], "desir": [13, 350, 359], "desktop": [82, 83], "despit": [285, 292], "destin": [118, 121], "destroi": [28, 30, 33, 35, 42, 43, 51, 53, 64, 66, 78, 267, 283, 285, 292], "destruct": [285, 292], "detach": [137, 143, 224, 237, 343, 349], "detail": [3, 5, 6, 7, 9, 10, 11, 13, 14, 16, 17, 20, 22, 24, 25, 35, 39, 43, 45, 53, 56, 66, 70, 86, 87, 88, 89, 96, 98, 110, 116, 118, 121, 128, 133, 137, 139, 147, 150, 166, 167, 168, 181, 187, 198, 204, 206, 208, 218, 220, 223, 224, 225, 230, 234, 238, 239, 240, 253, 255, 259, 261, 263, 278, 285, 286, 292, 294, 306, 307, 312, 315, 360], "detailsbr": [285, 292], "detect": [8, 118, 121, 159, 163, 191, 195, 245, 250, 267, 272, 273, 282, 317, 322, 350, 357], "determin": [10, 101, 102, 104, 206, 211, 285, 290], "determinist": [337, 339], "determint": [10, 206, 215], "dev": [28, 30, 43, 45, 53, 56, 126, 127, 324, 325], "deval": [337, 340], "develop": [2, 4, 8, 9, 12, 16, 28, 29, 30, 35, 37, 43, 44, 45, 53, 55, 56, 66, 68, 80, 81, 86, 87, 88, 89, 90, 91, 92, 93, 96, 98, 110, 111, 128, 130, 134, 155, 157, 164, 167, 171, 173, 175, 177, 191, 195, 196, 197, 198, 204, 219, 220, 267, 283, 293, 299, 350, 351, 360], "deviat": [350, 353], "devic": [2, 5, 6, 7, 10, 11, 13, 24, 27, 51, 52, 64, 65, 101, 102, 108, 128, 131, 136, 137, 139, 140, 141, 146, 147, 149, 150, 175, 177, 206, 213, 218, 222, 223, 224, 227, 228, 230, 231, 232, 234, 235, 237, 238, 240, 241, 253, 256, 257, 259, 262, 263, 266, 267, 275, 279, 283, 293, 295, 298, 317, 320, 321, 323, 324, 325, 328, 329, 343, 347, 349, 350, 351, 354, 355, 359], "devop": [79, 100], "df": [4, 6, 9, 171, 174, 198, 201, 202, 203, 204, 238, 242, 259, 262, 317, 321, 324, 328, 330, 332, 334, 336, 337, 339, 341, 342, 343, 345, 347, 350, 353, 357], "di": [3, 181, 185, 285, 291], "diagnos": [155, 157], "diagnost": [343, 347], "diagon": [337, 341], "diagram": [4, 6, 7, 8, 13, 14, 17, 21, 24, 26, 100, 101, 102, 107, 171, 174, 191, 195, 197, 224, 225, 234, 253, 257, 259, 262, 263, 293, 295], "diari": [267, 283], "dict": [3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 101, 102, 108, 110, 113, 116, 118, 121, 123, 137, 143, 159, 163, 171, 174, 181, 184, 206, 212, 213, 215, 218, 222, 224, 228, 234, 238, 240, 241, 243, 244, 245, 247, 248, 252, 253, 257, 259, 262, 263, 267, 269, 272, 273, 280, 282, 293, 296, 298, 317, 319, 324, 326, 343, 349], "dictat": [285, 292], "dictionari": [3, 4, 7, 14, 171, 174, 181, 187, 224, 229, 233, 238, 240, 241, 253, 257, 293, 298, 330, 334], "did": [3, 181, 185, 267, 283, 285, 291, 292, 317, 321, 323, 324, 328, 329], "didn": [267, 283], "diego": [267, 283], "differ": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 22, 24, 26, 66, 71, 79, 82, 83, 86, 87, 101, 102, 104, 105, 106, 112, 118, 120, 121, 126, 127, 128, 134, 135, 137, 144, 147, 151, 155, 158, 159, 162, 164, 165, 167, 169, 170, 171, 174, 175, 180, 191, 194, 198, 204, 206, 212, 215, 218, 221, 224, 225, 228, 230, 238, 239, 240, 241, 253, 257, 259, 261, 266, 267, 279, 283, 285, 288, 293, 298, 299, 317, 323, 324, 329, 337, 340, 343, 347, 350, 351, 359], "differenti": [337, 338], "difficult": [8, 159, 163, 191, 195], "diffus": [5, 13, 260, 263, 264, 321, 326, 328], "diffusionpolici": [328, 329], "digit": [7, 11, 14, 218, 222, 227, 238, 242, 253, 255, 256], "dii": [101, 102, 106], "dilat": 13, "dim": [137, 140, 143, 224, 237, 317, 320, 324, 325, 327, 330, 333, 350, 355, 359], "dimens": [224, 237, 330, 331, 334, 343, 345], "dimension": [337, 338, 339], "dinger": [267, 283], "dir": [84, 85, 86, 87, 88, 89, 147, 153, 224, 226, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352, 355, 359], "direct": [7, 14, 17, 22, 24, 26, 118, 124, 253, 257, 285, 292, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "directli": [3, 8, 17, 22, 24, 26, 27, 82, 83, 88, 89, 96, 98, 118, 122, 181, 183, 184, 191, 192, 238, 239, 317, 318, 321, 324, 325, 329, 330, 331, 332, 334, 337, 339, 340, 343, 344, 350, 351, 357], "director": [285, 291, 292], "directori": [0, 5, 9, 43, 46, 53, 58, 82, 83, 86, 87, 92, 93, 128, 135, 168, 198, 203, 224, 226, 234, 237, 245, 251, 317, 323, 330, 332, 336, 337, 338, 342, 343, 345, 349, 350, 355, 359], "dirpath": [317, 321, 324, 328], "disabl": [6, 13, 14, 16, 128, 133, 137, 139, 147, 150, 224, 237, 259, 263, 293, 300], "disaggreg": [10, 206, 217], "disappoint": [267, 281, 283], "discern": [285, 288], "disconnect": [330, 334], "discontinu": [324, 325], "discount": [330, 336], "discov": [285, 292], "discret": [8, 128, 135, 191, 195], "discuss": [7, 12, 14, 253, 256, 257], "disengag": [9, 198, 201], "disjoint": [224, 228, 285, 291, 292], "disk": [5, 10, 15, 86, 87, 88, 89, 155, 157, 159, 161, 164, 166, 167, 206, 208, 214, 224, 226, 238, 242, 245, 248, 317, 323, 330, 336, 337, 342, 343, 349, 350, 359], "dismiss": [267, 283], "displai": [1, 5, 13, 17, 22, 90, 91, 110, 113, 164, 167, 169, 170, 224, 226, 236, 237, 245, 250, 285, 288, 292, 321, 350, 357], "display_top_match": [128, 136], "disrupt": [285, 291], "dist": [238, 244], "dist_val_acc": [350, 355], "distanc": [4, 7, 9, 12, 14, 171, 174, 198, 201, 204, 253, 257, 317, 323, 337, 342], "distil": [101, 102, 105, 343, 349], "distilbert": [305, 306, 307, 315], "distinct": [24, 26, 101, 102, 104, 159, 162, 224, 227, 330, 332], "distract": [285, 292], "distribut": [1, 2, 3, 7, 9, 10, 12, 14, 15, 17, 22, 84, 85, 88, 89, 100, 101, 102, 106, 107, 110, 116, 117, 126, 127, 128, 129, 130, 132, 143, 147, 149, 151, 168, 169, 170, 172, 173, 175, 177, 181, 182, 183, 188, 192, 196, 197, 198, 199, 200, 201, 203, 205, 206, 207, 208, 210, 211, 214, 226, 227, 228, 230, 232, 235, 236, 238, 239, 242, 243, 244, 245, 252, 253, 256, 260, 261, 267, 277, 284, 285, 287, 289, 292, 296, 298, 299, 300, 302, 307, 313, 319, 323, 325, 326, 329, 332, 336, 339, 342, 345, 347, 352, 355, 359], "distributeddataparallel": [5, 6, 13, 224, 225, 228, 231, 234, 235, 259, 263, 330, 333], "distributedsampl": [5, 13, 224, 228, 232, 234, 350, 354, 355], "div_term": [343, 346], "dive": [100, 101, 102, 109, 118, 119, 120, 125, 343, 345], "divers": [8, 191, 192, 195], "divid": [8, 17, 22, 191, 195], "dl_dw": [7, 14, 253, 257], "dmatrix": [4, 12, 171, 174, 337, 339, 340, 341], "dn": [24, 26], "do": [3, 4, 7, 9, 12, 13, 14, 35, 37, 66, 68, 82, 83, 86, 87, 118, 121, 123, 137, 139, 147, 149, 171, 172, 181, 184, 187, 198, 202, 203, 204, 253, 256, 266, 267, 275, 279, 283, 285, 288, 291, 292], "doc": [1, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 22, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 88, 89, 96, 98, 101, 102, 108, 118, 121, 164, 167, 169, 170, 198, 201, 203, 204, 206, 215, 216, 218, 223, 224, 230, 236, 259, 263, 267, 271, 281, 293, 300, 302, 306, 307, 313, 315], "docker": [24, 26], "dockerfil": [101, 102, 108, 110, 115], "document": [7, 8, 14, 16, 17, 22, 24, 25, 82, 83, 90, 91, 101, 102, 105, 109, 110, 117, 118, 122, 124, 125, 126, 127, 164, 167, 191, 192, 196, 197, 253, 257], "documentari": [285, 288], "doe": [3, 4, 6, 7, 8, 9, 10, 14, 24, 27, 88, 89, 90, 91, 101, 102, 105, 106, 137, 139, 147, 154, 155, 158, 159, 163, 171, 174, 181, 187, 188, 191, 196, 198, 201, 206, 212, 238, 243, 253, 257, 259, 262, 267, 283, 284, 285, 288, 292, 317, 323, 324, 329, 330, 336], "doesn": [3, 5, 9, 10, 15, 24, 26, 86, 87, 101, 102, 106, 128, 130, 137, 138, 181, 184, 198, 204, 206, 215, 224, 226, 267, 283, 285, 288, 291, 292, 343, 345], "doesnt": [285, 292], "dog": [126, 127, 147, 150, 350, 351], "doggo": [126, 127, 128, 129, 130, 133, 135, 136, 137, 138, 139, 142, 143, 144, 146, 147, 148, 150, 153], "dogma": [285, 292], "dolocationid": [9, 198, 201], "domain": [337, 341, 342], "domin": [8, 191, 195, 337, 341], "don": [1, 2, 3, 5, 9, 10, 15, 100, 128, 135, 137, 139, 145, 147, 153, 169, 170, 175, 180, 181, 186, 198, 202, 206, 212, 224, 225, 226, 230, 267, 283, 285, 288, 292, 317, 319, 323, 330, 336, 350, 351, 354], "donald": [267, 283], "done": [5, 9, 13, 28, 33, 35, 42, 66, 78, 82, 83, 88, 89, 90, 91, 92, 93, 101, 102, 108, 110, 114, 115, 128, 129, 137, 138, 139, 147, 148, 164, 166, 198, 204, 224, 228, 237, 285, 291, 292, 307, 311, 316, 330, 332, 337, 338], "donkei": [137, 144], "dont": [285, 292], "dool": [267, 283], "dorset": [267, 283], "dot": [3, 181, 187, 267, 281, 283, 317, 318, 324, 325, 330, 331, 333, 336, 337, 338, 350, 351], "dot_product": [330, 333], "dotenv": [137, 138], "doubl": [267, 281, 283, 285, 288], "down": [5, 6, 13, 16, 24, 26, 27, 53, 63, 90, 91, 101, 102, 106, 116, 147, 151, 224, 234, 259, 264, 266, 267, 279, 283, 285, 287, 292, 293, 295, 324, 325], "down_block_typ": [6, 259, 262], "downblock2d": [6, 259, 262], "download": [1, 5, 7, 13, 14, 15, 16, 66, 69, 82, 83, 86, 87, 88, 89, 92, 93, 100, 118, 121, 169, 170, 232, 245, 251, 253, 255, 257, 304, 307, 314, 330, 332, 343, 344, 345], "downsampl": 13, "downsample_pad": [6, 259, 262], "downscal": [16, 128, 133, 136, 137, 146, 307, 311, 316], "downscale_delay_": 16, "downstream": [10, 15, 118, 120, 122, 206, 214, 285, 287, 330, 336], "downtim": [92, 93, 126, 127, 147, 151, 153, 164, 167], "dp": [137, 140, 143], "draft": [267, 281, 283, 284], "drag": [267, 283], "dragon": [285, 292], "drama": [285, 288], "drastic": [126, 127], "draw": [285, 292], "dread": [267, 283], "dream": [285, 291, 292], "dreambr": [285, 292], "dreamnightmar": [285, 292], "dress": [267, 283], "drill": [8, 191, 193], "drive": [285, 291], "driver": [3, 10, 24, 27, 128, 134, 159, 161, 164, 167, 181, 184, 188, 206, 208, 213, 267, 283, 330, 332, 337, 341, 343, 349, 350, 359], "driver_artifact": [12, 13, 293, 300], "drop": [101, 102, 107, 224, 232, 237, 267, 283, 317, 319, 337, 340, 343, 345, 350, 359], "drop_column": [12, 128, 131, 137, 139, 317, 319], "drop_last": [5, 7, 13, 14, 224, 232, 253, 255, 257, 343, 345], "dropdown": [82, 83, 110, 116, 164, 166], "dropna": [330, 334, 343, 347, 350, 357], "dropout": [137, 140, 343, 346], "dropout_p": [137, 140, 143, 144], "ds_adjust": [9, 198, 202, 203], "ds_block_based_shuffl": [9, 198, 204], "ds_file_shuffl": [9, 198, 204], "ds_gener": [137, 143], "ds_iter": [337, 340], "ds_label": [10, 206, 212], "ds_limit": [9, 198, 203], "ds_meta": [285, 289, 291], "ds_normal": [10, 15, 206, 212, 213], "ds_pred": [10, 15, 206, 213, 214, 215, 216], "ds_randomized_block": [10, 15, 206, 215], "ds_randomized_row": [10, 15, 206, 215], "ds_review": [285, 289, 290], "ds_row_based_shuffl": [9, 198, 204], "ds_tip": [9, 198, 202], "ds_tmp": [350, 359], "dsl": [8, 191, 194], "dtest": [4, 171, 174], "dtrain": [4, 171, 174, 337, 340], "dtype": [6, 12, 16, 128, 130, 137, 141, 144, 146, 159, 163, 238, 243, 259, 262, 267, 283, 284, 317, 319, 324, 326, 329, 330, 334, 343, 345, 346], "due": [3, 5, 6, 8, 10, 11, 101, 102, 105, 110, 116, 128, 133, 136, 137, 146, 159, 163, 181, 185, 191, 195, 206, 208, 212, 218, 220, 224, 225, 259, 261, 267, 271, 281, 285, 291, 292, 307, 309, 316], "dummi": [101, 102, 108, 118, 123], "dummy_data_1000_500": [86, 87], "dummy_data_1000_720": [86, 87], "dummy_data_xxl": [86, 87], "dummy_kei": [102, 108], "dump": [11, 16, 118, 123, 137, 139, 140, 143, 164, 167, 218, 222], "duplic": [224, 225, 234, 245, 248, 343, 345, 350, 351], "durabl": [8, 191, 192], "durat": [88, 89, 164, 167, 168], "dure": [0, 3, 5, 6, 13, 28, 30, 43, 45, 53, 56, 86, 87, 101, 102, 105, 106, 128, 130, 164, 167, 181, 187, 224, 225, 233, 236, 238, 244, 259, 263, 267, 277, 284, 285, 288, 292, 293, 295, 298, 317, 318, 319, 321, 330, 334, 336, 337, 339, 340, 343, 344, 346, 349, 350, 352, 357], "dustin": [267, 283], "dvd": [285, 291], "dynam": [8, 9, 11, 16, 17, 22, 24, 25, 118, 121, 137, 144, 147, 151, 191, 197, 198, 204, 218, 220, 343, 344], "dynamic_lora_loading_path": [118, 121], "e": [0, 2, 3, 5, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 22, 24, 26, 27, 28, 29, 30, 35, 36, 43, 44, 45, 49, 50, 51, 53, 54, 56, 61, 62, 64, 66, 67, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 101, 102, 105, 110, 116, 126, 127, 128, 129, 137, 138, 147, 148, 155, 157, 159, 161, 164, 166, 175, 177, 181, 184, 185, 187, 191, 192, 195, 198, 202, 203, 206, 208, 210, 212, 218, 220, 221, 223, 224, 228, 229, 234, 235, 236, 238, 239, 241, 245, 246, 249, 252, 253, 257, 266, 267, 279, 317, 318, 324, 325, 330, 331, 334, 337, 340, 343, 349, 350, 352], "each": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 22, 43, 46, 53, 58, 79, 84, 85, 86, 87, 88, 89, 92, 93, 96, 98, 101, 102, 104, 105, 110, 113, 116, 118, 120, 121, 125, 126, 127, 128, 130, 131, 137, 139, 143, 146, 147, 149, 151, 159, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 174, 175, 180, 181, 183, 191, 197, 198, 201, 202, 203, 206, 210, 212, 213, 218, 221, 224, 225, 226, 227, 228, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 248, 253, 255, 257, 259, 262, 263, 266, 267, 270, 271, 273, 275, 279, 281, 282, 283, 285, 289, 292, 293, 295, 298, 299, 302, 306, 307, 313, 315, 317, 318, 319, 324, 325, 328, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 341, 343, 344, 345, 347, 349, 350, 351, 352, 353, 355, 357, 359], "earli": [7, 14, 253, 257, 267, 283, 317, 323, 337, 342, 343, 345, 349, 350, 359], "earlier": [82, 83, 84, 85, 88, 89, 128, 132, 285, 291, 317, 319, 350, 358], "early_stopping_round": [337, 342], "earn": [267, 283], "earth": [285, 291], "eas": [8, 191, 194], "easi": [0, 8, 9, 16, 100, 101, 102, 106, 110, 116, 118, 122, 123, 125, 128, 132, 147, 149, 159, 163, 164, 167, 191, 197, 198, 200, 224, 236, 238, 239, 293, 298, 299, 302, 307, 313, 317, 318], "easier": [6, 8, 24, 26, 128, 133, 134, 191, 197, 259, 262, 285, 292, 293, 295, 330, 336], "easili": [2, 3, 4, 5, 6, 7, 9, 10, 11, 92, 93, 101, 102, 103, 107, 110, 111, 118, 119, 128, 135, 137, 142, 145, 147, 153, 159, 163, 171, 172, 175, 176, 181, 182, 198, 199, 206, 207, 218, 219, 253, 254, 259, 260, 266, 267, 279, 285, 287, 293, 295, 299, 302, 307, 313, 324, 325], "eastern": [285, 291, 292], "eat": [285, 291], "ec2": [18, 20, 22, 23, 24, 26, 27, 30, 31, 34, 43, 45, 53, 56, 88, 89, 360], "echo": [66, 69, 137, 138, 267, 283], "eclips": [267, 283], "ecolog": [337, 342], "ecosystem": [4, 5, 6, 8, 12, 126, 127, 171, 173, 191, 195, 196, 224, 225, 259, 261], "ed": [267, 283], "eddi": [267, 283, 285, 291, 292], "edg": [267, 283, 343, 349], "edgecolor": [330, 332], "edit": [82, 83, 84, 85, 126, 127, 267, 283], "editor": [35, 37, 66, 68, 82, 83, 84, 85, 90, 91], "educ": [350, 351], "ef": [15, 16, 21, 23, 24, 26, 28, 30, 34, 53, 57], "effect": [3, 4, 17, 22, 86, 87, 110, 112, 118, 121, 126, 127, 171, 174, 181, 184, 224, 229, 285, 291, 292, 330, 333, 334, 343, 349], "effici": [1, 8, 10, 11, 84, 85, 96, 99, 101, 102, 104, 110, 112, 118, 121, 126, 127, 128, 130, 132, 133, 169, 170, 191, 192, 195, 196, 206, 208, 218, 220, 221, 224, 225, 237, 238, 239, 242, 245, 252, 266, 267, 277, 279, 284, 285, 287, 292, 293, 295, 298, 299, 300, 317, 318, 319, 330, 331, 332, 334, 336, 343, 344, 345, 349, 350, 351, 353, 359], "efs_id": [17, 23, 28, 30], "egress": [24, 26], "eid": [267, 283], "eight": [324, 328, 350, 356], "eip": [28, 30, 43, 45, 53, 56], "eipalloc": [28, 30, 43, 45, 53, 56], "either": [9, 10, 15, 90, 91, 101, 102, 106, 164, 167, 198, 202, 206, 209, 224, 237, 267, 275, 283, 285, 292, 330, 332], "eject": [267, 283], "ek": [17, 18, 20, 22, 24, 27, 45, 46, 47, 52, 55, 56, 58, 59, 65, 126, 127, 360], "eks_cluster_nam": [43, 45, 46, 53, 56, 57, 58], "elam": [285, 291], "elaps": 12, "elast": [17, 22, 28, 30, 43, 45, 53, 56, 137, 144], "element": [16, 285, 292], "elev": [337, 338, 341], "elif": [293, 298, 317, 323, 343, 345], "elimin": [8, 101, 102, 105, 191, 192, 337, 339], "ellipsi": [94, 95], "els": [5, 7, 10, 92, 93, 118, 123, 137, 140, 146, 206, 213, 224, 237, 245, 251, 253, 256, 267, 275, 283, 293, 298, 305, 306, 307, 315, 317, 323, 324, 329, 330, 332, 334, 336, 337, 340, 343, 345, 347, 349, 350, 355, 359], "elsewher": [126, 127], "elt": [8, 191, 192], "email": [35, 39, 66, 70, 96, 98, 164, 167, 168], "emb": [128, 135, 136, 137, 139, 267, 272, 273, 275, 282, 283, 330, 331, 336], "embd": [147, 149], "embed": [0, 90, 91, 126, 127, 133, 135, 136, 137, 139, 140, 141, 146, 147, 149, 266, 267, 273, 275, 278, 279, 282, 283, 284, 286, 294, 312, 317, 323, 324, 329, 332, 333, 336], "embedd": 0, "embedding_dim": [137, 140, 143, 144, 224, 229, 330, 333, 334, 336], "embedding_gener": [128, 136], "embeddings_d": [128, 131, 133, 136], "embeddings_path": [128, 133, 136], "embedimag": [128, 131, 136, 137, 139], "emit": [101, 102, 105, 317, 321], "emmanuel": [267, 283], "emotion": [285, 291, 292], "emploi": [8, 191, 192], "empti": [28, 30, 33, 35, 42, 43, 45, 51, 53, 64, 66, 78, 92, 93], "emption": [343, 344], "en": [1, 14, 101, 102, 108, 169, 170, 267, 271, 281, 285, 292, 293, 300, 302, 306, 307, 313, 315], "enabl": [2, 3, 4, 8, 9, 10, 11, 12, 17, 21, 22, 24, 25, 27, 37, 68, 78, 84, 85, 86, 87, 88, 89, 92, 93, 100, 117, 118, 120, 121, 123, 125, 126, 127, 137, 138, 147, 151, 155, 158, 159, 163, 164, 167, 168, 171, 173, 175, 177, 181, 182, 187, 191, 192, 195, 197, 198, 203, 205, 206, 208, 218, 221, 238, 239, 246, 248, 249, 252, 266, 267, 277, 279, 284, 285, 287, 289, 292, 293, 299, 317, 318, 319, 323, 324, 325, 329, 330, 331, 334, 337, 338, 343, 344, 345, 349, 350, 351, 356, 359], "enable_access_log": [164, 167], "enable_auto_tool_choic": [118, 123], "enable_checkpoint": [6, 259, 263], "enable_filestor": [35, 39], "enable_lora": [118, 121], "enable_progress_bar": [317, 321, 324, 328], "encapsul": [224, 235, 350, 351], "encod": [101, 102, 104, 164, 167, 267, 272, 273, 275, 282, 283, 285, 287, 323, 324, 325, 331, 336, 343, 345, 346], "encode_batch": [330, 332], "encount": [3, 5, 6, 66, 71, 181, 185, 259, 260, 307, 309, 316], "encourag": [317, 318, 330, 331], "end": [3, 8, 16, 24, 26, 79, 88, 89, 101, 102, 104, 105, 106, 108, 110, 114, 115, 117, 118, 121, 124, 137, 143, 164, 167, 172, 173, 181, 187, 191, 195, 196, 238, 244, 245, 250, 252, 267, 283, 285, 291, 292, 307, 309, 316, 317, 318, 323, 324, 325, 329, 330, 336, 337, 338, 342, 343, 344, 345, 349, 350, 351, 359], "end_run": [137, 143], "end_tim": [137, 144], "endpoint": [11, 16, 24, 26, 92, 93, 101, 102, 106, 108, 110, 114, 115, 117, 218, 222, 305, 306, 307, 315, 317, 323, 330, 336, 337, 342], "enforc": [0, 3, 8, 10, 84, 85, 118, 122, 181, 187, 191, 192, 206, 212], "engag": [9, 118, 121, 198, 201], "engin": [2, 5, 6, 9, 10, 11, 16, 17, 20, 24, 27, 39, 69, 70, 78, 79, 94, 95, 108, 110, 116, 118, 121, 128, 130, 147, 151, 164, 165, 175, 177, 192, 194, 195, 196, 198, 205, 206, 208, 218, 220, 224, 225, 259, 261, 293, 295, 300, 337, 341, 342, 343, 344, 345], "engine_arg": [101, 102, 108], "engine_kwarg": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123], "english": [305, 306, 307, 315], "enhanc": [8, 118, 120, 123, 155, 157, 159, 162, 163, 191, 192, 196], "enjoi": [267, 283, 285, 292], "enough": [3, 10, 15, 84, 85, 181, 190, 206, 215, 224, 234, 317, 319, 330, 332, 350, 352], "ensembl": [147, 151, 172, 337, 338], "ensu": [285, 292], "ensur": [1, 3, 5, 6, 8, 9, 13, 28, 29, 31, 35, 37, 40, 43, 44, 47, 53, 55, 59, 63, 66, 68, 75, 80, 81, 84, 85, 118, 122, 126, 127, 128, 132, 137, 141, 155, 158, 164, 165, 169, 170, 181, 187, 190, 191, 192, 198, 203, 224, 225, 228, 231, 234, 237, 238, 239, 241, 243, 245, 248, 259, 261, 267, 275, 278, 283, 286, 293, 294, 298, 312, 317, 321, 324, 329, 330, 332, 334, 337, 339, 340, 342, 343, 345, 350, 355], "enter": [82, 83, 90, 91, 92, 93, 168, 285, 288], "enterpris": [92, 93, 101, 102, 107, 110, 115, 117, 118, 125, 126, 127], "enthus": [137, 144], "entir": [3, 5, 6, 8, 9, 10, 15, 24, 25, 66, 67, 94, 95, 101, 102, 105, 126, 127, 128, 130, 132, 137, 139, 146, 181, 187, 191, 195, 198, 201, 206, 211, 214, 215, 224, 225, 228, 259, 262, 266, 279, 285, 292, 293, 298, 317, 323, 330, 332, 336, 337, 340, 343, 345, 350, 351], "entiti": [94, 95], "entri": [168, 238, 243, 343, 349], "entropi": [350, 351], "entrypoint": [16, 90, 91], "enum": [118, 122, 123], "enumer": [7, 14, 137, 139, 143, 146, 253, 255, 285, 289, 330, 332, 336], "env": [3, 84, 85, 110, 115, 128, 129, 137, 138, 147, 148, 181, 186, 224, 226, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "env_var": [3, 110, 113, 116, 118, 121, 122, 123, 137, 138, 181, 186, 187], "environ": [0, 4, 5, 6, 8, 9, 10, 11, 17, 19, 22, 28, 29, 34, 35, 36, 43, 44, 52, 53, 54, 65, 66, 67, 69, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 100, 101, 102, 104, 110, 113, 116, 118, 121, 123, 126, 127, 128, 129, 130, 137, 138, 147, 148, 151, 155, 156, 158, 159, 163, 164, 165, 166, 167, 171, 174, 182, 187, 191, 195, 198, 203, 206, 213, 218, 221, 222, 259, 262, 266, 267, 279, 293, 295, 296, 299, 302, 307, 313, 317, 318, 319, 321, 326, 328, 329, 330, 332, 337, 338, 339, 343, 345, 350, 351, 352], "environment": [84, 85], "eot": [28, 30, 35, 39, 43, 45, 53, 56, 66, 70], "ep": [13, 137, 140], "ephemer": [224, 226], "epic": [267, 283], "episod": [324, 329], "epoch": [5, 7, 13, 14, 128, 130, 137, 143, 224, 228, 233, 234, 236, 238, 239, 240, 245, 246, 247, 248, 250, 252, 253, 256, 257, 293, 298, 299, 300, 317, 320, 321, 322, 323, 324, 325, 327, 328, 329, 330, 331, 334, 335, 336, 343, 344, 347, 349, 350, 351, 355, 356, 357, 358, 359], "epoch_loss": 5, "equival": [9, 198, 201, 203], "ergonom": [324, 326], "ernst": [285, 292], "ernsthugo": [285, 292], "erotica": [285, 288], "errno": [9, 198, 203], "error": [3, 5, 6, 8, 9, 10, 13, 14, 43, 51, 53, 64, 66, 71, 84, 85, 118, 122, 137, 144, 155, 157, 159, 163, 164, 167, 181, 185, 191, 195, 198, 203, 206, 208, 212, 259, 260, 266, 279, 307, 309, 316, 317, 318, 330, 331, 336, 337, 338], "erupt": [285, 292], "escap": [285, 292], "especi": [3, 5, 9, 15, 16, 84, 85, 128, 130, 132, 181, 184, 187, 198, 205, 224, 232, 238, 239, 266, 267, 279, 285, 291, 293, 298, 317, 319, 343, 347, 350, 352], "essenti": [8, 17, 21, 101, 102, 109, 110, 117, 137, 146, 191, 195, 245, 246, 293, 298], "establish": [7, 14, 17, 19, 118, 124, 253, 256], "estim": [5, 88, 89, 224, 228], "estimate_pi": [88, 89], "eta": [4, 171, 174, 317, 318, 324, 325, 337, 340, 342], "etc": [2, 3, 4, 5, 6, 9, 10, 12, 17, 18, 19, 22, 23, 24, 27, 110, 113, 126, 127, 128, 130, 132, 133, 134, 135, 136, 137, 139, 144, 146, 147, 151, 153, 154, 164, 166, 167, 171, 174, 175, 177, 181, 190, 198, 201, 203, 206, 210, 224, 225, 229, 245, 250, 259, 261, 337, 340], "ether": [285, 292], "etl": [8, 15, 128, 130, 191, 192, 195], "euler": [317, 323], "europ": [285, 291, 292], "europa": [267, 283, 285, 292], "ev": [267, 283], "eval": [4, 5, 6, 10, 11, 13, 15, 16, 137, 138, 143, 146, 171, 174, 206, 213, 218, 222, 224, 237, 259, 263, 293, 298, 317, 323, 324, 329, 330, 334, 336, 337, 340, 343, 347, 349, 350, 355, 359], "eval_arrow": [337, 340], "eval_dataset": [293, 298], "eval_epoch": [137, 143], "eval_metr": [4, 171, 174, 337, 340], "eval_pr": [293, 297], "evals_result": [4, 171, 174, 337, 340], "evalu": [3, 7, 14, 16, 17, 20, 79, 118, 124, 138, 181, 187, 224, 237, 253, 256, 293, 295, 296, 297, 298, 317, 318, 323, 324, 325, 329, 330, 331, 336, 338, 339, 342, 350, 351, 355, 359], "evan": [267, 283], "even": [3, 16, 88, 89, 101, 102, 105, 118, 125, 128, 132, 137, 144, 147, 149, 151, 159, 163, 181, 187, 224, 234, 267, 283, 285, 288, 292, 306, 307, 308, 309, 315, 316, 343, 349, 350, 355], "evenli": [224, 228, 229], "event": [5, 6, 8, 88, 89, 147, 154, 155, 157, 159, 161, 162, 191, 195, 224, 225, 259, 261], "eventu": [224, 237], "ever": [4, 12, 171, 173, 285, 288, 291], "everi": [0, 2, 86, 87, 90, 91, 94, 95, 175, 180, 224, 228, 233, 234, 238, 243, 244, 267, 283, 285, 289, 317, 320, 324, 325, 328, 330, 334, 337, 338, 339, 340, 350, 351, 352, 353, 357, 359], "every_n_epoch": [317, 321, 324, 328], "everyon": [285, 292], "everyth": [24, 26, 92, 93, 224, 235, 285, 288, 292, 317, 323, 324, 329, 330, 332, 336, 343, 347, 350, 352], "evil": [285, 291, 292], "evolut": [8, 191, 192], "evolv": [330, 334, 337, 338], "ex": [317, 319], "exact": [1, 3, 10, 80, 81, 110, 115, 118, 122, 169, 170, 181, 187, 206, 212], "exactli": [17, 22, 317, 319, 330, 335, 350, 351, 352, 353, 359], "examin": [43, 50, 53, 62], "exampl": [3, 5, 7, 8, 9, 10, 11, 15, 16, 18, 20, 22, 26, 28, 30, 31, 35, 38, 43, 44, 45, 53, 54, 56, 66, 67, 69, 71, 82, 83, 90, 91, 92, 93, 101, 102, 105, 106, 114, 116, 119, 120, 124, 125, 128, 130, 133, 137, 144, 155, 157, 158, 160, 164, 165, 166, 167, 181, 183, 186, 187, 188, 190, 191, 192, 198, 202, 203, 206, 208, 210, 211, 213, 218, 222, 223, 224, 226, 229, 234, 236, 238, 239, 245, 252, 253, 257, 266, 267, 273, 275, 279, 282, 283, 285, 287, 290, 293, 298, 306, 307, 315, 317, 318, 323, 324, 325, 330, 331, 336, 337, 338, 340, 341, 342, 343, 349, 350, 351, 352, 357, 359], "exce": [305, 306, 307, 315], "excel": [110, 112, 118, 124], "except": [3, 6, 16, 128, 132, 181, 185, 245, 247, 259, 263, 267, 283, 317, 319, 350, 352], "excess": [350, 351], "excit": [267, 283], "exclus": [5, 101, 102, 106], "exdb": [13, 14], "execut": [3, 4, 5, 6, 7, 8, 14, 17, 18, 22, 43, 50, 53, 62, 66, 69, 82, 83, 88, 89, 90, 91, 96, 98, 101, 102, 107, 118, 120, 123, 128, 129, 130, 132, 135, 137, 138, 147, 148, 155, 157, 164, 166, 171, 174, 176, 178, 180, 181, 183, 184, 186, 188, 189, 191, 192, 196, 197, 201, 205, 207, 208, 212, 213, 214, 224, 225, 226, 227, 228, 238, 243, 253, 257, 259, 262, 263, 267, 275, 277, 283, 284, 293, 295, 298, 317, 319, 324, 326, 330, 332, 334, 337, 338, 339, 340, 342, 343, 344, 345, 347, 350, 351, 352, 355, 359], "execute_notebook": 0, "exercis": 100, "exhaust": [101, 102, 106, 245, 246, 337, 340], "exhibit": [285, 292, 343, 345], "exisitng": [24, 27], "exist": [4, 9, 24, 25, 26, 27, 28, 29, 30, 43, 44, 45, 52, 55, 56, 65, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 118, 123, 126, 127, 128, 129, 133, 137, 138, 139, 147, 148, 153, 164, 166, 167, 171, 174, 198, 203, 245, 251, 267, 271, 281, 285, 288, 293, 299, 302, 307, 313, 317, 321, 323, 324, 328, 329, 330, 332, 334, 336, 337, 342, 343, 345, 347, 349, 350, 351, 355, 357, 359, 360], "exist_ok": [5, 13, 137, 140, 142, 317, 319, 321, 324, 328, 330, 332, 337, 339, 343, 345, 350, 352], "existing_vpc_id": [17, 22], "exit": [317, 322], "exogen": [343, 349], "exp": [343, 346], "expand": [28, 29, 43, 44, 53, 55, 63, 66, 68, 88, 89, 164, 166, 224, 237, 317, 320], "expect": [5, 7, 14, 16, 82, 83, 101, 102, 106, 224, 227, 234, 253, 257, 317, 321, 330, 332, 337, 339, 343, 349, 350, 352], "expens": [4, 7, 10, 11, 12, 14, 15, 110, 112, 118, 121, 171, 174, 206, 213, 218, 220, 253, 256, 267, 273, 282, 285, 291, 317, 319], "expensive_comput": [3, 181, 189], "expensive_squar": [2, 3, 175, 180, 181, 184, 188, 189], "experi": [2, 5, 7, 79, 80, 81, 82, 83, 84, 85, 92, 93, 100, 110, 117, 118, 121, 125, 126, 127, 137, 138, 142, 143, 144, 155, 157, 159, 163, 164, 165, 175, 177, 224, 226, 228, 245, 252, 253, 257, 285, 288, 307, 308, 309, 316, 324, 329, 337, 342, 350, 351, 358, 359], "experiment": [11, 218, 223, 330, 335], "experiment_id": [137, 144], "experiment_nam": [6, 137, 143, 144, 147, 150, 245, 248, 259, 263], "expert": [8, 191, 196, 285, 292, 317, 318, 324, 325, 350, 352], "expertli": [285, 292], "explain": [17, 21, 24, 26, 96, 97, 164, 167, 267, 283, 285, 291], "explan": [118, 121, 164, 166, 306, 307, 315], "explicit": [118, 124, 285, 288, 330, 331, 337, 340], "explicitli": [6, 7, 9, 10, 92, 93, 198, 202, 206, 211, 253, 257, 259, 262, 266, 267, 271, 279, 281], "explor": [8, 82, 83, 86, 87, 92, 93, 100, 101, 102, 103, 106, 110, 116, 117, 118, 119, 120, 125, 164, 167, 191, 192, 245, 252, 285, 288, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "explos": [159, 163], "export": [66, 69, 110, 113, 114, 118, 121, 155, 158, 168, 324, 329], "expos": [24, 27, 267, 283], "exposehead": [17, 22, 53, 63], "exposur": [17, 22], "expr": [285, 290], "express": [5, 88, 89, 224, 226, 285, 290, 317, 323], "expresswai": [267, 283], "expwrk_1dp3fa7w5hu3i83ldsi7lqvp9t": [128, 129, 137, 138, 147, 148], "extend": [3, 17, 22, 137, 143, 181, 190, 238, 239, 245, 247, 248, 252, 317, 318, 323, 324, 329, 330, 336, 337, 340, 342, 343, 349, 350, 353, 359], "extens": [10, 82, 83, 86, 87, 118, 123, 128, 130, 134, 137, 139, 143, 206, 210], "extern": [8, 9, 17, 22, 27, 53, 63, 66, 73, 118, 120, 123, 125, 159, 161, 191, 195, 198, 200, 245, 252], "extra": [101, 102, 106, 267, 283, 330, 331, 332, 334, 337, 339, 343, 347, 349, 350, 357], "extra_st": [245, 247, 248], "extract": [6, 8, 10, 13, 14, 53, 57, 191, 192, 206, 212, 259, 262, 330, 332, 336, 350, 357], "extract_dir": [330, 332], "extractal": [330, 332], "extrem": [16, 86, 87, 101, 102, 105, 128, 132, 159, 163, 307, 308, 309, 316, 337, 338], "f": [3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 28, 30, 84, 85, 86, 87, 88, 89, 92, 93, 101, 102, 108, 110, 115, 118, 121, 122, 123, 126, 127, 128, 135, 137, 139, 140, 143, 144, 145, 146, 147, 150, 153, 159, 163, 164, 166, 168, 171, 174, 181, 183, 186, 187, 189, 198, 201, 203, 206, 213, 216, 218, 222, 224, 228, 233, 237, 245, 251, 253, 256, 259, 260, 262, 293, 298, 299, 317, 319, 321, 323, 324, 328, 329, 330, 332, 334, 336, 337, 339, 340, 341, 342, 343, 344, 345, 346, 347, 349, 350, 352, 355, 359], "f1": [137, 146, 267, 283, 350, 359], "f_": [317, 318, 324, 325, 337, 338, 343, 344, 350, 351], "face": [5, 6, 16, 17, 22, 100, 101, 102, 106, 110, 113, 114, 118, 121, 123, 137, 143, 224, 225, 238, 239, 259, 261, 266, 267, 277, 279, 281, 283, 284, 285, 288, 292, 296, 297, 300, 304, 307, 311, 314, 316, 350, 351, 352, 359], "facial": [285, 292], "facilit": [8, 86, 87, 191, 192, 196], "fact": [267, 283, 285, 288, 291, 292], "factor": [137, 143, 334, 336], "fahrenheit": [3, 118, 123, 181, 190], "fail": [3, 5, 6, 10, 13, 14, 128, 132, 137, 144, 147, 153, 159, 163, 181, 185, 206, 208, 212, 224, 225, 245, 246, 248, 249, 259, 261, 330, 334, 337, 340, 343, 348], "failur": [5, 6, 8, 10, 90, 91, 92, 93, 110, 115, 128, 132, 147, 151, 155, 157, 159, 161, 163, 182, 185, 191, 195, 206, 208, 212, 224, 225, 245, 246, 248, 249, 250, 259, 261, 324, 328, 337, 338, 342, 343, 344, 347, 349, 350, 351, 355, 359], "failure_config": [245, 248, 250, 317, 321, 324, 328, 330, 334, 337, 340, 343, 347, 350, 356], "failureconfig": [246, 249, 252, 317, 318, 319, 321, 324, 325, 326, 328, 330, 331, 332, 334, 337, 338, 339, 340, 343, 344, 345, 347, 350, 352, 355, 356, 359], "fair": [267, 283], "fake": [285, 289], "fake_kei": [101, 110, 114, 118, 121, 122, 123], "fall": [330, 332], "fallback": [128, 132, 317, 323, 324, 329], "fals": [4, 5, 6, 7, 11, 12, 13, 14, 16, 137, 140, 146, 164, 167, 171, 174, 218, 222, 224, 227, 230, 253, 256, 257, 259, 262, 263, 317, 320, 321, 323, 324, 327, 328, 329, 330, 332, 334, 337, 339, 340, 343, 345, 347, 349, 350, 355, 357, 359], "famili": [267, 283], "familiar": [79, 126, 127, 164, 165, 238, 241], "fan": [267, 283, 285, 288], "fanatic": [285, 292], "fanchant": [267, 283], "fantasi": [285, 291], "fantast": [285, 292], "far": [285, 288, 291, 337, 340], "fare": [285, 292], "fare_amount": [4, 12, 171, 174], "fashion": [9, 15, 128, 130, 198, 205], "fast": [5, 9, 10, 24, 26, 100, 101, 102, 106, 107, 118, 124, 126, 127, 147, 151, 198, 200, 206, 212, 224, 225, 226, 324, 325, 337, 338, 342, 350, 352, 355], "fastapi": [4, 8, 92, 93, 147, 148, 150, 151, 168, 171, 172, 174, 191, 197, 311, 314, 316], "fastapideploy": [92, 93], "faster": [7, 110, 116, 117, 128, 132, 134, 147, 151, 224, 237, 253, 257, 285, 290, 293, 299, 317, 323], "fastest": [82, 83], "fate": [285, 292], "father": [285, 292], "fault": [5, 6, 8, 9, 10, 17, 21, 22, 110, 115, 126, 127, 137, 144, 147, 153, 191, 195, 198, 200, 205, 206, 208, 224, 225, 247, 248, 250, 259, 261, 317, 318, 319, 321, 323, 324, 325, 328, 329, 330, 331, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344, 348, 349, 351, 355, 356, 359, 360], "fc": 13, "fc1": [137, 140], "fc2": [137, 140], "fcb9ef8c96f844f08bcd0185601f3dbd": [137, 144], "feasibl": [324, 329], "featur": [4, 8, 17, 20, 24, 26, 82, 83, 101, 102, 106, 107, 110, 115, 116, 117, 125, 126, 127, 128, 130, 147, 151, 155, 158, 159, 160, 164, 167, 171, 174, 191, 192, 195, 196, 199, 201, 202, 285, 288, 302, 307, 313, 317, 319, 330, 336, 338, 339, 340, 342, 343, 349, 350, 352, 359], "feature_col": [337, 340, 341], "feature_column": [337, 339, 340, 341, 342], "feature_nam": [337, 340], "feb": [267, 283], "fed": [267, 283], "feder": [17, 22], "fee": [4, 12, 171, 174], "feed": [9, 198, 200, 202, 238, 239, 267, 283, 343, 344, 347, 350, 353], "feedback": [82, 83], "feel": [92, 93, 238, 241, 267, 283, 285, 288, 291, 292, 317, 323, 324, 329, 330, 336, 337, 342, 350, 359], "femal": [285, 288], "fenc": [267, 283], "fend": [285, 291], "ferrari": [267, 283], "ferri": [267, 283], "fetch": [5, 6, 128, 132, 182, 183, 184, 224, 225, 226, 237, 238, 241, 259, 261, 293, 298, 337, 339, 343, 345], "fetch_covtyp": [337, 339], "few": [66, 73, 80, 81, 82, 83, 84, 85, 118, 121, 164, 165, 267, 284, 285, 287, 288, 291, 292, 317, 318, 321, 323, 324, 329, 330, 331, 332, 336, 337, 342, 343, 349, 350, 359], "fewer": [8, 35, 39, 137, 144, 147, 151, 191, 195], "ff": [267, 283], "fiat": [267, 283], "fid": [317, 323], "field": [9, 92, 93, 118, 122, 198, 201, 238, 243, 324, 326], "fifo": [12, 13, 14, 293, 300], "fifoschedul": [7, 14, 253, 257], "fig": [7, 13, 14, 253, 255, 317, 319, 323, 350, 352], "figsiz": [5, 7, 13, 14, 224, 226, 237, 253, 255, 317, 319, 321, 323, 324, 328, 330, 332, 334, 337, 339, 343, 345, 347, 349, 350, 352, 357], "figur": [5, 9, 15, 198, 201, 224, 226, 237, 317, 321, 324, 328, 330, 332, 334, 343, 345, 347, 349, 350, 357], "file": [0, 1, 4, 5, 6, 7, 8, 11, 12, 13, 17, 22, 24, 26, 28, 30, 34, 35, 38, 39, 43, 45, 46, 53, 56, 58, 66, 69, 70, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 100, 118, 121, 126, 127, 128, 130, 133, 135, 137, 139, 143, 144, 145, 147, 150, 153, 159, 163, 164, 167, 169, 170, 171, 174, 191, 192, 201, 203, 205, 209, 210, 213, 217, 218, 223, 224, 226, 234, 238, 242, 245, 248, 251, 252, 253, 258, 259, 262, 264, 267, 278, 283, 286, 294, 312, 317, 319, 323, 324, 325, 329, 330, 331, 332, 336, 343, 345, 350, 351, 352, 353, 355, 359], "file_nam": [118, 121], "filenam": [82, 83, 128, 130, 224, 226, 317, 321, 324, 328], "filenotfounderror": [9, 198, 203, 317, 323, 350, 359], "filestor": [24, 26, 35, 36, 39, 66, 70], "filestore_capacity_gb": [35, 39], "filestore_instance_nam": [35, 39, 66, 70], "filestore_loc": [35, 39, 66, 70], "filestore_ti": [35, 39], "filesystem": [6, 13, 14, 164, 166, 224, 226, 259, 262, 293, 300, 343, 344], "fill": [267, 283, 285, 292], "film": [267, 283, 285, 288, 291, 292], "filmbr": [285, 292], "filmmak": [285, 288], "filter": [9, 28, 30, 35, 38, 43, 45, 53, 56, 66, 69, 71, 84, 85, 88, 89, 128, 130, 133, 137, 139, 164, 167, 198, 205, 224, 227, 287, 291, 292, 330, 331, 333, 336], "filterwarn": [317, 321, 324, 328], "final": [2, 3, 7, 10, 14, 15, 66, 69, 88, 89, 100, 118, 123, 175, 180, 181, 188, 206, 216, 224, 227, 232, 236, 237, 238, 244, 245, 249, 250, 251, 252, 253, 257, 285, 291, 292, 293, 300, 317, 320, 324, 329, 330, 332, 336, 337, 340, 342, 343, 347, 349, 350, 355, 356, 358, 359], "find": [3, 4, 5, 6, 10, 17, 22, 28, 30, 35, 42, 43, 45, 53, 56, 57, 63, 66, 69, 78, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 164, 166, 171, 174, 181, 186, 206, 212, 224, 225, 245, 252, 259, 261, 285, 288, 291, 292], "fine": [7, 24, 26, 90, 91, 118, 120, 121, 128, 135, 245, 252, 253, 256, 285, 291, 317, 321, 323, 337, 342, 350, 351, 355], "finer": [3, 181, 187], "finest": [267, 281, 283], "finetun": [5, 6, 7, 13, 14, 253, 257, 259, 264, 305, 306, 307, 315], "finish": [3, 101, 102, 105, 137, 144, 181, 189, 224, 236, 324, 329, 337, 340, 350, 358], "fiorentina": [267, 283], "fiorina": [267, 283], "fir": [267, 283, 337, 338], "fire": [267, 283], "firewal": [17, 22, 35, 39, 42, 66, 70], "firewall_policy_nam": [35, 39, 66, 70], "first": [1, 2, 3, 4, 6, 7, 11, 12, 13, 14, 16, 28, 31, 35, 38, 40, 43, 47, 53, 57, 59, 66, 69, 75, 86, 87, 88, 89, 101, 102, 105, 106, 110, 113, 118, 121, 128, 130, 147, 149, 159, 163, 169, 170, 171, 173, 175, 177, 178, 180, 181, 184, 186, 218, 220, 222, 224, 227, 237, 253, 257, 259, 263, 267, 270, 271, 281, 283, 285, 288, 291, 292, 317, 319, 321, 323, 330, 332, 336, 343, 345, 350, 355], "fit": [4, 5, 7, 10, 12, 13, 14, 15, 101, 102, 105, 137, 139, 143, 171, 174, 206, 215, 236, 238, 242, 244, 245, 249, 250, 253, 256, 257, 262, 285, 287, 293, 299, 317, 321, 322, 324, 328, 330, 334, 335, 337, 338, 340, 342, 343, 347, 348, 349, 350, 351, 356, 358], "fit_model": [7, 253, 256], "five": [285, 291, 292, 317, 321, 324, 328, 350, 356], "fix": [224, 228, 293, 298, 317, 318, 330, 332, 343, 345], "flag": [88, 89], "flap": [285, 288], "flashi": [285, 292], "flatten": [317, 319, 350, 352], "flavor": [84, 85], "flawless": [267, 283], "fleet": [126, 127, 285, 288, 292, 343, 344], "flew": [267, 283], "flexibl": [3, 8, 9, 11, 15, 16, 96, 99, 101, 102, 107, 118, 119, 120, 128, 132, 147, 151, 181, 188, 191, 192, 194, 195, 198, 201, 218, 221, 266, 267, 279, 285, 287, 293, 299], "flexibli": [16, 137, 144], "flink": [8, 128, 130, 191, 195], "flip": [350, 351], "flip_sin_to_co": [6, 259, 262], "flippen": [285, 291], "float": [3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 137, 143, 146, 171, 174, 181, 185, 198, 201, 206, 213, 218, 222, 224, 234, 245, 248, 253, 256, 257, 259, 262, 267, 284, 317, 320, 324, 327, 330, 334, 337, 341, 343, 347], "float16": [6, 259, 262], "float32": [11, 12, 137, 141, 146, 218, 222, 267, 283, 284, 317, 319, 324, 326, 329, 330, 334, 343, 345, 346, 349], "floral": [28, 30, 43, 45, 53, 56], "flore": [267, 283], "flow": [17, 22, 96, 99, 118, 123, 155, 157, 195, 324, 325], "flush": [3, 101, 102, 108, 110, 114, 115, 118, 121, 181, 185], "fly": [238, 239], "fmt": [337, 341], "fn": [137, 146], "fn_arg": [118, 123], "fn_call": [118, 123], "fn_callabl": [118, 123], "fn_constructor_arg": [337, 341, 342, 343, 349, 350, 359], "fn_constructor_kwarg": [10, 15, 16, 128, 131, 137, 139, 206, 213], "fn_kwarg": [15, 128, 131, 137, 139], "fname": [343, 345], "foam": [267, 283], "focu": [100, 118, 120, 126, 127, 159, 162, 238, 239, 285, 288, 343, 344], "focus": [0, 8, 24, 26, 101, 102, 104, 126, 127, 155, 158, 191, 192, 194, 267, 283, 343, 349], "folder": [4, 5, 9, 10, 11, 82, 83, 86, 87, 90, 91, 118, 121, 168, 171, 174, 198, 203, 206, 213, 218, 222, 278, 286, 294, 312, 337, 342, 343, 349, 350, 359], "follow": [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 22, 24, 26, 28, 29, 35, 37, 42, 43, 44, 46, 51, 53, 55, 58, 63, 64, 66, 68, 71, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 100, 101, 102, 108, 110, 112, 118, 121, 124, 128, 129, 137, 138, 147, 153, 155, 157, 158, 159, 163, 164, 167, 168, 169, 170, 175, 180, 181, 183, 191, 194, 197, 198, 201, 206, 208, 213, 218, 220, 224, 225, 227, 238, 239, 240, 253, 257, 259, 261, 262, 263, 285, 292, 293, 299, 324, 329, 330, 331, 336, 337, 338, 343, 344, 349, 350, 351, 353], "followup": [285, 292], "fontsiz": [317, 319, 350, 352], "foo": [128, 131], "food": [318, 323, 359], "food101": [317, 319, 350, 351, 352, 359], "food101_diffusion_ft": [317, 321], "food101_diffusion_result": [317, 321], "food101_ft_resum": [350, 356, 359], "food101_ft_run": [350, 359], "food101_lit": [317, 319, 350, 352, 353, 355, 356, 359], "food101_single_run": [350, 359], "food101dataset": [351, 354, 359], "footag": [267, 283], "footbal": [267, 281, 283], "footer": 0, "footprint": [128, 130], "forbidden": [13, 14], "forc": [137, 139, 224, 237, 344, 346, 349, 350, 359], "forcibli": [285, 291], "ford": [285, 288], "forecast": 349, "foreground": [285, 292], "foregroundbr": [285, 292], "forest": [339, 341], "forg": [1, 169, 170], "forget": [267, 283, 285, 292], "forgotten": [285, 292], "forgottenbr": [285, 292], "fork": 0, "form": [3, 12, 128, 131, 181, 183, 285, 292, 330, 332], "format": [7, 10, 14, 100, 118, 120, 121, 122, 123, 125, 128, 130, 132, 168, 206, 210, 212, 238, 239, 242, 253, 255, 293, 298, 317, 319, 330, 332, 343, 345, 350, 351, 352], "fort": [285, 291], "forum": [110, 117, 118, 125], "forward": [6, 13, 137, 139, 140, 143, 224, 225, 228, 238, 240, 245, 247, 259, 262, 267, 283, 293, 298, 320, 321, 324, 325, 327, 328, 330, 331, 333, 337, 339, 343, 346, 349], "found": [7, 14, 17, 22, 224, 230, 245, 247, 250, 251, 253, 257, 285, 292, 317, 323, 324, 329, 330, 331, 337, 340, 350, 359], "foundat": [7, 8, 14, 109, 118, 125, 191, 192, 253, 256, 330, 336], "four": [5, 6, 101, 102, 105, 224, 227, 259, 263, 330, 332], "fourth": [267, 283, 285, 291, 292], "fox": [267, 283], "foxx": [267, 283], "fp": [137, 139, 140, 143, 146], "fp16": [101, 102, 106, 110, 112, 116], "fp8": [110, 116], "frac": [350, 353], "fraction": [11, 137, 144, 147, 149, 151, 153, 182, 190, 218, 220, 306, 307, 315], "fragment": [147, 151], "fragrant": [267, 283], "frame": [337, 339, 340], "framework": [4, 5, 6, 9, 11, 12, 15, 16, 100, 125, 171, 173, 174, 192, 197, 198, 200, 218, 219, 221, 224, 225, 259, 261, 285, 287, 302, 307, 313], "franc": [101, 102, 108, 118, 121], "francisco": [118, 123], "frank": [267, 283], "fraud": [8, 191, 195], "freak": [285, 291], "free": [3, 88, 89, 90, 91, 92, 93, 100, 110, 116, 126, 127, 128, 136, 137, 146, 147, 154, 181, 186, 224, 237, 266, 267, 279, 283, 293, 295, 337, 342, 343, 349, 350, 359], "freed": [224, 237], "freeli": [9, 15, 198, 201], "freez": [126, 127, 137, 139], "freq_shift": [6, 259, 262], "frequenc": [6, 259, 262, 330, 332, 337, 339], "frequent": [8, 191, 192, 195], "fresh": [224, 237], "fri": [350, 351], "friction": [2, 175, 177], "fridai": [267, 283], "friend": [267, 283], "friendli": [90, 91, 110, 112, 159, 163, 317, 319, 330, 336, 350, 351, 352], "frighten": [285, 291, 292], "frill": [285, 292], "from": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 24, 27, 28, 29, 30, 32, 35, 36, 39, 41, 42, 43, 44, 45, 47, 48, 50, 53, 54, 57, 59, 60, 62, 63, 66, 67, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 101, 102, 104, 105, 106, 108, 110, 111, 113, 114, 115, 116, 118, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 153, 154, 155, 158, 159, 161, 163, 164, 166, 167, 169, 170, 171, 172, 173, 174, 181, 183, 186, 188, 190, 191, 195, 198, 200, 202, 203, 205, 206, 207, 208, 210, 212, 213, 218, 219, 220, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 239, 240, 242, 243, 244, 246, 247, 248, 251, 252, 253, 254, 257, 259, 260, 261, 263, 266, 267, 269, 271, 277, 279, 280, 281, 283, 284, 285, 287, 288, 291, 292, 293, 296, 297, 298, 304, 307, 311, 314, 316, 318, 319, 321, 325, 326, 328, 331, 334, 336, 338, 339, 340, 344, 345, 347, 349, 350, 351, 352, 353, 355, 357, 358, 359], "from_artifacts_dir": [137, 146, 147, 149], "from_directori": [5, 13, 224, 234, 245, 248, 330, 334, 343, 347, 349, 350, 355, 359], "from_huggingfac": [267, 271, 281], "from_item": [16, 159, 163, 285, 289, 317, 319, 324, 326, 343, 349], "from_numpi": [343, 349, 350, 359], "from_pretrain": [6, 128, 131, 147, 149, 259, 262, 293, 298], "from_pydict": [350, 352], "from_pylist": [343, 345], "from_torch": [10, 206, 210], "fromarrai": [128, 131, 147, 149, 238, 243], "front": [337, 339, 350, 352], "frontal": [285, 288], "fr\u00e9chet": [317, 323], "fsdp": [5, 224, 226, 231, 245, 252], "fspd": [137, 144], "ft": [267, 281, 283], "fuck": [267, 283], "full": [4, 6, 9, 10, 15, 17, 20, 22, 24, 26, 53, 63, 90, 91, 118, 121, 126, 127, 159, 163, 164, 167, 171, 174, 198, 201, 206, 208, 214, 224, 229, 236, 246, 252, 259, 262, 285, 291, 292, 317, 318, 324, 325, 330, 332, 334, 336, 337, 340, 343, 347, 350, 352, 353, 357, 359], "full_path": [350, 353], "fulli": [3, 5, 6, 16, 17, 18, 80, 81, 96, 98, 100, 181, 183, 224, 225, 227, 238, 239, 244, 259, 261, 317, 318, 321, 324, 325, 330, 331, 332, 334, 337, 338, 340, 350, 351, 355], "fullyshardeddataparallel": [5, 224, 231], "function": [3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 53, 63, 86, 87, 94, 95, 110, 113, 118, 120, 123, 125, 128, 130, 131, 132, 137, 140, 147, 151, 171, 174, 176, 180, 181, 185, 190, 192, 196, 198, 201, 202, 203, 204, 206, 210, 212, 215, 216, 218, 221, 224, 227, 228, 231, 233, 234, 238, 243, 253, 257, 259, 260, 262, 263, 267, 275, 283, 285, 290, 292, 295, 297, 300, 306, 307, 315, 317, 319, 324, 325, 328, 329, 330, 331, 332, 334, 337, 338, 342, 343, 344, 350, 351, 355], "fundament": [3, 11, 16, 24, 26, 79, 101, 102, 103, 104, 108, 126, 127, 164, 165, 181, 182, 218, 221], "further": [10, 84, 85, 128, 134, 206, 210, 213, 224, 236, 285, 292, 306, 307, 315, 350, 353], "fuse": [10, 206, 212], "futur": [2, 9, 128, 130, 164, 167, 175, 179, 198, 204, 285, 288, 291, 292, 324, 325, 343, 344, 345, 346, 347], "future_tru": [343, 349], "g": [0, 2, 3, 7, 8, 9, 10, 11, 12, 14, 15, 17, 22, 24, 26, 27, 28, 29, 35, 36, 42, 43, 44, 53, 54, 66, 67, 78, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 110, 116, 155, 157, 159, 161, 164, 166, 175, 177, 181, 184, 185, 187, 191, 192, 195, 198, 203, 206, 208, 210, 212, 218, 220, 221, 224, 228, 229, 234, 235, 236, 238, 239, 241, 245, 246, 249, 252, 253, 257, 266, 267, 279, 281, 283, 330, 334, 337, 340, 343, 349], "g4dn": [128, 133, 136, 137, 139, 143, 146], "g54aiirwj1": [86, 87], "g54aiirwj1s8t9ktgzikqur41k": [86, 87], "gain": [126, 127, 330, 336, 337, 341], "galaxi": [267, 283], "galleri": [267, 283], "gallo": [285, 288], "gambl": [285, 291], "game": [267, 283], "gan": [317, 318], "gandhi": [267, 283], "gang": [285, 291], "gannon": [285, 291], "gap": [8, 16, 118, 121, 126, 127, 191, 192, 343, 345], "gape": [285, 288], "garbag": [2, 175, 177, 224, 226, 237], "garden": [267, 283], "gate": [110, 113, 114, 118, 122, 123], "gatewai": [17, 22, 28, 30, 43, 45, 53, 56], "gather": [4, 88, 89, 171, 174, 350, 352], "gaussian": [317, 318, 323, 324, 326, 343, 349], "gb": [3, 86, 87, 110, 112, 116, 181, 183], "gc": [9, 24, 26, 35, 36, 42, 66, 78, 118, 121, 198, 203, 224, 226, 234, 237, 245, 252, 330, 336, 350, 359], "gca": [337, 341], "gce": [17, 18, 24, 27, 40, 66, 69, 360], "gcloud": [35, 37, 38, 66, 68, 69, 71, 72, 78], "gcp": [17, 18, 22, 24, 26, 27, 37, 38, 39, 41, 42, 66, 68, 69, 70, 74, 79, 94, 95, 96, 98], "gcp_if_": [35, 39], "gcp_project_id": [35, 38, 39, 66, 69, 70, 72, 78], "gcp_region": [35, 38, 39, 66, 69, 70, 71, 72, 76, 78], "gcs_bucket_nam": [35, 39, 42, 66, 70, 78], "gear": [267, 283], "gee": [267, 283], "gener": [1, 2, 9, 10, 11, 13, 15, 16, 35, 39, 86, 87, 88, 89, 90, 91, 105, 109, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 135, 147, 148, 149, 159, 163, 164, 166, 167, 168, 169, 170, 175, 177, 198, 202, 204, 206, 211, 213, 215, 218, 222, 224, 226, 237, 245, 252, 262, 266, 267, 273, 278, 279, 282, 285, 286, 288, 294, 312, 321, 325, 329, 330, 331, 336, 343, 344, 349], "generate_embed": [126, 127, 128, 135], "generate_synthetic_imag": [159, 163], "generated_bi": [159, 163], "generative_cv": [317, 321, 323], "genit": [285, 288], "geniu": [285, 292], "genr": [330, 336], "geo": [337, 338], "german": [285, 292], "german_shepherd": [147, 150], "germani": [267, 283, 285, 292], "get": [1, 4, 5, 6, 8, 11, 12, 16, 17, 22, 28, 29, 30, 31, 35, 39, 40, 43, 44, 45, 47, 49, 50, 51, 53, 55, 57, 59, 61, 62, 63, 64, 66, 72, 73, 75, 82, 83, 86, 87, 88, 89, 90, 91, 103, 109, 110, 113, 115, 116, 119, 121, 123, 126, 127, 128, 131, 136, 137, 138, 147, 150, 151, 153, 159, 162, 164, 166, 167, 169, 170, 171, 172, 174, 182, 183, 184, 185, 186, 187, 188, 190, 191, 195, 218, 222, 224, 227, 228, 237, 256, 259, 263, 267, 283, 285, 291, 292, 293, 298, 306, 307, 309, 315, 316, 317, 318, 321, 323, 324, 328, 329, 330, 332, 334, 337, 340, 343, 345, 347, 349, 350, 352, 355, 359], "get_best_result": [4, 7, 12, 14, 171, 174, 253, 257], "get_checkpoint": [245, 246, 247, 252, 317, 319, 321, 324, 326, 328, 330, 332, 334, 337, 339, 340, 342, 343, 345, 347, 350, 351, 352, 355], "get_config_dict": [6, 259, 262], "get_context": [4, 5, 13, 137, 143, 171, 174, 224, 228, 233, 234, 238, 240, 245, 247, 248, 317, 319, 321, 324, 326, 328, 330, 332, 334, 337, 339, 340, 343, 345, 347, 350, 352, 355], "get_current_temperatur": [118, 123], "get_dataset_shard": [6, 137, 143, 238, 241, 259, 263, 317, 321, 324, 328, 330, 332, 334, 337, 339, 340], "get_devic": [6, 137, 141, 259, 260, 262, 293, 298], "get_image_featur": [128, 131, 147, 149], "get_linear_schedule_with_warmup": [6, 259, 260, 262], "get_model": [337, 340, 341], "get_prob": [147, 149, 150], "get_scor": [337, 341, 342], "get_temperature_d": [118, 123], "get_top_match": [128, 136], "get_us": 168, "get_user_profil": 168, "get_world_rank": [4, 5, 13, 137, 143, 171, 174, 224, 233, 234, 245, 248, 317, 321, 324, 328, 330, 334, 337, 340, 343, 347, 350, 355], "get_world_s": [5, 13, 224, 228, 238, 240, 245, 247], "getbucketloc": [17, 22], "getenv": [84, 85, 86, 87], "getlogg": [164, 167], "getobject": [17, 22], "getsizeof": [3, 181, 183], "gettempdir": [317, 321, 324, 328], "gettingstart": 100, "getvalu": [317, 319, 350, 352], "gh": 0, "ghetto": [267, 283], "giant": [285, 292], "gib": [12, 14], "gift": [267, 283], "girl": [267, 283], "git": [0, 92, 93, 94, 95, 100], "github": [0, 1, 17, 22, 43, 46, 48, 53, 58, 60, 66, 73, 76, 92, 93, 100, 169, 170, 343, 345], "githubusercont": [343, 345], "give": [7, 14, 53, 57, 80, 81, 84, 85, 92, 93, 94, 95, 101, 102, 107, 224, 227, 245, 248, 253, 256, 285, 291, 330, 332, 336, 337, 339], "given": [2, 4, 5, 7, 8, 10, 13, 14, 15, 94, 95, 118, 123, 137, 139, 147, 149, 164, 167, 171, 174, 175, 178, 191, 195, 196, 197, 206, 215, 253, 257, 305, 306, 307, 315, 317, 320, 324, 325, 327, 337, 338, 343, 344], "gke": [17, 18, 20, 24, 27, 69, 70, 72, 75, 126, 127, 360], "glanc": [80, 81], "glass": [267, 283], "glob": [317, 323, 324, 326, 329], "global": [28, 30, 35, 39, 43, 45, 53, 56, 66, 69, 70, 137, 144, 224, 228, 317, 320, 330, 332, 350, 353, 355], "global_batch_s": [5, 13, 224, 228, 229, 235, 238, 240, 244, 245, 247, 248, 250, 293, 299], "gloriou": [285, 291], "gloss": [285, 292], "gm": [267, 281, 283], "go": [3, 17, 22, 80, 81, 101, 102, 107, 110, 116, 118, 121, 137, 144, 181, 184, 266, 267, 279, 283, 285, 291, 292, 350, 352], "goal": [4, 100, 171, 174, 267, 283, 293, 295, 324, 325, 350, 351], "goaldotcom": [267, 283], "god": [267, 283], "goe": [88, 89, 94, 95, 267, 281, 283, 285, 288], "gold": [285, 291], "golions2012": [267, 283], "gone": [285, 292], "gonna": [267, 283], "good": [7, 8, 14, 28, 30, 43, 45, 53, 56, 110, 112, 137, 138, 159, 163, 191, 197, 224, 226, 237, 238, 242, 253, 256, 267, 283, 285, 288, 291, 292, 337, 341, 343, 345], "googl": [8, 17, 19, 20, 24, 27, 37, 39, 68, 191, 192, 267, 283], "google_cloud": [66, 69], "google_project_id": [35, 39, 66, 70, 78], "google_region": [35, 39, 66, 70, 78], "googleapi": [35, 38, 66, 69], "gore": [285, 292], "got": [3, 181, 189, 267, 283, 285, 292], "gotrib": [267, 283], "gotten": [285, 292], "govern": [126, 127], "gpu": [1, 3, 4, 7, 8, 9, 10, 11, 12, 14, 15, 24, 27, 28, 34, 43, 46, 53, 58, 80, 81, 84, 85, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 113, 114, 116, 117, 118, 119, 124, 125, 126, 127, 128, 129, 132, 133, 136, 137, 139, 143, 144, 146, 155, 157, 159, 161, 169, 170, 171, 172, 174, 181, 187, 190, 191, 196, 198, 205, 206, 208, 212, 213, 217, 218, 220, 221, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 237, 238, 239, 244, 245, 252, 253, 254, 256, 257, 260, 261, 266, 267, 277, 279, 283, 284, 293, 295, 298, 299, 300, 306, 307, 315, 317, 318, 321, 323, 324, 325, 328, 329, 330, 331, 334, 336, 337, 340, 344, 350, 351, 354, 356, 359], "grab": [317, 319, 350, 359], "gracefulli": [92, 93, 307, 309, 316], "grad": [224, 228], "grade": [101, 102, 103, 107, 110, 117, 118, 125, 137, 145, 317, 323], "grader": [267, 283], "gradient": [4, 5, 6, 12, 13, 137, 143, 171, 174, 224, 225, 228, 231, 234, 259, 263, 293, 298, 337, 338, 339, 343, 345], "gradual": [159, 163, 317, 318, 337, 338], "grafana": [88, 89, 92, 93, 110, 116, 157, 164, 166, 167], "grai": [5, 7, 10, 13, 14, 16, 206, 211, 224, 226, 237, 253, 255], "grain": [3, 11, 24, 26, 181, 187, 218, 221], "grand": [267, 283], "grant": [17, 22, 24, 26, 285, 288], "granular": [94, 95, 164, 167], "granularli": 16, "graph": [2, 3, 137, 144, 155, 158, 159, 163, 175, 177, 181, 184], "grass": [285, 292], "grayscal": [5, 7, 13, 14, 224, 226, 227, 237, 253, 255], "great": [94, 95, 110, 115, 267, 283, 285, 292, 307, 308, 309, 316], "greater": [12, 86, 87, 88, 89], "greec": [267, 283], "green": [317, 318, 350, 351], "grei": [267, 283], "grep": [28, 30, 43, 45, 49, 50, 51, 53, 56, 61, 62, 64, 126, 127], "grid": [7, 14, 224, 226, 237, 253, 257, 317, 321, 324, 328, 329, 330, 334, 343, 345, 347, 349, 350, 357], "grim": [285, 291, 292], "grimnoir": [285, 292], "ground": [10, 15, 206, 212, 215, 224, 226, 324, 326, 343, 344, 347, 349, 350, 359], "ground_truth_label": [10, 15, 206, 215], "group": [14, 21, 23, 24, 26, 28, 30, 34, 43, 45, 53, 56, 57, 63, 79, 80, 81, 88, 89, 94, 95, 96, 98, 199, 207, 224, 225, 235, 267, 283, 317, 318, 330, 336, 343, 344, 350, 351, 352, 353, 357], "groupbi": [8, 191, 196, 205, 330, 332, 334, 343, 347, 350, 357], "grouplen": [330, 332], "grow": [2, 4, 12, 100, 171, 173, 175, 177, 285, 288], "grpc": [8, 11, 16, 147, 151, 191, 197, 218, 220, 222], "gserviceaccount": [35, 39, 66, 70], "gsutil": [35, 42, 66, 78], "gt": [285, 292], "guarante": [8, 118, 122, 191, 196, 224, 234], "guard": [330, 334, 350, 357], "gucci": [267, 283], "gui": [267, 283], "guid": [1, 5, 6, 10, 13, 14, 24, 26, 28, 29, 43, 44, 53, 54, 66, 67, 79, 92, 93, 100, 110, 117, 118, 120, 121, 122, 123, 125, 137, 139, 143, 155, 156, 158, 164, 165, 167, 169, 170, 206, 210, 224, 234, 259, 263, 267, 283, 293, 300, 306, 307, 315], "gunman": [285, 291], "gym": [324, 325, 326, 329], "gymnasium": [324, 325, 326], "gz": [13, 14], "h": [5, 11, 13, 137, 144, 147, 153, 168, 218, 222, 224, 237, 317, 318, 319, 320, 343, 345, 350, 359], "ha": [1, 3, 5, 6, 8, 16, 17, 20, 35, 42, 53, 63, 66, 78, 80, 81, 84, 85, 86, 87, 88, 89, 94, 95, 96, 98, 110, 112, 118, 121, 125, 128, 130, 132, 135, 147, 151, 159, 163, 164, 167, 169, 170, 181, 183, 191, 195, 196, 197, 224, 225, 227, 238, 242, 243, 259, 261, 267, 270, 271, 275, 277, 281, 283, 284, 285, 288, 291, 292, 293, 298, 307, 308, 309, 316, 330, 332], "had": [9, 198, 204, 267, 283, 285, 288], "hadoop": [8, 191, 195], "hahha": [267, 283], "hail": [285, 292, 343, 344], "half": [267, 283, 285, 291, 292, 343, 344, 345], "halfstarv": [285, 292], "halloween": [267, 281, 283], "halv": [7, 14, 253, 257], "ham": [267, 283], "hamburg": [350, 351], "hand": [1, 79, 100, 101, 102, 109, 118, 120, 126, 127, 169, 170, 267, 283, 285, 291, 330, 332, 350, 351], "handheld": [285, 292], "handl": [3, 4, 8, 10, 11, 12, 16, 17, 22, 24, 25, 26, 66, 73, 80, 81, 90, 91, 100, 101, 102, 104, 107, 110, 113, 115, 118, 122, 126, 127, 137, 144, 155, 157, 159, 163, 168, 171, 174, 181, 185, 187, 190, 191, 192, 195, 196, 197, 206, 208, 218, 220, 221, 224, 225, 226, 227, 228, 230, 232, 237, 238, 239, 240, 244, 245, 246, 248, 251, 267, 277, 284, 285, 287, 293, 295, 298, 302, 307, 313, 317, 318, 321, 323, 324, 325, 326, 330, 331, 332, 334, 336, 337, 339, 340, 343, 344, 350, 351, 352, 354, 355, 357], "handwritten": [7, 11, 14, 218, 222, 224, 226, 253, 255, 256], "hang": [137, 144, 155, 157, 285, 291, 292, 324, 325], "happen": [2, 7, 8, 14, 159, 163, 175, 179, 191, 197, 245, 248, 253, 257, 285, 291, 292], "happi": [267, 283], "happybirthdayremuslupin": [267, 281, 283, 284], "hard": [285, 291, 292], "hardli": [285, 288], "hardwar": [5, 6, 10, 11, 84, 85, 88, 89, 100, 101, 102, 106, 107, 112, 113, 114, 125, 147, 151, 206, 208, 212, 213, 218, 220, 221, 224, 225, 238, 239, 245, 248, 259, 261, 293, 295, 298, 299, 300, 337, 338], "hark": [285, 291], "harm": 176, "harri": [267, 283], "hasattr": [350, 355], "hasek": [267, 283], "hash": [9, 198, 204], "hashicorp": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68], "hashtag": [267, 283], "hat": [330, 331, 350, 351], "hater": [267, 283], "have": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 28, 29, 30, 34, 35, 37, 43, 44, 45, 46, 52, 53, 55, 56, 58, 65, 66, 68, 82, 83, 84, 85, 86, 87, 94, 95, 100, 101, 102, 105, 110, 117, 128, 131, 133, 137, 139, 141, 144, 147, 151, 159, 163, 164, 165, 169, 170, 171, 172, 181, 185, 188, 189, 191, 194, 195, 198, 203, 206, 208, 212, 213, 215, 218, 220, 222, 224, 234, 238, 244, 253, 257, 259, 263, 267, 275, 283, 285, 288, 289, 290, 291, 292, 293, 298, 302, 307, 313, 330, 331, 337, 339, 340, 350, 351, 359], "hdf": [8, 9, 191, 195, 198, 203], "he": [267, 283, 285, 291, 292], "head": [3, 9, 14, 17, 21, 22, 24, 26, 43, 50, 53, 62, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 128, 132, 147, 153, 159, 162, 163, 181, 183, 187, 198, 201, 203, 285, 292, 317, 321, 324, 328, 337, 339, 343, 345, 349], "head_nod": [43, 50, 53, 62], "header": [0, 5, 13, 330, 336], "headlei": [267, 281, 283], "headless": [24, 26], "headnodeconfig": [43, 50, 53, 62], "health": [24, 25, 101, 102, 106, 147, 152, 168, 317, 321], "healthi": [84, 85, 92, 93, 343, 347], "heap": [3, 181, 189], "hear": [267, 283], "heard": [285, 288], "heart": [285, 291, 343, 347], "heat": [307, 308, 309, 316], "heatmap": [337, 341], "heaven": [285, 291], "heavi": [5, 9, 10, 15, 80, 81, 198, 204, 206, 215, 238, 239], "heavili": [285, 292], "heavli": [224, 232], "hebdo": [267, 283], "hei": [17, 22, 267, 283], "height": [10, 15, 16, 159, 163, 206, 212, 317, 319, 337, 342], "held": [267, 283], "hell": [267, 283], "hello": [82, 83, 86, 87, 90, 91, 101, 164, 167, 267, 283], "hello_world": [1, 82, 83, 90, 91, 169, 170], "helm": [24, 25, 43, 44, 45, 46, 48, 51, 53, 55, 56, 58, 60, 64, 66, 68, 73, 76, 78], "helm_upgrade_command": [43, 45, 53, 56], "help": [1, 7, 8, 10, 14, 16, 17, 20, 24, 26, 82, 83, 101, 102, 105, 147, 154, 155, 157, 164, 166, 169, 170, 191, 195, 206, 212, 224, 226, 253, 256, 285, 292, 337, 339, 341, 343, 344, 345], "helper": [118, 123, 224, 226, 231, 232, 233, 234, 330, 332, 337, 339, 345, 351, 359], "helper_tool_map": [118, 123], "her": [267, 283, 285, 288, 291, 292], "here": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 28, 30, 43, 45, 53, 63, 100, 101, 102, 103, 104, 107, 110, 111, 117, 118, 119, 121, 122, 124, 125, 126, 127, 128, 129, 137, 138, 147, 148, 159, 163, 164, 165, 166, 167, 171, 172, 174, 175, 176, 179, 180, 181, 182, 190, 191, 192, 193, 194, 195, 197, 198, 199, 202, 203, 204, 205, 206, 207, 210, 212, 215, 218, 219, 221, 222, 223, 224, 226, 228, 229, 236, 237, 238, 240, 245, 248, 252, 253, 254, 256, 257, 258, 259, 260, 262, 263, 264, 266, 267, 279, 283, 285, 287, 289, 291, 292, 293, 295, 302, 306, 307, 313, 315, 317, 318, 319, 324, 325, 330, 331], "herm": [118, 123], "hero": [267, 283, 285, 292], "herself": [285, 292], "heteregen": [10, 206, 208], "heterogen": [4, 8, 9, 10, 15, 126, 127, 128, 129, 132, 137, 138, 144, 147, 151, 171, 174, 191, 196, 198, 205, 206, 208, 217], "hf": [110, 113], "hf_d": [317, 319], "hf_dataset": [267, 271, 281], "hf_token": [110, 113, 114, 115, 116, 118, 121, 122, 123], "hi": [267, 281, 283, 285, 288, 291, 292], "hidden": [0, 285, 292, 324, 329], "hidden_dim": [137, 140, 143, 144], "hide": [285, 291], "hierarch": [164, 167], "hierarchi": [96, 99], "high": [3, 5, 6, 7, 8, 10, 11, 13, 14, 16, 17, 22, 92, 93, 101, 102, 104, 105, 106, 107, 118, 124, 128, 132, 147, 151, 181, 187, 191, 192, 195, 196, 206, 212, 218, 220, 221, 224, 227, 235, 253, 256, 259, 263, 267, 277, 283, 284, 285, 292, 302, 307, 311, 313, 316, 324, 329, 330, 331, 332, 334, 350, 351], "higher": [8, 15, 86, 87, 101, 102, 105, 110, 116, 117, 118, 124, 128, 130, 147, 151, 191, 196, 238, 239, 330, 331, 332], "highest": [285, 291, 317, 321, 330, 336], "highli": [17, 22, 24, 26, 126, 127, 147, 151, 159, 163, 337, 339], "highlight": [3, 6, 8, 181, 183, 191, 195, 224, 234, 259, 263, 285, 292, 330, 332, 337, 341], "hike": [267, 283], "him": [267, 283, 285, 291, 292], "himself": [267, 283, 285, 291], "hindu": [267, 283], "hint": [2, 3, 5, 6, 7, 10, 13, 14, 175, 180, 181, 190, 206, 212, 253, 257, 259, 263, 293, 296], "hire": [285, 291], "hist": [4, 171, 174, 330, 332, 337, 340], "histor": [8, 191, 192, 195, 330, 331, 343, 344], "histori": [118, 123, 224, 236, 330, 334, 343, 345, 347, 349, 350, 357, 359], "hit": [28, 30, 43, 45, 53, 56, 159, 163, 267, 283, 330, 336], "hitchhik": [267, 283], "hiya": [267, 283], "hmu": [267, 283], "hmw": [267, 283], "hoc": [324, 325, 337, 341], "hogan": [267, 283], "hogwart": [267, 281, 283, 284], "hold": [9, 10, 15, 198, 201, 206, 210, 224, 228, 293, 298], "hole": [267, 283], "holidai": [343, 349], "hollywood": [267, 283, 285, 291, 292], "home": [14, 86, 87, 126, 127, 128, 129, 135, 137, 138, 144, 145, 147, 148, 153, 267, 283], "homebrew": [28, 29, 43, 44, 53, 55, 66, 68, 155, 158], "homecom": [267, 283], "homepath": [155, 158], "hong": [285, 291], "hood": [3, 10, 126, 127, 137, 144, 181, 184, 206, 210, 224, 230], "hook": [285, 291, 292, 324, 329], "hop": [267, 283], "hope": [267, 283, 285, 291], "horizon": [343, 345, 346, 347, 349], "horizont": [110, 113, 116, 147, 149, 151], "horribl": [285, 292], "horror": [285, 292], "hospit": [285, 292], "host": [5, 17, 20, 22, 24, 27, 80, 81, 82, 83, 96, 97, 98, 224, 232, 317, 318], "hostfil": [137, 144], "hostnam": 13, "hot": [11, 137, 143, 218, 223, 337, 341, 350, 351], "hotwif": [267, 283], "hour": [12, 101, 102, 106, 159, 163, 267, 283, 317, 319, 343, 344, 345, 349, 350, 352], "hourli": [344, 349], "hous": [285, 291], "housekeep": [224, 226], "hoverboard": [267, 283], "how": [3, 4, 5, 6, 7, 8, 13, 14, 15, 16, 18, 28, 30, 35, 36, 39, 43, 45, 53, 56, 63, 80, 81, 86, 87, 92, 93, 96, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 115, 119, 120, 123, 125, 126, 127, 128, 135, 137, 143, 147, 149, 159, 163, 164, 165, 167, 168, 171, 174, 181, 185, 191, 193, 199, 201, 202, 203, 204, 207, 213, 227, 228, 229, 230, 234, 238, 239, 240, 245, 246, 248, 253, 257, 258, 259, 260, 262, 263, 266, 267, 275, 277, 278, 279, 281, 283, 284, 285, 286, 287, 290, 291, 292, 293, 294, 295, 300, 307, 309, 312, 316, 332, 334, 336, 340, 345, 359], "howev": [3, 8, 9, 10, 15, 24, 26, 137, 139, 181, 187, 191, 195, 198, 203, 204, 206, 215, 224, 234, 285, 291], "html": [0, 1, 14, 28, 29, 43, 44, 53, 55, 101, 102, 108, 169, 170, 267, 271, 281, 293, 300, 302, 306, 307, 313, 315], "http": [0, 1, 4, 8, 11, 12, 13, 14, 16, 17, 22, 24, 27, 28, 29, 35, 37, 43, 44, 46, 48, 53, 55, 58, 60, 63, 66, 68, 73, 76, 88, 89, 92, 93, 100, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 126, 127, 128, 129, 136, 137, 138, 147, 148, 150, 151, 153, 155, 158, 164, 167, 168, 169, 170, 171, 174, 191, 197, 218, 222, 267, 271, 281, 293, 300, 302, 306, 307, 309, 313, 315, 316, 330, 332, 343, 345], "huddleston": [267, 283], "hudi": [8, 191, 192], "hug": [110, 113, 114, 118, 121, 123, 137, 143, 266, 267, 277, 279, 281, 284, 285, 288, 296, 297, 300, 304, 307, 311, 314, 316, 350, 351, 352, 359], "huge": [285, 292], "huggingfac": [5, 6, 101, 102, 108, 110, 113, 114, 115, 118, 121, 122, 259, 261, 267, 269, 280], "huggingface_hub": [118, 121], "hugo": [285, 292], "hulk": [267, 283], "human": [9, 198, 201, 224, 234, 267, 283], "humbl": [285, 291], "humor": [285, 291, 292], "hundr": [337, 338], "hunt": [267, 283], "hurt": [267, 283], "hustl": [267, 283], "hvar": [267, 283], "hxwxc": [159, 163], "hybrid": [118, 124, 330, 336], "hydrologi": [337, 338], "hyper": [137, 146], "hyperband": [7, 14, 253, 257], "hyperparam": [343, 349], "hyperparamet": [5, 6, 8, 12, 13, 137, 138, 140, 143, 172, 191, 195, 224, 228, 229, 235, 238, 244, 245, 248, 250, 254, 256, 258, 259, 263, 317, 323, 324, 329, 330, 336, 337, 340, 342, 343, 349, 350, 359], "hypnot": [285, 292], "i": [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 22, 24, 25, 27, 28, 32, 33, 35, 39, 41, 43, 44, 45, 49, 50, 51, 53, 54, 56, 61, 62, 63, 64, 66, 67, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 103, 105, 106, 107, 111, 114, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 130, 131, 132, 133, 136, 137, 138, 139, 143, 146, 147, 151, 155, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 192, 193, 195, 199, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 237, 238, 239, 241, 242, 243, 245, 246, 247, 248, 250, 253, 254, 255, 256, 257, 259, 260, 262, 263, 266, 267, 273, 275, 277, 279, 281, 282, 283, 284, 285, 287, 288, 289, 290, 291, 292, 293, 295, 298, 299, 300, 306, 309, 311, 315, 316, 317, 318, 319, 321, 323, 324, 325, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 343, 344, 345, 346, 347, 348, 350, 351, 352, 353, 355, 356, 357, 359], "iam": [20, 21, 23, 24, 26, 27, 28, 29, 30, 34, 35, 36, 38, 39, 43, 44, 45, 55, 56, 63, 66, 69, 70], "ic": [267, 283], "iceberg": [8, 10, 191, 192, 206, 210], "ichiro": [267, 283], "icon": [82, 83, 118, 122, 285, 292], "id": [23, 28, 30, 35, 38, 39, 43, 45, 47, 48, 53, 56, 59, 60, 66, 69, 70, 75, 76, 80, 81, 88, 89, 118, 121, 123, 126, 127, 147, 153, 159, 163, 164, 167, 168, 176, 224, 235, 285, 289, 290, 291, 292, 293, 298, 331, 350, 352], "idea": [28, 30, 43, 45, 53, 56, 285, 292], "ideal": [4, 8, 82, 83, 110, 112, 117, 128, 133, 171, 174, 191, 192, 195, 285, 292, 337, 339, 343, 349], "ident": [17, 22, 24, 27, 35, 39, 43, 45, 53, 56, 66, 70, 94, 95, 126, 127, 224, 225, 234, 238, 240], "identifi": [28, 30, 43, 45, 53, 56, 110, 113, 118, 121, 159, 163, 224, 233, 234], "idiot": [267, 283, 285, 292], "idl": [28, 30, 43, 45, 53, 56, 101, 102, 105, 106, 126, 127, 128, 130, 132, 133, 136, 137, 146], "idx": [6, 224, 237, 259, 262, 330, 336, 343, 345, 350, 353, 359], "idx1": [13, 14], "idx2item": [330, 336], "idx3": [13, 14], "ig": [267, 283], "ignor": [317, 321, 324, 328], "iid": [330, 332, 336], "ill": [267, 283], "illustr": [10, 24, 26, 100, 206, 210, 267, 283, 317, 323], "iloc": [6, 137, 144, 147, 150, 259, 262, 330, 336, 343, 345, 349, 350, 353], "im": [267, 283, 285, 292], "imag": [3, 5, 6, 7, 8, 10, 13, 14, 15, 16, 24, 26, 80, 81, 86, 87, 92, 93, 101, 102, 108, 110, 115, 126, 127, 130, 131, 135, 137, 139, 145, 147, 148, 149, 153, 159, 163, 164, 167, 181, 186, 191, 192, 206, 210, 211, 212, 213, 219, 224, 226, 227, 228, 232, 237, 240, 241, 242, 245, 247, 253, 255, 256, 257, 259, 262, 263, 285, 292, 320, 323, 359], "image_arr": [238, 243], "image_arrai": [159, 163], "image_batch": 16, "image_byt": [317, 319, 350, 352, 353, 359], "image_bytes_raw": [317, 319], "image_classifi": 16, "image_classifier_ingress": 16, "image_height": [159, 163], "image_id": [10, 159, 163, 206, 212], "image_latents_256": [6, 259, 262], "image_uri": [101, 102, 108, 110, 115, 159, 163], "image_width": [159, 163], "imagebatchpredictor": [350, 359], "imagenet": [224, 227, 350, 352, 353], "imagenet_mean": [350, 353, 359], "imagenet_std": [350, 353, 359], "imageri": [285, 292], "imageserviceingress": 16, "imagin": [285, 291, 292], "imbalanc": [337, 339], "imdb": [285, 287, 288, 289, 291, 292], "img": [5, 10, 128, 131, 206, 211, 224, 226, 237, 317, 319, 323, 350, 352, 353, 359], "immatur": [8, 191, 196], "immedi": [2, 3, 8, 10, 92, 93, 101, 102, 105, 175, 179, 181, 187, 191, 195, 206, 211, 245, 250, 317, 322, 350, 353], "immut": [3, 181, 183], "impact": [8, 86, 87, 101, 102, 105, 106, 118, 124, 191, 196, 285, 292], "implement": [2, 5, 6, 7, 8, 9, 10, 14, 15, 17, 21, 92, 93, 126, 127, 164, 167, 168, 175, 180, 191, 192, 196, 197, 198, 204, 206, 212, 213, 219, 220, 224, 227, 228, 238, 239, 253, 256, 259, 262, 263, 266, 267, 273, 279, 282, 350, 351], "implementaiton": 13, "impli": [285, 288], "implicit": [317, 323], "import": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 108, 110, 113, 114, 115, 116, 118, 121, 122, 123, 128, 129, 131, 133, 136, 137, 138, 139, 140, 141, 142, 143, 146, 147, 148, 159, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 180, 181, 182, 187, 198, 199, 206, 207, 218, 219, 223, 233, 253, 254, 259, 260, 266, 279, 287, 292, 295, 302, 309, 313, 316, 321, 323, 328, 334, 338, 342, 347, 349, 355, 359], "import_path": [92, 93, 101, 102, 108, 110, 115, 118, 122], "importance_typ": [337, 341], "imposs": [285, 292], "impress": [285, 292], "improv": [2, 9, 10, 11, 118, 121, 122, 126, 127, 128, 130, 132, 137, 144, 147, 151, 155, 157, 159, 162, 175, 180, 198, 204, 205, 206, 208, 217, 218, 220, 238, 239, 317, 323, 337, 338, 342, 343, 347, 349], "imshow": [5, 7, 10, 13, 14, 16, 206, 211, 224, 226, 237, 253, 255, 317, 319, 323, 350, 352, 359], "in_channel": [5, 6, 13, 224, 227, 259, 262], "in_count": [88, 89], "in_featur": [13, 137, 140], "in_proj": [343, 346], "inaccess": [159, 163], "incept": [317, 323], "includ": [0, 3, 4, 5, 8, 9, 10, 11, 12, 13, 17, 20, 24, 26, 28, 34, 43, 52, 53, 65, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 106, 118, 121, 128, 134, 135, 147, 151, 153, 155, 157, 158, 164, 166, 171, 174, 181, 186, 187, 191, 192, 195, 196, 198, 201, 206, 215, 218, 221, 224, 236, 245, 248, 251, 293, 295, 298, 299, 302, 307, 313, 330, 332, 334, 343, 347, 350, 357], "include_path": [10, 15, 16, 128, 130, 137, 139, 146, 206, 210, 212], "incom": [16, 147, 148, 350, 359], "incomplet": [224, 232], "incorpor": [8, 191, 192, 350, 351], "incorrect_squar": [3, 181, 185], "increas": [101, 102, 105, 110, 116, 128, 130, 133, 136, 137, 139, 143, 146, 159, 163, 245, 252, 307, 311, 316, 317, 323, 330, 334], "increasingli": [1, 169, 170], "increment": [8, 92, 93, 147, 151, 191, 192, 350, 351], "incur": [8, 164, 167, 191, 196], "indent": [137, 140, 143], "independ": [9, 10, 11, 86, 87, 110, 116, 147, 149, 198, 205, 206, 210, 218, 221, 224, 230, 238, 239, 243, 343, 344, 347, 350, 353], "index": [0, 4, 8, 128, 133, 164, 166, 171, 174, 191, 192, 224, 226, 237, 302, 307, 313, 330, 332, 337, 339, 340, 343, 345, 350, 353, 359], "index_col": [337, 340], "indi": [285, 288], "indian": [267, 283], "indic": [1, 3, 169, 170, 181, 187, 224, 237, 330, 331, 332, 336, 337, 341, 350, 353], "individu": [2, 4, 12, 137, 144, 159, 162, 171, 173, 175, 180], "indonesiasayshbdforjustinbieb": [267, 283], "industri": 360, "ineffect": [285, 291], "ineffici": [101, 102, 105, 147, 151, 266, 267, 279], "inf": [137, 143], "infer": [3, 8, 9, 10, 11, 12, 15, 16, 84, 85, 90, 91, 106, 111, 117, 118, 121, 124, 126, 127, 130, 135, 137, 138, 139, 146, 147, 148, 151, 172, 181, 187, 190, 191, 195, 196, 197, 198, 201, 205, 206, 208, 213, 217, 218, 222, 225, 226, 234, 236, 245, 252, 278, 283, 286, 294, 306, 307, 312, 315, 317, 318, 323, 324, 325, 329, 332, 338, 340, 344, 345, 351], "inference_01": 360, "inference_02": 360, "inference_03": 360, "inference_04": 360, "inference_05": 360, "inference_06": 360, "inference_07": 360, "inference_08": 360, "inference_mod": [128, 131, 137, 140, 143, 147, 149, 224, 237], "inference_row": [350, 359], "inferf": [330, 331], "infinit": [10, 206, 212], "influenc": [285, 292, 324, 325], "info": [1, 12, 14, 110, 116, 164, 167, 169, 170, 267, 283], "inform": [5, 13, 43, 44, 53, 54, 66, 67, 86, 87, 118, 123, 155, 158, 159, 163, 164, 166, 168, 224, 228, 267, 283, 302, 307, 313, 350, 357], "infrastructur": [1, 5, 6, 18, 19, 25, 26, 35, 36, 79, 82, 83, 92, 93, 96, 97, 98, 100, 106, 110, 115, 117, 118, 121, 125, 159, 162, 163, 169, 170, 224, 225, 245, 249, 259, 261, 324, 325, 330, 336, 350, 351], "ingest": [4, 12, 15, 126, 127, 137, 139, 171, 174, 238, 239, 244, 245, 252, 337, 338, 339, 342, 350, 351], "ingmar": [285, 288], "ingress": [4, 16, 24, 26, 27, 51, 52, 64, 65, 78, 147, 150, 171, 174, 305, 306, 307, 315], "ingress_from_cidr_map": [17, 22], "ingress_with_self": [17, 22], "inher": 100, "inherit": [4, 12, 92, 93, 171, 173], "ini": [155, 158], "init": [1, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 88, 89, 126, 127, 128, 129, 137, 138, 147, 148, 169, 170, 224, 226, 267, 271, 281, 285, 288, 293, 299], "initi": [1, 3, 6, 8, 10, 13, 15, 24, 26, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 92, 93, 118, 121, 128, 131, 137, 140, 147, 149, 153, 169, 170, 181, 184, 191, 196, 206, 213, 224, 225, 235, 237, 259, 262, 263, 267, 271, 273, 278, 281, 282, 286, 287, 293, 294, 295, 298, 299, 312, 330, 331, 343, 344], "initial_replica": 16, "inject": [164, 167, 224, 229, 232, 317, 318, 324, 325, 326, 327, 350, 354], "inlin": [128, 135, 137, 145, 147, 153, 306, 307, 315], "inner": [285, 291], "inning": [267, 281, 283], "innings": [267, 281, 283], "innoc": [285, 291, 292], "inplac": [13, 137, 140, 337, 339], "input": [6, 9, 10, 11, 12, 15, 16, 86, 87, 101, 102, 104, 128, 130, 131, 136, 137, 139, 147, 149, 164, 166, 198, 201, 203, 204, 206, 208, 212, 213, 214, 215, 216, 218, 222, 224, 226, 227, 228, 238, 239, 245, 252, 259, 262, 267, 273, 282, 285, 290, 293, 298, 306, 307, 315, 320, 337, 338, 343, 344, 345, 346, 347, 349], "input_window": [343, 345, 346, 347, 349], "inputdatabuff": [10, 206, 214], "inscrut": [8, 191, 195], "insert": [101, 102, 105], "insid": [3, 9, 86, 87, 88, 89, 96, 98, 128, 131, 133, 164, 166, 167, 181, 188, 190, 198, 205, 224, 226, 229, 238, 244, 285, 288, 317, 321, 324, 325, 337, 340, 342, 350, 351, 353, 359], "insight": [8, 137, 144, 159, 161, 163, 191, 195], "inspect": [3, 4, 5, 6, 13, 86, 87, 92, 93, 171, 174, 181, 187, 225, 226, 245, 252, 259, 262, 266, 267, 279, 317, 319, 352], "inspir": [267, 283], "instal": [3, 4, 24, 25, 27, 28, 29, 30, 37, 44, 45, 51, 52, 54, 55, 56, 64, 65, 67, 68, 72, 74, 79, 82, 83, 84, 85, 86, 87, 110, 114, 126, 127, 128, 129, 137, 138, 147, 148, 171, 172, 181, 186, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "instanc": [1, 3, 4, 5, 9, 10, 12, 15, 21, 23, 24, 26, 30, 34, 39, 43, 50, 53, 62, 66, 70, 80, 81, 84, 85, 88, 89, 110, 116, 126, 127, 128, 131, 132, 133, 134, 136, 137, 139, 143, 144, 146, 159, 163, 164, 167, 168, 169, 170, 171, 174, 181, 187, 190, 198, 204, 206, 208, 215, 224, 226, 232, 237, 266, 267, 279, 285, 292, 306, 309, 315], "instance_iam_role_arn": [17, 23], "instance_profile_arn": [28, 30], "instance_typ": [43, 50, 53, 62, 159, 163], "instanceid": [28, 30, 43, 45, 53, 56], "instant": [82, 83], "instanti": [3, 6, 16, 181, 190, 259, 262, 350, 351, 356], "instead": [3, 4, 5, 7, 9, 13, 14, 86, 87, 88, 89, 92, 93, 128, 129, 130, 131, 137, 138, 147, 148, 171, 174, 181, 184, 185, 189, 198, 201, 203, 204, 224, 225, 230, 231, 238, 239, 240, 241, 245, 247, 253, 257, 266, 267, 273, 279, 282, 285, 291, 292, 317, 318, 330, 331, 332, 337, 338, 340, 341, 343, 344, 350, 352, 355], "instruct": [24, 25, 84, 85, 100, 101, 102, 108, 110, 112, 113, 116, 118, 121, 122, 124, 126, 127], "instrument": [168, 267, 283], "insubordin": [285, 291], "int": [3, 4, 5, 6, 7, 10, 13, 14, 15, 159, 163, 171, 174, 181, 183, 185, 187, 206, 212, 215, 224, 232, 233, 237, 238, 241, 245, 248, 253, 255, 256, 257, 259, 262, 317, 319, 320, 324, 326, 327, 330, 333, 337, 341, 342, 343, 345, 347, 350, 352, 354, 355, 359], "int32": [337, 341, 343, 345], "int4": [110, 116], "int64": [137, 141, 267, 281, 284, 285, 289, 290, 324, 326, 330, 332], "intact": [224, 228], "integ": [12, 137, 139, 238, 243, 330, 331, 332, 350, 351], "integr": [4, 5, 6, 8, 9, 12, 13, 17, 21, 24, 27, 79, 80, 81, 92, 93, 101, 102, 107, 110, 116, 118, 120, 122, 123, 125, 126, 127, 147, 151, 154, 155, 157, 158, 159, 163, 164, 167, 171, 173, 191, 192, 195, 196, 197, 198, 205, 224, 225, 232, 240, 241, 244, 259, 261, 266, 267, 279, 285, 287, 293, 299, 302, 307, 313, 318, 324, 325, 328, 329, 330, 331, 337, 342, 343, 345, 350, 351, 359, 360], "intellig": [8, 118, 123, 191, 192], "intend": [86, 87, 92, 93, 285, 288, 293, 299], "intens": [1, 3, 4, 7, 8, 14, 101, 102, 106, 110, 112, 169, 170, 171, 174, 181, 187, 191, 196, 238, 239, 253, 255, 285, 292], "intent": [285, 291, 292, 317, 318, 337, 338], "interact": [8, 16, 24, 26, 82, 83, 96, 98, 100, 101, 102, 106, 107, 118, 123, 125, 191, 195, 285, 292, 330, 331, 332, 337, 342], "interconnect": [110, 116], "interest": [267, 283, 285, 288], "interfac": [4, 5, 8, 155, 158, 171, 173, 191, 194, 224, 226, 238, 239, 302, 307, 313, 330, 334], "intermedi": [3, 5, 10, 13, 101, 102, 104, 128, 130, 181, 184, 206, 208, 224, 234, 337, 342, 343, 349, 350, 359], "intern": [7, 17, 22, 159, 161, 238, 240, 253, 258, 330, 334, 336, 350, 354], "internet": [17, 22], "interoper": [8, 191, 192], "interpret": [330, 336, 337, 342, 343, 349], "interrupt": [84, 85, 147, 151, 245, 248, 250, 343, 348, 350, 351], "interv": [6, 8, 16, 191, 197, 259, 262, 343, 344, 345], "intervent": [245, 248, 330, 335, 343, 348, 350, 351], "interview": [9, 15, 198, 205, 267, 283], "intrigu": [285, 291, 292], "intro": [11, 12, 43, 44, 53, 55, 66, 68, 218, 223, 360], "introduc": [2, 8, 11, 16, 82, 83, 126, 127, 155, 156, 159, 162, 175, 177, 191, 195, 218, 219], "introduct": [15, 16, 238, 239, 293, 295], "introductori": 100, "intuit": [8, 191, 197], "invari": [285, 288], "invent": [285, 291, 292, 343, 344], "invert_yaxi": [337, 341], "invest": [267, 281, 283], "investig": [137, 144], "invit": [96, 98], "invoc": [2, 175, 178, 180], "invok": [2, 3, 175, 179, 180, 181, 188, 337, 342], "involv": [9, 10, 15, 101, 102, 108, 198, 204, 206, 215, 267, 283, 285, 291, 292, 302, 307, 313], "io": [0, 1, 10, 14, 43, 44, 46, 48, 53, 55, 58, 60, 66, 68, 73, 76, 128, 136, 169, 170, 182, 206, 209, 224, 226, 267, 271, 281, 293, 300, 302, 306, 307, 313, 315, 317, 319, 343, 345, 350, 352, 353, 359], "iot": [8, 191, 195], "ip": [13, 14, 17, 20, 22, 28, 30, 43, 45, 53, 56, 66, 73, 128, 133, 136, 137, 146, 224, 235, 350, 351], "ipykernel": [1, 169, 170], "ipynb": [82, 83, 86, 87, 100, 126, 127, 360], "ipywidget": [128, 129, 137, 138, 147, 148], "iran": [267, 283], "iron": [267, 283], "irrupt": [267, 283], "is_avail": [5, 7, 253, 256, 293, 298, 317, 323, 324, 329, 343, 349, 350, 359], "is_big_tip": 12, "isdir": [137, 142, 245, 251, 350, 359], "isfil": [350, 359], "isinst": [3, 181, 184], "islam": [267, 283], "isn": [5, 13, 53, 63, 101, 102, 106, 128, 130, 137, 143, 146, 147, 151, 224, 232, 267, 283, 285, 288, 317, 323, 350, 351], "isol": [8, 11, 17, 19, 22, 191, 192, 218, 221], "issu": [2, 5, 6, 8, 53, 63, 66, 71, 88, 89, 118, 125, 155, 157, 159, 163, 175, 177, 191, 195, 224, 225, 237, 245, 248, 259, 261, 285, 288, 350, 351], "itali": [267, 283], "itbr": [285, 292], "item": [2, 3, 5, 6, 7, 10, 13, 14, 118, 121, 137, 139, 143, 147, 150, 155, 158, 159, 163, 175, 180, 181, 189, 206, 212, 224, 233, 253, 255, 257, 259, 262, 285, 292, 293, 298, 317, 319, 324, 326, 333, 334, 337, 341, 343, 347, 349, 350, 355, 359], "item2idx": [330, 332, 336], "item_col": [330, 332], "item_embed": [330, 333, 336], "item_id": [330, 332, 336], "item_idx": [330, 331, 332, 333, 334], "item_metadata": [330, 336], "item_vec": [330, 333], "item_vector": [330, 336], "iter": [9, 10, 11, 12, 14, 80, 81, 82, 83, 101, 102, 105, 118, 124, 126, 127, 128, 130, 198, 202, 206, 209, 218, 220, 224, 228, 238, 241, 293, 298, 317, 318, 321, 323, 324, 328, 329, 330, 332, 334, 335, 337, 342, 343, 345], "iter_torch_batch": [137, 143, 238, 239, 241, 245, 252, 317, 321, 324, 328, 330, 331, 334, 336], "iterations_since_restor": 13, "iterrow": [330, 336], "its": [2, 3, 5, 7, 8, 9, 10, 11, 14, 15, 53, 63, 86, 87, 92, 93, 96, 98, 101, 102, 105, 107, 110, 116, 159, 162, 163, 164, 166, 167, 175, 177, 181, 183, 187, 191, 192, 198, 203, 205, 206, 210, 218, 221, 224, 225, 226, 228, 235, 236, 237, 253, 257, 266, 267, 279, 283, 285, 292, 302, 307, 308, 309, 313, 316, 317, 318, 319, 330, 331, 334, 337, 338, 339, 340, 341, 343, 344, 350, 353, 355], "itself": [9, 159, 163, 198, 201], "iv": [285, 292], "j": [0, 2, 137, 143, 175, 180, 267, 283, 330, 332, 336], "jack": [285, 291], "jackson": [267, 283], "jadwal": [267, 283], "jai": [267, 281, 283, 285, 291], "jail": [267, 283], "jame": [267, 283, 285, 291], "jami": [267, 283], "jan": [267, 283], "janet": [267, 283], "januari": [267, 281, 283], "java": [8, 191, 195], "jean": [285, 292], "jeanmarc": [285, 292], "jeff": [285, 291], "jgz99": [110, 115], "jit": [10, 11, 15, 16, 206, 213, 218, 222], "job": [4, 6, 7, 14, 17, 22, 24, 26, 27, 28, 32, 34, 35, 41, 43, 50, 53, 62, 63, 66, 77, 84, 85, 86, 87, 88, 89, 94, 95, 96, 98, 118, 121, 126, 127, 132, 147, 154, 159, 161, 163, 164, 166, 171, 174, 224, 225, 227, 228, 230, 234, 235, 237, 238, 244, 245, 246, 248, 249, 252, 253, 257, 259, 261, 263, 267, 281, 283, 284, 285, 291, 292, 317, 323, 324, 329, 330, 336, 337, 338, 342, 343, 349, 350, 351, 356, 359, 360], "job_config": [90, 91], "job_descript": [118, 121], "job_id": [90, 91], "jobconfig": [90, 91], "joe": [267, 283], "john": [267, 281, 283, 285, 288, 291], "johnson": [267, 283, 285, 288], "joi": [267, 283], "join": [5, 6, 8, 9, 13, 17, 22, 86, 87, 118, 121, 125, 128, 133, 137, 139, 143, 146, 159, 163, 168, 191, 196, 198, 205, 224, 234, 237, 245, 247, 248, 259, 263, 267, 283, 287, 289, 292, 317, 321, 323, 324, 328, 329, 332, 334, 337, 339, 340, 343, 345, 347, 349, 350, 352, 355, 359], "join_typ": [285, 291], "joint": [317, 318, 343, 344, 350, 352], "jointli": [137, 144], "joke": [110, 114], "journei": [168, 285, 292], "jpeg": [8, 191, 192, 317, 318, 319, 323, 350, 352], "jpg": [128, 130, 137, 146], "jq": [28, 30, 43, 45, 53, 56], "json": [4, 8, 10, 11, 12, 16, 102, 108, 119, 120, 121, 123, 125, 137, 139, 140, 143, 146, 147, 150, 153, 164, 167, 168, 171, 174, 191, 192, 206, 210, 218, 219, 222, 307, 309, 316, 317, 319, 324, 326, 330, 332, 337, 339, 350, 352, 355], "json_method1": [118, 122], "json_request": [11, 16, 164, 167, 218, 222], "json_schema": [118, 122], "juda": [267, 283], "judg": [285, 291, 337, 339], "juggl": [11, 218, 220], "jule": [267, 283], "jump": [11, 16, 218, 222, 285, 292, 324, 325], "jun": [267, 283], "june": [4, 12, 66, 69, 100, 171, 174], "jupyt": [84, 85, 128, 130], "jupyter_execute_notebook": 0, "jupyterlab": [82, 83], "just": [7, 9, 14, 15, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 94, 95, 101, 102, 106, 118, 123, 126, 127, 128, 133, 135, 136, 137, 139, 140, 198, 201, 203, 238, 239, 245, 248, 253, 256, 257, 267, 275, 283, 285, 288, 289, 291, 292, 317, 318, 319, 330, 331, 350, 352, 359], "justifi": [267, 283, 285, 291], "justin": [267, 283], "j\u00e4reg\u00e5rd": [285, 292], "k": [101, 102, 105, 126, 127, 137, 139, 147, 153, 267, 281, 283, 293, 298, 324, 325, 329, 330, 336, 337, 338, 343, 349, 350, 359], "k8": [17, 18, 20, 27, 43, 45, 46, 50, 53, 56, 58, 62, 66, 69, 77, 79, 155, 158, 360], "kafka": [8, 128, 130, 191, 195], "katharina": [285, 292], "ke": [267, 283], "keep": [6, 84, 85, 94, 95, 101, 102, 108, 224, 225, 227, 228, 234, 237, 238, 239, 240, 245, 248, 251, 259, 262, 267, 277, 283, 284, 285, 290, 317, 319, 321, 324, 325, 328, 329, 330, 331, 332, 334, 337, 339, 340, 343, 347, 349, 350, 352, 355, 356, 357, 359], "keeper": [285, 291], "kei": [7, 8, 9, 10, 11, 14, 15, 20, 22, 24, 26, 28, 30, 43, 45, 53, 56, 86, 87, 100, 103, 104, 108, 116, 137, 141, 147, 150, 164, 166, 191, 192, 198, 204, 206, 207, 215, 218, 221, 224, 226, 227, 228, 234, 238, 240, 245, 247, 253, 257, 293, 295, 330, 336, 337, 340, 341, 350, 355], "kenni": [267, 283], "kept": [267, 283, 330, 332], "kernel": [1, 24, 27, 101, 102, 107, 128, 129, 137, 138, 147, 148, 169, 170, 224, 227], "kernel_s": [5, 7, 13, 14, 224, 227, 253, 256, 257], "kerri": [267, 281, 283], "kessler": [285, 292], "keylogg": [118, 121], "kick": [330, 334, 350, 356], "kiddo": [267, 283], "kill": [7, 14, 224, 237, 253, 257, 285, 288, 291, 292, 317, 319], "kim": [267, 283], "kind": [1, 169, 170, 337, 339], "kingdom": [285, 292], "kitchen": [267, 283], "klaviyo": 16, "klondik": [285, 291], "km": [17, 22], "know": [3, 8, 118, 121, 159, 163, 181, 189, 191, 196, 245, 246, 248, 285, 291, 292, 343, 344], "knowledg": [79, 110, 117, 164, 165, 337, 341, 342], "known": [317, 318], "kong": [285, 291], "kpop": [267, 283], "kri": [267, 283], "kserv": [147, 151], "kube": [43, 46, 51, 53, 58, 63, 64], "kubeconfig": [43, 46, 53, 58], "kubectl": [43, 44, 46, 49, 50, 51, 53, 55, 58, 61, 62, 63, 64, 68, 73, 78], "kuberai": [155, 158], "kubernet": [11, 17, 19, 20, 44, 45, 47, 52, 55, 56, 59, 65, 68, 69, 73, 75, 78, 79, 126, 127, 218, 220], "kucinich": [267, 283], "kueue": [24, 26], "kurt": [267, 283], "kv": [104, 106, 109, 110, 116, 337, 341], "kv_cache_util": [110, 116], "kvedzwag2qa8i5bj": [110, 115, 147, 153], "kwarg": [128, 131], "l": [4, 5, 9, 10, 13, 15, 53, 63, 171, 174, 198, 201, 203, 206, 210, 317, 318, 324, 325, 330, 331, 350, 351], "l4": [101, 102, 108, 118, 121, 122], "l40": [110, 113, 114, 116, 118, 123], "l6": [267, 273, 282], "lab": [7, 14, 253, 257], "label": [4, 5, 7, 10, 11, 12, 13, 14, 15, 128, 130, 137, 139, 141, 143, 146, 164, 165, 171, 174, 206, 212, 213, 215, 218, 222, 224, 226, 228, 237, 238, 240, 241, 242, 243, 245, 247, 253, 255, 256, 257, 267, 270, 271, 281, 283, 284, 285, 287, 288, 289, 290, 291, 292, 293, 297, 298, 307, 309, 316, 317, 319, 321, 323, 324, 328, 330, 334, 337, 339, 340, 341, 342, 343, 347, 349, 352, 353, 357, 359], "label_col": [337, 340], "label_column": [4, 12, 171, 174, 337, 340], "label_nam": [317, 319, 350, 352, 359], "label_to_class": [137, 139, 146], "labeled_batch": [10, 206, 212], "labia": [285, 288], "labour": [267, 283], "lack": [8, 191, 192, 196], "lag": [343, 349], "lai": [267, 283], "lakehous": [10, 206, 210, 238, 239, 245, 252], "lambda": [6, 86, 87, 147, 150, 159, 163, 259, 262, 285, 290, 337, 341, 342], "land": [0, 337, 338], "landscap": [195, 307, 308, 309, 316, 360], "languag": [5, 6, 8, 13, 104, 191, 192, 194, 195, 259, 264, 330, 336], "laptop": [1, 126, 127, 169, 170, 285, 287], "lar": [285, 292], "larami": [285, 291], "larg": [0, 4, 5, 6, 8, 9, 10, 13, 15, 86, 87, 104, 105, 106, 110, 112, 117, 118, 124, 128, 132, 137, 139, 147, 151, 159, 163, 171, 174, 182, 183, 184, 190, 191, 192, 195, 198, 205, 206, 208, 245, 246, 259, 262, 263, 266, 267, 273, 277, 279, 282, 284, 285, 287, 290, 292, 293, 295, 298, 299, 300, 302, 307, 313, 317, 318, 319, 323, 330, 332, 336, 337, 338, 339, 342, 343, 344, 349, 350, 352], "large_mat_from_object_stor": [3, 181, 183], "large_matrix": [3, 181, 183], "larger": [8, 16, 86, 87, 128, 136, 147, 154, 191, 196, 245, 252, 285, 290, 317, 323, 324, 329, 330, 331, 350, 359], "largest": [118, 121], "last": [3, 5, 6, 8, 15, 53, 57, 92, 93, 101, 102, 105, 118, 123, 137, 139, 144, 181, 189, 191, 195, 196, 224, 225, 232, 236, 246, 247, 248, 259, 261, 267, 281, 283, 284, 293, 300, 330, 334, 335, 337, 340, 342, 343, 345, 347, 350, 353, 357], "last_login": 168, "lastmodifi": [86, 87], "latenc": [8, 11, 86, 87, 105, 109, 110, 116, 118, 124, 128, 130, 164, 167, 191, 195, 197, 218, 220, 324, 329, 350, 359], "latent": [6, 259, 262, 330, 331], "later": [7, 13, 14, 16, 82, 83, 101, 102, 107, 128, 130, 137, 139, 168, 224, 226, 230, 234, 236, 238, 242, 243, 253, 256, 285, 292, 317, 319, 320, 324, 327, 330, 331, 332, 334, 337, 339, 350, 353], "latest": [1, 5, 13, 28, 29, 43, 44, 53, 55, 66, 74, 82, 83, 169, 170, 224, 236, 245, 246, 247, 248, 249, 250, 267, 271, 281, 285, 291, 302, 306, 307, 313, 315, 321, 324, 325, 328, 329, 330, 331, 334, 335, 336, 343, 344, 347, 348, 349, 350, 351, 358], "latin": [330, 336], "laugh": [267, 283], "laughter": [267, 283], "launch": [2, 3, 4, 6, 7, 9, 10, 11, 17, 19, 84, 85, 92, 93, 94, 95, 96, 98, 101, 102, 103, 111, 118, 119, 128, 133, 135, 136, 137, 139, 143, 145, 146, 147, 148, 153, 165, 171, 172, 175, 176, 181, 182, 189, 198, 199, 206, 207, 218, 219, 228, 230, 237, 253, 254, 259, 260, 266, 267, 279, 285, 287, 290, 293, 295, 302, 307, 313, 318, 325, 337, 338, 340, 342, 344, 349, 351], "layer": [4, 17, 19, 110, 113, 116, 137, 139, 140, 171, 173, 224, 227, 317, 323, 330, 332, 343, 344], "layer1": 13, "layer2": 13, "layer3": 13, "layer4": 13, "layers_per_block": [6, 259, 262], "layout": [330, 332], "lazi": [9, 15, 128, 130, 198, 201, 202, 207, 267, 275, 277, 283, 284, 330, 332, 337, 339], "lbc": [43, 46, 53, 58], "lbl": [317, 319], "le": [267, 283], "lead": [3, 8, 181, 187, 189, 191, 196, 267, 283, 285, 292], "leader": [267, 283], "leagu": [267, 283], "leakag": [343, 345], "lean": [350, 351], "learn": [3, 4, 5, 6, 7, 9, 10, 13, 14, 15, 28, 34, 101, 102, 103, 110, 117, 125, 126, 127, 128, 135, 159, 160, 161, 171, 173, 181, 185, 187, 192, 195, 198, 205, 206, 217, 226, 228, 236, 253, 257, 259, 262, 266, 267, 273, 277, 278, 279, 282, 284, 285, 286, 287, 288, 292, 293, 294, 295, 296, 298, 299, 300, 302, 307, 312, 313, 323, 329, 332, 333, 334, 336, 339, 345, 347, 349, 352, 359], "learning_r": [224, 229], "least": [3, 17, 22, 24, 26, 27, 101, 102, 108, 181, 185, 187], "leav": [5, 28, 30, 43, 45, 80, 81, 84, 85, 92, 93, 285, 291, 317, 321, 337, 339, 342, 343, 349, 350, 359], "lecun": [13, 14], "left": [2, 8, 82, 83, 84, 85, 101, 102, 105, 175, 180, 191, 197, 285, 292, 330, 335, 336, 337, 342, 343, 348], "leftarrow": [317, 318, 324, 325], "leftov": [245, 251], "legend": [317, 321, 324, 328, 330, 334, 343, 347, 349, 350, 357], "leigh": [267, 283], "lemieux": [267, 283], "len": [3, 5, 6, 88, 89, 137, 140, 143, 181, 189, 224, 226, 237, 259, 262, 285, 289, 305, 306, 307, 315, 330, 332, 334, 336, 337, 339, 340, 341, 342, 343, 345, 347, 350, 352, 353], "lena": [285, 288], "length": [10, 28, 30, 43, 45, 53, 56, 101, 102, 104, 105, 106, 110, 113, 118, 124, 206, 208, 305, 306, 307, 315, 343, 345], "leo": [285, 292], "leopold": [285, 292], "lesnar": [267, 283], "less": [118, 121, 285, 288, 317, 323], "lesson": 12, "let": [2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 35, 39, 43, 45, 46, 53, 58, 82, 83, 84, 85, 86, 87, 92, 93, 94, 95, 101, 102, 106, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 125, 164, 166, 171, 172, 174, 175, 180, 181, 183, 184, 185, 187, 189, 190, 198, 201, 202, 203, 204, 206, 210, 211, 213, 215, 216, 218, 222, 224, 226, 227, 228, 232, 234, 253, 255, 256, 257, 259, 262, 263, 317, 318, 319, 320, 321, 324, 325, 330, 331, 337, 338, 339, 343, 347, 350, 352, 354, 359], "level": [0, 5, 6, 7, 8, 11, 13, 14, 16, 17, 22, 24, 27, 88, 89, 94, 95, 101, 102, 105, 107, 128, 133, 155, 157, 159, 162, 164, 167, 182, 185, 191, 195, 218, 221, 224, 227, 235, 253, 256, 259, 263, 285, 288, 330, 332, 334, 343, 344, 349, 350, 351], "leverag": [8, 10, 11, 24, 25, 126, 127, 164, 167, 191, 192, 206, 208, 217, 218, 220, 266, 267, 277, 279, 284, 285, 287, 293, 295, 299, 317, 323, 324, 329, 337, 342], "lexu": [118, 122, 267, 283], "lgbm": [5, 6, 224, 225, 259, 261], "lh": [9, 198, 203], "li": [285, 292], "liar": [267, 283], "lib": [86, 87, 137, 144, 267, 283], "libomp": [4, 171, 172], "librari": [2, 5, 7, 8, 9, 10, 11, 14, 15, 16, 80, 81, 86, 87, 88, 89, 147, 151, 159, 163, 175, 177, 191, 195, 196, 198, 200, 201, 206, 210, 212, 213, 215, 218, 221, 222, 224, 226, 253, 256, 257, 266, 278, 279, 286, 287, 294, 295, 297, 299, 302, 309, 312, 313, 316, 317, 319, 324, 326, 330, 332, 343, 345, 350, 352], "licens": [1, 169, 170], "lie": [267, 283, 324, 326], "life": [267, 283, 285, 288, 292], "lifecycl": [8, 11, 13, 24, 25, 26, 86, 87, 90, 91, 100, 191, 197, 218, 221, 225], "lift": [80, 81], "light": [0, 10, 206, 212, 267, 283], "lightli": 100, "lightn": [5, 137, 143, 261, 318, 319, 320, 323, 324, 325, 326, 328, 329], "lightning_training_loop": [6, 259, 262], "lightningmodul": [6, 259, 262, 318, 325], "lightweight": [11, 88, 89, 218, 220, 267, 273, 282, 330, 331, 332, 336, 337, 339, 343, 345, 349, 350, 351], "lik": [285, 292], "like": [3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 23, 24, 26, 27, 28, 30, 31, 35, 38, 39, 40, 41, 43, 45, 47, 53, 56, 59, 62, 66, 75, 82, 83, 84, 85, 101, 102, 107, 110, 113, 114, 118, 122, 126, 127, 128, 130, 132, 133, 134, 137, 139, 144, 147, 154, 171, 174, 181, 184, 185, 186, 187, 191, 192, 195, 196, 197, 198, 201, 203, 205, 206, 213, 224, 225, 228, 229, 233, 235, 238, 241, 253, 257, 259, 263, 267, 283, 285, 287, 288, 291, 292, 293, 299, 302, 307, 309, 313, 316, 317, 323, 324, 329, 330, 331, 332, 336, 337, 338, 340], "likeeeeeeeee": [267, 283], "likelihood": [350, 351], "limit": [7, 8, 9, 24, 27, 28, 30, 43, 45, 53, 56, 84, 85, 86, 87, 101, 102, 104, 118, 121, 191, 192, 198, 203, 213, 253, 257], "limousin": [4, 9, 12, 171, 174, 198, 201], "line": [82, 83, 92, 93, 128, 129, 137, 138, 147, 148, 155, 158, 285, 292, 317, 318, 343, 344, 350, 351], "linear": [3, 7, 13, 14, 137, 140, 181, 190, 253, 257, 324, 327, 343, 346], "linearmodel": [3, 181, 190], "lineup": [267, 281, 283, 284], "link": [0, 24, 26, 92, 93, 118, 120, 125, 224, 230], "linux": [155, 158], "list": [3, 4, 9, 10, 15, 28, 30, 32, 33, 35, 38, 41, 43, 45, 49, 50, 51, 53, 56, 57, 61, 62, 64, 66, 69, 71, 77, 78, 82, 83, 86, 87, 90, 91, 96, 98, 101, 102, 106, 118, 121, 128, 130, 137, 143, 159, 163, 171, 174, 181, 184, 198, 201, 203, 206, 210, 216, 224, 236, 237, 238, 242, 285, 291, 317, 319, 324, 326, 330, 332, 336, 337, 339, 340, 343, 349], "list_": [343, 345], "list_objects_v2": [86, 87, 118, 121], "listbucket": [17, 22], "listbucketmultipartupload": [17, 22], "listdir": [350, 359], "listfil": [10, 206, 214], "listmultipartuploadpart": [17, 22], "lit": [267, 281, 283], "lite": [317, 318, 319, 352], "liter": [267, 283], "littl": [267, 283, 285, 291, 292], "live": [267, 283, 285, 291, 307, 308, 309, 316, 330, 336, 343, 344], "ll": [1, 5, 6, 7, 14, 17, 18, 35, 37, 43, 47, 50, 53, 59, 62, 66, 68, 70, 75, 80, 81, 82, 83, 88, 89, 100, 101, 102, 103, 109, 110, 111, 112, 113, 115, 119, 121, 159, 160, 169, 170, 226, 227, 229, 230, 235, 252, 253, 256, 259, 262, 267, 283, 285, 291, 343, 344], "llama": [101, 102, 108, 113, 114, 115, 116, 118, 121, 124], "llamafactoryai": [118, 121], "llm": [10, 105, 109, 112, 114, 115, 117, 120, 122, 123, 125, 128, 132, 147, 151, 206, 208], "llm_config": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123], "llmconfig": [101, 102, 108, 110, 113, 116, 118, 121, 123], "lo": [1, 169, 170], "load": [3, 4, 8, 11, 12, 16, 17, 22, 24, 27, 28, 34, 51, 52, 64, 65, 82, 83, 84, 85, 92, 93, 101, 102, 107, 108, 110, 113, 115, 118, 121, 123, 128, 130, 131, 137, 138, 139, 140, 143, 146, 147, 151, 171, 174, 181, 187, 190, 191, 192, 197, 199, 205, 207, 208, 213, 218, 220, 222, 225, 226, 232, 234, 239, 246, 248, 250, 252, 254, 262, 266, 273, 277, 279, 282, 284, 287, 292, 293, 295, 296, 297, 298, 299, 300, 302, 306, 307, 313, 315, 318, 322, 323, 324, 329, 331, 334, 336, 338, 341, 342, 344, 347, 349, 351, 353, 355, 359], "load_data": [4, 171, 174], "load_dataset": [267, 269, 271, 280, 281, 285, 288, 293, 296, 298, 317, 319, 343, 345, 350, 352, 359], "load_dotenv": [137, 138], "load_ext": [128, 129, 137, 138, 147, 148], "load_from_checkpoint": [6, 259, 263], "load_model": [4, 12, 128, 131, 171, 174, 337, 340], "load_model_ray_train": [5, 13, 224, 228, 231, 238, 240, 245, 247], "load_model_torch": [5, 13], "load_state_dict": [5, 13, 137, 140, 224, 237, 245, 247, 317, 323, 324, 329, 330, 334, 336, 343, 347, 349, 350, 355, 359], "loadbalanc": [24, 27], "loaded_df": [6, 259, 262], "loaded_model": [5, 13], "loaded_model_ray_train": [5, 6, 13, 259, 263], "loader": [5, 238, 240, 241, 243, 245, 247, 293, 295, 317, 321, 330, 331, 343, 345, 350, 351, 353, 354, 359], "loan": [267, 283], "loc": [12, 14], "local": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 22, 28, 31, 35, 40, 43, 47, 51, 53, 59, 64, 66, 75, 92, 93, 100, 101, 102, 103, 108, 111, 117, 118, 119, 126, 127, 128, 130, 137, 144, 147, 150, 153, 156, 157, 164, 165, 166, 167, 171, 172, 174, 175, 176, 181, 182, 188, 191, 195, 198, 199, 203, 206, 207, 213, 216, 218, 219, 220, 222, 223, 224, 225, 226, 234, 237, 238, 239, 253, 254, 259, 260, 266, 267, 271, 279, 281, 283, 285, 287, 293, 295, 299, 300, 302, 307, 313, 317, 318, 319, 321, 324, 325, 328, 330, 331, 337, 338, 340, 343, 344, 350, 355, 359], "local_fil": [86, 87], "local_file_path": [118, 121], "local_idx": [350, 353], "local_path": [5, 10, 11, 13, 15, 16, 118, 121, 164, 167, 206, 213, 218, 222], "local_pred_fold": [10, 206, 216], "local_storag": [6, 13, 259, 262], "local_zip": [330, 332], "localhost": [0, 4, 11, 12, 16, 101, 102, 108, 110, 114, 118, 121, 122, 123, 126, 127, 164, 167, 171, 174, 218, 222, 307, 309, 316], "locat": [1, 5, 35, 39, 66, 70, 86, 87, 94, 95, 118, 123, 128, 132, 136, 137, 144, 146, 147, 154, 164, 167, 169, 170, 224, 226, 234, 236, 337, 338], "lock": [1, 88, 89, 169, 170, 278, 285, 286, 291, 294, 312], "lockfil": [126, 127], "lodg": [337, 338], "lofton": [267, 283], "log": [5, 6, 8, 10, 12, 13, 14, 17, 21, 22, 28, 31, 32, 35, 36, 40, 41, 43, 47, 50, 53, 59, 62, 63, 66, 75, 77, 82, 83, 90, 91, 92, 93, 100, 101, 102, 107, 110, 116, 128, 134, 147, 153, 155, 157, 159, 161, 162, 163, 165, 168, 191, 192, 195, 206, 214, 224, 225, 226, 228, 233, 234, 235, 236, 238, 240, 244, 245, 250, 252, 259, 261, 262, 293, 300, 317, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 334, 337, 338, 340, 342, 343, 346, 347, 349, 350, 351, 355, 360], "log_artifact": [137, 143], "log_engine_metr": [110, 113, 116], "log_every_n_step": [6, 259, 262], "log_level": [164, 167], "log_metr": [137, 143], "log_param": [137, 143], "log_result": [88, 89], "logdir": [12, 13, 293, 300], "logger": [164, 167], "logging_config": [164, 167], "logic": [3, 5, 6, 8, 10, 11, 12, 13, 14, 16, 17, 19, 22, 118, 124, 137, 138, 147, 149, 151, 181, 187, 191, 196, 197, 206, 212, 218, 221, 224, 227, 238, 240, 245, 247, 259, 263, 293, 300, 306, 307, 308, 309, 315, 316, 317, 318, 321, 323, 324, 325, 330, 331, 334, 343, 345, 350, 351, 355], "login": [28, 31, 35, 38, 40, 43, 47, 53, 59, 66, 69, 75, 82, 83], "logist": 12, "logit": [10, 11, 15, 16, 206, 213, 218, 222, 224, 227, 237, 293, 297, 298, 350, 355, 359], "logloss": 12, "loguniform": [7, 14, 253, 257], "loki": [267, 283], "lol": [267, 283], "london": [267, 283], "long": [3, 4, 5, 6, 7, 12, 13, 14, 17, 21, 96, 98, 101, 102, 105, 106, 118, 124, 171, 174, 181, 187, 224, 225, 245, 246, 253, 256, 259, 261, 263, 267, 283, 285, 292, 330, 332, 334, 343, 344], "longer": [5, 8, 191, 197, 224, 228, 237, 238, 240, 245, 251, 252, 317, 323], "longrightarrow": [324, 325, 343, 344], "look": [3, 4, 5, 7, 8, 9, 10, 13, 15, 16, 28, 30, 31, 35, 40, 43, 45, 47, 53, 56, 57, 59, 66, 75, 110, 117, 118, 121, 125, 137, 139, 141, 164, 166, 171, 174, 181, 190, 191, 197, 198, 204, 206, 211, 215, 224, 226, 228, 238, 241, 253, 257, 267, 281, 283, 285, 291, 292, 317, 323, 337, 339], "look_back_period_": 16, "lookup": [330, 331], "loop": [118, 123, 137, 143, 176, 225, 226, 227, 229, 230, 234, 235, 236, 239, 243, 244, 246, 248, 249, 250, 252, 261, 263, 293, 295, 298, 317, 318, 321, 323, 329, 331, 338, 342, 344, 350, 351, 355, 359], "lora": [101, 102, 105, 109, 119, 120, 125, 147, 151], "lora_checkpoint": [118, 121], "lora_config": [118, 121], "lose": [245, 246, 249], "loss": [5, 6, 7, 12, 13, 14, 137, 143, 224, 226, 227, 228, 233, 234, 236, 238, 240, 245, 247, 253, 256, 257, 259, 262, 285, 292, 293, 298, 299, 300, 318, 320, 325, 327, 329, 337, 338, 340, 342, 349, 351, 352, 355, 356], "loss_fn": [6, 137, 143, 259, 262, 317, 320, 324, 327, 343, 347], "loss_funct": 13, "loss_ms": [6, 259, 262], "lot": [5, 6, 128, 134, 224, 225, 259, 261], "louboutin": [267, 281, 283], "loung": [267, 283], "love": [126, 127, 267, 283, 285, 292, 307, 309, 316], "low": [8, 101, 102, 105, 107, 118, 121, 124, 191, 195, 197, 324, 327, 329, 343, 349, 350, 351, 359], "lower": [86, 87, 88, 89, 110, 112, 117, 118, 124, 267, 283, 305, 306, 307, 315], "lowercas": [305, 306, 307, 315], "lowest": [350, 356], "lr": [5, 6, 7, 13, 14, 137, 143, 144, 224, 228, 238, 240, 245, 247, 253, 256, 257, 259, 262, 263, 293, 298, 299, 317, 320, 324, 327, 330, 334, 343, 347, 349, 350, 355, 356], "lr_factor": [137, 143, 144], "lr_patienc": [137, 143, 144], "lr_schedul": [6, 137, 143, 259, 262], "lssf": [1, 169, 170], "lstm": [343, 349], "lstrip": [86, 87], "lt": [285, 292], "luck": [267, 283, 285, 291], "lucki": [267, 283], "lupin": [267, 281, 283, 284], "lustr": [8, 191, 192], "ly": [285, 292], "m": [0, 1, 3, 5, 13, 169, 170, 181, 189, 267, 283, 285, 291, 292, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "m1": [1, 169, 170], "m2": [1, 169, 170], "m3": [1, 169, 170], "m5": [88, 89, 128, 136, 137, 139, 159, 163], "mac": [267, 283, 293, 298], "machin": [1, 3, 4, 9, 16, 17, 20, 27, 80, 81, 82, 83, 84, 85, 90, 91, 137, 144, 155, 156, 159, 161, 169, 170, 171, 173, 181, 185, 192, 195, 198, 205, 266, 267, 273, 275, 278, 279, 282, 283, 284, 285, 286, 287, 292, 293, 294, 295, 299, 300, 302, 307, 312, 313, 337, 338], "maco": [1, 4, 155, 158, 169, 170, 171, 172], "macosx": [1, 169, 170], "maddon": [267, 283], "made": [6, 11, 218, 221, 259, 263, 285, 288, 291, 292, 330, 334], "madison": [267, 283], "magnitud": [343, 345], "mai": [3, 5, 6, 28, 30, 35, 39, 42, 43, 45, 53, 56, 63, 66, 70, 71, 73, 78, 84, 85, 86, 87, 88, 89, 94, 95, 128, 129, 137, 138, 139, 147, 148, 155, 158, 181, 187, 189, 190, 238, 239, 259, 260, 267, 283, 285, 291, 292, 302, 307, 313, 337, 341, 350, 359], "maiden": [267, 283], "main": [0, 3, 5, 6, 9, 11, 15, 28, 30, 35, 39, 43, 45, 53, 56, 90, 91, 92, 93, 101, 102, 108, 110, 113, 128, 135, 137, 143, 159, 163, 164, 167, 168, 181, 188, 198, 204, 218, 223, 224, 228, 259, 262, 267, 283, 285, 292, 295, 330, 332, 343, 345], "maintain": [3, 8, 11, 17, 19, 86, 87, 101, 102, 105, 126, 127, 181, 190, 191, 192, 197, 218, 221, 285, 292, 324, 325], "mainten": [24, 25], "major": [285, 288], "make": [0, 1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 66, 78, 82, 83, 86, 87, 92, 93, 94, 95, 100, 101, 102, 104, 118, 120, 121, 128, 130, 132, 133, 134, 147, 149, 159, 163, 169, 170, 171, 174, 181, 184, 188, 191, 197, 198, 201, 204, 205, 218, 223, 224, 226, 232, 236, 238, 239, 242, 243, 244, 245, 246, 248, 249, 253, 257, 258, 259, 262, 263, 266, 267, 279, 283, 285, 287, 288, 291, 292, 293, 295, 299, 302, 305, 306, 307, 313, 315, 317, 318, 319, 324, 326, 330, 336, 337, 338, 339, 343, 344, 345, 348, 349, 350, 351, 353], "make_pendulum_dataset": [324, 326], "makedir": [137, 142, 317, 319, 321, 324, 328, 330, 332, 337, 339, 343, 345, 350, 352], "malaga": [267, 283], "male": [285, 288, 292], "mall": [267, 283], "malloc": [155, 158], "mamba": [278, 286, 294, 312], "man": [267, 283, 285, 288, 291, 292], "manag": [2, 5, 6, 8, 12, 17, 19, 20, 21, 22, 25, 27, 35, 37, 39, 43, 47, 51, 53, 59, 64, 66, 68, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 107, 109, 110, 115, 126, 127, 128, 135, 147, 153, 155, 158, 159, 162, 164, 167, 175, 177, 182, 191, 192, 224, 225, 238, 240, 259, 261, 267, 283, 293, 299, 302, 307, 313, 317, 318, 321, 323, 324, 325, 328, 331, 336, 337, 338, 343, 344, 350, 351, 354, 355, 359], "mani": [2, 4, 9, 10, 11, 28, 30, 43, 45, 53, 56, 88, 89, 92, 93, 101, 102, 105, 110, 117, 118, 122, 124, 128, 134, 135, 171, 174, 175, 177, 182, 186, 187, 198, 201, 206, 212, 218, 220, 224, 227, 229, 230, 232, 267, 273, 275, 281, 282, 283, 285, 288, 292, 307, 311, 316, 330, 331, 332, 337, 340, 343, 349, 350, 351], "manipul": [285, 291], "mann": [285, 291, 292], "manner": [4, 9, 10, 15, 171, 174, 198, 199, 201, 203, 206, 207, 285, 287, 293, 299, 350, 351], "mansbridg": [267, 283], "manual": [5, 35, 42, 66, 78, 137, 144, 224, 225, 226, 228, 230, 231, 234, 246, 248, 252, 267, 283, 293, 298, 317, 318, 323, 324, 325, 330, 331, 335, 337, 338, 340, 342, 343, 344, 348, 350, 351, 354, 355, 357], "manual_se": [343, 347], "map": [6, 12, 16, 118, 121, 123, 128, 130, 131, 137, 139, 146, 159, 163, 238, 243, 259, 262, 293, 298, 324, 325, 330, 332, 336, 337, 338, 339, 343, 349, 350, 351, 352, 353, 355, 359], "map_batch": [4, 9, 10, 12, 15, 16, 128, 131, 137, 139, 146, 164, 166, 171, 174, 198, 202, 206, 212, 213, 215, 266, 267, 273, 275, 277, 279, 282, 283, 284, 317, 319, 324, 326, 330, 332, 337, 341, 342, 343, 349, 350, 359], "map_group": [9, 10, 15, 198, 204, 205, 206, 215], "map_loc": [5, 6, 13, 137, 140, 224, 237, 259, 263, 317, 323, 324, 329, 330, 334, 336, 343, 347, 349, 350, 355, 359], "mapbatch": [10, 206, 214, 267, 284], "mapreduc": [8, 191, 195], "mar": [307, 308, 309, 316], "marathon": [267, 283], "marc": [285, 292], "march": [267, 283], "mario": [267, 283], "mark": [155, 158, 285, 292], "markdown": 0, "marker": [317, 321, 324, 328, 330, 334, 343, 347, 349, 350, 357], "market_typ": [159, 163], "marlei": [267, 283], "marri": [285, 288], "martial": [285, 291], "martin": [267, 283], "mask": [293, 298], "mass": [267, 283], "massiv": [10, 206, 208], "master": [1, 14, 118, 120, 169, 170, 285, 291, 293, 300, 343, 345], "mat1_ref": [3, 181, 183], "mat2_ref": [3, 181, 183], "match": [3, 10, 43, 51, 53, 64, 88, 89, 92, 93, 110, 113, 118, 121, 122, 124, 128, 136, 181, 187, 206, 213, 245, 250, 267, 283, 337, 339], "matching_analysi": [118, 121], "materi": [4, 9, 11, 12, 16, 128, 130, 137, 139, 159, 163, 164, 166, 171, 174, 198, 201, 202, 204, 207, 208, 210, 211, 212, 213, 215, 218, 222, 266, 267, 279, 284, 330, 332, 337, 339, 340, 342, 350, 359], "materialized_d": [267, 277, 284], "materializeddataset": [267, 284, 285, 289], "math": [2, 175, 180, 324, 326, 343, 345, 346], "mathbb": [317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351], "mathcal": [317, 318, 324, 325, 330, 331, 350, 351], "mathemat": [118, 124], "matmul": [3, 181, 183, 330, 336], "matplotlib": [5, 7, 10, 13, 14, 16, 128, 129, 137, 138, 147, 148, 206, 207, 224, 226, 253, 254, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "matric": [350, 359], "matrix": [137, 146, 334, 336], "matrixfactorizationmodel": [330, 333, 334, 336], "matt": [267, 283], "matter": [101, 102, 106, 267, 283, 285, 288, 350, 353], "matur": [8, 92, 93, 191, 196], "max": [9, 10, 15, 16, 86, 87, 110, 113, 198, 204, 206, 212, 215, 285, 292, 317, 321, 324, 328, 330, 334, 343, 345, 347, 350, 355], "max_": 15, "max_depth": [4, 12, 171, 174, 337, 340, 342], "max_epoch": [6, 259, 262, 263, 317, 321, 323, 324, 328], "max_failur": [245, 248, 252, 317, 321, 324, 328, 330, 334, 337, 340, 343, 347, 350, 351, 356], "max_len": [343, 346], "max_length": [293, 298], "max_lora": [118, 121], "max_lora_rank": [118, 121], "max_model_len": [101, 102, 108, 113, 118, 121, 122, 123], "max_nod": [43, 50, 53, 62, 159, 163], "max_num_adapters_per_replica": [118, 121], "max_ongoing_request": [8, 191, 197], "max_replica": [16, 101, 102, 108, 110, 113, 116, 118, 123], "max_retri": [3, 181, 185], "max_siz": [10, 206, 213], "max_step": [6, 259, 263], "max_t": [317, 320, 324, 327], "maxim": [8, 9, 101, 102, 104, 106, 107, 191, 192, 197, 198, 204, 266, 267, 279], "maximum": [16, 43, 50, 53, 62, 80, 81, 84, 85, 101, 102, 104, 105, 110, 112, 116, 117, 118, 121, 124, 245, 250], "maxpool": 13, "maxpool2d": 13, "maxpumperla": [0, 293, 300], "mayb": [285, 291], "mb": [343, 345], "mcintir": [285, 291], "mcm": [137, 146], "md": [0, 164, 167, 278, 286, 294, 312], "mdmad": [267, 283], "me": [17, 22, 110, 114, 115, 267, 283, 285, 288, 291, 292], "mean": [1, 3, 7, 9, 10, 14, 15, 66, 69, 101, 102, 105, 128, 130, 169, 170, 181, 183, 187, 198, 201, 202, 204, 206, 211, 215, 224, 231, 237, 253, 257, 317, 318, 324, 325, 330, 331, 336, 343, 345, 349, 350, 353, 354, 359], "meant": [8, 191, 192, 194, 330, 336], "meantim": [285, 291], "measur": [330, 336], "meat": [285, 288], "mechan": [8, 10, 155, 157, 191, 192, 206, 208, 350, 351, 355], "medium": [101, 102, 106, 109, 114, 117, 118, 124], "meet": [1, 10, 11, 16, 101, 102, 105, 126, 127, 169, 170, 206, 208, 218, 220, 267, 283], "megatron": [137, 144], "melodrama": [285, 292], "member": [94, 95], "memori": [2, 3, 5, 6, 9, 10, 12, 14, 15, 88, 89, 104, 105, 107, 109, 110, 112, 114, 116, 118, 120, 121, 124, 128, 130, 131, 134, 137, 139, 155, 157, 159, 161, 163, 175, 177, 181, 183, 187, 189, 195, 196, 198, 200, 201, 206, 208, 210, 211, 212, 214, 215, 224, 226, 232, 237, 238, 243, 259, 260, 262, 266, 275, 279, 283, 285, 287, 292, 317, 319, 323, 330, 332, 339, 343, 349, 350, 352, 353, 359], "memory_usag": [9, 198, 201], "memorydb": 21, "men": [285, 288, 291], "mental": [285, 288], "mention": [285, 292, 343, 349], "menu": [128, 136, 137, 146, 147, 154], "merg": [9, 198, 205, 330, 336], "merlin": [267, 283], "messag": [1, 3, 8, 13, 14, 16, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 128, 133, 137, 139, 147, 150, 159, 161, 168, 169, 170, 181, 190, 191, 195, 317, 321, 324, 328], "messages_cv": [118, 121], "messages_nemoguard": [118, 121], "messages_yara": [118, 121], "messi": [267, 281, 283], "meta": [101, 102, 108, 110, 112, 113, 116, 118, 121, 330, 334, 343, 347, 350, 355], "meta_path": [350, 355], "metadata": [8, 9, 88, 89, 128, 132, 159, 163, 191, 197, 198, 205, 245, 248, 267, 271, 281, 284, 285, 287, 289, 291, 324, 328, 330, 334, 336, 337, 342, 350, 353], "metadataprint": 270, "method": [3, 5, 6, 7, 9, 10, 11, 14, 15, 16, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 128, 131, 155, 158, 181, 183, 190, 198, 202, 206, 211, 213, 214, 215, 218, 222, 224, 225, 253, 257, 259, 261, 262, 266, 267, 273, 279, 282, 285, 292, 293, 298, 317, 323, 330, 331, 336], "method_nam": [3, 181, 190], "metric": [4, 6, 7, 12, 14, 16, 17, 22, 82, 83, 92, 93, 110, 113, 116, 118, 125, 137, 143, 144, 146, 147, 150, 152, 155, 157, 158, 159, 161, 162, 163, 165, 171, 174, 225, 226, 227, 228, 235, 238, 240, 244, 245, 247, 248, 250, 252, 253, 256, 257, 259, 261, 263, 295, 299, 300, 317, 321, 322, 323, 324, 325, 328, 329, 331, 332, 336, 337, 339, 340, 342, 343, 344, 347, 348, 349, 350, 351, 355, 356, 357, 358, 359, 360], "metrics_d": [137, 146], "metrics_datafram": [5, 6, 13, 224, 236, 259, 263, 317, 321, 324, 328, 330, 334, 343, 347, 350, 357], "metrics_interval_": 16, "mf_ray_train": [330, 334], "miami": [267, 283], "mic": [267, 281, 283], "michael": [267, 283, 285, 292], "micro": [350, 355], "microservic": [96, 98, 101, 102, 106, 147, 151, 168], "mid": [245, 250, 337, 338, 350, 358], "mid_block_scale_factor": [6, 259, 262], "middl": [267, 283], "midwai": [267, 283], "might": [4, 5, 6, 7, 10, 12, 24, 26, 28, 30, 43, 45, 53, 56, 66, 69, 101, 102, 105, 171, 172, 174, 206, 212, 224, 225, 253, 257, 259, 261, 285, 288, 317, 323, 337, 340, 342, 350, 359], "migrat": [11, 16, 218, 222, 224, 227, 262, 324, 325], "milan": [267, 283], "mile": [4, 8, 9, 12, 15, 137, 139, 171, 174, 191, 195, 196, 198, 201], "million": [4, 12, 171, 174, 267, 281, 283, 285, 292], "min": [4, 7, 9, 10, 12, 14, 15, 16, 110, 113, 137, 143, 171, 174, 198, 204, 206, 211, 212, 215, 253, 257, 337, 340, 347, 349, 350, 356], "min_": 15, "min_nod": [43, 50, 53, 62], "min_replica": [16, 101, 102, 108, 110, 113, 116, 118, 123], "min_siz": [10, 206, 213], "mind": [267, 283, 285, 288, 292], "mine": [267, 283], "minecraft": [267, 281, 283, 284], "miner": [285, 291], "mini": [5, 6, 224, 225, 259, 263, 317, 318, 324, 325], "miniconda": [1, 169, 170], "miniforge3": [1, 169, 170], "minilm": [267, 273, 282], "minim": [5, 6, 7, 8, 12, 14, 17, 20, 22, 24, 26, 128, 130, 132, 137, 143, 191, 192, 195, 224, 225, 253, 257, 259, 261, 262, 293, 299, 317, 320, 324, 325, 329, 330, 331, 337, 338, 350, 351, 359], "minimalist": 0, "minimum": [16, 17, 22, 43, 50, 53, 62, 80, 81, 84, 85], "minu": 12, "minut": [5, 9, 35, 39, 43, 45, 53, 56, 66, 70, 73, 80, 81, 84, 85, 198, 201, 267, 283, 343, 344, 345], "mirror": [343, 344, 350, 351], "mise": [285, 292], "miseenscen": [285, 292], "misfortun": [285, 291], "mismatch": [343, 347], "miss": [267, 283, 285, 291, 292, 317, 323, 324, 328], "missouri": [267, 283], "mistak": [285, 291], "mistral": [118, 124], "mitch": [267, 283], "mix": [6, 259, 262, 263, 317, 323, 324, 329, 350, 359], "mkdir": [5, 13, 137, 140], "ml": [1, 4, 8, 9, 11, 12, 15, 16, 84, 85, 86, 87, 90, 91, 94, 95, 100, 126, 127, 128, 130, 147, 149, 154, 169, 170, 171, 173, 174, 191, 194, 196, 197, 198, 205, 218, 219, 220, 221, 222, 266, 267, 277, 279, 284, 302, 304, 306, 307, 313, 314, 315, 330, 332, 336, 350, 351], "mlbcentral": [267, 283], "mlflow": [137, 142, 143, 144, 147, 148, 150, 245, 252, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "mlogloss": [337, 340], "mlop": [4, 12, 118, 121, 126, 127, 137, 138, 171, 173, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "mlp": [324, 327, 329, 330, 336], "mm": [3, 181, 187], "mmlu": [118, 124], "mnist": [5, 7, 10, 11, 14, 15, 16, 164, 167, 206, 210, 212, 213, 215, 218, 222, 225, 228, 231, 232, 234, 236, 237, 238, 242, 245, 251, 252, 253, 254, 255, 256, 257], "mnist_app": [11, 16, 164, 167, 218, 222, 223], "mnist_app_handl": [11, 218, 222], "mnist_classifi": [11, 16, 218, 222], "mnist_classifier_arg": [10, 206, 213], "mnist_deploy": 16, "mnist_deployment_handl": 16, "mnist_pr": [10, 15, 206, 216, 217], "mnist_preprocessor": 16, "mnistclassifi": [10, 11, 15, 16, 206, 213, 214, 218, 222], "mnt": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 86, 87, 128, 133, 137, 139, 142, 144, 147, 150, 164, 166, 171, 174, 198, 203, 206, 213, 218, 222, 224, 226, 227, 232, 234, 236, 238, 242, 245, 251, 259, 262, 263, 264, 317, 319, 321, 323, 324, 328, 329, 330, 332, 334, 336, 337, 339, 340, 342, 343, 344, 345, 350, 351, 352, 353, 355, 356, 359], "mock": [285, 291], "modal": [9, 10, 15, 198, 205, 206, 217], "modano": [267, 283], "mode": [0, 4, 7, 11, 12, 14, 118, 122, 128, 130, 137, 143, 171, 174, 207, 218, 223, 224, 228, 237, 253, 257, 293, 298], "model": [3, 8, 9, 10, 11, 12, 15, 16, 84, 85, 86, 87, 90, 91, 104, 106, 107, 108, 111, 114, 115, 117, 119, 120, 121, 122, 123, 125, 126, 127, 128, 131, 135, 138, 139, 143, 144, 145, 146, 147, 148, 149, 150, 151, 181, 187, 190, 191, 192, 193, 196, 197, 198, 202, 205, 206, 208, 213, 215, 217, 218, 220, 221, 222, 223, 225, 226, 228, 234, 235, 236, 237, 238, 240, 245, 246, 247, 248, 250, 252, 254, 256, 260, 264, 266, 273, 277, 278, 279, 282, 284, 285, 286, 292, 293, 294, 295, 296, 297, 298, 299, 300, 311, 312, 314, 316, 317, 318, 320, 321, 323, 324, 325, 326, 328, 329, 332, 334, 335, 336, 338, 339, 341, 342, 344, 345, 347, 349, 352, 355, 359], "model1": [4, 171, 174], "model1_predict": [4, 171, 174], "model2": [4, 171, 174], "model2_predict": [4, 171, 174], "model_config": [6, 259, 262], "model_dump": [4, 118, 123, 171, 174], "model_id": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123, 128, 131, 136, 137, 139, 147, 149, 150], "model_json_schema": [118, 122], "model_kwarg": [343, 349], "model_loading_config": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123], "model_nam": [6, 84, 85, 259, 262, 263, 267, 272, 273, 282], "model_path": [4, 5, 11, 13, 171, 174, 218, 222, 224, 237, 337, 340], "model_predict": [4, 171, 174], "model_ref": [128, 131], "model_registri": [137, 142, 143, 144, 147, 150], "model_select": [4, 171, 172, 337, 339], "model_sourc": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123], "model_state_dict": [245, 247], "modelcheckpoint": [317, 321, 323, 324, 328], "modelclass": 272, "modelwork": [224, 237], "moder": [101, 102, 105, 118, 121, 124], "modern": [8, 128, 130, 191, 192, 195, 266, 267, 277, 279, 284, 285, 287, 293, 295, 300], "modif": [92, 93], "modifi": [3, 4, 5, 9, 10, 11, 28, 30, 35, 39, 43, 45, 53, 56, 66, 71, 86, 87, 88, 89, 92, 93, 164, 166, 167, 171, 174, 181, 189, 198, 203, 206, 213, 218, 222, 246, 317, 318, 350, 351], "modul": [5, 13, 17, 21, 22, 35, 39, 53, 57, 101, 102, 103, 108, 109, 110, 115, 117, 118, 120, 125, 137, 140, 143, 164, 167, 224, 231, 234, 238, 239, 246, 247, 248, 330, 333, 336, 343, 346, 349, 350, 351, 359], "modular": [17, 21, 22, 330, 331], "mofo": [267, 283], "mom": [267, 283], "momentum": [13, 137, 140, 293, 298], "mondai": [267, 283], "monei": [285, 288, 291], "mongodb": [8, 191, 192], "monitor": [5, 6, 8, 13, 24, 25, 82, 83, 90, 91, 92, 93, 100, 101, 111, 113, 115, 117, 118, 124, 125, 137, 144, 147, 153, 155, 157, 158, 159, 161, 162, 163, 164, 165, 166, 191, 195, 224, 225, 233, 234, 259, 261, 337, 342, 343, 349], "monro": [267, 283], "month": [4, 9, 118, 123, 171, 174, 198, 201, 267, 283], "moon": [267, 283], "more": [2, 3, 7, 8, 9, 10, 11, 13, 14, 16, 17, 22, 26, 28, 30, 43, 44, 45, 53, 54, 56, 66, 67, 88, 89, 96, 98, 101, 102, 105, 107, 108, 112, 117, 120, 126, 127, 128, 132, 135, 137, 139, 144, 147, 151, 155, 158, 159, 161, 163, 164, 166, 175, 180, 181, 184, 185, 187, 188, 189, 191, 194, 196, 197, 198, 204, 205, 206, 208, 212, 215, 217, 218, 220, 221, 223, 224, 225, 226, 230, 234, 236, 245, 252, 253, 256, 257, 261, 264, 278, 285, 286, 291, 292, 293, 294, 298, 300, 302, 306, 307, 311, 312, 313, 315, 316, 317, 323, 330, 336, 337, 341, 342, 343, 347, 350, 359], "morn": [267, 283], "moron": [285, 291], "morri": [285, 291], "morti": [267, 283], "mosh": [267, 283], "most": [3, 8, 9, 10, 15, 24, 26, 80, 81, 84, 85, 86, 87, 96, 98, 118, 121, 122, 181, 184, 190, 191, 194, 198, 202, 206, 209, 211, 224, 236, 245, 250, 285, 291, 292, 307, 309, 316, 317, 321, 324, 328, 330, 332, 335, 337, 341, 350, 352, 355, 357], "most_rec": [86, 87], "mostli": [101, 102, 106], "motion": [324, 325], "motiv": [285, 288], "mount": [17, 22, 86, 87], "mountaincar": [324, 329], "mouth": [285, 291], "move": [5, 6, 7, 8, 13, 14, 100, 101, 102, 109, 137, 139, 191, 192, 224, 227, 228, 231, 232, 234, 235, 237, 238, 240, 241, 253, 256, 257, 259, 262, 266, 267, 279, 285, 292, 293, 298, 317, 323, 324, 325, 329, 337, 339, 343, 345, 350, 351], "movement": [6, 259, 262], "movi": [267, 283, 285, 288, 291, 292, 331, 332], "movielen": [331, 336], "mp": [267, 272, 273, 277, 282, 283, 284, 293, 295, 298, 299], "mse": [317, 318, 321, 324, 328, 330, 331, 334], "mse_loss": [6, 259, 262, 330, 334], "mseloss": [317, 320, 324, 327], "mta": [267, 283], "mtv": [285, 292], "mtvstar": [267, 283], "mtvstarsof2015": [267, 283], "much": [8, 9, 101, 102, 106, 118, 121, 128, 130, 137, 143, 159, 163, 191, 196, 198, 201, 203, 267, 283, 285, 288, 291, 292, 317, 323, 330, 331, 337, 339], "muck": [285, 292], "muddi": [285, 291], "multi": [9, 10, 11, 15, 16, 17, 19, 84, 85, 101, 102, 105, 106, 107, 110, 112, 116, 117, 118, 121, 124, 125, 137, 144, 147, 151, 198, 203, 205, 206, 217, 218, 220, 224, 225, 226, 235, 238, 239, 244, 245, 252, 317, 318, 321, 323, 324, 325, 330, 331, 332, 336, 337, 338, 340, 350, 351, 359], "multi_actor_tracing_ray_serve_exampl": 168, "multiclass": 13, "multiclassaccuraci": [350, 355], "multilabel_confusion_matrix": [137, 146], "multimod": [126, 127, 317, 318], "multipl": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 22, 24, 26, 79, 84, 85, 96, 98, 101, 102, 105, 106, 107, 110, 111, 112, 113, 116, 117, 118, 120, 121, 124, 125, 137, 144, 147, 149, 153, 159, 161, 164, 167, 168, 171, 173, 175, 180, 181, 183, 187, 188, 191, 192, 195, 198, 203, 205, 206, 212, 213, 218, 220, 221, 224, 225, 232, 234, 238, 239, 242, 253, 257, 259, 261, 263, 266, 267, 273, 279, 282, 285, 287, 290, 293, 295, 298, 299, 300, 302, 307, 313, 317, 318, 319, 323, 324, 325, 329, 330, 332, 334, 336, 337, 338, 342, 343, 347, 349, 350, 352, 357, 359, 360], "multiplex": [10, 11, 16, 206, 208, 218, 220], "multipli": [3, 181, 190, 343, 349], "multiprocess": [3, 181, 187, 350, 353], "multithread": [3, 10, 181, 187, 206, 212], "multivari": [343, 349], "mum": [267, 281, 283], "muslim": [267, 283], "must": [118, 121, 245, 250, 317, 318, 324, 325], "mutabl": [137, 139], "mutat": [3, 181, 190, 267, 284], "mutual": [101, 102, 106], "my": [0, 3, 17, 22, 35, 38, 39, 82, 83, 101, 102, 108, 110, 113, 114, 115, 116, 118, 121, 122, 123, 168, 181, 186, 267, 281, 283, 285, 288, 292], "my_custom_env": [3, 181, 186], "my_simple_model": [7, 14, 253, 257], "my_xgboost_func": [4, 171, 174], "myself": [267, 283, 285, 288, 292], "mysentimentmodel": [306, 307, 309, 315], "mysql": [8, 191, 192], "n": [1, 3, 5, 24, 26, 53, 63, 86, 87, 110, 115, 118, 121, 123, 128, 136, 169, 170, 181, 187, 189, 224, 228, 267, 277, 278, 281, 283, 284, 285, 286, 292, 294, 312, 317, 318, 320, 321, 324, 325, 329, 331, 337, 341, 342, 350, 359], "n_step": [324, 326, 329], "nab": [343, 345], "naiv": [10, 11, 206, 208, 218, 220, 317, 323], "naiveti": [285, 291, 292], "nake": [285, 291], "nam": [53, 57], "name": [1, 5, 6, 7, 8, 11, 12, 13, 14, 16, 17, 21, 22, 23, 28, 30, 31, 32, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 53, 56, 57, 58, 59, 62, 66, 69, 70, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 101, 102, 108, 110, 115, 118, 121, 122, 123, 137, 144, 147, 153, 159, 163, 164, 166, 167, 168, 169, 170, 191, 197, 218, 222, 223, 224, 228, 234, 235, 237, 238, 244, 245, 248, 250, 253, 257, 259, 263, 285, 288, 291, 317, 319, 321, 323, 324, 328, 329, 330, 332, 334, 336, 337, 340, 343, 347, 350, 352, 356, 359], "namespac": [43, 45, 46, 48, 49, 50, 51, 53, 56, 58, 60, 61, 62, 64, 66, 73, 76, 78, 224, 234], "nandito": [267, 283], "narrat": [285, 292], "naruto": [267, 283], "nash": [267, 283], "nashnewvideo": [267, 283], "nat": [17, 22, 24, 26, 28, 30, 43, 45, 53, 56], "natgatewai": [28, 30, 43, 45, 53, 56], "nation": [267, 283, 285, 291, 307, 308, 309, 316], "nativ": [2, 4, 8, 24, 25, 27, 100, 101, 102, 106, 107, 159, 162, 163, 171, 173, 175, 179, 191, 192, 194, 195, 196, 285, 291, 292, 317, 321, 323, 324, 325, 328, 329, 343, 344, 350, 351], "nativesbr": [285, 292], "natur": [118, 123, 307, 308, 309, 316], "navig": [82, 83, 90, 91, 92, 93, 94, 95, 155, 158, 164, 166, 168], "nbsp": [80, 81, 86, 87, 126, 127, 128, 129, 137, 138, 147, 148], "nc": [285, 288], "nccl": [293, 299, 350, 351], "ndarrai": [7, 10, 11, 14, 15, 16, 206, 212, 213, 215, 218, 222, 253, 257, 267, 272, 273, 282, 284, 343, 349], "ndcg": [330, 336], "ndim": [224, 237], "nearli": [126, 127], "necessari": [5, 9, 13, 16, 28, 29, 30, 34, 35, 38, 39, 43, 44, 45, 52, 53, 54, 56, 63, 65, 66, 67, 69, 198, 202, 224, 232, 267, 271, 281, 293, 295, 296, 305, 306, 307, 315, 337, 339, 350, 351], "need": [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 27, 28, 29, 30, 31, 35, 36, 37, 39, 40, 42, 43, 44, 45, 47, 53, 55, 56, 57, 59, 63, 66, 68, 70, 71, 75, 78, 79, 80, 81, 82, 83, 84, 85, 92, 93, 101, 102, 106, 110, 111, 112, 113, 115, 117, 118, 119, 121, 122, 123, 124, 128, 129, 130, 137, 138, 139, 143, 147, 148, 155, 158, 159, 163, 169, 170, 175, 180, 181, 183, 184, 186, 191, 192, 195, 198, 201, 203, 205, 206, 208, 213, 216, 218, 220, 222, 224, 225, 226, 227, 230, 232, 234, 238, 239, 240, 243, 245, 251, 252, 253, 257, 259, 261, 263, 267, 275, 283, 285, 291, 292, 293, 299, 302, 307, 313, 317, 319, 323, 324, 325, 330, 332, 334, 336, 337, 339, 340, 341, 350, 351, 352, 353, 354, 355, 357], "neg": [8, 137, 146, 191, 196, 285, 288, 307, 308, 309, 316], "nemoguard": [118, 121], "nephew": [267, 283], "nest": [182, 184], "net": [6, 137, 140, 259, 262, 317, 320, 324, 327, 350, 355], "netflix": [9, 10, 15, 198, 205, 206, 217], "network": [5, 6, 7, 9, 10, 14, 15, 17, 20, 22, 24, 26, 27, 53, 63, 66, 78, 79, 86, 87, 101, 102, 106, 147, 151, 155, 157, 159, 161, 163, 198, 204, 206, 208, 215, 224, 225, 253, 256, 259, 261, 317, 318, 320, 350, 351], "networkinterfaceid": [28, 30, 43, 45, 53, 56], "neural": [7, 14, 137, 140, 253, 256, 330, 336, 343, 346, 350, 351], "never": [0, 147, 151, 267, 283, 285, 291, 292], "new": [3, 4, 8, 9, 13, 16, 24, 27, 28, 30, 46, 50, 53, 58, 62, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 101, 102, 105, 110, 115, 118, 123, 128, 136, 137, 143, 171, 174, 181, 189, 190, 191, 193, 198, 201, 245, 250, 267, 278, 283, 285, 286, 288, 291, 292, 294, 312, 321, 324, 329, 337, 338, 342, 343, 344, 350, 358, 360], "newaxi": [337, 341], "newli": [66, 72], "newsha": [267, 283], "next": [3, 7, 9, 13, 14, 16, 17, 20, 35, 37, 43, 47, 53, 59, 66, 68, 75, 80, 81, 84, 85, 90, 91, 92, 93, 104, 119, 121, 124, 128, 132, 157, 164, 166, 181, 189, 198, 203, 224, 226, 231, 253, 257, 267, 283, 319, 332, 340, 344, 345, 352, 355], "nf": [86, 87], "nfl": [267, 281, 283], "nginx": [24, 27, 51, 52, 64, 65, 78], "nhead": [343, 346, 347, 349], "nhl": [267, 281, 283, 284], "nia": [267, 283], "niall": [267, 283], "nice": [245, 252], "nicer": [8, 191, 196], "nick": [267, 283], "nicki": [267, 283], "nigga": [267, 283], "night": [267, 281, 283, 284, 285, 291, 292], "nightli": [324, 329, 343, 349], "nightmar": [285, 291, 292], "nightmarish": [285, 292], "nine": [317, 319, 350, 352], "nirvana": [267, 283], "nlb": [24, 27], "nlp": [245, 252], "nn": [5, 6, 7, 13, 14, 137, 140, 143, 224, 226, 227, 231, 234, 245, 248, 253, 254, 256, 257, 259, 260, 293, 296, 317, 319, 320, 324, 326, 327, 330, 332, 333, 343, 345, 346, 347, 350, 352, 355], "no_grad": [5, 10, 11, 13, 15, 16, 206, 213, 218, 222, 317, 323, 324, 329, 330, 334, 336, 343, 347, 350, 355], "no_restart": [224, 237], "node": [1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 14, 16, 17, 20, 21, 22, 24, 26, 27, 28, 30, 35, 36, 39, 43, 45, 46, 50, 56, 58, 62, 63, 66, 70, 72, 80, 81, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 105, 106, 110, 112, 113, 115, 116, 117, 118, 124, 126, 127, 128, 129, 132, 133, 136, 137, 138, 139, 143, 144, 146, 147, 148, 151, 153, 154, 157, 159, 161, 162, 163, 169, 170, 175, 177, 181, 183, 185, 187, 198, 201, 203, 206, 212, 218, 220, 224, 225, 226, 228, 234, 235, 238, 239, 243, 245, 248, 249, 252, 253, 256, 259, 261, 267, 275, 283, 285, 287, 289, 317, 318, 323, 324, 325, 330, 331, 336, 337, 338, 339, 340, 345, 348, 349, 350, 351, 355, 359], "node_ip": 13, "nodegroup": [53, 57], "noderol": [53, 57], "nofril": [285, 292], "noir": [285, 291, 292], "noirlik": [285, 292], "nois": [6, 259, 262, 320, 323, 324, 325, 326, 327, 329, 343, 345], "noise_schedul": [6, 259, 262], "noised_lat": [6, 259, 262], "noiser": [317, 323], "noisi": [317, 320, 324, 325, 327], "noisy_act": [324, 326, 327], "noisy_img": [317, 320], "non": [5, 11, 101, 102, 106, 107, 108, 110, 114, 116, 118, 121, 122, 123, 128, 130, 164, 167, 218, 223, 224, 225, 226, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352, 355], "non_block": [224, 237], "none": [5, 6, 7, 10, 13, 14, 128, 129, 137, 138, 139, 147, 148, 206, 213, 224, 233, 234, 237, 245, 248, 253, 257, 259, 262, 293, 300, 317, 321, 323, 324, 328, 329, 330, 334, 336, 337, 340, 343, 346, 347, 350, 353, 359], "norm": [224, 237, 343, 345, 349], "norm_ep": [6, 259, 262], "norm_num_group": [6, 259, 262], "normal": [5, 7, 10, 13, 14, 15, 16, 206, 207, 212, 214, 224, 226, 232, 237, 238, 239, 243, 253, 254, 255, 257, 317, 318, 323, 325, 327, 329, 330, 331, 336, 337, 339, 341, 344, 349, 350, 351, 353, 359], "normalci": [285, 292], "normalis": [343, 345, 350, 353], "normalize_cpu": [224, 237], "normalized_batch": [10, 15, 16, 206, 212], "normalized_img": 5, "north": [267, 283, 285, 291], "not_ready_ref": [3, 181, 189], "note": [1, 7, 9, 11, 13, 14, 15, 16, 17, 20, 24, 27, 28, 30, 35, 39, 43, 45, 47, 53, 56, 59, 66, 70, 75, 101, 102, 107, 126, 127, 128, 129, 130, 133, 135, 136, 137, 138, 145, 146, 147, 148, 153, 154, 155, 157, 158, 164, 167, 169, 170, 176, 182, 184, 185, 198, 202, 203, 204, 211, 212, 215, 218, 222, 223, 226, 238, 239, 241, 243, 253, 257, 262, 267, 283, 284, 285, 290, 291, 292, 307, 309, 316, 330, 336, 337, 339, 350, 351, 355], "notebook": [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 28, 29, 35, 36, 43, 44, 53, 54, 80, 81, 84, 85, 86, 87, 96, 97, 100, 101, 102, 103, 110, 111, 118, 119, 128, 129, 130, 136, 137, 138, 139, 146, 147, 148, 154, 155, 156, 159, 160, 164, 165, 171, 172, 175, 176, 181, 182, 184, 198, 199, 206, 207, 218, 219, 224, 225, 226, 253, 254, 259, 260, 266, 267, 278, 279, 284, 286, 292, 293, 294, 295, 300, 302, 307, 309, 311, 312, 313, 316, 317, 318, 324, 325, 329, 330, 331, 332, 337, 342, 343, 344, 350, 351, 352, 359, 360], "noth": [137, 140, 224, 237, 267, 283, 285, 292], "notic": [5, 9, 137, 140, 143, 198, 203, 224, 228, 267, 283, 285, 292], "notif": [164, 167, 168], "notificationservic": 168, "nov": [13, 267, 283], "novelti": [330, 336], "now": [1, 3, 4, 5, 6, 10, 11, 13, 15, 16, 17, 22, 28, 30, 34, 35, 39, 43, 45, 51, 52, 53, 63, 64, 65, 66, 70, 82, 83, 88, 89, 90, 91, 94, 95, 101, 102, 108, 110, 114, 115, 116, 117, 118, 120, 121, 122, 123, 125, 128, 129, 131, 137, 138, 147, 148, 164, 166, 167, 169, 170, 171, 174, 181, 184, 206, 213, 218, 222, 224, 227, 232, 234, 235, 237, 238, 240, 241, 242, 243, 244, 245, 247, 248, 259, 263, 267, 283, 285, 291, 292, 317, 318, 319, 321, 323, 324, 329, 330, 332, 334, 336, 337, 339, 342, 350, 359], "nowher": [285, 288], "np": [2, 3, 5, 6, 7, 10, 11, 14, 15, 16, 128, 131, 136, 137, 139, 143, 147, 148, 149, 159, 163, 164, 167, 175, 176, 181, 182, 183, 187, 189, 206, 207, 212, 213, 215, 218, 219, 222, 224, 226, 237, 238, 243, 253, 254, 257, 259, 260, 262, 267, 269, 272, 273, 280, 282, 283, 293, 296, 297, 317, 319, 324, 326, 329, 330, 332, 337, 339, 340, 341, 343, 345, 349, 350, 352, 359], "nthread": [337, 340], "ntop": [330, 336], "nude": [285, 288], "nuditi": [285, 288], "nuge": [267, 283], "nugent": [267, 283], "null": [28, 30, 43, 45, 53, 56], "num": [13, 155, 158], "num_actor": [350, 359], "num_block": [267, 284, 285, 289], "num_boost_round": [4, 171, 174, 337, 340], "num_class": [5, 13, 137, 140, 143, 144, 224, 227, 337, 340, 350, 355, 359], "num_cpu": [3, 9, 10, 128, 130, 181, 187, 198, 204, 206, 212, 317, 319, 337, 341, 342], "num_decoder_lay": [343, 346], "num_encoder_lay": [343, 346], "num_epoch": [5, 7, 13, 14, 137, 143, 144, 224, 228, 229, 235, 238, 240, 244, 245, 247, 248, 250, 253, 256, 257], "num_gpu": [3, 7, 10, 15, 16, 128, 130, 131, 137, 139, 146, 147, 149, 181, 187, 206, 212, 213, 224, 237, 253, 256, 267, 275, 283, 343, 349, 350, 359], "num_imag": [159, 163], "num_item": [330, 332, 333, 334, 336], "num_label": [293, 298], "num_lay": [343, 346, 347, 349], "num_parquet_shard": [330, 332], "num_partit": [285, 291], "num_replica": [16, 147, 149, 151, 153, 305, 306, 307, 311, 315, 316], "num_return": [3, 181, 189], "num_row": [267, 281, 284, 285, 288, 289, 290, 350, 353], "num_row_group": [350, 353], "num_sampl": [4, 7, 12, 14, 171, 174, 253, 257], "num_to_keep": [317, 321, 324, 328, 330, 334, 337, 340, 343, 347, 350, 356], "num_training_step": [6, 259, 262], "num_us": [330, 332, 333, 334, 336], "num_warmup_step": [6, 259, 262, 263], "num_work": [4, 5, 6, 12, 13, 137, 143, 171, 174, 224, 225, 230, 245, 252, 259, 262, 263, 293, 299, 300, 317, 321, 324, 328, 330, 334, 337, 340, 343, 345, 347, 350, 351, 353, 354, 356, 359], "number": [3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 24, 27, 80, 81, 84, 85, 88, 89, 110, 113, 116, 118, 122, 137, 144, 159, 161, 164, 166, 167, 171, 174, 181, 187, 189, 198, 201, 203, 206, 210, 212, 213, 224, 227, 228, 229, 235, 238, 244, 245, 250, 253, 255, 257, 259, 263, 266, 267, 273, 279, 282, 285, 288, 290, 293, 295, 298, 299, 300, 306, 307, 311, 315, 316, 324, 326, 330, 332, 334, 350, 359], "numenta": [343, 345], "numer": [9, 118, 121, 198, 201, 224, 226, 317, 323, 337, 338, 350, 352], "numpi": [2, 3, 5, 6, 7, 9, 10, 11, 14, 15, 16, 128, 131, 136, 137, 139, 140, 143, 147, 148, 149, 159, 163, 164, 167, 175, 176, 181, 182, 198, 202, 206, 207, 210, 213, 218, 219, 222, 224, 226, 237, 238, 243, 253, 254, 259, 260, 267, 269, 280, 284, 293, 296, 317, 319, 323, 324, 326, 330, 332, 337, 339, 340, 343, 345, 349, 350, 352, 359], "nuremburg": [267, 283], "nutshel": 12, "nvdp": [43, 46, 51, 53, 58, 64], "nvidia": [24, 27, 51, 52, 64, 65, 118, 121, 267, 275, 283], "nvlink": [110, 116], "nvme": [5, 224, 225, 226], "nyc": [4, 9, 164, 166, 171, 174, 198, 201, 204, 349], "nyc_taxi": [343, 345], "nyc_taxi_2021": 12, "nyc_taxi_t": [343, 345], "nyc_taxi_transform": [343, 347], "o": [2, 3, 5, 6, 11, 13, 84, 85, 86, 87, 101, 110, 113, 116, 118, 121, 123, 128, 129, 133, 137, 138, 139, 142, 143, 146, 147, 148, 155, 157, 159, 163, 175, 176, 181, 182, 186, 187, 218, 223, 224, 226, 234, 237, 245, 247, 248, 251, 259, 260, 262, 263, 267, 278, 281, 283, 286, 293, 294, 296, 312, 317, 319, 321, 323, 324, 326, 328, 329, 330, 332, 334, 336, 337, 339, 340, 342, 343, 345, 347, 349, 350, 352, 355, 357, 359], "ob": [324, 326, 327, 329], "obj": [86, 87, 118, 121], "obj_ref": [3, 181, 183], "object": [2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 35, 42, 66, 78, 88, 89, 92, 93, 101, 102, 105, 110, 113, 118, 121, 123, 128, 130, 131, 137, 144, 171, 174, 175, 179, 180, 184, 190, 191, 196, 197, 198, 201, 206, 214, 215, 218, 220, 222, 224, 234, 235, 236, 238, 243, 245, 250, 252, 253, 256, 257, 266, 267, 277, 279, 283, 284, 332, 336, 337, 340, 342, 350, 352, 357], "object_ref": [3, 181, 189], "objectref": [2, 3, 175, 179, 181, 183, 184, 189], "oblig": [285, 292], "oblivi": [285, 292], "obs_dim": [324, 327, 329], "obs_sampl": [324, 329], "observ": [5, 6, 14, 92, 93, 101, 118, 125, 126, 127, 128, 134, 135, 137, 138, 163, 224, 225, 259, 261, 293, 300, 324, 325, 326, 327, 337, 339, 343, 346, 350, 353], "observed_data": [164, 166], "obtain": [86, 87, 293, 298], "obtus": [285, 288], "obviou": [285, 288], "occupi": [245, 251], "occur": [8, 159, 163, 191, 195, 238, 243, 245, 249, 330, 332], "ocean": [267, 283], "oct": [267, 281, 283], "octob": [267, 283], "off": [0, 5, 7, 10, 13, 14, 110, 116, 118, 121, 124, 128, 129, 137, 138, 147, 148, 206, 211, 224, 226, 237, 253, 255, 267, 281, 283, 285, 292, 317, 319, 323, 330, 334, 335, 337, 341, 342, 343, 348, 350, 352, 353, 356, 359], "offenc": [267, 283], "offend": [267, 283], "offer": [4, 8, 11, 24, 25, 26, 90, 91, 96, 99, 110, 112, 116, 128, 133, 134, 137, 144, 147, 153, 171, 173, 191, 192, 194, 195, 196, 218, 220, 221, 285, 288, 292, 317, 319], "offici": [17, 22, 35, 39, 82, 83, 84, 85, 126, 127, 155, 158, 164, 167], "offlin": [12, 88, 89, 126, 127, 324, 325, 326, 330, 332, 337, 338], "offlinemnistclassifi": 16, "offlinepredictor": [4, 12, 171, 174], "offload": [86, 87, 350, 351], "often": [8, 100, 137, 139, 191, 192, 195, 196, 224, 237, 330, 332, 337, 341], "oh": [267, 283], "olap": [8, 191, 192], "old": [267, 283, 285, 288, 292, 337, 342, 343, 349, 350, 359], "older": [164, 167, 317, 323], "oltp": [8, 191, 192], "olympics2012": [267, 283], "omp_num_thread": [3, 181, 187], "on_demand": [159, 163], "on_epoch": [317, 320, 324, 327], "on_fit_start": [6, 259, 262], "on_step": [6, 259, 262], "onc": [2, 10, 12, 15, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 84, 85, 86, 87, 90, 91, 101, 102, 105, 108, 110, 114, 115, 118, 121, 128, 130, 131, 137, 138, 147, 153, 175, 180, 182, 184, 188, 190, 206, 212, 213, 224, 225, 226, 234, 237, 266, 267, 273, 279, 282, 293, 298, 317, 321, 330, 331, 337, 341, 343, 344, 349, 350, 351, 353, 359], "one": [2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 16, 17, 22, 24, 26, 27, 28, 30, 43, 45, 53, 56, 84, 85, 86, 87, 88, 89, 96, 98, 101, 102, 104, 105, 106, 107, 118, 120, 121, 124, 137, 143, 144, 147, 151, 155, 158, 164, 166, 167, 171, 174, 175, 180, 181, 185, 187, 188, 189, 191, 193, 197, 198, 203, 205, 206, 208, 218, 220, 221, 224, 225, 227, 230, 232, 236, 238, 239, 245, 252, 259, 261, 266, 267, 279, 283, 285, 288, 291, 292, 317, 321, 337, 339, 340, 344, 347, 348, 350, 351, 353, 355, 356], "one_hot": [137, 143], "onehellofanighttour": [267, 283], "ones": [3, 8, 28, 30, 43, 45, 53, 56, 82, 83, 101, 102, 105, 110, 112, 181, 189, 191, 194, 267, 283, 302, 307, 313], "ongo": [16, 24, 25, 79], "onli": [2, 3, 5, 9, 10, 13, 15, 16, 17, 20, 24, 26, 28, 31, 35, 40, 43, 47, 53, 59, 66, 75, 84, 85, 86, 87, 88, 89, 94, 95, 96, 98, 100, 101, 102, 105, 107, 118, 121, 128, 130, 132, 137, 143, 144, 155, 158, 159, 162, 163, 166, 175, 180, 181, 190, 198, 201, 203, 206, 211, 212, 214, 215, 225, 227, 228, 233, 245, 248, 251, 252, 267, 283, 285, 287, 288, 290, 291, 292, 293, 298, 299, 317, 323, 324, 325, 329, 330, 332, 334, 336, 337, 338, 340, 343, 345, 347, 349, 350, 351, 353, 355, 357], "onlin": [8, 11, 12, 16, 88, 89, 126, 127, 128, 130, 151, 153, 191, 192, 218, 222, 238, 239, 267, 278, 283, 286, 294, 306, 312, 315, 343, 349, 350, 359], "onlinemnistclassifi": [11, 16, 218, 222], "onlinemnistpreprocessor": 16, "onlinepredictor": 12, "onto": [3, 7, 10, 14, 181, 187, 206, 211, 224, 237, 253, 256], "onu": [8, 191, 195], "oom": [5, 6, 10, 128, 130, 159, 163, 206, 212, 259, 260], "op": [126, 127, 350, 352], "open": [0, 1, 2, 5, 8, 13, 35, 38, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 100, 118, 123, 124, 128, 136, 137, 139, 140, 143, 144, 146, 155, 158, 159, 162, 164, 166, 168, 169, 170, 175, 177, 191, 192, 267, 283, 285, 291, 317, 319, 350, 352, 353, 359], "openai": [101, 102, 107, 108, 110, 113, 114, 115, 118, 121, 122, 123, 128, 131, 136, 137, 139, 147, 150], "openapi": 16, "opentelemetri": [155, 157, 168], "oper": [8, 12, 17, 19, 26, 35, 39, 44, 45, 46, 49, 51, 52, 54, 56, 58, 61, 63, 64, 65, 67, 70, 78, 79, 96, 98, 101, 102, 104, 106, 126, 127, 128, 130, 131, 137, 139, 144, 155, 157, 158, 164, 165, 166, 168, 191, 192, 193, 196, 199, 205, 207, 208, 211, 212, 267, 277, 284, 285, 287, 289, 290, 291, 293, 296, 317, 318, 319, 350, 351], "opinion": [17, 21, 285, 288], "oppos": [285, 292], "opt": [155, 158], "opt_path": [350, 355], "opt_state_path": [343, 347], "optim": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 22, 24, 26, 103, 107, 109, 111, 117, 118, 124, 125, 126, 127, 128, 132, 137, 143, 144, 146, 147, 153, 191, 192, 196, 198, 202, 204, 205, 206, 212, 218, 220, 224, 226, 228, 238, 240, 245, 246, 247, 248, 250, 252, 253, 254, 256, 257, 259, 262, 266, 267, 273, 279, 282, 293, 298, 317, 320, 324, 327, 330, 334, 336, 343, 345, 347, 350, 351, 352, 355, 359], "optimizerlrschedul": [6, 259, 260, 262], "option": [3, 4, 5, 9, 10, 11, 12, 14, 15, 16, 20, 21, 26, 28, 30, 34, 35, 36, 39, 42, 45, 51, 52, 56, 64, 65, 78, 84, 85, 101, 102, 105, 108, 118, 121, 122, 128, 133, 159, 163, 164, 167, 171, 172, 173, 181, 185, 187, 198, 204, 206, 210, 213, 215, 218, 221, 223, 224, 226, 237, 238, 241, 266, 267, 275, 279, 283, 293, 300, 306, 307, 311, 315, 316, 317, 319, 321, 324, 328, 330, 334, 340, 342, 343, 346, 347, 349, 360], "optuna": [7, 14, 253, 254, 257], "optunasearch": [7, 14, 253, 257], "orang": [3, 181, 183], "orc": [8, 191, 192], "orchestr": [24, 25, 26, 106, 110, 113, 147, 154, 168, 192, 224, 225, 226, 230, 235, 317, 318, 323, 324, 325, 330, 331, 334, 336, 337, 342, 343, 344, 345, 347, 349, 350, 351, 352, 355], "order": [2, 7, 14, 80, 81, 128, 130, 137, 146, 159, 161, 175, 177, 253, 257, 267, 283, 285, 291, 337, 339, 340], "order_bi": [137, 144, 147, 150], "ordinari": [285, 288], "oregon": [267, 283], "org": [96, 98, 99, 330, 332, 360], "org_967t9ah1lbk1yqf1zau6a1v247": [86, 87], "org_xxxxxxx": [35, 39], "organ": [8, 9, 17, 19, 22, 35, 39, 79, 88, 89, 94, 95, 99, 100, 137, 139, 191, 192, 198, 201, 224, 234], "organiz": [24, 26, 100], "orient": 12, "origin": [5, 13, 53, 63, 224, 227, 267, 281, 283, 284, 285, 292, 317, 319, 330, 335, 336, 343, 345, 350, 351], "original_user_id": [330, 336], "oscar": [267, 283], "oss": [128, 132, 134, 137, 142, 144, 147, 153, 159, 162, 163], "ossci": [13, 14], "other": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 17, 22, 24, 25, 66, 78, 82, 83, 86, 87, 88, 89, 118, 120, 128, 129, 133, 137, 138, 142, 147, 148, 155, 157, 169, 170, 171, 173, 174, 175, 177, 181, 188, 190, 191, 192, 194, 196, 198, 201, 218, 221, 224, 225, 226, 229, 237, 238, 239, 253, 257, 259, 261, 263, 267, 283, 284, 285, 291, 292, 293, 295, 302, 307, 313, 317, 321, 323, 324, 329, 330, 332, 334, 343, 347, 350, 351], "otherwis": [5, 8, 118, 121, 191, 197, 330, 332], "otlp": 168, "our": [3, 5, 6, 7, 9, 10, 11, 14, 15, 16, 88, 89, 90, 91, 113, 114, 115, 118, 121, 122, 123, 128, 131, 164, 166, 168, 181, 188, 198, 201, 206, 210, 216, 218, 222, 224, 234, 253, 255, 256, 257, 259, 262, 263, 267, 283, 285, 292, 293, 297], "out": [3, 4, 5, 6, 8, 9, 10, 16, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 128, 134, 137, 143, 155, 157, 159, 163, 171, 174, 181, 185, 189, 191, 196, 198, 205, 206, 209, 224, 225, 254, 257, 259, 260, 266, 279, 281, 283, 285, 288, 291, 292, 317, 318, 319, 324, 326, 330, 332, 336, 337, 342, 343, 349, 350, 353, 359], "out_channel": [5, 6, 13, 224, 227, 259, 262], "out_featur": [13, 137, 140], "out_img_byt": [317, 319], "out_label": [317, 319], "out_proj": [343, 346], "out_ref": [3, 181, 184], "outbound": [17, 22], "outbr": [285, 292], "outdoor": [267, 283], "outhous": [267, 283], "outlier": [11, 218, 220], "outlook": 111, "output": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 28, 30, 31, 35, 39, 40, 42, 43, 45, 47, 53, 56, 57, 59, 66, 69, 70, 73, 75, 78, 88, 89, 92, 93, 101, 102, 104, 106, 109, 110, 113, 119, 120, 121, 123, 125, 128, 130, 147, 149, 164, 166, 167, 169, 170, 171, 174, 175, 180, 181, 185, 198, 200, 201, 203, 206, 210, 212, 214, 216, 218, 222, 224, 225, 226, 227, 228, 234, 236, 238, 240, 245, 247, 251, 253, 256, 257, 259, 262, 266, 267, 279, 293, 298, 317, 320, 323, 330, 332, 336, 343, 344, 346, 349, 360], "output_column": [285, 292], "output_csv": [330, 332], "output_dir": [317, 319, 350, 352], "output_path": [159, 163], "output_s": 13, "outsid": [5, 6, 13, 24, 27, 137, 139, 259, 263], "outstand": [8, 191, 197], "over": [3, 4, 7, 9, 10, 12, 14, 24, 26, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 128, 130, 171, 174, 181, 190, 198, 202, 205, 206, 208, 209, 224, 228, 238, 240, 245, 246, 253, 257, 267, 283, 285, 291, 292, 293, 298, 317, 323, 330, 334, 336, 337, 338, 342, 347, 349, 350, 351, 359], "overal": [15, 293, 295, 337, 340], "overcom": [267, 283], "overfit": [350, 357], "overhead": [2, 5, 6, 8, 82, 83, 100, 101, 102, 105, 110, 116, 175, 177, 191, 195, 224, 225, 259, 261, 324, 329], "overlap": [159, 162, 224, 225, 343, 344], "overload": [159, 163], "overr": [285, 292], "overrid": [28, 30, 35, 39, 43, 45, 53, 56, 92, 93, 224, 227], "overriden": [90, 91], "overse": 13, "oversubscrib": [3, 181, 187], "overview": [7, 8, 9, 10, 14, 15, 86, 87, 111, 117, 119, 137, 144, 156, 158, 159, 160, 164, 166, 172, 176, 191, 192, 198, 199, 206, 207, 219, 224, 225, 253, 256, 263, 360], "overwhelm": [159, 163], "overwrit": [82, 83], "own": [3, 5, 9, 11, 12, 24, 26, 35, 39, 80, 81, 86, 87, 92, 93, 96, 98, 110, 115, 118, 125, 181, 183, 198, 203, 218, 221, 224, 226, 267, 281, 283, 285, 292, 302, 307, 308, 309, 313, 316, 317, 318, 319, 321, 324, 325, 328, 330, 331, 334, 337, 338, 339, 340, 343, 344, 350, 353, 355, 359], "owner": [35, 37, 66, 68, 96, 98], "ownership": [285, 291], "ox": [267, 283], "p": [137, 140, 144, 285, 292, 317, 318, 343, 349], "p50": [164, 167], "p90": [164, 167], "p99": [164, 167], "pa": [317, 319, 337, 339, 340, 343, 345, 350, 352], "pack": [267, 283, 317, 320], "packag": [1, 84, 85, 86, 87, 128, 129, 137, 138, 144, 147, 148, 169, 170, 245, 248, 278, 286, 294, 312, 317, 323, 324, 326, 330, 336, 337, 342], "pad": [5, 7, 13, 14, 128, 131, 147, 149, 224, 227, 253, 256, 257, 293, 298, 317, 320], "page": [0, 9, 10, 15, 101, 102, 109, 110, 116, 147, 153, 164, 166, 198, 204, 206, 215], "pagedattent": [101, 102, 107], "pagerduti": [164, 167], "pai": [101, 102, 107, 285, 292], "paid": [4, 9, 171, 174, 198, 201], "pain": [8, 191, 195], "pair": [5, 330, 331, 350, 353], "pal": [267, 283], "pale": [285, 292], "pan": [285, 291], "pancak": [350, 351], "panda": [4, 5, 6, 9, 12, 13, 171, 172, 198, 199, 201, 202, 203, 204, 224, 226, 236, 238, 242, 259, 260, 287, 288, 317, 319, 324, 326, 330, 332, 337, 339, 340, 341, 342, 343, 345, 349, 350, 352, 359], "panel": [2, 3, 175, 180, 181, 189], "pant": [267, 283], "paper": [267, 283], "par": [285, 292], "parallel": [1, 7, 8, 9, 10, 14, 15, 104, 106, 113, 117, 128, 130, 137, 144, 169, 170, 176, 191, 195, 198, 200, 202, 204, 206, 208, 210, 228, 230, 238, 239, 242, 243, 244, 253, 257, 262, 266, 267, 271, 275, 277, 279, 281, 283, 284, 285, 287, 289, 290, 293, 298, 302, 307, 313, 317, 318, 319, 323, 324, 325, 326, 328, 330, 331, 332, 336, 337, 338, 339, 341, 343, 344, 349, 350, 351, 352, 359], "parallel_strategi": [5, 224, 231, 245, 252], "parallel_strategy_kwarg": [5, 224, 231], "param": [4, 12, 137, 144, 171, 174, 307, 309, 316, 330, 334, 337, 340], "param_group": [137, 143], "param_nam": [224, 229], "param_spac": [4, 7, 12, 14, 171, 174, 253, 257], "paramet": [2, 4, 5, 6, 7, 9, 10, 13, 14, 15, 28, 31, 35, 40, 43, 47, 53, 59, 66, 75, 84, 85, 92, 93, 101, 102, 106, 110, 112, 117, 118, 121, 123, 125, 137, 143, 144, 171, 174, 175, 180, 198, 202, 204, 206, 212, 213, 215, 224, 228, 229, 231, 238, 239, 240, 244, 245, 247, 252, 253, 256, 257, 259, 262, 267, 275, 283, 285, 290, 293, 295, 298, 306, 307, 315, 317, 320, 324, 327, 330, 334, 337, 340, 343, 347, 350, 351, 355], "parameter": [11, 218, 223, 245, 247], "paramor": [267, 283], "parcel": [285, 292], "parent": [5, 13, 137, 140], "parish": [267, 283], "pariti": [24, 26], "park": [267, 283, 307, 308, 309, 316], "parquet": [4, 6, 8, 9, 10, 12, 15, 86, 87, 159, 163, 164, 166, 171, 174, 191, 192, 198, 201, 203, 204, 205, 206, 210, 216, 238, 239, 242, 245, 251, 252, 259, 262, 318, 323, 324, 329, 331, 338, 340, 342, 344, 347, 351, 355, 359], "parquet_256": [317, 319, 350, 352, 353], "parquet_dir": [330, 332, 337, 339, 343, 345, 347], "parquet_fil": [350, 353], "parquet_path": [317, 319, 343, 345, 350, 353, 354, 359], "parquetdataset": [6, 259, 262], "parquetfil": [350, 353], "pars": [8, 11, 16, 88, 89, 102, 108, 110, 114, 118, 122, 137, 146, 147, 148, 191, 197, 218, 222, 343, 345], "parseabl": [118, 120, 122], "part": [0, 7, 9, 13, 14, 15, 16, 53, 57, 82, 83, 110, 113, 137, 139, 198, 199, 201, 253, 254, 285, 292, 293, 295, 299, 330, 332], "parti": [155, 157, 267, 283], "particular": [10, 206, 212, 285, 288], "particularli": [3, 181, 188, 267, 283], "partit": [10, 206, 208, 266, 267, 271, 279, 281, 317, 318, 324, 325, 337, 339, 350, 353], "partner": [267, 283, 285, 291], "pass": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 92, 93, 137, 139, 143, 171, 174, 182, 187, 190, 191, 196, 198, 201, 203, 206, 208, 213, 218, 222, 224, 225, 228, 229, 230, 231, 235, 238, 239, 240, 242, 243, 244, 245, 247, 248, 250, 252, 253, 257, 259, 262, 263, 267, 283, 293, 298, 330, 331, 337, 340, 350, 359], "passeng": [4, 9, 12, 171, 174, 198, 201, 204, 285, 292, 349], "passenger_count": [4, 9, 12, 171, 174, 198, 201], "passrol": [17, 22], "past": [82, 83, 84, 85, 88, 89, 90, 91, 267, 283, 343, 344, 345, 346, 347, 349], "past_list": [343, 349], "past_norm": [343, 349], "patch": [8, 24, 26, 191, 196], "patch32": [128, 131, 136, 137, 139, 147, 150], "path": [4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 24, 27, 66, 69, 86, 87, 110, 113, 118, 121, 128, 129, 130, 133, 137, 138, 139, 140, 142, 143, 146, 147, 148, 150, 153, 159, 163, 164, 166, 167, 171, 174, 191, 197, 198, 203, 206, 211, 212, 213, 215, 216, 218, 222, 223, 224, 226, 234, 235, 236, 237, 238, 244, 245, 247, 248, 250, 251, 259, 262, 263, 293, 300, 317, 319, 321, 322, 323, 324, 328, 329, 330, 332, 334, 336, 337, 339, 340, 342, 343, 345, 347, 349, 350, 352, 355, 359], "pathlib": [4, 5, 13, 137, 140, 171, 174, 224, 226, 343, 345], "paths_to_delet": [245, 251, 350, 359], "patienc": [137, 143], "patient": [267, 283], "pattern": [8, 11, 176, 182, 184, 187, 191, 192, 218, 223, 224, 234, 238, 240, 245, 252, 323, 332, 336, 342, 359], "payload": [4, 12, 171, 174], "payment_typ": [9, 198, 201, 204], "pb": [159, 163], "pc": [267, 281, 283, 284], "pd": [4, 5, 6, 9, 12, 13, 171, 172, 174, 198, 199, 201, 202, 224, 226, 238, 242, 259, 260, 262, 285, 288, 317, 319, 324, 326, 330, 332, 336, 337, 339, 341, 342, 343, 345, 349, 350, 352], "pdf": [330, 332], "pe": [343, 346], "peac": [267, 283], "peak": [101, 102, 106], "peer": [24, 26], "penalti": [164, 167], "pend": [12, 13, 293, 300], "pendulum": [328, 329], "pendulum_diffus": [324, 328, 329], "pendulum_diffusion_ft": [324, 328], "pendulum_diffusion_result": [324, 328], "peopl": [267, 283, 285, 288, 292], "per": [9, 10, 15, 16, 86, 87, 101, 102, 106, 107, 110, 113, 116, 118, 121, 128, 130, 137, 144, 147, 151, 155, 157, 164, 166, 167, 198, 201, 204, 206, 210, 211, 227, 229, 230, 234, 235, 238, 244, 295, 299, 317, 320, 321, 324, 327, 328, 332, 334, 337, 338, 340, 341, 342, 343, 344, 347, 349, 350, 351, 355, 357, 359], "per_worker_batch": [224, 228], "percentag": [9, 198, 202], "percentil": [343, 349], "perceptu": [317, 323], "perfect": [110, 112, 267, 283, 285, 292, 317, 319], "perform": [2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 84, 85, 86, 87, 88, 89, 92, 93, 101, 102, 103, 104, 110, 112, 116, 117, 118, 122, 123, 124, 125, 126, 127, 137, 139, 144, 155, 157, 159, 162, 164, 167, 171, 172, 173, 174, 175, 180, 181, 187, 191, 192, 195, 196, 197, 198, 199, 202, 204, 205, 206, 207, 208, 212, 213, 215, 218, 219, 220, 222, 224, 234, 236, 238, 239, 241, 245, 246, 252, 253, 254, 257, 259, 262, 263, 267, 273, 275, 277, 282, 283, 284, 285, 290, 291, 292, 293, 295, 300, 317, 319, 323, 324, 325, 329, 330, 336, 337, 339, 341, 342, 343, 344, 347, 349, 350, 351, 355, 359], "performantli": [1, 169, 170], "perhap": [7, 8, 14, 191, 196, 253, 257, 285, 291, 292], "period": [16, 80, 81, 84, 85, 101, 102, 106, 343, 349], "permiss": [17, 22, 24, 26, 28, 29, 35, 36, 43, 44, 53, 55, 63, 86, 87, 94, 95], "permut": [317, 323], "persi": [267, 283], "persis": [224, 226], "persist": [5, 6, 17, 21, 22, 24, 26, 86, 87, 88, 89, 128, 130, 155, 157, 159, 162, 207, 208, 225, 226, 227, 228, 232, 238, 242, 245, 246, 248, 251, 259, 263, 324, 328, 330, 332, 335, 337, 338, 339, 351, 355], "person": [285, 292, 330, 331, 336], "perspect": [285, 288, 292], "pertain": [88, 89], "petabyt": [128, 132], "phase": [109, 293, 298], "philip": [267, 283], "philosop": [285, 292], "philosophi": [285, 291], "photo": [267, 283, 350, 351], "photograph": [285, 292, 317, 318, 350, 352], "physic": [3, 10, 159, 161, 181, 187, 206, 212], "pi": [88, 89, 324, 325, 326, 329], "pi4_sampl": [88, 89], "pi_": [324, 325], "pic": [267, 283], "pick": [53, 57, 101, 102, 108, 245, 250, 252, 267, 283, 285, 292, 330, 335, 343, 348, 350, 351, 358, 359], "pickup": [343, 344], "pid": [13, 14, 293, 300], "piec": [164, 166, 267, 283], "pil": [5, 128, 131, 136, 137, 139, 147, 148, 224, 226, 232, 237, 238, 243, 317, 319, 350, 352], "pile": [285, 288], "pin": [224, 237, 317, 319, 350, 351, 354], "pine": [337, 338], "pinecon": [8, 191, 192], "pink": [285, 288], "pinterest": [10, 206, 217], "pioneer": [8, 191, 195], "pip": [0, 1, 66, 74, 82, 83, 84, 85, 86, 87, 110, 114, 126, 127, 128, 129, 137, 138, 147, 148, 153, 155, 158, 168, 169, 170, 182, 278, 286, 294, 312, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "pipelin": [4, 6, 8, 9, 10, 15, 84, 85, 101, 102, 105, 106, 117, 118, 125, 128, 130, 147, 151, 159, 163, 165, 171, 174, 182, 191, 192, 195, 198, 203, 205, 206, 208, 212, 213, 217, 224, 226, 238, 239, 241, 242, 244, 245, 252, 259, 262, 285, 287, 292, 304, 305, 306, 307, 314, 315, 317, 318, 319, 324, 325, 329, 330, 331, 337, 338, 342, 343, 344, 349, 350, 351, 353, 359], "pipeline_parallel_s": [110, 116], "pitch": [267, 283], "pivot": [324, 325], "pixel": [7, 10, 14, 206, 212, 224, 226, 232, 238, 242, 243, 253, 255, 318, 319, 321, 324, 326, 350, 352], "pixeldiffus": [317, 320, 321, 323], "pizza": [350, 351], "pl": [6, 259, 260, 262, 263, 317, 319, 320, 321, 324, 326, 327, 328], "pl_ckpt": [317, 323], "place": [0, 8, 101, 102, 105, 191, 197, 224, 226, 245, 247, 249, 267, 283, 285, 292, 307, 308, 309, 316], "placehold": [28, 29, 35, 36, 43, 44, 51, 53, 54, 64, 66, 67, 82, 83, 101, 102, 108, 110, 114], "placement": [224, 228, 231, 232, 235, 324, 325, 343, 349, 350, 351, 354, 355, 359], "plai": [82, 83, 267, 283, 285, 291, 350, 359], "plain": [317, 319, 350, 351], "plan": [10, 17, 22, 24, 26, 27, 28, 30, 35, 39, 43, 45, 51, 53, 56, 64, 66, 70, 79, 118, 121, 164, 167, 206, 211, 212, 214, 267, 281, 283], "plane": [19, 21], "planner": [343, 344], "plate": [267, 283], "plateau": [343, 349], "platform": [8, 16, 17, 20, 24, 26, 79, 88, 89, 100, 101, 102, 106, 128, 130, 147, 154, 155, 157, 158, 159, 162, 164, 165, 166, 167, 191, 192, 194, 195, 224, 226, 267, 271, 281, 293, 295, 299], "plausibl": [324, 329], "pleas": [17, 22, 28, 31, 35, 40, 43, 44, 47, 51, 53, 54, 59, 64, 66, 67, 75, 78, 100, 118, 121, 164, 166, 167, 278, 286, 294, 312, 330, 336, 350, 351], "plenti": [128, 133], "plot": [16, 224, 226, 236, 237, 285, 288, 291, 292, 319, 320, 326, 327, 332, 337, 339, 342, 345, 349, 352, 359], "plotlin": [285, 292], "plt": [5, 7, 10, 13, 14, 16, 206, 207, 211, 224, 226, 237, 253, 254, 255, 317, 319, 321, 323, 324, 326, 328, 330, 332, 334, 337, 339, 341, 343, 345, 347, 349, 350, 352, 357, 359], "plu": [17, 22, 224, 226, 324, 326, 350, 352], "plugin": [6, 24, 27, 51, 52, 64, 65, 66, 72, 259, 263, 317, 321, 324, 328], "pm": [324, 325], "pndm": [317, 323], "png": [10, 128, 136, 147, 150, 153, 206, 212], "poc": [24, 26], "pod": [24, 26, 27, 43, 49, 50, 51, 53, 61, 62, 64, 66, 73, 101, 102, 106], "point": [5, 10, 11, 17, 20, 24, 26, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 86, 87, 101, 102, 108, 110, 115, 128, 130, 137, 144, 168, 206, 212, 218, 223, 224, 228, 234, 245, 250, 266, 267, 279, 283, 285, 291, 292, 337, 339], "pointless": [285, 288], "pole": [337, 338], "polici": [17, 21, 22, 24, 26, 27, 35, 39, 42, 63, 66, 70, 320, 323, 326], "polish": [285, 292], "polit": [118, 121, 285, 288, 292], "politician": [285, 288], "poll": [267, 283], "pomeranian": [137, 144], "pont": [267, 283], "pool": [10, 128, 132, 206, 213], "poor": [11, 218, 220], "poorli": [7, 14, 253, 257], "pop": [128, 129, 137, 138, 147, 148], "popul": [9, 28, 30, 43, 45, 198, 201], "popular": [4, 8, 118, 121, 171, 173, 174, 191, 194, 195, 267, 272, 273, 282, 330, 332], "porn": [285, 288], "porno": [285, 288], "pornograph": [285, 288], "port": [137, 144, 155, 158, 337, 338], "portion": [293, 298], "pos_enc": [343, 346], "posit": [8, 137, 146, 191, 196, 267, 281, 283, 285, 287, 288, 290, 292, 307, 309, 316, 343, 346], "posix": [17, 22], "possibl": [110, 117, 118, 123, 125, 147, 151, 285, 287], "possibli": [8, 191, 197], "post": [4, 5, 6, 7, 11, 12, 13, 16, 17, 22, 147, 150, 153, 164, 167, 168, 171, 174, 218, 222, 253, 258, 259, 264, 306, 307, 315, 317, 318, 330, 331, 336], "poster": [285, 292], "postgresql": [8, 191, 192], "postwar": [285, 291, 292], "potato": [285, 288], "potemkin": [285, 292], "potenti": [3, 159, 161, 181, 185], "potter": [267, 283], "power": [3, 118, 119, 120, 123, 126, 127, 181, 188, 266, 267, 279, 285, 287, 293, 295, 299, 337, 342], "powershel": [118, 121], "pq": [317, 319, 343, 345, 350, 352, 353], "practic": [1, 5, 17, 20, 35, 39, 79, 96, 97, 101, 119, 120, 121, 125, 128, 130, 155, 157, 169, 170, 224, 225, 234, 237, 245, 252, 267, 277, 284, 285, 289, 293, 295, 343, 345, 350, 351], "practition": [4, 12, 171, 173], "prayer": [267, 283], "pre": [10, 15, 35, 39, 82, 83, 84, 85, 86, 87, 126, 127, 128, 135, 137, 145, 147, 153, 206, 213, 293, 298, 305, 306, 307, 314, 315, 317, 323, 330, 336, 343, 344, 350, 351], "preced": [137, 144, 285, 292], "precis": [6, 101, 102, 106, 110, 112, 137, 146, 259, 262, 263, 317, 323, 324, 329, 350, 359], "precomput": [101, 102, 104, 350, 353], "preconfigur": [224, 227], "pred": [5, 137, 146, 224, 237, 324, 327, 330, 334, 337, 341, 342, 343, 346, 347, 349, 350, 355, 359], "pred_d": [137, 146, 337, 341, 342, 343, 349, 350, 359], "pred_label": [337, 340, 341], "pred_nois": [317, 320, 323, 324, 329], "pred_norm": [343, 349], "pred_prob": [337, 340], "pred_row": [343, 349, 350, 359], "predefin": [84, 85], "predic": [9, 198, 205], "predict": [4, 7, 8, 10, 11, 13, 14, 15, 16, 118, 124, 126, 127, 137, 139, 140, 146, 147, 148, 150, 153, 171, 174, 191, 193, 206, 213, 216, 218, 222, 253, 257, 266, 267, 279, 293, 297, 298, 302, 306, 307, 309, 313, 315, 316, 317, 318, 320, 323, 324, 325, 327, 330, 331, 333, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 349, 350, 352, 359], "predict_prob": [137, 140, 146, 147, 149], "predicted_label": [10, 11, 15, 16, 164, 167, 206, 213, 215, 218, 222, 350, 359], "predicted_prob": [12, 137, 146], "prediction_pipelin": [4, 171, 174], "predictor": [4, 12, 101, 102, 104, 137, 146, 147, 149, 171, 174, 350, 359], "preemption": [10, 88, 89, 206, 208, 245, 249], "prefect": [147, 154], "prefer": [24, 26, 28, 31, 35, 40, 43, 47, 53, 59, 66, 73, 75, 82, 83, 118, 124, 168, 285, 292, 317, 323], "prefer_spot": [159, 163], "prefetch": [350, 359], "prefetch_batch": [6, 238, 241, 259, 263], "prefil": 109, "prefix": [5, 9, 35, 39, 86, 87, 198, 203, 330, 336, 343, 349, 350, 359], "prefix_for_the_resources_ad": [35, 39], "preinstal": [80, 81], "prem": [80, 81], "premier": [267, 283], "premis": [17, 20, 94, 95], "prepar": [3, 6, 79, 92, 93, 118, 121, 128, 130, 181, 186, 224, 228, 231, 235, 239, 245, 252, 259, 263, 285, 287, 291, 292, 293, 295, 298, 324, 328, 347, 349, 351, 359], "prepare_data_load": [5, 13, 225, 227, 228, 234, 245, 252, 343, 345, 350, 351, 352, 354, 359], "prepare_model": [5, 13, 137, 143, 225, 227, 228, 234, 245, 252, 330, 331, 332, 334, 343, 345, 347, 350, 351, 352, 355, 359], "prepare_train": [317, 321, 324, 328], "preproc": [137, 146], "preprocess": [5, 6, 8, 9, 10, 12, 15, 16, 84, 85, 126, 127, 128, 129, 130, 131, 138, 143, 144, 191, 192, 198, 201, 205, 206, 212, 224, 226, 232, 237, 238, 239, 243, 244, 245, 252, 259, 262, 278, 286, 287, 293, 294, 295, 312, 317, 318, 319, 324, 325, 329, 330, 332, 336, 343, 344, 350, 351, 352, 353], "preprocess_imag": [317, 319], "preprocessed_data": [137, 139], "preprocessed_data_path": [137, 139], "preprocessed_df": [285, 292], "preprocessed_train": [137, 139], "preprocessed_train_d": [137, 143], "preprocessed_train_path": [137, 139, 143], "preprocessed_v": [137, 139], "preprocessed_val_d": [137, 143], "preprocessed_val_path": [137, 139, 143], "preprocessor": [9, 16, 137, 139, 140, 143, 146, 147, 149, 198, 205, 285, 288], "preprocessor_app": 16, "preprocessor_handl": 16, "prerequisit": [36, 54, 67], "presenc": [337, 342], "present": [5, 8, 101, 102, 106, 118, 122, 164, 165, 191, 195, 224, 226, 285, 291, 292, 330, 332, 337, 338, 340, 343, 345, 349, 350, 359], "preserv": [137, 146, 224, 225, 227, 337, 339, 343, 344], "press": [1, 169, 170], "pressur": [101, 102, 105, 128, 130, 159, 163, 307, 308, 309, 316], "pretend": [285, 289], "pretenti": [285, 288], "pretrain": [6, 259, 262, 263, 264, 267, 272, 273, 282], "pretrainedconfig": [6, 259, 262], "pretti": [285, 291, 350, 359], "prevent": [10, 206, 212, 343, 345], "preview": [0, 28, 30, 35, 39, 43, 45, 53, 56, 119, 126, 127], "previou": [43, 48, 53, 60, 66, 76, 82, 83, 101, 102, 104, 105, 118, 123, 128, 131, 137, 138, 139, 164, 165, 238, 239, 240, 245, 247, 250, 307, 309, 316, 330, 336, 337, 338, 350, 351], "previous": [5, 245, 247], "price": [4, 12, 137, 144, 171, 174, 267, 281, 283, 343, 344], "priest": [267, 283], "primari": [8, 155, 158, 191, 195], "primarili": [6, 10, 15, 16, 24, 26, 206, 214, 259, 262, 343, 347], "prime": [267, 283], "primit": [350, 352], "princip": [17, 22], "print": [1, 3, 4, 5, 6, 7, 10, 13, 14, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 137, 140, 146, 164, 166, 169, 170, 171, 174, 181, 183, 185, 187, 189, 206, 212, 224, 228, 233, 245, 251, 253, 256, 257, 259, 262, 267, 271, 277, 281, 284, 285, 288, 289, 290, 291, 292, 293, 298, 299, 307, 309, 316, 317, 319, 321, 322, 323, 324, 326, 328, 329, 330, 332, 334, 336, 337, 339, 340, 341, 342, 343, 345, 347, 348, 349, 350, 352, 353, 355, 356, 358, 359], "print_metrics_ray_train": [5, 13, 224, 228, 233, 238, 240, 245, 247], "printout": [350, 359], "prior": [8, 82, 83, 137, 138, 139, 191, 192, 337, 340], "priorit": [7, 14, 253, 257], "prioriti": [126, 127, 137, 146], "privat": [17, 20, 22, 24, 26, 28, 30, 31, 35, 40, 43, 45, 47, 53, 56, 59, 66, 69, 75, 86, 87, 94, 95], "private_subnet": [17, 22], "privileg": [17, 22, 24, 26], "prj_cz951f43jjdybtzkx1s5sjgz99": [128, 129, 137, 138, 147, 148], "pro": [118, 124], "prob": [3, 137, 146, 181, 185, 337, 341], "probabilist": [343, 349], "probabl": [137, 140, 146, 147, 149, 150, 267, 283, 350, 351], "problem": [9, 88, 89, 101, 102, 107, 118, 124, 155, 157, 198, 201], "proce": [6, 13, 16, 259, 262], "process": [2, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 20, 22, 66, 67, 84, 85, 88, 89, 92, 93, 105, 107, 108, 109, 110, 111, 122, 123, 128, 130, 132, 134, 136, 137, 139, 144, 146, 159, 161, 164, 166, 171, 174, 175, 178, 179, 180, 182, 184, 186, 188, 192, 197, 198, 200, 203, 205, 206, 208, 209, 210, 212, 213, 218, 221, 224, 225, 227, 228, 230, 232, 234, 235, 237, 238, 239, 242, 253, 256, 257, 259, 262, 263, 266, 267, 271, 273, 275, 277, 279, 281, 282, 283, 284, 292, 293, 295, 298, 299, 300, 302, 307, 313, 319, 324, 325, 329, 330, 331, 332, 337, 338, 341, 343, 344, 345, 349, 350, 353], "processed_d": [317, 319], "processor": [128, 131, 147, 149], "prod": [3, 126, 127, 181, 186], "produc": [4, 5, 6, 8, 9, 10, 13, 15, 171, 174, 191, 193, 198, 203, 206, 210, 224, 234, 259, 263, 285, 291, 317, 318, 330, 336, 343, 349], "product": [1, 3, 8, 17, 20, 79, 84, 85, 86, 87, 101, 102, 103, 104, 106, 107, 110, 111, 112, 115, 116, 117, 118, 119, 120, 122, 124, 125, 164, 167, 169, 170, 181, 187, 191, 196, 199, 207, 224, 234, 245, 252, 254, 260, 266, 267, 279, 285, 292, 302, 307, 313, 317, 323, 330, 331, 332, 333, 336, 337, 338, 340, 350, 351], "production": [90, 91, 100, 147, 154, 245, 252], "profession": 79, "profil": [24, 26, 28, 30, 34, 137, 144, 155, 157, 324, 329, 350, 359], "profile_data": 168, "prog_bar": [6, 259, 262, 317, 320, 324, 327], "program": [3, 118, 124, 164, 167, 181, 189], "programm": 16, "programmat": [16, 90, 91, 164, 167], "progress": [5, 13, 88, 89, 224, 225, 233, 236, 245, 246, 248, 249, 330, 332, 334, 350, 351, 352], "project": [0, 10, 11, 17, 19, 22, 35, 37, 38, 39, 66, 68, 69, 70, 72, 78, 86, 87, 97, 99, 168, 206, 208, 218, 220, 267, 283, 285, 292], "project_numb": [35, 39], "prometheu": 157, "promot": [343, 349], "promote_opt": [337, 340], "prompt": [1, 80, 81, 82, 83, 101, 102, 104, 105, 118, 121, 169, 170], "promptli": [285, 291], "proof": [24, 26, 267, 283], "propag": [126, 127], "proper": [28, 29, 35, 36, 43, 44, 53, 55, 137, 141, 224, 228], "properli": [1, 8, 118, 123, 155, 158, 169, 170, 191, 195, 278, 286, 293, 294, 295, 307, 309, 312, 316, 350, 352], "properti": [8, 118, 123, 191, 192], "proport": [9, 10, 15, 198, 203, 206, 210, 213, 330, 332], "proprietari": [8, 126, 127, 191, 192], "prosper": [267, 283], "protect": [337, 338], "protocol": [8, 11, 168, 191, 192, 197, 218, 220], "prototyp": [82, 83, 110, 112, 117], "prove": [16, 317, 322, 350, 358], "provid": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 24, 27, 28, 30, 35, 39, 41, 45, 50, 53, 56, 66, 70, 79, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 100, 101, 102, 103, 105, 107, 110, 112, 113, 115, 116, 117, 118, 120, 121, 122, 123, 126, 127, 128, 130, 135, 147, 151, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 173, 175, 176, 177, 181, 182, 191, 192, 195, 197, 198, 199, 200, 206, 207, 215, 218, 220, 221, 223, 224, 225, 226, 229, 238, 239, 244, 245, 250, 259, 261, 285, 287, 293, 295, 299, 302, 307, 313, 317, 321, 330, 332, 334, 337, 341, 343, 345, 347, 350, 351], "provis": [5, 6, 10, 12, 24, 25, 26, 27, 79, 96, 98, 126, 127, 206, 208, 259, 261, 317, 318, 337, 338, 350, 351], "proxi": [8, 147, 151, 164, 167, 168, 191, 197], "proxim": [337, 338], "proxy_http_request": [164, 167, 168], "proxy_route_to_replica": [164, 167, 168], "prune": [317, 323, 324, 329, 337, 342], "pseudo": [317, 323], "pt": [5, 10, 11, 13, 15, 16, 128, 131, 137, 140, 146, 147, 149, 206, 213, 217, 218, 222, 223, 224, 234, 237, 245, 247, 248, 317, 323, 330, 334, 336, 343, 347, 349, 350, 355, 359], "public": [4, 9, 10, 11, 15, 16, 17, 20, 22, 110, 113, 128, 130, 137, 138, 164, 166, 171, 174, 198, 201, 204, 206, 210, 212, 213, 215, 218, 222, 266, 267, 277, 279, 283, 284, 285, 287, 288, 291, 292], "public_subnet": [17, 22], "publicli": [94, 95, 118, 121], "publish": [92, 93], "pull": [82, 83, 267, 283, 317, 319, 330, 334, 337, 340, 343, 347, 350, 351, 352, 353, 357], "pulocationid": [9, 198, 201], "pumpkin": [267, 283], "pun": [285, 288], "punchestown": [267, 283], "punctuat": [285, 292], "pure": [317, 318, 330, 331], "purpl": [267, 283], "purpos": [1, 2, 15, 22, 137, 144, 147, 154, 169, 170, 175, 177, 192, 285, 288, 289, 350, 351], "push": [82, 83, 267, 283, 285, 292, 350, 351], "pushdown": [9, 198, 205], "put": [3, 17, 22, 128, 131, 176, 181, 183, 267, 283, 285, 291, 292], "putobject": [17, 22], "pwd": 5, "py": [0, 1, 10, 11, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 108, 110, 113, 116, 118, 121, 122, 123, 128, 129, 135, 137, 138, 147, 148, 155, 158, 159, 163, 164, 166, 169, 170, 206, 211, 215, 218, 223], "py311": [101, 102, 108, 110, 115], "py312": [159, 163], "py_execut": [128, 129, 137, 138, 147, 148], "pyarrow": [8, 10, 164, 166, 191, 192, 206, 210, 317, 319, 324, 326, 330, 332, 337, 339, 340, 343, 345, 350, 351, 352], "pydant": [4, 16, 118, 122, 171, 172], "pydata": [343, 345], "pyflink": [8, 191, 195], "pypi": [86, 87], "pyplot": [5, 7, 10, 13, 14, 16, 206, 207, 224, 226, 253, 254, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "pyproj": [84, 85], "pyproject": [126, 127], "pyspark": [8, 191, 195], "python": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 16, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 100, 101, 102, 106, 108, 110, 113, 115, 118, 121, 123, 126, 127, 128, 129, 135, 137, 138, 147, 148, 155, 158, 164, 166, 167, 168, 169, 170, 171, 173, 175, 177, 178, 179, 181, 184, 185, 190, 191, 192, 194, 195, 196, 198, 200, 218, 220, 224, 227, 237, 253, 257, 259, 263, 278, 286, 293, 294, 295, 300, 302, 307, 312, 313, 317, 319, 324, 326, 330, 331, 332, 337, 339, 343, 345, 350, 352], "python3": [0, 86, 87, 137, 144], "pythonmalloc": [155, 158], "pytorch": [10, 137, 138, 140, 143, 206, 210, 224, 225, 226, 227, 228, 232, 234, 238, 239, 240, 241, 243, 245, 252, 254, 261, 296, 299, 300, 302, 307, 313, 317, 318, 319, 321, 323, 324, 325, 326, 328, 330, 331, 332, 333, 334, 336, 344, 346, 350, 351, 352, 353, 359], "pyyaml": 0, "q": [101, 102, 105, 118, 124, 126, 127, 128, 129, 137, 138, 147, 148, 330, 332], "q2": [267, 283], "q_q": [267, 283], "qp": [147, 151, 164, 167], "qt": [267, 281, 283, 284], "qtr": [267, 283], "quad": [317, 318, 324, 325, 330, 331], "qualif": [118, 121], "qualit": [350, 359], "qualiti": [8, 110, 112, 191, 192, 317, 323, 330, 336], "quantiz": [101, 102, 105, 117], "queri": [8, 28, 30, 43, 45, 53, 56, 57, 88, 89, 92, 93, 110, 114, 118, 121, 147, 153, 159, 162, 163, 164, 167, 168, 191, 192, 307, 316], "question": [267, 283], "queu": [24, 26], "queue": [2, 8, 11, 16, 24, 26, 128, 135, 164, 167, 175, 177, 191, 197, 218, 221], "quick": [2, 5, 12, 17, 20, 24, 26, 88, 89, 118, 121, 172, 175, 176, 224, 226, 234, 235, 317, 319, 321, 330, 332, 337, 339, 340, 350, 352, 359], "quickli": [1, 9, 82, 83, 94, 95, 101, 102, 106, 169, 170, 198, 201, 285, 292, 317, 319, 337, 339, 343, 347, 350, 352, 357], "quickstart": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68], "quit": [267, 283, 285, 292], "quot": [267, 283], "quota": [17, 19, 101, 102, 106, 126, 127], "qwen": [118, 122, 123, 124], "qwen2": [118, 122], "qwen3": [118, 123], "r": [0, 1, 8, 13, 35, 42, 66, 78, 126, 127, 128, 129, 137, 138, 140, 146, 147, 148, 168, 169, 170, 191, 192, 267, 278, 283, 285, 286, 288, 294, 312, 324, 325, 330, 331, 332, 337, 338, 340, 343, 344, 350, 351, 352], "r1": [118, 124], "r2": [8, 191, 192], "race": [267, 283, 285, 288, 291], "radio": [267, 283], "rafe": [267, 283], "ragnarok": [267, 283], "rahul": [267, 283], "rai": [17, 19, 20, 22, 25, 27, 34, 42, 49, 50, 51, 52, 61, 62, 64, 65, 79, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 96, 98, 99, 106, 109, 115, 116, 117, 120, 122, 123, 125, 126, 127, 129, 130, 131, 133, 134, 135, 136, 138, 139, 141, 142, 143, 145, 146, 148, 149, 150, 152, 153, 156, 157, 162, 163, 177, 178, 179, 183, 184, 185, 186, 187, 188, 195, 201, 202, 203, 204, 210, 211, 212, 214, 215, 216, 222, 223, 226, 227, 229, 230, 231, 232, 233, 235, 236, 247, 248, 249, 250, 251, 256, 262, 269, 272, 273, 280, 281, 282, 283, 296, 298, 299, 306, 309, 314, 315, 320, 323, 326, 329, 333, 335, 336, 342, 348, 352, 355, 356, 357, 358], "railwai": [285, 292], "rais": [3, 181, 185, 317, 321, 323, 324, 328, 350, 359], "ram": [10, 206, 212], "ramen": [350, 351], "rammstein": [267, 283], "rand": [3, 11, 16, 164, 167, 181, 183, 187, 218, 222], "randint": [3, 5, 6, 7, 12, 14, 118, 123, 159, 163, 181, 189, 224, 226, 237, 253, 257, 259, 262, 317, 320, 324, 326], "randn": [317, 323, 324, 326, 329], "randn_lik": [6, 259, 262, 317, 320], "random": [2, 3, 5, 7, 9, 10, 11, 12, 14, 15, 16, 35, 41, 53, 62, 88, 89, 118, 123, 159, 163, 164, 167, 175, 176, 181, 182, 183, 185, 187, 189, 198, 204, 206, 215, 218, 222, 224, 226, 237, 253, 257, 317, 319, 323, 324, 325, 326, 329, 330, 332, 336, 337, 339, 350, 351, 352], "random_shuffl": [9, 10, 15, 198, 204, 206, 215, 317, 319, 324, 326, 337, 339], "random_st": [4, 171, 174, 337, 339, 350, 353], "randomize_block_ord": [9, 10, 15, 198, 204, 206, 215, 330, 332], "randomli": [5, 9, 10, 15, 128, 130, 198, 204, 206, 215, 224, 226, 330, 336], "rang": [2, 3, 5, 7, 10, 12, 13, 14, 16, 17, 22, 88, 89, 128, 130, 137, 143, 146, 159, 163, 175, 180, 181, 183, 185, 189, 206, 212, 224, 226, 228, 237, 238, 240, 243, 245, 247, 253, 256, 257, 285, 289, 293, 298, 302, 307, 313, 317, 323, 324, 326, 329, 330, 334, 337, 341, 343, 344, 345, 347, 350, 353, 355], "rank": [4, 5, 6, 13, 118, 121, 171, 174, 225, 227, 228, 233, 235, 245, 248, 252, 259, 263, 317, 321, 334, 336, 337, 340, 341, 342, 343, 347, 350, 354, 355], "rap": [267, 283], "rapid": [82, 83], "rapidli": [80, 81], "rate": [6, 7, 14, 24, 27, 101, 102, 105, 164, 167, 224, 228, 253, 257, 259, 262, 267, 283, 285, 288, 293, 298, 299, 317, 323, 324, 329, 333, 334, 336, 343, 349, 350, 359], "rather": [10, 101, 102, 104, 206, 211, 285, 288, 292, 330, 336, 337, 339, 343, 344], "ratings_d": [330, 332], "ratings_parquet": [330, 332], "ratings_parquet_uri": [330, 332], "ratio": [137, 144, 168, 337, 339, 341], "rattl": [285, 292], "rattler": [267, 283], "ravenstein": [285, 292], "raw": [8, 12, 13, 14, 53, 57, 159, 162, 163, 191, 192, 238, 242, 243, 267, 283, 293, 298, 317, 319, 323, 330, 332, 336, 337, 338, 339, 341, 343, 345, 350, 351, 352, 359], "raw_path": [330, 332], "ray_actor_opt": [16, 147, 149], "ray_address": [137, 138], "ray_data_synthet": [159, 163], "ray_dedup_log": [14, 293, 300], "ray_enable_windows_or_osx_clust": [155, 158], "ray_pl_ckpt": [317, 321, 324, 328], "ray_result": [14, 293, 300], "ray_runtime_env_hook": [128, 129, 137, 138, 147, 148], "ray_scheduler_ev": [13, 14, 16, 128, 133, 137, 139, 147, 150], "ray_train_v2_en": [137, 138, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "ray_wheel_url": [126, 127], "rayddp": [317, 321], "rayddpstrategi": [6, 259, 260, 263, 317, 321, 324, 328], "raylightningenviron": [6, 259, 260, 263, 317, 319, 321, 324, 326, 328], "rayproject": [84, 85], "rayserv": [306, 307, 315], "raytaskerror": [3, 181, 185], "raytrainreportcallback": [6, 259, 260, 263, 317, 318, 321, 324, 325, 328, 329, 337, 338, 339, 340, 341, 342], "raytrainwork": [13, 293, 300], "raytrainxgboosttrain": [4, 171, 172, 174], "rayturbo": [10, 11, 126, 127, 128, 132, 137, 144, 147, 151, 206, 208, 218, 220], "rbac": [96, 98, 99], "rbi": [267, 281, 283], "rd": [17, 22, 267, 283, 337, 339], "rdata": [343, 345, 349, 350, 359], "re": [1, 3, 7, 10, 11, 15, 28, 31, 33, 35, 40, 42, 43, 46, 47, 53, 58, 59, 66, 75, 78, 79, 82, 83, 88, 89, 90, 91, 92, 93, 110, 114, 115, 118, 125, 128, 130, 131, 134, 137, 139, 144, 147, 154, 169, 170, 181, 185, 206, 215, 218, 223, 224, 231, 232, 237, 245, 251, 252, 253, 256, 267, 283, 285, 288, 317, 318, 319, 323, 324, 325, 329, 330, 331, 336, 337, 338, 339, 342, 343, 349, 350, 351, 352], "reach": [24, 26, 101, 102, 104, 245, 250, 285, 291], "read": [2, 3, 5, 6, 8, 11, 12, 13, 17, 22, 96, 98, 128, 129, 130, 132, 175, 180, 181, 184, 188, 189, 191, 192, 201, 203, 207, 210, 218, 223, 224, 226, 237, 238, 239, 242, 259, 264, 285, 291, 292, 317, 319, 330, 332, 337, 339, 343, 345, 350, 351, 352, 353, 359], "read_csv": [5, 9, 13, 198, 201, 330, 332, 336, 343, 345], "read_databricks_t": [10, 206, 210], "read_imag": [10, 15, 16, 128, 130, 137, 139, 146, 206, 210, 212, 215], "read_json": [9, 198, 201], "read_parquet": [4, 6, 9, 10, 12, 128, 136, 137, 143, 164, 166, 171, 174, 198, 201, 204, 206, 210, 238, 242, 259, 262, 317, 319, 330, 332, 337, 339, 350, 359], "read_row_group": [350, 353], "read_tabl": [343, 345, 350, 353], "readabl": [9, 198, 201, 224, 234], "readfil": [10, 206, 214], "readi": [1, 3, 28, 31, 35, 40, 43, 46, 47, 53, 58, 59, 66, 75, 79, 80, 81, 82, 83, 92, 93, 101, 102, 109, 110, 112, 117, 118, 121, 122, 125, 128, 131, 132, 169, 170, 181, 189, 224, 232, 238, 243, 245, 252, 267, 283, 293, 298, 324, 326, 329, 330, 331, 332, 343, 345, 350, 351, 353], "readm": [164, 167, 278, 286, 294, 312, 360], "readme_01": 360, "ready_ref": [3, 181, 189], "real": [8, 101, 102, 104, 118, 123, 125, 128, 130, 191, 195, 267, 283, 285, 289, 293, 298, 325, 329, 330, 331, 336, 337, 342, 343, 349, 350, 351], "realist": [285, 292, 317, 318, 330, 332, 350, 359], "realiti": [285, 288], "realknowncaus": [343, 345], "realli": [267, 283, 285, 288, 291, 292], "reason": [86, 87, 101, 102, 105, 107, 110, 112, 118, 124, 159, 161, 324, 329], "reasoning_pars": [118, 123], "reassur": [350, 353], "rebuild": [0, 224, 237, 330, 336], "rec": [317, 319, 343, 345, 350, 352], "rec_sys_tutori": [330, 332, 334, 336], "recal": [16, 137, 146], "recalcul": [101, 102, 105], "recap": [7, 14, 253, 257], "receiv": [5, 6, 8, 24, 26, 118, 121, 123, 191, 197, 224, 225, 259, 263, 302, 307, 313, 330, 331, 332, 334, 343, 345], "recent": [86, 87, 224, 236, 245, 250, 285, 292, 317, 321, 324, 328, 330, 335, 343, 349, 350, 355, 357], "recent_kei": [86, 87], "recent_nam": [86, 87], "recip": [5, 267, 283], "recipi": 168, "reclaim": [317, 323, 330, 336], "recommend": [0, 4, 5, 6, 7, 8, 9, 10, 11, 17, 22, 24, 26, 84, 85, 101, 102, 103, 110, 111, 119, 121, 122, 164, 165, 171, 172, 191, 195, 198, 199, 202, 206, 207, 217, 218, 219, 223, 224, 226, 234, 245, 252, 253, 254, 259, 260, 262, 333], "recomput": [330, 336], "record": [4, 9, 12, 137, 139, 155, 157, 171, 174, 198, 201, 317, 319, 343, 345, 350, 352, 357], "recov": [10, 128, 132, 147, 153, 206, 208, 245, 246, 317, 318, 337, 342, 343, 344, 349], "recoveri": [224, 234, 245, 246, 248, 252, 317, 318, 324, 329, 330, 331, 334, 343, 347, 350, 351, 359], "recreat": [16, 343, 349], "recurr": [343, 344], "recurs": [28, 33, 43, 51, 53, 64, 245, 251], "red": [101, 102, 105, 267, 283, 285, 292, 317, 318, 350, 351], "redefin": [4, 171, 174], "redeploi": [324, 325], "redi": [17, 21], "redshift": [8, 191, 192], "reduc": [4, 7, 8, 9, 10, 11, 84, 85, 101, 102, 103, 118, 120, 122, 126, 127, 128, 130, 147, 151, 171, 172, 191, 192, 198, 199, 206, 207, 211, 212, 218, 219, 253, 254, 267, 277, 284, 293, 299, 343, 349], "reducelronplateau": [137, 143], "reduct": [110, 116], "redund": [101, 102, 105, 147, 151, 224, 234, 350, 359], "ref": [2, 3, 128, 135, 137, 145, 147, 153, 175, 179, 180, 181, 183, 184, 187, 189, 190], "refer": [2, 3, 6, 9, 10, 13, 15, 17, 22, 28, 29, 35, 37, 43, 44, 53, 54, 55, 66, 67, 68, 82, 83, 86, 87, 96, 98, 101, 102, 104, 128, 131, 155, 157, 158, 164, 167, 175, 177, 179, 181, 183, 184, 185, 187, 198, 201, 206, 212, 216, 224, 236, 237, 259, 262, 293, 297], "reflect": [8, 191, 192, 350, 357], "refresh": [285, 291], "reg": [4, 171, 174], "regard": [267, 283], "regardless": [5, 6, 259, 262, 343, 347], "region": [12, 13, 14, 17, 23, 28, 30, 35, 38, 39, 43, 45, 46, 48, 53, 56, 58, 60, 66, 69, 70, 72, 76, 86, 87, 118, 121], "regist": [20, 22, 24, 26, 29, 30, 33, 34, 36, 39, 41, 44, 45, 49, 52, 54, 56, 61, 65, 67, 70, 84, 85, 96, 98, 100, 128, 129, 137, 138, 147, 148, 155, 157, 164, 165, 305, 306, 307, 315, 317, 323, 330, 336, 350, 359], "register_buff": [343, 346], "register_us": 168, "registr": [35, 39, 42, 66, 78, 79], "registration_complet": 168, "registri": [126, 127, 138, 144, 147, 150, 350, 359], "regress": [7, 14, 253, 257, 337, 342], "regular": [2, 88, 89, 175, 178, 224, 227, 330, 336, 350, 353, 359], "reimplement": [238, 240], "reinforc": [8, 191, 195], "rel": [0, 5, 343, 349], "rel_path": [118, 121], "relat": [43, 51, 53, 57, 64, 88, 89, 168], "relationship": [7, 14, 17, 19, 22, 253, 257], "releas": [1, 28, 30, 43, 45, 51, 53, 56, 64, 137, 138, 169, 170, 224, 237, 267, 283, 285, 288, 324, 329], "relev": [5, 9, 10, 15, 118, 121, 198, 204, 205, 206, 215, 330, 336, 343, 347, 350, 353], "reli": [8, 11, 191, 192, 195, 197, 218, 220, 238, 239, 337, 339, 350, 352], "reliabl": [5, 6, 8, 10, 90, 91, 92, 93, 118, 120, 122, 125, 126, 127, 128, 132, 191, 195, 206, 208, 224, 225, 245, 246, 252, 259, 261, 350, 355], "religi": [267, 283], "religion": [267, 283], "reload": [11, 118, 121, 218, 223, 224, 237, 245, 248, 249, 330, 336], "relpath": [118, 121], "relu": [13, 137, 140, 317, 320, 324, 327], "remain": [110, 112, 159, 163, 224, 227, 234, 267, 281, 283, 284, 317, 319, 330, 332, 343, 347], "remaind": [330, 332], "remark": [267, 283], "remast": [267, 283], "remateri": [337, 339], "rememb": [88, 89, 90, 91, 92, 93, 110, 116, 224, 234, 285, 292, 317, 323], "remind": [267, 283, 285, 292], "remot": [1, 3, 4, 7, 10, 11, 15, 16, 82, 83, 88, 89, 90, 91, 126, 127, 147, 150, 168, 169, 170, 171, 174, 176, 180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 206, 210, 218, 222, 224, 237, 253, 256, 337, 338, 341, 342, 350, 359], "remote_add": [2, 3, 175, 178, 179, 181, 184, 187, 188], "remote_funct": [2, 175, 179], "remote_path": [10, 11, 206, 213, 218, 222], "remov": [1, 3, 6, 9, 43, 51, 53, 64, 101, 102, 107, 128, 131, 137, 138, 169, 170, 181, 190, 198, 203, 224, 226, 234, 245, 251, 259, 262, 330, 336, 350, 359], "remove_code_output": 0, "remu": [267, 281, 283, 284], "renam": [82, 83, 337, 339, 343, 345], "renew": [24, 27], "rent": [285, 288], "repackag": [324, 325], "repartit": [9, 159, 163, 198, 204, 266, 267, 271, 279, 281, 284], "repeat": [13, 14, 293, 300, 337, 341, 343, 346], "repeatedli": [137, 139, 267, 273, 282], "replac": [28, 29, 30, 31, 35, 36, 38, 40, 41, 43, 44, 45, 47, 48, 53, 54, 56, 57, 59, 60, 62, 66, 67, 69, 72, 75, 76, 82, 83, 86, 87, 92, 93, 101, 102, 105, 107, 118, 121, 159, 163, 238, 239, 240, 243, 267, 283, 285, 292, 317, 323, 324, 325, 329, 330, 336, 343, 349, 350, 359], "replic": [3, 5, 6, 181, 183, 224, 225, 234, 259, 263], "replica": [8, 16, 92, 93, 101, 102, 107, 108, 113, 118, 121, 147, 151, 152, 153, 164, 167, 168, 191, 197, 220, 224, 225, 305, 306, 307, 311, 315, 316, 350, 359], "replica_handle_request": [164, 167, 168], "repo": [0, 43, 46, 48, 53, 58, 60, 66, 73, 76, 101, 102, 108, 118, 121, 126, 127], "repo_id": [118, 121], "report": [6, 7, 8, 12, 14, 16, 191, 193, 225, 226, 227, 228, 236, 238, 240, 244, 245, 247, 248, 252, 253, 256, 257, 259, 263, 267, 283, 317, 318, 320, 321, 323, 324, 325, 326, 330, 331, 332, 334, 337, 340, 343, 344, 347, 349, 350, 351, 355, 357, 359], "report_metrics_torch": [5, 13], "reportedli": [267, 283], "repositori": [0, 66, 73, 92, 93], "repres": [4, 7, 9, 12, 101, 102, 106, 171, 174, 198, 201, 253, 255, 285, 292, 330, 331, 333], "represent": [101, 102, 104, 106], "reproduc": [1, 169, 170, 278, 286, 294, 312, 317, 319, 330, 332, 337, 338, 339, 350, 353], "republican": [267, 283], "req": [82, 83], "request": [2, 4, 8, 10, 11, 12, 16, 24, 26, 27, 101, 102, 104, 105, 107, 113, 116, 118, 121, 122, 123, 128, 136, 147, 148, 150, 164, 167, 171, 172, 174, 175, 180, 182, 188, 191, 197, 206, 213, 218, 219, 220, 221, 222, 224, 237, 302, 306, 313, 315, 343, 345, 349, 350, 359], "request_data": 12, "requir": [0, 3, 5, 6, 8, 9, 10, 11, 15, 17, 19, 20, 21, 22, 24, 26, 27, 37, 39, 46, 52, 58, 65, 68, 74, 78, 79, 84, 85, 86, 87, 90, 91, 96, 98, 100, 105, 107, 110, 112, 113, 114, 117, 122, 123, 126, 127, 128, 129, 135, 137, 138, 144, 147, 148, 168, 181, 187, 188, 191, 192, 195, 198, 204, 206, 214, 215, 218, 220, 221, 224, 225, 231, 238, 239, 245, 248, 259, 260, 261, 267, 275, 278, 283, 286, 293, 294, 298, 299, 312, 317, 318, 324, 325, 330, 332, 343, 345, 350, 351, 355, 357], "rerun": [128, 130, 337, 342], "res18": [238, 244], "resampl": 344, "rescal": [317, 323], "research": [110, 112, 117, 118, 122, 266, 267, 279, 293, 299], "reserv": [2, 3, 4, 5, 6, 7, 8, 9, 10, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 101, 102, 103, 110, 111, 118, 119, 126, 127, 155, 156, 159, 160, 164, 165, 171, 172, 175, 176, 181, 182, 187, 191, 192, 198, 199, 206, 207, 253, 254, 259, 260, 266, 267, 279, 285, 287, 293, 295, 302, 307, 313], "reset": [13, 128, 136, 137, 143, 146, 147, 154, 224, 228, 324, 326], "reshap": [6, 10, 206, 208, 259, 262], "resid": [343, 349], "residu": [343, 344], "resili": [92, 93, 110, 115, 245, 246, 248, 252, 324, 325, 337, 338, 343, 344], "resiz": [128, 133, 136, 137, 146, 351], "resnet": [13, 225, 226, 231, 237, 317, 323, 350, 351], "resnet18": [5, 7, 13, 14, 224, 226, 227, 234, 236, 253, 254, 256, 257, 350, 352, 355, 359], "resolut": [6, 259, 262, 263], "resolv": [5, 53, 63], "resourc": [5, 6, 7, 8, 11, 12, 13, 14, 16, 18, 19, 22, 23, 24, 25, 26, 27, 33, 34, 36, 37, 42, 50, 51, 52, 57, 62, 63, 64, 65, 68, 78, 79, 80, 81, 84, 85, 88, 89, 90, 91, 92, 93, 96, 98, 99, 104, 105, 112, 124, 126, 127, 128, 130, 132, 137, 144, 147, 149, 151, 155, 157, 164, 166, 182, 188, 190, 191, 196, 197, 208, 218, 220, 221, 224, 227, 230, 235, 237, 238, 244, 245, 248, 250, 253, 257, 259, 263, 266, 267, 279, 285, 290, 293, 295, 299, 300, 324, 325, 329, 343, 344, 350, 351], "resources_per_work": [137, 143, 293, 299, 337, 340], "resp": [164, 167], "respect": [285, 291, 292], "respond": [118, 121], "respons": [11, 16, 24, 27, 84, 85, 86, 87, 101, 102, 104, 106, 108, 110, 114, 115, 118, 120, 121, 122, 123, 147, 150, 164, 167, 168, 218, 222, 302, 307, 309, 313, 316], "response_format": [118, 122], "rest": [43, 51, 53, 64, 224, 227, 238, 240, 245, 247, 285, 292, 302, 307, 313, 330, 332, 350, 352], "restart": [1, 84, 85, 86, 87, 118, 121, 128, 129, 136, 137, 138, 146, 147, 148, 153, 154, 169, 170, 224, 226, 245, 246, 248, 249, 307, 309, 316, 324, 328, 329, 330, 334, 337, 338], "restor": [224, 236, 246, 247, 248, 252, 285, 292, 317, 318, 323, 330, 335, 336], "restored_train": [245, 250], "restrict": [94, 95], "result": [1, 4, 7, 10, 12, 14, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 88, 89, 90, 91, 118, 123, 128, 130, 137, 143, 147, 154, 169, 170, 171, 174, 176, 180, 182, 183, 184, 190, 206, 208, 225, 226, 227, 234, 235, 237, 245, 250, 252, 253, 257, 266, 267, 279, 285, 291, 292, 293, 298, 299, 300, 305, 306, 307, 309, 315, 316, 317, 321, 322, 324, 325, 328, 330, 332, 334, 335, 336, 337, 340, 342, 347, 348, 350, 352, 356, 357, 358, 359], "resum": [5, 6, 126, 127, 128, 132, 224, 225, 234, 246, 247, 248, 249, 252, 259, 261, 318, 321, 323, 324, 325, 328, 331, 334, 336, 337, 338, 340, 342, 347, 349, 350, 351, 355, 358], "resume_from_checkpoint": [330, 335], "retain": [88, 89, 159, 162, 337, 340, 343, 347], "retent": [285, 292, 350, 351], "rethink": [285, 292], "retrain": [324, 329, 330, 331, 337, 342, 343, 349, 350, 359], "retri": [5, 6, 10, 137, 144, 182, 206, 212, 224, 225, 246, 249, 250, 252, 259, 261, 317, 321, 324, 325, 330, 331, 334, 337, 340, 350, 351, 355, 356], "retriev": [2, 3, 128, 136, 175, 179, 181, 190, 224, 237, 238, 241, 330, 334, 337, 340], "retrigg": [147, 154], "retry_except": [3, 181, 185], "return": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 82, 83, 88, 89, 90, 91, 118, 122, 123, 128, 130, 131, 136, 137, 139, 140, 141, 143, 146, 147, 149, 150, 159, 163, 164, 166, 169, 170, 171, 174, 175, 178, 179, 180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 198, 202, 206, 212, 213, 215, 218, 222, 223, 224, 227, 228, 231, 232, 233, 236, 237, 238, 241, 243, 245, 250, 253, 255, 257, 259, 262, 267, 272, 273, 275, 282, 283, 285, 292, 293, 297, 298, 305, 306, 307, 315, 317, 319, 320, 323, 324, 326, 327, 329, 330, 331, 332, 333, 337, 339, 340, 341, 342, 343, 345, 346, 349, 350, 353, 354, 359], "return_tensor": [128, 131, 147, 149], "reus": [3, 10, 15, 17, 22, 101, 102, 105, 181, 183, 206, 213, 224, 226, 237, 267, 273, 282, 337, 339, 341, 342, 343, 349], "reusabl": [343, 344], "reveal": [330, 332], "reveng": [285, 291], "revers": [147, 150, 337, 341, 343, 345, 350, 359], "review": [10, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 86, 87, 206, 210, 285, 287, 290, 291, 292, 293, 295, 300], "rewrit": [5, 6, 224, 225, 259, 261, 330, 336, 350, 351], "rf": [4, 5, 6, 7, 9, 10, 12, 13, 15, 16, 171, 174, 198, 203, 205, 206, 217, 253, 258, 259, 264], "rg_idx": [350, 353], "rg_meta": [350, 353], "rgb": [128, 131, 136, 147, 149, 224, 227, 237, 317, 318, 319, 350, 351, 353, 359], "rice": [350, 351], "rich": [8, 82, 83, 191, 195], "richer": [324, 329], "rick": [267, 283], "ricki": [267, 283], "ride": [4, 12, 171, 174, 285, 291, 343, 344, 349], "ridicul": [285, 292], "ridlei": [285, 292], "rifl": [285, 291], "riget": [285, 292], "right": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 16, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 82, 83, 101, 102, 103, 105, 110, 111, 118, 119, 124, 125, 128, 129, 137, 138, 147, 148, 155, 156, 159, 160, 164, 165, 171, 172, 175, 176, 180, 181, 182, 191, 192, 198, 199, 206, 207, 211, 218, 222, 224, 227, 234, 238, 241, 253, 254, 259, 260, 266, 267, 279, 283, 285, 287, 291, 292, 293, 295, 302, 307, 313, 324, 325], "rightarrow": [337, 338], "rigid": [8, 191, 194], "rip": [285, 291], "rise": [350, 357], "risibl": [285, 288], "risk": [285, 292], "riskbr": [285, 292], "river": [267, 283, 285, 292], "riverboat": [285, 291], "rkn": [159, 163], "rllib": [8, 191, 195], "rm": [4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 16, 28, 33, 35, 42, 43, 51, 53, 64, 66, 78, 171, 174, 198, 203, 205, 206, 217, 218, 223, 253, 258, 259, 264], "rmse": [4, 7, 14, 171, 174, 253, 257, 330, 336], "rmtree": [128, 133, 137, 139, 142, 245, 251, 317, 323, 324, 329, 330, 332, 336, 337, 342, 343, 349, 350, 359], "road": [337, 338], "roadmap": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 101, 102, 103, 110, 111, 118, 119, 164, 165, 171, 172, 175, 176, 181, 182, 191, 192, 198, 199, 206, 207, 218, 219, 224, 227, 253, 254, 259, 260], "roar": [285, 291], "robert": [285, 291], "robin": [24, 27, 267, 283], "robot": [324, 329], "robust": [0, 8, 118, 122, 191, 192, 245, 249, 252, 324, 325, 343, 349, 350, 351], "rock": [267, 281, 283], "role": [21, 23, 24, 26, 27, 28, 30, 34, 35, 37, 43, 45, 56, 63, 66, 68, 86, 87, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 159, 162, 285, 292], "roll": [92, 93, 110, 115, 164, 167, 267, 283, 324, 326], "rollin": [267, 283], "rollout": [11, 147, 151, 153, 164, 167, 218, 220, 324, 325], "roma": [267, 283], "roman": [285, 291], "ronda": [285, 291], "roof": [267, 283], "root": [2, 5, 6, 7, 13, 14, 86, 87, 118, 121, 155, 158, 164, 167, 168, 175, 180, 224, 225, 226, 232, 253, 255, 257, 259, 261, 330, 336], "roughli": [317, 319, 324, 326, 350, 352], "round": [24, 27, 267, 283, 285, 292, 330, 332, 337, 338, 340, 342], "rout": [3, 11, 16, 17, 22, 24, 26, 27, 101, 102, 106, 168, 181, 190, 218, 220, 306, 307, 315], "route_prefix": [4, 16, 92, 93, 118, 122, 147, 150, 171, 174], "row": [5, 6, 128, 130, 131, 137, 139, 146, 164, 166, 201, 210, 212, 224, 226, 236, 237, 238, 242, 243, 259, 262, 266, 267, 270, 271, 279, 281, 283, 284, 285, 287, 288, 289, 290, 291, 292, 317, 319, 321, 330, 331, 332, 334, 336, 337, 338, 339, 341, 343, 345, 349, 350, 351, 353, 357, 359], "row_group": [350, 353], "row_group_idx": [350, 353], "row_group_map": [350, 353], "royal": [267, 283], "rpc": [3, 8, 181, 190, 191, 192], "rsplit": [128, 130, 137, 139], "rstrip": [86, 87], "rubbish": [285, 292], "rubbl": [285, 291, 292], "rube": [285, 291], "ruin": [285, 292], "rule": [8, 17, 22, 84, 85, 118, 121, 191, 197, 267, 283], "rumor": [267, 283], "run": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 19, 21, 25, 27, 28, 30, 31, 32, 33, 34, 40, 41, 42, 43, 45, 46, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 61, 62, 64, 65, 66, 69, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 96, 98, 100, 101, 102, 103, 106, 108, 111, 112, 114, 116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 132, 134, 135, 137, 138, 144, 147, 148, 150, 151, 153, 155, 157, 158, 159, 161, 162, 163, 165, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 181, 182, 184, 186, 190, 191, 196, 198, 199, 200, 201, 203, 205, 206, 207, 208, 211, 213, 217, 218, 219, 220, 221, 222, 223, 225, 226, 227, 228, 229, 230, 232, 234, 235, 236, 238, 239, 243, 244, 245, 246, 249, 250, 251, 252, 253, 254, 256, 257, 258, 260, 262, 264, 266, 273, 279, 282, 283, 285, 287, 291, 292, 293, 295, 298, 299, 300, 302, 307, 309, 311, 313, 315, 316, 317, 318, 319, 321, 322, 323, 324, 325, 328, 329, 330, 331, 334, 335, 336, 337, 338, 340, 342, 344, 347, 348, 351, 353, 355, 356, 357, 358], "run_command": [82, 83], "run_config": [4, 5, 6, 12, 13, 171, 174, 224, 234, 235, 238, 244, 245, 248, 250, 259, 263, 317, 321, 324, 328, 330, 334, 337, 340, 343, 347, 350, 356], "run_id": [137, 144], "runawai": [267, 283], "runconfig": [4, 5, 6, 12, 13, 171, 172, 174, 225, 226, 238, 244, 245, 248, 250, 252, 259, 263, 317, 319, 321, 324, 326, 328, 330, 332, 334, 337, 338, 339, 340, 343, 344, 345, 347, 350, 351, 352, 355, 356, 357, 359], "runnabl": [164, 165], "runnam": [137, 144], "runnng": [88, 89], "runtim": [11, 80, 81, 84, 85, 118, 120, 121, 126, 127, 128, 129, 137, 138, 147, 148, 164, 167, 182, 218, 221, 293, 296, 307, 309, 315, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352, 360], "runtime_env": [3, 110, 113, 116, 118, 121, 122, 123, 128, 129, 137, 138, 147, 148, 181, 186, 187, 293, 296], "runtimeenv": [293, 296], "runwai": [9, 15, 198, 205], "ruse": [285, 291], "rust": [8, 191, 192], "ruth": [285, 291], "rw": 13, "ryan": [267, 283], "s3": [4, 8, 9, 10, 11, 12, 13, 14, 15, 16, 21, 23, 24, 26, 28, 30, 33, 34, 43, 45, 51, 53, 56, 57, 63, 64, 86, 87, 118, 121, 128, 130, 136, 137, 139, 146, 147, 150, 153, 164, 166, 171, 174, 191, 192, 198, 201, 203, 204, 206, 210, 212, 213, 215, 218, 222, 224, 234, 245, 252, 317, 323, 330, 336, 350, 359], "s3_bucket_id": [17, 23, 28, 30], "s3_f": [164, 166], "s3_kei": [86, 87, 118, 121], "s3_path": [6, 259, 262], "s3f": [6, 259, 260, 262], "s3filesystem": [6, 164, 166, 259, 262], "s5": [101, 102, 105], "s6": [101, 102, 105, 267, 283], "s7": [101, 102, 105], "s_": [324, 325], "s_k": [324, 325], "saatchi": [267, 283], "sacrif": [324, 325], "safe": [28, 30, 43, 45, 53, 56, 224, 226, 245, 246, 267, 283, 317, 319, 343, 347, 350, 355], "safetensor": [118, 121], "safeti": [118, 122], "sagemak": [128, 130, 147, 151], "sai": [3, 17, 22, 90, 91, 181, 184, 267, 283, 285, 291, 292], "said": [267, 283, 285, 288], "sake": [9, 198, 202], "salad": [350, 351], "sam": [267, 283], "samara": [128, 136, 147, 150, 153], "same": [1, 3, 4, 5, 6, 7, 9, 11, 12, 13, 14, 16, 24, 26, 28, 30, 43, 45, 53, 56, 86, 87, 90, 91, 101, 102, 108, 110, 115, 117, 118, 125, 126, 127, 128, 131, 137, 139, 146, 169, 170, 171, 174, 181, 186, 198, 202, 204, 218, 221, 222, 224, 227, 230, 231, 232, 234, 238, 240, 243, 244, 245, 247, 250, 252, 253, 257, 259, 262, 263, 267, 283, 284, 285, 288, 292, 317, 318, 319, 321, 324, 329, 330, 336, 337, 340, 342, 343, 349, 350, 353, 357, 359], "sampl": [4, 5, 6, 7, 12, 14, 15, 16, 17, 22, 28, 30, 34, 35, 39, 43, 45, 46, 53, 56, 58, 66, 70, 73, 86, 87, 137, 141, 155, 157, 171, 174, 225, 228, 229, 237, 253, 257, 259, 262, 285, 288, 292, 306, 307, 315, 319, 326, 330, 336, 337, 339, 343, 345, 350, 351, 352, 353, 359], "sample_act": [324, 329], "sample_batch": [12, 137, 141, 337, 341], "sample_count": [88, 89], "sample_idx": [5, 224, 226], "sample_imag": [317, 323], "sample_s": [6, 259, 262], "sampler": [5, 6, 13, 224, 225, 228, 238, 240, 259, 263, 350, 355], "samsara": 16, "samsung": [267, 283], "san": [118, 123, 267, 283], "sander": [267, 283], "saniti": [224, 226, 235, 321, 324, 328, 330, 332, 337, 339], "sat": [267, 283], "satisfi": [3, 181, 183], "satur": [159, 163], "saturdai": [267, 283], "save": [3, 5, 6, 53, 63, 84, 85, 86, 87, 88, 89, 101, 102, 106, 118, 121, 126, 127, 128, 130, 132, 133, 137, 138, 139, 140, 142, 143, 181, 187, 225, 227, 228, 236, 238, 240, 244, 246, 247, 250, 252, 259, 263, 285, 291, 317, 318, 319, 320, 323, 324, 325, 328, 329, 330, 334, 335, 336, 337, 338, 340, 342, 343, 344, 347, 350, 351, 352, 355, 359], "save_checkpoint_and_metrics_ray_train": [5, 13, 224, 228, 234, 238, 240], "save_checkpoint_and_metrics_ray_train_with_extra_st": [245, 247, 248], "save_checkpoint_and_metrics_torch": [5, 13], "save_hyperparamet": [6, 259, 262], "save_last": [317, 321, 324, 328], "save_model": [4, 171, 174], "save_top_k": [317, 321, 324, 328], "saw": [267, 283], "sayhellodebuglog": [164, 167], "sayhellodefaultlog": [164, 167], "scaffold": [11, 218, 223], "scala": [8, 191, 195], "scalabl": [4, 8, 9, 11, 12, 16, 17, 22, 86, 87, 90, 91, 92, 93, 96, 99, 100, 104, 109, 110, 117, 118, 125, 126, 127, 128, 130, 132, 147, 151, 153, 171, 173, 191, 192, 195, 196, 198, 200, 218, 219, 221, 238, 239, 245, 252, 266, 267, 277, 279, 284, 285, 287, 292, 293, 299, 300, 302, 307, 313, 317, 318, 324, 329, 330, 331, 332, 337, 338, 341, 342, 343, 344, 349, 350, 351, 353], "scalar": [3, 181, 190, 317, 320, 324, 325, 327], "scale": [1, 2, 3, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 22, 24, 25, 26, 27, 43, 50, 53, 62, 63, 79, 84, 85, 88, 89, 92, 93, 96, 99, 100, 101, 102, 103, 106, 107, 112, 113, 115, 118, 124, 126, 127, 128, 132, 137, 138, 143, 144, 147, 149, 151, 153, 159, 163, 169, 170, 175, 177, 181, 187, 191, 192, 195, 197, 198, 201, 205, 206, 212, 213, 218, 221, 225, 226, 227, 232, 235, 238, 239, 243, 245, 246, 248, 250, 252, 253, 257, 261, 262, 266, 277, 279, 284, 285, 287, 293, 295, 299, 302, 311, 313, 316, 317, 318, 319, 323, 326, 330, 331, 332, 333, 336, 337, 338, 339, 342, 343, 344, 345, 349, 350, 351, 352, 353, 359], "scaling_config": [4, 5, 6, 12, 13, 137, 143, 171, 174, 224, 230, 235, 238, 244, 245, 248, 250, 259, 263, 293, 299, 317, 321, 324, 328, 330, 334, 337, 340, 343, 347, 350, 356], "scalingconfig": [4, 5, 6, 12, 13, 137, 143, 171, 174, 225, 226, 245, 252, 259, 263, 293, 296, 299, 317, 318, 319, 321, 324, 325, 326, 328, 330, 331, 332, 334, 337, 338, 339, 340, 343, 344, 345, 347, 350, 352, 356], "scan": [285, 291], "scari": [285, 292], "scenario": [3, 8, 16, 17, 22, 28, 30, 43, 45, 53, 56, 110, 112, 117, 181, 187, 189, 191, 192, 293, 298, 350, 351], "scene": [285, 288, 292, 330, 331], "schedul": [2, 3, 4, 5, 6, 7, 9, 10, 12, 14, 24, 26, 88, 89, 101, 102, 107, 128, 132, 135, 137, 143, 147, 151, 171, 174, 175, 177, 179, 180, 181, 187, 188, 190, 198, 203, 206, 212, 213, 253, 257, 259, 262, 267, 283, 293, 300, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 351, 359], "schema": [8, 9, 118, 122, 123, 191, 192, 198, 201, 267, 281, 284, 285, 289, 290, 317, 319, 330, 332, 343, 345], "schemat": [5, 6, 224, 225, 259, 263], "schlong": [285, 288], "school": [267, 283], "schumer": [267, 283], "scienc": [8, 191, 195], "scientif": [8, 118, 124, 191, 195, 324, 326, 337, 339], "scikit": [302, 307, 313, 337, 339], "scipt": [164, 167], "scope": [17, 19, 86, 87, 96, 98, 159, 162], "score": [118, 121, 267, 283, 307, 309, 316, 317, 321, 323, 330, 331, 332, 336, 337, 340, 342, 343, 345, 347], "scoreless": [267, 281, 283], "scott": [285, 292], "scotu": [267, 283], "scratch": [7, 14, 28, 30, 43, 45, 53, 56, 128, 130, 253, 257, 317, 321, 324, 328, 350, 351, 359], "screen": [80, 81, 82, 83, 94, 95, 285, 292], "script": [0, 82, 83, 88, 89, 92, 93, 96, 98, 118, 121, 159, 163, 164, 167, 317, 323, 324, 329, 337, 338, 350, 351], "scroll": [53, 63], "scrumptiou": [28, 30, 43, 45, 53, 56], "sdk": [35, 37, 66, 68, 69, 90, 91, 126, 127, 128, 135], "sea": [267, 283], "seaborn": [337, 339], "seal": [285, 292], "seamless": [8, 86, 87, 101, 102, 107, 118, 123, 191, 192, 195, 196, 285, 287, 324, 325, 329, 343, 348, 349], "seamlessli": [1, 8, 9, 100, 126, 127, 169, 170, 191, 196, 198, 205, 238, 240, 245, 248, 266, 267, 279, 317, 318, 324, 325, 330, 336, 337, 338, 342, 343, 344, 345, 350, 351], "search": [4, 7, 12, 14, 17, 22, 88, 89, 126, 127, 128, 133, 171, 174, 253, 254, 257, 317, 323, 324, 329, 337, 342, 343, 349, 350, 359], "search_alg": [7, 14, 253, 257], "search_run": [137, 144, 147, 150], "season": [343, 345], "seattl": [285, 291], "second": [0, 2, 3, 7, 12, 14, 53, 57, 66, 69, 110, 113, 147, 151, 164, 166, 167, 175, 180, 181, 184, 253, 257, 267, 275, 283, 287, 337, 342, 350, 351, 358], "secondarili": [10, 15, 206, 214], "secret": [17, 22, 267, 283, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "section": [0, 7, 14, 17, 20, 21, 24, 26, 82, 83, 84, 85, 88, 89, 128, 131, 155, 157, 158, 164, 166, 167, 224, 226, 253, 256], "secur": [19, 21, 23, 24, 26, 28, 30, 34, 35, 39, 53, 63, 79, 96, 99, 101, 102, 106, 107, 110, 115, 118, 125], "security_group_descript": [17, 22], "security_group_id": [17, 23, 28, 30], "security_group_nam": [17, 22], "securitygroup": [17, 21, 22], "sedan": [118, 122], "see": [1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 22, 24, 25, 28, 30, 32, 35, 39, 41, 43, 45, 50, 53, 56, 62, 63, 66, 69, 77, 86, 87, 92, 93, 94, 95, 96, 98, 101, 102, 106, 108, 110, 113, 115, 118, 121, 122, 123, 128, 134, 137, 138, 139, 143, 144, 147, 152, 155, 158, 159, 163, 164, 166, 167, 169, 170, 175, 180, 181, 184, 190, 198, 203, 204, 205, 206, 215, 217, 218, 223, 224, 228, 230, 235, 236, 245, 246, 253, 256, 258, 259, 262, 263, 267, 278, 281, 283, 285, 286, 288, 291, 292, 293, 294, 300, 306, 307, 309, 312, 315, 316, 317, 318, 323, 324, 325, 343, 347, 349, 350, 351], "seed": [317, 323, 324, 326, 330, 332, 350, 353], "seek": [317, 319], "seem": [285, 292], "seen": [110, 117, 267, 283, 285, 288, 292], "segment": [15, 17, 22, 343, 349], "seiz": [285, 288], "select": [1, 5, 28, 30, 43, 45, 53, 56, 80, 81, 82, 83, 92, 93, 94, 95, 110, 116, 123, 125, 128, 129, 137, 138, 147, 148, 164, 166, 169, 170, 224, 226, 293, 295, 298, 330, 336, 337, 340, 343, 349, 350, 357], "select_column": [4, 171, 174], "selector": [94, 95], "self": [3, 4, 6, 10, 11, 12, 15, 16, 128, 131, 137, 139, 140, 146, 147, 149, 150, 164, 167, 171, 174, 181, 190, 206, 213, 218, 222, 224, 237, 259, 262, 267, 272, 273, 282, 305, 306, 307, 315, 317, 320, 321, 324, 327, 328, 330, 333, 337, 341, 343, 344, 345, 346, 349, 350, 351, 353, 359], "sell": [285, 291], "semant": [3, 181, 188, 190], "semi": [8, 191, 192], "send": [3, 12, 16, 118, 122, 123, 147, 150, 159, 163, 164, 167, 168, 181, 190, 302, 306, 313, 315], "send_welcome_email": 168, "sens": [4, 7, 12, 14, 171, 172, 253, 257, 285, 291, 292], "sent": [8, 164, 167, 168, 191, 197, 224, 237, 285, 291], "sentenc": [118, 121, 124, 224, 230, 266, 267, 269, 273, 275, 279, 280, 282, 283], "sentence_transform": [267, 269, 280], "sentencetransform": [266, 267, 269, 272, 273, 275, 277, 279, 280, 282, 283, 284], "sentiment": [267, 271, 281, 302, 305, 306, 307, 309, 311, 313, 315, 316], "sep": [330, 332, 336], "separ": [2, 3, 5, 7, 8, 11, 14, 86, 87, 96, 99, 147, 151, 155, 158, 175, 179, 181, 188, 191, 192, 218, 221, 224, 232, 253, 257, 330, 331, 337, 339], "sept": [267, 283], "sequenc": [9, 101, 102, 105, 198, 201, 285, 292, 293, 295, 296, 298, 300, 337, 338, 345, 346], "sequenti": [7, 13, 14, 101, 102, 104, 253, 256, 317, 320, 324, 327, 343, 344], "sequoia": [155, 158], "seri": [79, 147, 152, 345, 349], "serial": [2, 8, 175, 177, 191, 192, 195, 337, 340, 350, 351], "serializ": [317, 319], "series_id": [343, 345], "serious": [285, 288, 292], "serv": [12, 17, 19, 20, 28, 29, 43, 44, 53, 54, 92, 93, 94, 95, 105, 109, 112, 115, 116, 117, 120, 122, 123, 125, 126, 127, 149, 150, 152, 153, 154, 155, 158, 159, 162, 165, 195, 196, 222, 223, 224, 237, 245, 252, 278, 286, 294, 306, 309, 312, 314, 315, 317, 323, 324, 329, 330, 332, 333, 336, 337, 342, 343, 349, 350, 359], "serve_llama": [101, 102, 108], "serve_llama_3_1_70b": [110, 113, 114, 115, 116], "serve_my_lora_app": [118, 121], "serve_my_qwen": [118, 122], "serve_my_qwen3": [118, 123], "serveclass": 305, "server": [0, 101, 137, 144, 155, 158], "serverless": [17, 20, 126, 127], "servic": [7, 8, 17, 21, 22, 24, 26, 27, 28, 30, 35, 36, 38, 39, 43, 45, 53, 56, 63, 66, 69, 70, 73, 78, 84, 85, 86, 87, 88, 89, 94, 95, 96, 98, 101, 102, 105, 108, 111, 114, 116, 117, 126, 127, 148, 150, 151, 152, 154, 164, 167, 168, 191, 197, 219, 220, 253, 258, 278, 286, 294, 302, 307, 312, 313, 360], "service2_6hxismeqf1fkd2h7pfmljmncvm": [147, 153], "serving_01": 360, "serving_02": 360, "serving_03": 360, "serving_04": 360, "serving_05": 360, "serving_06": 360, "serving_07": 360, "session": [86, 87, 96, 98, 126, 127, 168], "session_2024": [12, 13], "session_2025": [293, 300], "session_latest": [155, 158, 164, 167, 168], "set": [0, 2, 3, 5, 6, 7, 10, 11, 12, 13, 16, 17, 18, 21, 24, 27, 28, 30, 34, 35, 38, 39, 43, 45, 46, 48, 52, 53, 56, 58, 60, 65, 66, 69, 70, 76, 80, 81, 84, 85, 92, 93, 96, 97, 111, 116, 117, 119, 121, 122, 126, 127, 128, 133, 135, 137, 139, 142, 144, 147, 150, 151, 153, 156, 164, 167, 175, 180, 181, 186, 187, 206, 212, 218, 222, 223, 224, 225, 226, 227, 230, 232, 235, 237, 245, 246, 253, 255, 257, 259, 261, 262, 263, 267, 283, 285, 290, 291, 292, 293, 295, 298, 299, 300, 306, 307, 315, 317, 319, 330, 332, 334, 337, 340, 343, 344, 347, 350, 351, 353], "set_epoch": [5, 13, 224, 228, 238, 240, 350, 355], "set_experi": [137, 143], "set_float32_matmul_precis": [324, 329], "set_grad_en": [293, 298, 343, 349, 350, 359], "set_index": [343, 345], "set_titl": [7, 13, 14, 253, 255, 317, 319, 350, 352], "set_tracking_uri": [137, 143, 144, 147, 150], "seth": [267, 283], "setup": [6, 8, 12, 17, 18, 20, 24, 25, 26, 43, 51, 53, 64, 79, 84, 85, 94, 95, 98, 99, 100, 110, 112, 116, 117, 118, 123, 126, 127, 137, 144, 157, 164, 165, 167, 191, 195, 224, 227, 235, 238, 244, 245, 247, 248, 250, 259, 262, 263, 278, 286, 294, 295, 300, 312, 321, 325, 332, 334, 339, 342, 345, 352, 360], "seven": [101, 102, 105], "sever": [8, 17, 21, 88, 89, 101, 102, 105, 106, 107, 191, 195, 285, 291, 302, 307, 313], "sevigni": [285, 288], "sex": [267, 283, 285, 288], "sexist": [267, 283], "sg": [17, 22, 28, 30], "sgd": [293, 298], "sh": [1, 43, 44, 53, 55, 66, 68, 169, 170, 317, 319], "shallow": [337, 338], "shame": [285, 292], "shape": [3, 6, 9, 10, 15, 16, 24, 26, 84, 85, 86, 87, 128, 136, 137, 146, 159, 163, 181, 187, 198, 203, 206, 212, 224, 237, 259, 262, 267, 283, 284, 317, 320, 324, 326, 329, 337, 339, 343, 345, 346, 349, 350, 353], "shard": [4, 5, 6, 13, 101, 102, 106, 171, 174, 224, 225, 227, 228, 230, 232, 234, 238, 239, 240, 241, 242, 244, 245, 248, 259, 263, 317, 318, 319, 321, 323, 324, 325, 326, 328, 329, 330, 331, 332, 334, 336, 337, 338, 340, 342, 343, 344, 345, 350, 359], "shard_0": [350, 352, 353], "share": [0, 3, 5, 8, 9, 11, 17, 21, 22, 24, 26, 28, 34, 35, 36, 53, 63, 94, 95, 100, 118, 121, 125, 128, 130, 131, 133, 137, 139, 155, 158, 181, 183, 191, 192, 198, 203, 218, 220, 224, 226, 234, 245, 251, 267, 283, 320, 324, 327, 337, 338, 339, 343, 344, 345, 349, 350, 351], "shared_path": [86, 87], "shared_storag": [86, 87], "sharetea": [267, 283], "she": [267, 283, 285, 288, 291, 292], "sheeran": [267, 283], "shell": [10, 11, 66, 69, 206, 213, 218, 222], "sheriff": [285, 291], "shift": [82, 83, 90, 91, 92, 93, 337, 339, 343, 344, 347], "shine": [267, 283, 285, 291, 292], "shippuden": [267, 283], "shit": [267, 283], "shock": [285, 288], "shoe": [285, 292], "shoot": [267, 283, 285, 291], "shootout": [285, 291], "short": [285, 288, 337, 340, 343, 344], "shorter": [110, 116], "shot": [285, 288, 292], "should": [1, 3, 5, 6, 9, 10, 11, 13, 24, 26, 53, 63, 92, 93, 118, 121, 164, 166, 169, 170, 181, 189, 190, 198, 200, 204, 206, 214, 218, 222, 224, 227, 230, 259, 263, 267, 283, 285, 288, 291, 307, 309, 316, 317, 323, 324, 329, 330, 336, 337, 342, 350, 359], "should_checkpoint": 13, "shouldn": [137, 139], "show": [3, 4, 6, 7, 10, 14, 16, 28, 33, 35, 36, 43, 51, 53, 64, 88, 89, 101, 102, 105, 107, 118, 120, 126, 127, 164, 165, 168, 171, 174, 181, 187, 206, 211, 213, 214, 224, 225, 226, 234, 235, 236, 237, 238, 240, 253, 257, 259, 262, 263, 266, 267, 270, 271, 275, 278, 279, 281, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294, 312, 317, 319, 321, 323, 324, 325, 328, 330, 332, 334, 337, 339, 341, 343, 345, 347, 349, 350, 351, 352, 357, 359], "showcas": [118, 119, 267, 275, 278, 283, 285, 286, 288, 292, 294, 302, 307, 312, 313], "shown": [164, 167, 224, 226, 285, 288], "shuffl": [5, 6, 7, 13, 14, 128, 130, 137, 139, 199, 207, 224, 228, 232, 238, 239, 240, 241, 245, 252, 253, 255, 257, 259, 262, 263, 293, 296, 298, 324, 326, 330, 332, 337, 342, 343, 345, 347, 350, 353, 354, 355], "shut": [116, 224, 234, 266, 267, 279, 285, 287, 293, 295], "shutdown": [12, 16, 110, 114, 116, 118, 121, 122, 123, 295, 302, 309, 313], "shutil": [128, 133, 137, 139, 142, 224, 226, 245, 251, 317, 319, 323, 324, 326, 329, 330, 332, 336, 337, 339, 342, 343, 345, 349, 350, 352, 359], "sick": [267, 283], "sid": [17, 22], "side": [8, 191, 196, 285, 291, 292, 317, 323, 330, 336, 350, 359], "sidebar": 0, "sidecar": [101, 102, 106], "sidestep": [317, 318], "sidewalk": [285, 291], "sight": [285, 288], "sign": [10, 11, 155, 157, 164, 165, 206, 213, 218, 222, 267, 283], "signal": [159, 161, 343, 345], "signatur": [6, 7, 14, 253, 257, 259, 263], "signifi": [9, 198, 201], "signific": [5, 6, 8, 101, 102, 106, 191, 196, 224, 225, 259, 261, 266, 267, 279], "significantli": [84, 85, 159, 162], "signup": 100, "silicon": [1, 169, 170, 266, 267, 275, 279, 283, 293, 295, 298], "silu": [6, 259, 262], "sim": [317, 318, 324, 325], "similar": [4, 12, 24, 27, 110, 113, 114, 155, 158, 164, 166, 171, 174, 317, 323], "similarli": [3, 8, 181, 190, 191, 193, 306, 307, 315, 330, 332], "simpl": [2, 4, 5, 6, 7, 8, 9, 11, 12, 14, 16, 82, 83, 86, 87, 92, 93, 94, 95, 101, 102, 105, 108, 109, 110, 112, 118, 124, 137, 139, 140, 165, 171, 174, 175, 177, 178, 191, 197, 198, 200, 202, 218, 222, 224, 226, 229, 235, 237, 253, 256, 257, 259, 262, 278, 285, 286, 291, 292, 294, 302, 305, 306, 307, 312, 313, 315, 317, 318, 323, 330, 331, 332, 333, 336, 350, 351, 359], "simple_pipelin": [164, 166], "simpler": [110, 117, 118, 121], "simpli": [5, 147, 154, 302, 307, 313, 337, 342, 350, 358], "simplifi": [15, 35, 39, 80, 81, 100, 350, 351], "simul": [3, 168, 181, 185, 302, 313, 324, 325, 329, 330, 332, 350, 351], "simultan": [101, 102, 104, 159, 163], "sin": [324, 325, 326, 329, 343, 346], "sinc": [3, 9, 86, 87, 101, 102, 105, 137, 139, 159, 163, 164, 167, 181, 187, 198, 201, 224, 227, 234, 267, 283, 285, 292, 317, 319, 330, 335], "sing": [267, 283], "singl": [1, 3, 8, 9, 10, 12, 16, 101, 102, 104, 105, 106, 110, 112, 117, 118, 120, 121, 125, 147, 149, 153, 164, 167, 169, 170, 181, 187, 191, 195, 198, 201, 206, 211, 224, 225, 226, 228, 237, 245, 251, 260, 263, 267, 283, 285, 287, 292, 317, 318, 319, 324, 328, 330, 331, 337, 338, 339, 342, 343, 344, 350, 351, 352, 353], "single_gpu_mnist": 13, "sink": [4, 10, 171, 174, 206, 209], "sinusoid": [343, 346], "sisterlif": [267, 283], "sit": [267, 281, 283, 285, 288], "site": [0, 17, 22, 86, 87, 137, 144, 285, 288], "situat": [285, 292], "six": [101, 102, 105], "size": [5, 7, 9, 10, 13, 14, 80, 81, 84, 85, 88, 89, 101, 102, 105, 106, 109, 114, 116, 118, 121, 147, 151, 159, 163, 164, 167, 198, 202, 206, 208, 212, 224, 226, 227, 228, 229, 253, 255, 267, 271, 275, 281, 283, 285, 291, 293, 298, 299, 317, 320, 321, 323, 324, 326, 329, 330, 336, 340, 343, 345, 346, 349, 350, 352], "size_in_byt": [3, 181, 183], "sj": [267, 281, 283, 284], "skagwai": [285, 291], "skew": [10, 206, 208, 330, 332, 337, 339], "skill": [118, 121], "skip": [317, 319, 337, 338, 339, 350, 359], "sklearn": [4, 137, 146, 171, 172, 337, 339], "skylynn": [267, 283], "sla": [126, 127], "slack": [164, 167], "sleazi": [285, 291], "sleep": [2, 3, 164, 166, 175, 180, 181, 184, 189, 285, 292], "slice": [4, 171, 174, 317, 318, 330, 332, 337, 338, 340, 350, 351, 353], "slick": [285, 292], "slide": [267, 283, 344], "slim": [84, 85], "slip": [285, 291], "slo": [101, 102, 105], "slope": [337, 338], "slot": [224, 226, 343, 344], "slow": [11, 110, 112, 218, 220, 266, 267, 279], "slow_adjust_total_amount": [164, 166], "slower": [285, 290], "slowest": [9, 10, 15, 198, 204, 206, 215], "slowli": [285, 292], "slowyourrol": [267, 283], "sm": [267, 283], "small": [8, 9, 10, 15, 16, 88, 89, 101, 102, 106, 110, 112, 117, 118, 121, 124, 128, 130, 191, 192, 198, 201, 202, 206, 211, 215, 317, 318, 324, 329, 330, 332, 336, 343, 349, 350, 351], "small_siz": [293, 298], "small_unet_model_config": [6, 259, 262], "smaller": [0, 3, 17, 22, 118, 121, 124, 159, 163, 181, 187], "smallest": [80, 81], "smart": [101, 102, 107, 285, 291], "smith": [267, 281, 283, 284], "smoke": [245, 252], "smoothl1": [343, 347], "smoothl1loss": [343, 347], "smoothli": [285, 287], "sn": [337, 339, 341], "snake": [267, 283], "snap": [267, 281, 283], "snapshot": [86, 87, 317, 322, 337, 342], "snapshot_download": [118, 121], "snicker": [285, 292], "snippet": [5, 6, 13, 82, 83, 86, 87, 259, 263], "snowflak": [8, 10, 191, 192, 206, 210], "so": [0, 6, 7, 9, 13, 14, 80, 81, 82, 83, 84, 85, 88, 89, 92, 93, 118, 124, 126, 127, 128, 130, 135, 137, 139, 144, 146, 147, 151, 155, 157, 159, 163, 198, 202, 203, 224, 225, 226, 227, 232, 233, 234, 235, 237, 238, 239, 240, 242, 243, 245, 246, 247, 248, 250, 253, 257, 259, 262, 267, 281, 283, 285, 288, 291, 292, 302, 307, 313, 317, 320, 324, 327, 330, 332, 337, 339, 340, 343, 344, 345, 347, 350, 351, 353, 359], "socket": [8, 191, 195], "softbal": [267, 283], "softmax": [137, 140, 337, 338], "softprob": [337, 340], "softwar": [110, 114, 118, 121, 155, 157], "soil": [337, 338, 341], "sole": [17, 22, 285, 292], "solid": [330, 336], "solut": [2, 3, 5, 6, 7, 8, 10, 11, 13, 14, 15, 17, 21, 101, 102, 103, 107, 109, 118, 125, 126, 127, 128, 130, 175, 180, 181, 190, 191, 196, 206, 208, 212, 218, 220, 224, 225, 238, 239, 253, 257, 259, 261, 263, 293, 299, 343, 349], "solv": [101, 102, 106, 118, 124], "some": [4, 5, 6, 7, 8, 9, 10, 13, 15, 53, 63, 86, 87, 88, 89, 101, 102, 105, 110, 114, 118, 120, 122, 147, 154, 159, 162, 163, 164, 165, 171, 174, 191, 194, 195, 198, 204, 205, 206, 210, 211, 215, 253, 258, 259, 264, 267, 283, 285, 288, 289, 290, 291, 292], "someth": [3, 43, 47, 53, 59, 66, 75, 181, 184, 267, 283, 285, 291, 350, 351], "sometim": [3, 28, 30, 43, 45, 53, 56, 181, 187, 285, 292], "somewher": [3, 181, 190], "song": [267, 283], "sonnet": [118, 124], "soon": [3, 181, 189, 267, 283], "sophist": [7, 8, 14, 118, 120, 123, 125, 191, 196, 253, 257], "sorri": [267, 281, 283, 284], "sort": [8, 9, 86, 87, 137, 144, 147, 150, 191, 192, 198, 204, 285, 288, 330, 331, 332, 336, 337, 341, 350, 359], "sort_index": [330, 334, 343, 347, 350, 357], "sorted_prob": [147, 150], "sorted_run": [137, 144, 147, 150], "soul": [267, 281, 283], "sound": [267, 283, 285, 292], "soup": [267, 283], "sourc": [0, 1, 2, 10, 17, 22, 66, 69, 100, 128, 130, 132, 137, 144, 155, 158, 159, 162, 169, 170, 175, 177, 206, 210, 238, 239, 245, 252, 285, 289, 291, 292], "south": [267, 283], "sox": [267, 283], "space": [3, 4, 7, 8, 14, 17, 22, 171, 174, 181, 189, 191, 193, 245, 251, 253, 257, 285, 291, 317, 323, 324, 329, 330, 331, 336, 337, 342, 343, 349, 350, 359], "span": [168, 267, 283, 343, 345], "spark": [9, 128, 130, 195, 198, 205], "spatial": [337, 338], "spawn": [224, 235, 237], "speak": [285, 292], "speci": [337, 342], "special": [8, 9, 110, 117, 118, 120, 121, 124, 125, 191, 192, 198, 203, 267, 281, 283, 285, 291], "specif": [3, 6, 9, 15, 24, 27, 35, 36, 39, 66, 70, 84, 85, 86, 87, 88, 89, 94, 95, 101, 102, 107, 110, 113, 118, 124, 128, 134, 137, 144, 147, 149, 151, 152, 155, 157, 158, 159, 161, 164, 167, 176, 177, 181, 187, 190, 198, 204, 215, 224, 235, 237, 259, 262, 263, 317, 318, 323, 337, 341, 342, 350, 353], "specifi": [1, 3, 5, 6, 7, 9, 10, 11, 13, 14, 16, 24, 26, 80, 81, 82, 83, 84, 85, 92, 93, 94, 95, 118, 121, 122, 128, 135, 137, 143, 145, 147, 149, 153, 168, 169, 170, 181, 185, 187, 198, 201, 206, 212, 213, 218, 221, 223, 224, 231, 238, 244, 245, 250, 253, 257, 259, 263, 293, 295, 298, 299, 300, 305, 306, 307, 315, 330, 334], "specific": [224, 227], "speed": [4, 5, 6, 7, 8, 11, 14, 16, 86, 87, 118, 124, 128, 130, 171, 174, 191, 192, 218, 220, 224, 225, 253, 256, 259, 261, 285, 287, 292, 317, 319, 350, 352, 359], "speedup": [266, 267, 279], "speific": [88, 89], "spend": [285, 288], "spike": [101, 102, 106, 107, 110, 115], "spiki": [80, 81, 84, 85], "spill": [10, 15, 206, 214], "spillov": [88, 89], "spin": [10, 15, 88, 89, 92, 93, 126, 127, 206, 213, 266, 267, 273, 279, 282, 337, 338], "split": [4, 10, 12, 15, 86, 87, 101, 102, 106, 110, 113, 137, 139, 171, 174, 206, 212, 215, 224, 225, 226, 228, 229, 238, 240, 267, 271, 281, 283, 285, 288, 292, 318, 340, 341, 343, 345, 352, 359], "split_at_indic": [317, 319, 324, 326], "split_idx": [324, 326], "split_notebook": 0, "split_proportion": [330, 332], "spoil": [285, 291, 292], "spoiler": [285, 292], "spoken": [285, 291], "spot": [10, 84, 85, 88, 89, 126, 127, 128, 132, 137, 144, 206, 208, 267, 283, 337, 341], "spotifi": [7, 9, 198, 205, 253, 258], "sprai": [285, 292], "sprang": [285, 292], "spruce": [337, 338], "spur": [285, 291], "spy": [155, 158], "sql": [8, 191, 192, 196], "sqrt": [2, 7, 14, 175, 180, 253, 257, 343, 346], "sqrt_add": [2, 175, 180], "squad": [267, 283], "squar": [2, 3, 175, 180, 181, 184, 224, 226, 267, 283, 317, 318, 330, 331, 336], "square_ref": [3, 181, 184], "square_ref_1": [3, 181, 188], "square_ref_2": [3, 181, 188], "square_valu": [3, 181, 184], "squarederror": [4, 171, 174], "squeez": [7, 14, 253, 255, 317, 323, 324, 329, 330, 336, 343, 346, 347], "src": [343, 346], "ssh": [17, 20, 22, 137, 144, 350, 351], "ssl": [24, 27], "sso": [101, 102, 107], "sst": [305, 306, 307, 315], "st": [17, 22, 267, 283], "stabil": [317, 323, 330, 334, 343, 345], "stabilityai": [6, 259, 262, 263], "stabl": [5, 13, 101, 102, 108, 260, 263, 264], "stablediffus": [6, 259, 262, 263], "stack": [6, 28, 32, 35, 39, 41, 43, 45, 50, 53, 56, 62, 66, 70, 77, 100, 159, 162, 163, 259, 262, 324, 326, 337, 339, 343, 344, 349, 350, 359], "stadium": [267, 283], "stage": [3, 5, 6, 8, 10, 164, 166, 181, 187, 191, 194, 206, 208, 212, 213, 224, 226, 234, 259, 262, 267, 283], "stagnant": [343, 347], "stai": [224, 225, 238, 240, 267, 283, 317, 318], "stakehold": [343, 349], "standalon": [224, 227], "standard": [6, 8, 35, 39, 84, 85, 90, 91, 164, 167, 191, 192, 224, 226, 228, 232, 234, 238, 243, 259, 262, 263, 285, 288, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 346, 350, 351, 352, 353], "stander": [285, 292], "stapl": [285, 288], "star": [267, 283, 285, 292, 324, 325, 330, 332], "stare": [285, 288], "starlett": [11, 12, 16, 147, 148, 218, 219], "start": [1, 3, 4, 5, 6, 9, 10, 11, 12, 13, 16, 17, 20, 28, 29, 32, 35, 36, 41, 43, 44, 50, 53, 54, 55, 62, 66, 67, 69, 77, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 103, 105, 109, 110, 111, 114, 118, 119, 121, 124, 126, 127, 128, 130, 132, 157, 159, 163, 164, 165, 166, 167, 169, 170, 171, 172, 174, 181, 182, 186, 187, 198, 199, 206, 207, 210, 212, 213, 218, 219, 224, 225, 226, 227, 230, 231, 235, 238, 244, 245, 246, 247, 249, 254, 259, 260, 262, 263, 266, 267, 279, 285, 287, 288, 291, 295, 298, 299, 302, 307, 309, 313, 316, 317, 318, 323, 324, 325, 329, 330, 332, 343, 345, 347, 350, 351, 352, 358], "start_epoch": [245, 247, 330, 334, 343, 347, 350, 355], "start_run": [137, 143], "start_tim": [137, 144], "start_token": [343, 347], "starter": [92, 93], "startswith": [330, 336, 350, 359], "startup": [3, 24, 26, 110, 117, 128, 130, 155, 158, 181, 186], "starv": [285, 292], "state": [3, 4, 8, 11, 12, 17, 22, 28, 30, 43, 45, 53, 56, 82, 83, 101, 102, 107, 118, 123, 128, 130, 159, 161, 164, 166, 171, 174, 181, 190, 191, 195, 196, 207, 218, 220, 221, 246, 247, 250, 252, 267, 273, 282, 283, 285, 288, 292, 317, 323, 326, 327, 329, 330, 335, 337, 341, 343, 344, 349, 350, 359], "state_dict": [5, 13, 137, 140, 224, 234, 237, 245, 248, 317, 323, 324, 329, 330, 334, 336, 343, 347, 349, 350, 355, 359], "state_dict_fp": [137, 140, 146], "stateless": [2, 10, 175, 178, 206, 213, 317, 319], "statement": [5, 17, 22], "static": [8, 10, 101, 102, 105, 191, 197, 206, 208], "station": [285, 292], "statist": [9, 159, 161, 198, 205], "stats_d": [337, 341, 342], "statu": [12, 13, 14, 16, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 90, 91, 92, 93, 128, 133, 137, 139, 144, 147, 150, 159, 163, 164, 166, 168, 267, 283, 293, 300], "std": [9, 10, 15, 198, 204, 206, 215, 224, 237, 343, 345, 349, 350, 353, 359], "stderr": [164, 167], "steadi": [317, 321], "steadili": [224, 236, 343, 347], "steak": [350, 351], "steal": [285, 291], "steam": [285, 288], "step": [2, 3, 5, 6, 7, 8, 10, 13, 14, 28, 29, 30, 35, 37, 39, 42, 43, 44, 45, 47, 48, 53, 54, 55, 56, 59, 60, 66, 67, 68, 71, 75, 76, 78, 92, 93, 100, 105, 119, 124, 128, 130, 134, 135, 137, 139, 143, 159, 163, 164, 165, 175, 176, 178, 180, 181, 182, 191, 197, 206, 209, 212, 224, 225, 227, 228, 229, 232, 236, 237, 238, 240, 242, 243, 247, 253, 256, 257, 259, 262, 267, 283, 285, 287, 293, 298, 318, 325, 326, 331, 332, 334, 338, 339, 344, 345, 347, 351, 355], "step_size_hour": [343, 349], "sterl": [267, 283], "steven": [267, 283], "stewart": [285, 291], "still": [3, 5, 8, 13, 82, 83, 126, 127, 137, 138, 181, 184, 191, 196, 224, 234, 238, 240, 241, 245, 248, 250, 267, 283, 285, 291, 307, 311, 316, 317, 319, 343, 347, 350, 352], "stillkidrauhl": [267, 283], "stockholm": [285, 288], "stop": [1, 7, 14, 101, 102, 104, 169, 170, 224, 237, 253, 257, 285, 292, 307, 316, 317, 323, 337, 342, 343, 349, 350, 359], "storag": [6, 8, 9, 10, 15, 16, 17, 21, 22, 28, 34, 35, 36, 38, 39, 66, 69, 70, 80, 81, 100, 118, 121, 129, 130, 137, 139, 142, 147, 153, 159, 162, 163, 191, 192, 195, 198, 200, 202, 203, 206, 210, 216, 225, 226, 227, 232, 235, 236, 238, 244, 246, 248, 250, 252, 259, 262, 263, 324, 329, 332, 343, 344, 345, 350, 351, 352, 359, 360], "storage_fold": [4, 5, 9, 10, 11, 171, 174, 198, 203, 205, 206, 213, 216, 217, 218, 222, 223], "storage_path": [4, 5, 6, 12, 13, 171, 174, 224, 225, 234, 238, 244, 245, 248, 250, 259, 262, 263, 317, 321, 324, 328, 330, 334, 335, 337, 340, 343, 347, 350, 355, 356], "store": [6, 8, 10, 11, 12, 15, 17, 21, 88, 89, 126, 127, 128, 130, 131, 133, 137, 139, 144, 147, 154, 155, 158, 164, 166, 189, 190, 191, 192, 196, 197, 206, 214, 215, 218, 220, 224, 226, 227, 234, 235, 236, 238, 242, 243, 244, 245, 248, 252, 259, 262, 263, 266, 267, 277, 279, 283, 284, 285, 288, 292, 317, 318, 321, 323, 324, 329, 330, 332, 334, 336, 337, 338, 339, 342, 343, 345, 347, 350, 351, 352, 355, 357], "stori": [285, 288, 292], "storylin": [285, 291, 292], "str": [4, 5, 6, 7, 10, 11, 13, 14, 15, 16, 118, 122, 123, 159, 163, 171, 174, 206, 212, 213, 215, 218, 222, 224, 234, 245, 248, 253, 257, 259, 262, 267, 272, 273, 282, 305, 306, 307, 315, 337, 340, 343, 349, 350, 353, 354, 359], "strang": [285, 292], "stranger": [285, 291], "strategi": [6, 92, 93, 117, 128, 130, 137, 144, 259, 263, 317, 319, 321, 324, 328], "stratifi": [337, 339], "streak": [267, 281, 283], "stream": [4, 9, 10, 11, 15, 16, 101, 102, 106, 108, 110, 114, 115, 118, 121, 128, 130, 132, 147, 151, 164, 167, 171, 174, 197, 198, 205, 206, 208, 218, 220, 238, 239, 240, 241, 243, 244, 245, 252, 267, 281, 283, 284, 317, 318, 324, 325, 329, 330, 331, 332, 334, 336, 337, 338, 339, 340, 343, 345, 349], "streaming_split": [10, 206, 211], "streamlin": [17, 21, 24, 25, 90, 91, 137, 144], "street": [285, 291], "strength": [118, 121], "stretch": [267, 283], "strftime": [5, 13], "strict": [317, 323, 324, 329], "stride": [5, 7, 13, 14, 224, 227, 253, 256, 257, 343, 345], "strike": [285, 291, 292], "string": [35, 39, 43, 45, 48, 53, 56, 60, 66, 76, 88, 89, 118, 122, 123, 267, 281, 284, 285, 289, 290, 292, 350, 352], "strip": [330, 336, 343, 349, 350, 359], "strong": [101, 102, 106, 110, 112, 159, 163, 330, 333, 343, 345], "stronger": [110, 112], "structur": [10, 12, 97, 99, 102, 109, 119, 120, 121, 123, 125, 128, 130, 137, 139, 164, 167, 196, 205, 206, 210, 285, 288, 317, 318, 324, 325, 329, 330, 331, 343, 346, 347, 360], "stuck": [11, 159, 163, 218, 220], "student": [285, 288], "studi": [5, 6, 13, 259, 264, 285, 288], "studio": [82, 83, 267, 283, 285, 291], "stuff": [267, 283], "stun": [285, 291, 292], "stupid": [285, 292], "style": [0, 6, 128, 130, 238, 241, 245, 252, 259, 262, 285, 292, 317, 323, 330, 332, 334, 343, 344, 350, 351], "sub": [3, 181, 188], "subdirectori": 0, "subfold": [6, 259, 262], "subject": [118, 124], "submiss": [7, 14, 90, 91, 253, 257], "submit": [7, 14, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 88, 89, 126, 127, 128, 135, 137, 145, 159, 163, 253, 256, 267, 283], "subnet": [20, 23, 28, 30, 34, 35, 36, 39, 66, 70, 79], "subnet_id": [17, 23, 28, 30], "suboptim": [10, 206, 208], "subplot": [7, 13, 14, 224, 226, 237, 253, 255, 317, 319, 323, 330, 332, 350, 352], "subprocess": [10, 11, 206, 207, 213, 218, 219, 222, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "subraman": [267, 283], "subsampl": [337, 342], "subsequ": [101, 102, 106, 155, 157, 158, 164, 167, 285, 292], "subset": [5, 7, 9, 10, 15, 94, 95, 198, 202, 206, 210, 211, 253, 257, 293, 298, 317, 318, 319, 330, 332, 337, 339, 343, 345, 350, 351, 352], "substanc": [285, 292], "substanti": [343, 347], "subtract": [317, 323], "subword": [101, 102, 104], "success": [7, 14, 168, 253, 257], "successfulli": [28, 31, 34, 35, 40, 43, 47, 52, 53, 59, 65, 66, 75, 84, 85, 90, 91, 110, 117, 118, 121, 324, 329], "suck": [267, 283], "sudo": [155, 158], "suffer": [11, 218, 220], "suffici": [110, 114, 317, 319], "suffix": [2, 175, 179], "suggest": [10, 118, 121, 206, 213, 337, 341], "suit": [8, 128, 134, 147, 151, 191, 192, 224, 225, 330, 333], "suitabl": [0, 285, 292, 293, 295, 317, 323, 330, 332], "sum": [3, 9, 10, 15, 88, 89, 137, 146, 181, 183, 198, 201, 204, 206, 211, 215, 285, 288, 330, 333, 334, 337, 341, 342], "sum_ref": [3, 181, 184], "sum_valu": [3, 181, 184], "summar": [101, 102, 105, 110, 117, 118, 121, 124, 125, 267, 277, 284], "summari": [111, 118, 121, 125, 266, 279, 295], "summer": [267, 281, 283, 285, 292], "summerslam": [267, 281, 283], "summit": [9, 10, 13, 15, 198, 205, 206, 217], "sun": [267, 283, 285, 291, 292], "sunbeam": [28, 30], "sunda": [267, 283], "sundai": [267, 281, 283], "super": [6, 137, 140, 259, 262, 267, 278, 283, 286, 294, 312, 317, 320, 324, 327, 330, 333, 343, 346], "superior": [285, 292], "supervis": [324, 326, 343, 345, 350, 351], "suppli": [285, 291, 324, 329], "support": [1, 3, 5, 6, 8, 9, 10, 11, 15, 22, 24, 26, 27, 100, 101, 110, 116, 118, 121, 122, 125, 126, 127, 128, 130, 137, 139, 144, 147, 151, 154, 155, 157, 159, 162, 164, 165, 167, 169, 170, 181, 187, 190, 191, 192, 195, 196, 197, 198, 205, 206, 208, 210, 215, 216, 218, 220, 223, 224, 225, 226, 238, 241, 245, 247, 248, 250, 259, 261, 285, 287, 293, 299, 302, 307, 313, 324, 325, 330, 335, 336, 337, 339, 342, 350, 355], "suppos": [267, 283], "suptitl": [317, 319, 323, 350, 352], "sur": [267, 283], "sure": [0, 5, 66, 78, 82, 83, 118, 121, 245, 251, 267, 281, 283, 284, 317, 323, 330, 336, 350, 353], "surfac": [164, 167], "surg": [343, 349], "surpris": [285, 292], "surprisingli": [3, 181, 190, 267, 283], "surround": [285, 288], "surviv": [267, 281, 283, 284, 324, 329], "sushi": [350, 351], "suspens": [285, 292], "suv": [118, 122], "swai": [285, 292], "swap": [84, 85, 317, 323, 324, 329, 350, 359], "swede": [285, 288], "swedish": [285, 288], "sweep": [317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "swing": [267, 283, 285, 288, 324, 325], "switch": [118, 120, 121, 245, 252, 330, 336], "switcher": 0, "sy": [2, 3, 128, 129, 137, 138, 147, 148, 175, 176, 181, 182, 183, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "sydow": [285, 292], "symbol": [1, 169, 170, 324, 325], "sync": [82, 83, 224, 225, 267, 283, 350, 351, 355], "sync_dist": [6, 259, 262, 317, 320, 324, 327], "sync_on_comput": [350, 355], "synchron": [3, 5, 13, 181, 183, 224, 225, 230, 231, 234, 245, 248, 317, 321, 350, 355], "synthet": [159, 163, 324, 329], "synthetic_image_output": [159, 163], "system": [2, 3, 6, 8, 9, 10, 12, 14, 17, 22, 24, 26, 43, 46, 51, 53, 58, 63, 64, 66, 69, 86, 87, 92, 93, 100, 118, 120, 121, 122, 123, 128, 133, 137, 144, 147, 151, 155, 157, 158, 159, 161, 162, 164, 167, 175, 177, 181, 185, 191, 192, 195, 198, 203, 206, 212, 217, 245, 252, 259, 262, 336, 343, 348, 350, 351], "t": [1, 2, 3, 5, 8, 9, 10, 13, 15, 24, 26, 53, 63, 86, 87, 100, 101, 102, 106, 128, 130, 135, 137, 138, 139, 143, 145, 146, 147, 151, 153, 164, 166, 169, 170, 175, 180, 181, 184, 185, 186, 191, 196, 198, 202, 204, 206, 212, 215, 224, 225, 226, 230, 232, 237, 267, 281, 283, 285, 288, 291, 292, 317, 318, 319, 320, 323, 324, 325, 327, 329, 330, 332, 336, 343, 344, 345, 346, 347, 349, 350, 351, 352, 353, 354, 359], "t10k": [13, 14], "t4": [10, 12, 13, 14, 66, 71, 84, 85, 128, 131, 137, 139, 143, 146, 147, 149, 206, 213], "t_": [324, 325], "t_futur": [343, 349], "t_img": [317, 320], "t_past": [343, 349], "t_scale": [317, 320], "tab": [80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 128, 129, 137, 138, 144, 147, 148, 164, 166], "tabl": [7, 8, 10, 28, 30, 43, 45, 53, 56, 191, 192, 206, 210, 253, 257, 337, 340, 343, 345, 350, 352, 353], "tabular": [9, 15, 198, 201, 238, 242, 330, 331, 336, 339, 342, 350, 352], "tackl": [343, 344], "tag": [8, 28, 30, 43, 45, 53, 56, 137, 139, 144, 191, 196, 330, 336, 337, 342], "tail": [330, 332], "tailor": [224, 227], "take": [3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 23, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 73, 118, 120, 128, 130, 134, 137, 139, 141, 146, 164, 166, 171, 174, 181, 190, 198, 201, 202, 206, 211, 212, 218, 222, 226, 227, 234, 253, 256, 257, 259, 261, 263, 267, 275, 283, 285, 288, 291, 292, 293, 297, 319, 333, 339, 345, 352], "take_al": [330, 332], "take_batch": [9, 10, 12, 15, 16, 137, 141, 198, 202, 206, 211, 212, 213, 267, 275, 283, 337, 339], "takeawai": 103, "taken": [285, 288, 350, 359], "talent": [267, 283, 285, 292], "talk": [9, 10, 15, 118, 121, 198, 205, 206, 217, 267, 283, 285, 292], "taman": [267, 283], "tank": [267, 283], "target": [4, 5, 6, 10, 16, 17, 22, 118, 121, 137, 143, 171, 174, 206, 212, 224, 234, 238, 242, 259, 262, 278, 286, 294, 312, 324, 326, 330, 331, 332, 337, 339, 342, 343, 345, 347], "target_num_rows_per_block": [159, 163], "target_ongoing_request": 16, "target_path": [317, 323, 324, 329, 330, 336], "task": [2, 4, 7, 8, 9, 10, 11, 13, 14, 15, 43, 44, 53, 55, 66, 68, 82, 83, 84, 85, 88, 89, 101, 102, 106, 110, 112, 121, 128, 134, 164, 166, 171, 174, 175, 177, 178, 179, 180, 183, 189, 190, 191, 194, 195, 196, 198, 201, 203, 206, 208, 210, 212, 213, 218, 220, 253, 256, 285, 287, 292, 293, 296, 299, 317, 319, 323, 324, 329, 330, 333, 336, 337, 341, 342, 350, 351], "task_id": [88, 89], "taskpoolmapoper": [10, 206, 214], "tatum": [285, 291], "tavakolian": [267, 283], "tax": [4, 12, 171, 174, 267, 283], "taxi": [4, 9, 164, 166, 171, 174, 198, 201, 204, 349], "taximet": [9, 198, 201], "taxiwindowdataset": [343, 345], "tb": [10, 206, 208], "tbh": [267, 283], "tbl": [337, 340], "tc": [118, 123], "tcm": [285, 291], "tcp": [17, 22], "td3": [324, 329], "tea": [267, 283], "teach": [317, 318, 324, 325], "teacher": [285, 288, 344, 346], "team": [8, 94, 95, 96, 97, 99, 191, 195, 267, 283, 350, 351], "tear": [90, 91], "teardown": [24, 26], "tech": [267, 283], "technic": [2, 175, 177, 285, 292], "techniqu": [7, 14, 253, 256, 285, 292], "technologi": [8, 191, 192], "ted": [267, 283], "teen": [267, 283], "telemetri": [155, 157, 159, 163], "tell": [4, 6, 110, 114, 115, 171, 174, 224, 230, 234, 259, 263, 267, 283, 285, 288, 291, 350, 356], "temp": [28, 30, 43, 45, 53, 56, 224, 226, 234, 343, 347, 350, 352, 355], "temp_checkpoint_dir": [5, 13, 224, 234, 245, 248], "tempdir": [350, 355], "temperatur": [3, 118, 123, 181, 190], "tempfil": [5, 13, 137, 143, 224, 226, 234, 245, 248, 317, 319, 321, 324, 328, 330, 332, 334, 337, 339, 343, 347, 350, 352, 355], "templat": [24, 27, 92, 93, 101, 102, 109, 112, 126, 127, 245, 252], "tempor": [324, 329], "temporari": [101, 102, 106, 224, 234, 324, 329, 330, 334, 337, 342, 343, 349, 350, 355, 359], "temporarydirectori": [5, 13, 137, 143, 224, 234, 245, 248, 330, 334, 343, 347, 350, 355], "ten": [350, 351], "tenant": [118, 125, 330, 336], "tenni": [267, 283], "tensor": [5, 6, 10, 11, 13, 15, 16, 101, 102, 106, 110, 113, 117, 137, 141, 206, 213, 218, 222, 224, 232, 233, 237, 238, 241, 243, 259, 262, 293, 296, 319, 320, 323, 324, 329, 330, 336, 343, 345], "tensor_batch": [137, 141], "tensor_parallel_s": [110, 113, 116, 118, 123], "tensorflow": [302, 307, 313], "term": [3, 8, 17, 21, 181, 184, 191, 197, 285, 288, 343, 344], "termin": [1, 5, 12, 13, 24, 27, 28, 31, 32, 33, 35, 40, 42, 43, 47, 50, 51, 53, 59, 62, 64, 66, 69, 75, 78, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 102, 108, 110, 113, 115, 128, 133, 136, 137, 146, 147, 153, 164, 166, 168, 169, 170, 224, 226, 245, 250, 293, 300, 324, 326], "terminologi": [96, 97], "terraform": [17, 21, 22, 23, 29, 31, 33, 34, 37, 40, 42, 44, 47, 51, 52, 55, 57, 59, 64, 65, 68, 71, 75, 78, 79], "terrain": [337, 342], "terribl": [285, 292], "test": [4, 7, 11, 12, 14, 16, 30, 39, 45, 56, 70, 79, 82, 83, 92, 93, 110, 114, 117, 118, 122, 123, 124, 125, 137, 146, 155, 158, 171, 174, 218, 220, 222, 223, 245, 252, 253, 255, 267, 283, 293, 298, 299, 302, 313, 350, 353], "test_d": [137, 146], "test_job": [28, 32, 35, 41, 43, 50, 53, 62, 66, 77], "test_siz": [4, 12, 171, 174, 337, 339], "texan": [267, 283], "text": [6, 8, 53, 57, 86, 87, 105, 109, 118, 123, 159, 162, 191, 192, 259, 262, 266, 267, 270, 271, 273, 275, 277, 279, 281, 282, 283, 284, 285, 287, 288, 289, 290, 291, 292, 293, 295, 298, 305, 306, 307, 309, 315, 316, 324, 325, 330, 331, 343, 344], "text_token": [285, 292], "textembedd": [267, 272, 273, 275, 277, 282, 283, 284], "tf": [28, 30, 35, 39, 43, 45, 53, 56], "tfenv": [28, 29, 43, 44, 53, 55, 66, 68], "tfi": [267, 283], "tfvar": [28, 30, 43, 45, 53, 56], "tgt": [343, 346], "than": [5, 8, 10, 12, 82, 83, 101, 102, 104, 108, 110, 112, 118, 121, 159, 163, 191, 194, 196, 197, 206, 211, 212, 285, 288, 290, 291, 292, 330, 336, 337, 339, 341, 343, 344, 347], "thank": [118, 125, 168, 267, 283, 343, 347], "thats": [285, 292], "theater": [285, 288, 292], "thei": [1, 3, 8, 9, 10, 11, 15, 28, 30, 43, 45, 53, 56, 80, 81, 84, 85, 101, 102, 104, 106, 126, 127, 128, 130, 132, 147, 154, 159, 163, 169, 170, 181, 186, 187, 189, 191, 195, 196, 198, 202, 206, 211, 217, 218, 221, 224, 229, 238, 239, 245, 246, 267, 283, 285, 288, 291, 292, 302, 307, 313, 337, 339], "them": [0, 2, 3, 5, 6, 8, 10, 17, 20, 23, 24, 27, 53, 63, 82, 83, 86, 87, 88, 89, 94, 95, 110, 115, 118, 121, 126, 127, 147, 151, 159, 163, 175, 177, 179, 181, 186, 188, 189, 190, 191, 197, 206, 213, 224, 226, 237, 259, 262, 267, 275, 283, 285, 292, 317, 319, 320, 323, 324, 325, 328, 329, 330, 331, 334, 336, 350, 351, 352, 353, 357], "theme": [0, 168], "themselv": [285, 291], "theoret": [285, 292], "therefor": [90, 91, 285, 288], "theta": [317, 318, 324, 325, 329, 337, 338, 343, 344, 350, 351], "theta_": [324, 325], "theta_dot": [324, 329], "thfc": [267, 283], "thi": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 34, 35, 36, 37, 38, 39, 43, 44, 45, 47, 50, 52, 53, 54, 56, 57, 59, 62, 63, 65, 66, 67, 68, 69, 70, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 107, 109, 110, 111, 112, 116, 118, 119, 120, 121, 123, 126, 127, 128, 129, 130, 133, 135, 136, 137, 138, 139, 143, 145, 146, 147, 148, 149, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 171, 172, 174, 175, 176, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 251, 253, 254, 256, 257, 258, 259, 260, 262, 263, 264, 266, 267, 273, 275, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 297, 298, 299, 300, 302, 306, 307, 311, 312, 313, 315, 316, 319, 321, 328, 332, 333, 334, 335, 339, 340, 345, 346, 347, 348, 352, 353, 354, 355, 356, 357, 360], "thine": [267, 283], "thing": [3, 118, 123, 128, 133, 181, 186, 267, 283, 285, 291, 292], "think": [2, 3, 118, 124, 175, 176, 181, 182, 266, 267, 279, 283, 285, 287, 291, 292, 293, 295, 302, 307, 313], "third": [7, 14, 53, 57, 155, 157, 253, 257, 267, 283, 285, 291, 292], "tho": [267, 283], "thoma": [267, 283], "thor": [267, 283], "those": [8, 43, 50, 53, 62, 88, 89, 147, 154, 191, 196, 285, 288, 292, 337, 341], "though": [84, 85, 101, 102, 105, 267, 283], "thought": [285, 288], "three": [2, 6, 9, 10, 15, 101, 102, 107, 108, 109, 159, 162, 175, 180, 198, 201, 206, 209, 210, 245, 252, 259, 262, 285, 292, 317, 323, 350, 356], "thriller": [285, 292], "throb": [285, 288], "through": [3, 4, 5, 6, 7, 8, 13, 14, 17, 22, 23, 24, 27, 28, 29, 43, 44, 53, 54, 66, 67, 80, 81, 82, 83, 100, 101, 102, 104, 107, 109, 110, 111, 118, 120, 123, 147, 153, 159, 161, 162, 164, 167, 168, 171, 174, 181, 184, 191, 193, 196, 224, 226, 229, 245, 252, 253, 254, 259, 262, 266, 267, 277, 279, 283, 284, 285, 291, 292, 317, 318, 324, 329, 337, 338, 340, 343, 344, 350, 351], "throughout": [1, 169, 170, 350, 352], "throughput": [8, 10, 101, 102, 105, 128, 130, 137, 144, 146, 164, 166, 167, 191, 197, 206, 217, 238, 239, 245, 252, 266, 267, 277, 279, 284, 337, 341, 350, 359], "throw": [43, 51, 53, 64, 267, 277, 284], "thru": [285, 288], "thu": [5, 24, 27], "thumb": [8, 191, 197], "thursdai": [267, 281, 283, 284], "ti": [5, 6, 224, 227, 259, 263], "ticket": [267, 281, 283], "tidi": [337, 342, 343, 345, 349, 350, 359], "tie": [267, 283], "tiger": [267, 283, 285, 292], "tight": [267, 283], "tight_layout": [224, 237, 317, 319, 321, 323, 324, 328, 330, 332, 334, 343, 345, 347, 349, 350, 352, 357], "tightli": [17, 22], "tild": [324, 325], "tilt": [8, 191, 194], "timber": [267, 283], "time": [2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 24, 26, 84, 85, 88, 89, 96, 98, 101, 102, 104, 105, 106, 110, 113, 117, 118, 123, 126, 127, 128, 130, 132, 137, 139, 144, 147, 151, 152, 154, 159, 163, 164, 166, 167, 171, 174, 175, 176, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 195, 197, 206, 212, 218, 220, 224, 225, 226, 238, 239, 240, 245, 248, 253, 256, 259, 261, 263, 266, 267, 279, 283, 285, 288, 291, 292, 293, 300, 317, 318, 324, 325, 328, 329, 330, 331, 334, 336, 337, 340, 342, 345, 347, 349, 350, 351, 358], "time_since_restor": 13, "time_this_iter_": 13, "time_total_": 13, "timedelta": [343, 345], "timelin": [3, 181, 189], "timeseri": [164, 166], "timeseriesbatchpredictor": [343, 349], "timeseriestransform": [343, 346, 347, 349], "timestamp": [5, 13, 168, 224, 226, 330, 332, 336, 343, 345], "timestep": [6, 259, 262, 317, 318, 320, 323, 324, 325, 326, 327, 329], "tini": [317, 320, 323, 324, 327, 337, 339, 343, 349, 350, 353], "tint": [285, 292], "tip": [4, 9, 13, 14, 16, 128, 133, 137, 139, 147, 150, 171, 174, 198, 201, 202], "tip_amount": [4, 9, 164, 166, 171, 174, 198, 201, 202], "tip_percentag": [9, 198, 202], "titan": [267, 283], "titl": [5, 10, 66, 69, 147, 150, 206, 211, 224, 226, 237, 267, 281, 283, 317, 321, 324, 328, 332, 334, 337, 339, 341, 343, 345, 347, 349, 350, 357, 359], "tl": [24, 27], "tlc": [9, 198, 201], "tloss": [293, 298], "tmp": [12, 13, 88, 89, 155, 158, 164, 167, 168, 293, 300], "tmp_checkpoint": [350, 359], "tmpdir": [330, 334, 343, 347, 350, 355], "tn": [137, 146], "to_arrow_ref": [337, 340], "to_csv": [9, 198, 203, 330, 332, 343, 345], "to_datetim": [343, 345], "to_json": 12, "to_numpi": [337, 340, 341, 343, 345, 349], "to_panda": [9, 10, 15, 198, 204, 206, 215, 285, 292, 337, 340, 350, 353], "to_parquet": [9, 198, 203, 238, 242, 330, 332, 337, 339, 350, 353], "to_pylist": [343, 345], "to_tensor": [224, 237], "todai": [1, 169, 170, 267, 283], "todo": [164, 167], "togeth": [4, 5, 6, 10, 11, 16, 147, 151, 154, 171, 173, 176, 206, 212, 218, 221, 224, 227, 234, 235, 259, 263, 267, 283], "toggl": [128, 129, 137, 138, 147, 148], "toke": [110, 113], "token": [92, 93, 101, 102, 104, 105, 106, 107, 108, 110, 113, 114, 115, 116, 118, 121, 122, 123, 124, 147, 151, 168, 287, 288, 295, 296, 300], "tokenization_fn": [285, 292], "tokenize_funct": [293, 298], "toler": [5, 6, 8, 9, 10, 17, 21, 22, 92, 93, 110, 115, 126, 127, 137, 144, 147, 153, 191, 195, 198, 200, 205, 206, 208, 224, 225, 247, 248, 250, 259, 261, 317, 318, 319, 321, 323, 324, 325, 328, 329, 330, 331, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344, 348, 349, 351, 355, 356, 359, 360], "tolist": [11, 16, 164, 167, 218, 222, 224, 237, 238, 242, 330, 336, 343, 345], "toll": [4, 9, 171, 174, 198, 201], "tolls_amount": [4, 9, 171, 174, 198, 201], "toml": [126, 127], "tomorrow": [118, 123, 267, 281, 283, 284], "ton": [285, 292], "tone": [118, 121], "tonight": [267, 281, 283, 284], "tonit": [267, 283], "too": [4, 9, 10, 28, 30, 43, 45, 53, 56, 110, 112, 137, 138, 171, 174, 182, 198, 203, 206, 212, 267, 277, 284, 285, 287, 291, 292, 343, 345], "took": [3, 181, 187], "tool": [4, 8, 12, 24, 25, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 82, 83, 88, 89, 100, 102, 109, 119, 120, 125, 126, 127, 155, 157, 158, 159, 161, 162, 163, 164, 166, 171, 173, 191, 192, 196, 197, 266, 267, 279, 285, 287, 293, 295, 299, 300, 324, 325, 330, 332, 337, 339, 343, 345], "tool_cal": [118, 123], "tool_call_cli": [118, 123], "tool_call_id": [118, 123], "tool_call_pars": [118, 123], "tool_choic": [118, 123], "top": [2, 4, 9, 12, 80, 81, 94, 95, 118, 121, 128, 129, 132, 136, 137, 138, 143, 147, 148, 151, 171, 173, 175, 177, 182, 186, 189, 198, 200, 267, 283, 302, 307, 313, 324, 329, 331, 337, 341, 343, 347, 350, 359], "top20": [267, 283], "top_item_id": [330, 336], "top_items_df": [330, 336], "top_match": [128, 136], "top_scor": [330, 336], "topic": [111, 117, 121, 267, 283], "topic_safety_output_restrict": [118, 121], "topk": [330, 336], "torch": [5, 7, 10, 11, 13, 14, 15, 16, 128, 131, 137, 139, 140, 141, 143, 144, 147, 148, 149, 206, 207, 213, 218, 219, 222, 224, 226, 227, 228, 231, 232, 233, 234, 237, 245, 247, 248, 253, 254, 255, 256, 257, 260, 261, 263, 267, 269, 280, 293, 296, 298, 317, 319, 320, 321, 323, 324, 326, 327, 328, 329, 330, 331, 332, 334, 336, 343, 345, 346, 347, 349, 350, 351, 352, 355, 359], "torch_": [5, 13], "torch_config": [293, 299], "torch_d": [10, 206, 210], "torchconfig": [293, 296, 299], "torchmetr": [13, 350, 355], "torchpredictor": [137, 146, 147, 148, 149], "torchrec": [330, 331], "torchscript": [324, 329], "torchtrain": [5, 137, 143, 226, 228, 229, 230, 239, 242, 245, 248, 250, 252, 260, 293, 295, 296, 299, 318, 319, 323, 325, 326, 330, 331, 332, 334, 335, 336, 343, 344, 345, 347, 349, 351, 352, 359], "torchtrainer_2025": [293, 300], "torchtrainer_4dd7a_00000": [293, 300], "torchtrainer_4dd7a_00000_0_2025": [293, 300], "torchtrainer_d89d0_00000_0_2024": 13, "torchvis": [5, 7, 10, 13, 14, 15, 16, 206, 207, 224, 226, 227, 238, 243, 253, 254, 317, 319, 350, 351, 352], "torqu": [324, 325, 329], "torranc": [267, 283], "total": [3, 4, 5, 9, 12, 13, 14, 101, 102, 106, 110, 116, 171, 174, 181, 190, 198, 201, 224, 228, 229, 293, 299, 317, 319, 324, 326], "total_amount": [9, 164, 166, 198, 201, 202], "total_amt": [7, 14, 253, 257], "totensor": [5, 7, 10, 13, 14, 15, 16, 206, 207, 212, 224, 226, 232, 237, 238, 243, 253, 254, 255, 257, 350, 353, 359], "touch": [285, 288, 330, 336, 337, 339], "tough": [307, 308, 309, 316], "tougher": [307, 308, 309, 316], "tour": [2, 175, 176, 267, 283], "tourism": [267, 283], "tourist": [118, 121], "toward": [8, 191, 194, 278, 286, 294, 307, 309, 312, 316, 324, 325, 330, 332], "tower": [330, 336], "town": [267, 283, 285, 291], "tp": [137, 146], "tpot": [101, 102, 106], "tpu": [3, 5, 6, 181, 187, 224, 227, 259, 263], "tqdm": [5, 293, 296, 317, 319, 330, 332, 350, 352], "tr_model": [343, 346], "trace": [5, 6, 8, 147, 153, 155, 157, 165, 191, 195, 224, 225, 259, 261, 285, 288], "traceback": [8, 191, 195], "track": [1, 5, 88, 89, 90, 91, 137, 143, 159, 162, 169, 170, 224, 225, 236, 245, 252, 278, 285, 286, 292, 294, 312, 317, 321, 324, 325, 329, 330, 331, 334, 336, 337, 342], "track_running_stat": [13, 137, 140], "tracker": [137, 142], "tractabl": [317, 318], "trade": [110, 116, 118, 124], "tradit": [101, 102, 106, 128, 130, 330, 331, 337, 338], "traffic": [17, 22, 53, 63, 66, 73, 92, 93, 101, 102, 106, 107, 110, 115, 147, 148, 151, 164, 167, 302, 307, 311, 313, 316, 343, 344], "trail": [267, 283, 285, 291], "train": [7, 8, 9, 10, 12, 14, 15, 84, 85, 86, 87, 90, 91, 101, 102, 104, 126, 127, 128, 130, 134, 139, 141, 145, 146, 147, 148, 149, 191, 193, 195, 196, 198, 200, 202, 205, 206, 209, 211, 213, 217, 226, 227, 229, 230, 231, 232, 237, 241, 242, 243, 248, 251, 253, 255, 256, 257, 258, 267, 271, 281, 283, 285, 288, 292, 296, 305, 306, 307, 314, 315, 320, 322, 323, 326, 327, 333, 336, 341, 345, 346, 352, 355, 359], "train_arrow": [337, 340], "train_batch": [350, 355], "train_bert": [293, 295, 299, 300], "train_config": [293, 299, 330, 334, 336], "train_count": [317, 319], "train_ctx": [4, 171, 174], "train_d": [137, 139, 141, 143, 238, 242, 243, 244, 317, 319, 321, 324, 326, 328, 330, 332, 334, 337, 339, 340], "train_data": [5, 7, 13, 14, 224, 232, 253, 255, 257], "train_dataload": [6, 259, 262, 263, 317, 321, 324, 328], "train_dataset": [12, 293, 298], "train_df": [337, 339], "train_epoch": [137, 143], "train_frac": [330, 332], "train_func": [337, 338, 340, 342], "train_func_per_work": [293, 298, 299], "train_label": 13, "train_linear_model": [7, 14, 253, 257], "train_load": [5, 6, 13, 224, 232, 259, 262, 317, 321, 324, 328, 330, 334, 343, 347, 350, 355], "train_loop": [318, 323, 324, 325, 328], "train_loop_config": [4, 5, 6, 13, 137, 143, 171, 174, 235, 238, 244, 245, 248, 250, 259, 263, 293, 299, 330, 334, 337, 340, 343, 347, 350, 356], "train_loop_per_work": [6, 137, 143, 245, 248, 250, 259, 263, 293, 299, 317, 321, 330, 331, 334, 343, 344, 347, 351, 356], "train_loop_ray_train": [5, 6, 13, 224, 227, 228, 229, 230, 235, 259, 263], "train_loop_ray_train_ray_data": [238, 240, 244], "train_loop_ray_train_with_checkpoint_load": [245, 247, 248, 250], "train_loop_torch": [5, 7, 13, 14, 253, 256], "train_loss": [137, 143, 144, 317, 320, 321, 324, 327, 328, 330, 331, 334, 343, 347, 350, 355, 357], "train_loss_sum": [343, 347], "train_loss_tot": [350, 355], "train_model": [126, 127, 137, 145], "train_my_simple_model": [7, 14, 253, 257], "train_my_simple_model_2024": 14, "train_my_simple_model_3207e_00000_0_a": 14, "train_my_simple_model_3207e_00000terminated10": 14, "train_my_simple_model_3207e_00001terminated10": 14, "train_my_simple_model_3207e_00002terminated10": 14, "train_my_simple_model_3207e_00003terminated10": 14, "train_my_simple_model_3207e_00004terminated10": 14, "train_parquet": [337, 339], "train_pytorch": [7, 14, 253, 257], "train_pytorch_7cf0c_00000terminated10": 14, "train_pytorch_7cf0c_00001terminated10": 14, "train_record": [343, 345], "train_test_split": [4, 12, 171, 172, 174, 330, 332, 337, 339], "trainabl": [4, 7, 14, 171, 174, 253, 257], "trainbr": [285, 292], "traincontext": [5, 224, 228], "trainer": [4, 5, 6, 12, 13, 137, 143, 171, 174, 226, 227, 236, 238, 244, 245, 248, 249, 250, 252, 259, 262, 263, 293, 296, 299, 317, 321, 322, 324, 328, 330, 331, 334, 335, 342, 343, 347, 348, 350, 351, 356, 358, 359], "training_01": 360, "training_02": 360, "training_03": 360, "training_04": 360, "training_05": 360, "training_06": 360, "training_07": 360, "training_08": 360, "training_09": 360, "training_iter": 13, "training_step": [6, 259, 262, 317, 320, 324, 327], "trainingargu": [293, 296], "trajectori": [324, 329], "transact": [8, 191, 192], "transfer": [2, 3, 8, 9, 10, 15, 159, 163, 175, 177, 181, 184, 191, 192, 198, 204, 206, 212, 215, 350, 359], "transform": [5, 6, 7, 8, 11, 12, 13, 14, 16, 24, 27, 128, 130, 131, 132, 137, 139, 143, 146, 147, 148, 191, 192, 193, 195, 199, 200, 201, 204, 205, 207, 209, 211, 215, 218, 221, 222, 224, 226, 232, 239, 244, 253, 254, 255, 257, 259, 260, 267, 269, 273, 277, 280, 282, 284, 285, 287, 288, 292, 293, 295, 296, 298, 300, 304, 306, 307, 314, 315, 317, 318, 319, 324, 326, 329, 330, 332, 337, 339, 349, 351, 352, 354, 359], "transform_imag": [238, 243], "transient": [245, 248, 249, 350, 355], "transit": [86, 87, 126, 127, 317, 318, 324, 325, 330, 331, 337, 338, 343, 344], "transpar": [317, 321], "transpos": [317, 319], "travel": [118, 121, 267, 283], "treat": [285, 288, 291, 350, 351], "tree": [4, 5, 6, 12, 171, 174, 224, 225, 259, 261, 262, 267, 281, 283, 285, 288, 337, 338, 339], "tree_method": [4, 171, 174, 337, 340], "tremend": [285, 292], "trend": [267, 283], "tri": [285, 288, 337, 338], "trial": [4, 7, 12, 13, 14, 171, 174, 253, 257, 285, 292, 293, 300], "trial_id": 13, "tribul": [285, 292], "trier": [285, 292], "trigger": [10, 128, 130, 137, 139, 147, 154, 206, 211, 214, 267, 277, 284, 324, 326, 337, 339, 342], "trim": [317, 319, 350, 352], "trip": [4, 9, 12, 171, 174, 198, 201, 204, 285, 291, 343, 345, 349], "trip_amount": [4, 171, 174], "trip_dist": [4, 9, 12, 171, 174, 198, 201, 204], "trip_dur": 12, "trivial": [101, 102, 107], "trndnl": [267, 283], "troubleshoot": [82, 83, 88, 89, 118, 125, 155, 158], "truck": [118, 122], "true": [0, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 35, 39, 101, 102, 108, 110, 113, 114, 115, 116, 118, 121, 123, 128, 130, 131, 137, 138, 139, 140, 142, 143, 146, 147, 149, 150, 164, 166, 167, 168, 181, 185, 198, 201, 206, 210, 212, 213, 218, 222, 224, 225, 226, 230, 232, 237, 253, 255, 257, 259, 262, 263, 285, 288, 293, 298, 317, 319, 320, 321, 324, 327, 328, 330, 332, 334, 335, 337, 339, 341, 343, 345, 346, 347, 349, 350, 351, 352, 353, 354, 355, 356, 357, 359], "truli": [267, 283, 285, 292], "trump": [267, 283], "truncat": [293, 298, 305, 306, 307, 308, 309, 315, 316, 317, 319, 324, 326], "trust": [17, 19, 22, 267, 283, 343, 349], "truth": [10, 15, 206, 212, 215, 224, 226, 324, 326, 343, 344, 347, 349, 350, 359], "try": [2, 3, 7, 8, 13, 14, 28, 29, 43, 44, 51, 53, 55, 64, 66, 68, 110, 117, 118, 125, 175, 180, 181, 185, 191, 196, 245, 252, 253, 257, 285, 291, 292, 317, 319, 323, 337, 342, 350, 352], "tryna": [267, 283], "tsui": [285, 291], "ttft": [101, 102, 106], "ttm": [285, 291, 292], "tuesdai": [267, 283], "tune": [8, 10, 12, 90, 91, 101, 102, 107, 110, 117, 118, 120, 121, 124, 125, 126, 127, 128, 135, 137, 138, 191, 195, 196, 206, 212, 238, 241, 245, 252, 256, 285, 292, 317, 323, 324, 329, 330, 336, 337, 341, 342, 343, 349, 350, 351, 359], "tune_config": [4, 7, 12, 14, 171, 174, 253, 257], "tuneconfig": [4, 7, 12, 14, 171, 174, 253, 257], "tuner": [4, 7, 12, 14, 171, 174, 253, 257], "tupl": [3, 6, 10, 181, 184, 206, 213, 238, 239, 240, 259, 262], "turn": [2, 7, 9, 128, 129, 137, 138, 147, 148, 175, 178, 198, 203, 253, 257, 267, 283, 285, 292, 337, 338], "tutori": [17, 20, 82, 83, 86, 87, 110, 112, 126, 127, 128, 129, 133, 137, 138, 146, 147, 148, 153, 224, 226, 251, 317, 318, 323, 324, 325, 330, 331, 332, 336, 337, 338, 343, 344, 345, 347, 349, 350, 351, 352], "tv": [267, 283, 285, 291, 292], "tweet": [267, 283], "tweet_ev": [267, 271, 281], "twilight": [267, 283], "twitter": [267, 283], "two": [2, 3, 17, 22, 24, 26, 84, 85, 90, 91, 105, 137, 140, 155, 158, 164, 166, 168, 175, 180, 181, 183, 184, 185, 224, 227, 236, 238, 242, 267, 275, 283, 289, 292, 330, 332, 334, 336, 337, 339, 343, 345, 350, 351, 357], "txt": [0, 1, 86, 87, 126, 127, 128, 129, 137, 138, 147, 148, 168, 169, 170, 278, 286, 294, 312], "type": [3, 5, 6, 7, 8, 9, 10, 11, 14, 16, 18, 22, 24, 26, 28, 32, 35, 41, 43, 50, 53, 62, 66, 71, 77, 80, 81, 84, 85, 96, 98, 101, 102, 106, 110, 113, 123, 124, 128, 134, 137, 141, 144, 147, 153, 159, 163, 164, 167, 168, 181, 185, 191, 192, 195, 198, 205, 206, 210, 212, 213, 218, 219, 253, 254, 259, 260, 262, 267, 269, 280, 285, 288, 293, 296, 338, 341, 342], "typic": [9, 24, 27, 100, 110, 112, 118, 121, 128, 130, 198, 200, 266, 267, 279, 285, 292, 317, 319, 330, 332, 350, 351], "u": [6, 7, 12, 13, 14, 15, 16, 28, 30, 35, 38, 39, 43, 45, 53, 56, 66, 69, 70, 71, 86, 87, 118, 121, 125, 128, 133, 136, 137, 139, 143, 146, 147, 150, 153, 155, 158, 164, 166, 224, 227, 253, 256, 259, 262, 263, 267, 283, 285, 288, 292, 324, 325, 330, 331, 332, 336], "u002c": [267, 283], "u002c000": [267, 283], "u2019": [267, 283], "u2019ll": [267, 283], "u2019m": [267, 283], "u2019r": [267, 283], "u2019t": [267, 283], "u2019v": [267, 283], "u_": [324, 325, 330, 331], "u_k": [324, 325], "uber": [7, 253, 258], "ubj": [4, 12, 171, 174], "ubyt": [13, 14], "udf": [8, 191, 195], "ui": [0, 16, 17, 22, 86, 87, 90, 91, 147, 153], "uid": [330, 332, 336], "uint8": [16, 128, 130, 131, 147, 149, 159, 163, 238, 243], "un": [285, 292], "unabl": [285, 292], "unassoci": [28, 30, 43, 45, 53, 56], "unattach": [28, 30, 43, 45, 53, 56], "unavail": [3, 181, 187], "unavoid": [285, 288], "unbound": [10, 206, 212], "uncaptur": [128, 132], "uncas": [305, 306, 307, 315], "uncertainti": [343, 349], "unchang": [238, 243], "uncl": [285, 292], "uncom": [35, 39, 43, 46, 53, 58, 118, 122, 123, 293, 299], "uncondit": [317, 323], "unconnect": [285, 292], "unconnectedbr": [285, 292], "under": [3, 10, 66, 69, 84, 85, 92, 93, 126, 127, 137, 144, 164, 166, 181, 184, 206, 210, 224, 226, 227, 230, 234, 238, 242, 285, 291, 317, 319, 321, 323, 324, 328, 329, 330, 332, 336, 343, 344, 345, 347, 350, 352, 355], "underbrac": [343, 344], "underli": [3, 4, 5, 6, 10, 94, 95, 110, 113, 171, 174, 181, 183, 206, 211, 224, 225, 259, 261], "undersid": [285, 292], "understand": [7, 10, 14, 17, 18, 20, 79, 88, 89, 92, 93, 101, 102, 103, 104, 106, 108, 109, 110, 112, 113, 155, 157, 159, 163, 164, 167, 206, 212, 253, 256, 285, 291, 292, 293, 298, 330, 336, 337, 338, 350, 351], "understat": [285, 292], "understood": [110, 117], "underutil": [101, 102, 104, 105], "uneasy": [267, 283], "unet": [6, 259, 262], "unet2dconditionmodel": [6, 259, 260, 262], "unexpect": [5, 6, 155, 157, 224, 225, 259, 261, 267, 283, 343, 345], "ungat": [110, 113, 118, 121], "unifi": [1, 4, 8, 17, 23, 100, 101, 102, 107, 128, 134, 147, 153, 169, 170, 171, 173, 191, 192, 195], "uniform": [3, 4, 171, 174, 181, 188, 293, 298, 324, 325], "uniformli": [10, 206, 208], "uniniti": [10, 15, 206, 213], "uninstal": [43, 51, 53, 64, 66, 78], "uniqu": [9, 15, 86, 87, 101, 102, 106, 110, 113, 137, 139, 198, 201, 224, 232, 234, 285, 289, 292, 330, 336, 350, 352], "unique_item": [330, 336], "unique_us": [330, 336], "unit": [10, 11, 15, 101, 102, 106, 118, 123, 206, 215, 218, 221, 223, 285, 288, 350, 352], "univari": [343, 346], "univers": [285, 291], "unless": [9, 90, 91, 94, 95, 198, 202, 224, 237, 245, 247, 285, 288], "unlik": [90, 91, 224, 226, 238, 243, 267, 283], "unnecessari": [10, 11, 88, 89, 90, 91, 92, 93, 137, 139, 206, 212, 218, 220, 224, 234, 237, 267, 283, 324, 329, 337, 340], "unnot": [285, 292], "unpredict": [101, 102, 106], "unread": [317, 319], "unregist": [28, 33, 43, 51, 53, 64], "unreleas": [267, 283], "unrelentingli": [285, 291, 292], "unrival": [285, 292], "uns4": [267, 283], "unshuffl": [337, 339], "unsloth": [101, 102, 108, 110, 113, 118, 121], "unsqueez": [5, 13, 224, 237, 324, 329, 343, 345, 346, 347, 349], "unstabl": [317, 318], "unstructur": [8, 191, 192, 208, 360], "until": [3, 5, 8, 9, 13, 15, 101, 102, 105, 106, 159, 163, 181, 183, 191, 197, 198, 201, 202, 224, 235, 267, 275, 283, 285, 292, 337, 340], "untitl": [82, 83], "unus": [28, 30, 43, 45, 53, 56], "unveil": [267, 283], "unwrap": [5, 13, 224, 234, 245, 248], "up": [0, 3, 5, 6, 7, 8, 9, 10, 11, 15, 17, 18, 21, 24, 27, 28, 30, 33, 34, 35, 38, 39, 42, 45, 46, 50, 52, 56, 58, 62, 65, 66, 69, 70, 78, 84, 85, 96, 97, 101, 102, 106, 108, 111, 116, 117, 119, 121, 124, 126, 127, 128, 129, 130, 132, 133, 135, 136, 137, 138, 139, 142, 146, 147, 148, 151, 153, 154, 156, 157, 164, 165, 181, 185, 186, 191, 197, 198, 201, 206, 211, 213, 218, 221, 222, 225, 235, 248, 250, 253, 256, 257, 259, 261, 262, 266, 267, 273, 279, 282, 283, 285, 287, 288, 291, 292, 293, 295, 299, 319, 321, 325, 328, 334, 335, 338, 339, 340, 348, 351, 352, 356, 358], "up_block_typ": [6, 259, 262], "upblock2d": [6, 259, 262], "upcom": 199, "updat": [0, 1, 4, 5, 6, 7, 8, 11, 14, 16, 43, 46, 51, 53, 58, 64, 66, 73, 84, 85, 88, 89, 92, 93, 110, 115, 128, 129, 137, 138, 143, 147, 148, 153, 154, 169, 170, 171, 174, 191, 192, 218, 223, 224, 225, 228, 234, 245, 247, 253, 257, 259, 263, 278, 286, 293, 294, 298, 302, 307, 312, 313, 317, 323, 343, 349, 350, 355], "upgrad": [11, 43, 45, 46, 48, 53, 56, 58, 60, 73, 76, 82, 83, 92, 93, 147, 153, 218, 221, 317, 323, 324, 325, 329], "upload": [5, 13, 86, 87, 118, 121, 147, 153, 337, 342], "upload_fil": [86, 87, 118, 121], "upon": [88, 89], "upper": [82, 83], "upright": [324, 325], "upscal": [16, 128, 133, 136, 137, 139, 143, 146], "upscale_delay_": 16, "upset": [267, 283], "ur": [267, 283], "uri": [86, 87, 137, 144], "url": [8, 90, 91, 128, 136, 147, 149, 150, 153, 191, 197, 343, 345], "url_to_arrai": [128, 136, 147, 148, 149], "urljoin": [102, 108, 110, 114], "urllib": [102, 108, 110, 114, 137, 146, 147, 148], "urlpars": [137, 146, 147, 148, 150], "urmitz": [285, 292], "us": [2, 3, 11, 13, 20, 21, 23, 25, 27, 28, 29, 31, 34, 35, 36, 39, 40, 43, 44, 47, 52, 53, 54, 55, 59, 63, 65, 66, 67, 68, 69, 70, 71, 75, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 111, 112, 114, 115, 117, 120, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 142, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 157, 158, 159, 162, 163, 165, 166, 168, 173, 174, 175, 177, 178, 179, 181, 183, 184, 185, 186, 187, 188, 189, 190, 192, 193, 194, 195, 197, 199, 201, 202, 203, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 220, 222, 223, 226, 227, 228, 230, 231, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 250, 251, 252, 256, 258, 260, 262, 263, 264, 266, 267, 271, 273, 275, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 289, 290, 291, 292, 293, 294, 295, 297, 298, 299, 300, 306, 309, 311, 312, 315, 316, 319, 321, 323, 326, 328, 329, 333, 334, 336, 340, 341, 342, 345, 347, 349, 352, 355, 357, 359], "usabl": [118, 122, 238, 243], "usag": [11, 12, 13, 14, 16, 88, 89, 101, 102, 105, 107, 118, 120, 121, 124, 155, 157, 159, 161, 163, 218, 220, 293, 300, 330, 334], "use_gpu": [4, 5, 6, 12, 13, 137, 143, 171, 174, 224, 225, 230, 259, 263, 317, 321, 324, 328, 330, 334, 337, 340, 343, 347, 350, 351, 356], "use_gpu_actor": [350, 359], "usecol": [330, 336], "user": [1, 3, 5, 8, 10, 13, 14, 17, 19, 24, 26, 66, 69, 80, 81, 86, 87, 88, 89, 90, 91, 94, 95, 97, 101, 102, 104, 105, 106, 108, 110, 114, 115, 118, 121, 122, 123, 124, 126, 127, 128, 133, 137, 142, 143, 144, 159, 161, 162, 163, 164, 167, 169, 170, 181, 183, 187, 191, 195, 196, 206, 212, 224, 228, 231, 232, 267, 281, 283, 284, 293, 300, 302, 307, 313, 333, 334], "user2idx": [330, 332, 336], "user_col": [330, 332], "user_embed": [330, 333, 336], "user_id": [168, 330, 332, 336], "user_idx": [330, 331, 332, 333, 334, 336], "user_nam": [66, 69], "user_storag": [86, 87], "user_vec": [330, 333], "user_vector": [330, 336], "userguid": [28, 29, 43, 44, 53, 55], "userservic": [164, 167, 168], "usual": [8, 9, 191, 194, 198, 202, 224, 237, 285, 291], "utc": 13, "util": [3, 5, 6, 7, 10, 11, 13, 14, 16, 88, 89, 92, 93, 101, 102, 104, 105, 106, 107, 126, 127, 128, 129, 130, 134, 136, 137, 146, 147, 148, 151, 153, 154, 164, 167, 181, 187, 206, 208, 212, 217, 218, 220, 221, 224, 225, 226, 232, 234, 238, 239, 245, 252, 253, 254, 255, 257, 259, 260, 262, 293, 295, 296, 298, 317, 319, 324, 326, 329, 330, 332, 343, 345, 350, 351, 352], "uuid": [324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "uv": [126, 127, 128, 129, 137, 138, 147, 148, 278, 286, 294, 312], "ux": [8, 191, 195], "v": [17, 20, 27, 43, 49, 50, 53, 61, 62, 82, 83, 90, 91, 101, 102, 105, 118, 124, 126, 127, 137, 139, 194, 195, 224, 225, 237, 245, 251, 267, 283, 293, 298, 330, 331, 336, 343, 347, 349, 350, 359, 360], "v1": [24, 27, 102, 108, 110, 114, 115, 118, 121, 122, 123, 324, 325, 326, 329], "v2": [137, 138, 224, 225, 267, 273, 282, 317, 318, 321, 324, 325, 330, 331, 337, 338, 343, 344], "v_": [330, 331], "val": [137, 139, 143, 321, 326, 330, 332, 334, 337, 339, 340, 343, 345, 347, 349, 350, 353, 355, 357, 359], "val_batch": [350, 355], "val_d": [137, 139, 143, 317, 319, 321, 324, 326, 328, 330, 332, 334, 337, 339, 340, 341, 342], "val_dataload": [317, 321, 324, 328], "val_df": [337, 339], "val_load": [317, 321, 324, 328, 330, 334, 343, 347, 350, 355], "val_loss": [137, 143, 144, 147, 150, 317, 320, 321, 324, 327, 328, 330, 331, 334, 343, 347, 350, 355, 356, 357], "val_loss_sum": [343, 347], "val_loss_tot": [350, 355], "val_parquet": [337, 339], "val_pd": [337, 340, 341], "val_record": [343, 345], "val_xb": [350, 355], "val_yb": [350, 355], "valid": [4, 12, 16, 79, 92, 93, 118, 122, 137, 143, 171, 174, 293, 298, 317, 320, 321, 324, 325, 326, 327, 328, 331, 340, 341, 342, 351, 355, 356, 359], "valid_dataset": 12, "valid_dataset_featur": 12, "validation_step": [317, 320, 324, 327], "valu": [3, 5, 7, 10, 13, 14, 28, 29, 30, 35, 36, 39, 43, 44, 45, 46, 51, 53, 54, 56, 58, 64, 66, 67, 72, 73, 92, 93, 104, 164, 166, 167, 181, 183, 184, 188, 206, 211, 212, 224, 229, 232, 233, 236, 238, 243, 253, 255, 285, 290, 317, 318, 321, 324, 325, 326, 330, 332, 336, 343, 344, 345, 346, 350, 355, 357], "valuabl": [285, 291], "value_count": [330, 332, 337, 339], "valueerror": [3, 181, 185, 317, 321, 324, 328], "values_nginx": [43, 46, 53, 58], "values_nginx_gke_priv": [66, 73], "values_nginx_gke_publ": [66, 73], "values_nvdp": [43, 46, 53, 58], "vamp": [267, 283], "vampett": [267, 283], "vampir": [267, 283], "van": [267, 283], "vanilla": [6, 101, 102, 105, 172, 254, 259, 263, 343, 344, 345], "var": [17, 22, 66, 78, 137, 138, 224, 226, 317, 319, 324, 326, 330, 332, 337, 339, 343, 345, 350, 352], "varepsilon": [317, 318], "varepsilon_": [324, 325], "varepsilon_k": [324, 325], "vari": [8, 9, 10, 15, 101, 102, 106, 191, 192, 198, 204, 206, 208, 213, 215], "variabl": [3, 28, 30, 35, 39, 43, 45, 53, 56, 66, 69, 84, 85, 86, 87, 88, 89, 128, 136, 137, 146, 147, 154, 181, 186, 187], "variat": [7, 14, 253, 257], "varieti": [10, 15, 206, 210, 215, 285, 287], "variou": [8, 84, 85, 86, 87, 110, 117, 191, 193, 238, 239, 293, 299], "vast": [8, 191, 192], "ve": [28, 34, 101, 102, 109, 110, 117, 118, 120, 125, 137, 139, 267, 283, 285, 288, 292, 350, 359], "vector": [8, 10, 101, 102, 104, 128, 133, 191, 192, 206, 212, 266, 267, 279, 324, 326, 329, 330, 331, 333, 337, 342], "veget": [337, 338], "veloc": [324, 325], "venu": [307, 308, 309, 316], "venv": [0, 128, 129, 137, 138, 147, 148], "verbos": [164, 167], "veri": [5, 6, 7, 8, 9, 13, 14, 15, 110, 112, 191, 196, 198, 202, 224, 226, 253, 257, 259, 262, 263, 267, 283, 285, 292, 343, 349], "verif": 79, "verifi": [6, 10, 28, 33, 35, 38, 51, 63, 64, 66, 69, 72, 73, 74, 84, 85, 88, 89, 90, 91, 206, 212, 259, 262, 339, 343, 345], "vermaelen": [267, 283], "version": [1, 8, 15, 17, 22, 28, 29, 35, 37, 43, 44, 46, 53, 55, 58, 66, 68, 73, 74, 80, 81, 118, 121, 147, 150, 153, 155, 158, 159, 162, 164, 167, 169, 170, 191, 192, 238, 240, 267, 278, 283, 286, 294, 302, 307, 312, 313, 317, 319, 321, 343, 345], "versu": [137, 146, 317, 323, 337, 342, 350, 359], "vertex": [147, 151], "via": [0, 2, 3, 5, 6, 8, 10, 17, 20, 24, 26, 27, 28, 30, 35, 39, 43, 45, 53, 56, 82, 83, 94, 95, 96, 98, 126, 127, 155, 157, 164, 165, 167, 168, 175, 177, 181, 187, 191, 195, 196, 206, 208, 224, 225, 228, 229, 230, 237, 238, 243, 245, 252, 259, 261, 267, 283, 285, 291, 317, 320, 324, 328, 330, 332, 334, 350, 354, 355], "vicki": [267, 281, 283], "vid": [267, 283], "video": [8, 10, 191, 192, 206, 208, 267, 283, 285, 288, 292], "vietnam": [285, 288], "view": [1, 7, 9, 10, 11, 13, 14, 16, 17, 22, 28, 30, 32, 35, 39, 41, 43, 45, 50, 53, 56, 62, 63, 66, 77, 84, 85, 88, 89, 110, 116, 128, 133, 134, 137, 139, 144, 147, 150, 152, 153, 155, 157, 159, 163, 164, 166, 167, 169, 170, 198, 201, 202, 206, 212, 218, 223, 253, 257, 285, 288, 292, 317, 320, 324, 327], "viewer": [128, 134, 147, 153, 285, 288], "vincent": [285, 288], "violat": [118, 121], "viridi": [337, 341], "virtual": [0, 17, 20, 22, 27, 84, 85, 101, 102, 105, 128, 129, 137, 138, 147, 148, 155, 158], "virtuou": [285, 288], "visibl": [110, 116, 224, 233, 285, 288, 343, 349], "vision": [245, 252, 323, 359], "visit": [7, 14, 16, 155, 158, 253, 257], "visual": [7, 8, 9, 10, 15, 82, 83, 88, 89, 110, 116, 155, 157, 158, 159, 163, 191, 193, 198, 201, 206, 211, 253, 255, 285, 291, 292, 321, 323, 324, 325, 328, 334, 338, 347, 357], "visualis": [343, 345], "vit": [128, 131, 136, 137, 139, 147, 150, 245, 248, 250], "vit_b_16": [350, 359], "vit_l_32": [350, 359], "vllm": [108, 109, 110, 116, 117, 118, 122], "vm": [17, 18, 20, 27, 28, 32, 35, 39, 41, 66, 70, 79, 360], "vocal": [267, 283], "voic": [267, 283, 285, 292], "volatil": [5, 224, 226], "volleybal": [267, 283], "volum": [5, 8, 86, 87, 159, 163, 191, 192, 195, 224, 226, 337, 339, 350, 351, 359], "von": [285, 292], "vpc": [20, 21, 23, 24, 26, 28, 30, 34, 35, 36, 39, 43, 44, 45, 53, 55, 56, 66, 70, 78, 79, 101, 102, 107], "vpc_cidr": [17, 22], "vpc_id": [17, 22, 23, 28, 30], "vpcid": [28, 30, 43, 45, 53, 56], "vram": [110, 113, 267, 283], "vscode": [92, 93], "vstack": [137, 143], "vtripl": [118, 121], "vulva": [285, 288], "w": [7, 11, 14, 86, 87, 137, 139, 140, 143, 218, 222, 224, 237, 253, 257, 267, 281, 283, 285, 292, 317, 318, 319, 320, 350, 359], "w0": [3, 181, 190], "w1": [3, 181, 190], "wa": [6, 9, 12, 84, 85, 90, 91, 159, 163, 198, 201, 245, 250, 259, 262, 267, 283, 285, 288, 291, 292, 330, 335], "wai": [1, 3, 8, 9, 10, 16, 17, 20, 82, 83, 90, 91, 96, 99, 110, 116, 128, 130, 135, 136, 137, 146, 147, 153, 154, 169, 170, 181, 184, 191, 193, 194, 198, 200, 206, 211, 212, 224, 234, 267, 283, 285, 287, 291, 292, 302, 307, 308, 309, 313, 316, 324, 329, 350, 351, 353, 359], "wait": [2, 66, 73, 84, 85, 101, 102, 105, 128, 130, 137, 138, 175, 179, 180, 182, 188, 267, 283], "wake": [267, 283, 285, 292], "walk": [5, 7, 13, 14, 28, 29, 43, 44, 53, 54, 66, 67, 82, 83, 86, 87, 110, 111, 118, 121, 253, 254, 267, 277, 283, 284, 317, 318, 337, 338, 343, 344], "walter": [285, 291], "wander": [285, 288], "wanna": [267, 283], "want": [0, 2, 3, 5, 6, 8, 9, 10, 11, 12, 15, 16, 17, 22, 35, 39, 42, 53, 56, 66, 69, 78, 82, 83, 88, 89, 101, 102, 106, 128, 130, 137, 143, 147, 149, 153, 175, 179, 181, 183, 184, 189, 190, 191, 193, 198, 201, 203, 204, 206, 212, 213, 215, 218, 222, 223, 224, 226, 232, 233, 237, 259, 262, 267, 273, 281, 282, 283, 285, 288, 291, 292, 293, 300, 343, 344, 347], "war": [267, 283, 285, 288], "warehous": [10, 206, 210], "warm": [6, 259, 262, 267, 283], "warmth": [267, 283], "warn": [267, 283, 285, 291, 292, 317, 321, 324, 328], "warner": [267, 281, 283], "wasn": [285, 291, 343, 345], "wast": [285, 292], "watch": [66, 73, 267, 283, 285, 292], "water": [285, 292], "wave": [285, 292], "wc": [9, 198, 201], "we": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 28, 29, 35, 37, 43, 44, 46, 50, 53, 55, 58, 62, 66, 68, 70, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 100, 101, 102, 103, 105, 108, 109, 111, 112, 113, 115, 119, 121, 122, 128, 131, 135, 136, 137, 146, 147, 154, 164, 166, 171, 174, 175, 179, 180, 181, 184, 185, 186, 188, 190, 191, 193, 194, 198, 201, 202, 203, 204, 206, 210, 212, 213, 214, 215, 218, 222, 224, 226, 227, 228, 229, 230, 231, 234, 235, 237, 238, 240, 241, 242, 243, 244, 245, 247, 248, 253, 256, 257, 259, 262, 263, 266, 267, 279, 283, 285, 288, 289, 291, 292, 293, 297, 299, 300, 302, 304, 307, 309, 311, 313, 314, 316, 330, 331, 334, 350, 357], "wealth": [285, 292], "weather": [343, 349], "weaviat": [8, 191, 192], "web": [82, 83, 88, 89, 155, 157, 165, 302, 307, 313, 324, 329], "webservic": [304, 314], "websit": [0, 155, 158], "webster": [285, 291], "wed": [267, 283], "wednesdai": [267, 281, 283], "week": [267, 283, 343, 344, 345], "weight": [3, 13, 101, 102, 106, 110, 113, 137, 139, 143, 181, 190, 224, 225, 228, 234, 237, 245, 248, 252, 317, 323, 324, 329, 330, 334, 336, 337, 339, 342, 343, 349, 350, 359], "weight_decai": [6, 259, 262, 263], "weights_onli": [5, 6, 13, 224, 237, 259, 263], "welbeck": [267, 283], "welcom": [12, 168], "well": [3, 8, 17, 22, 24, 26, 84, 85, 96, 99, 128, 133, 135, 137, 145, 147, 153, 181, 190, 191, 196, 267, 283, 285, 291, 292, 302, 307, 313, 330, 332, 333], "wellcraft": [285, 292], "welllll": [267, 283], "went": [285, 292], "were": [6, 245, 250, 259, 262, 267, 283, 285, 291, 292, 337, 339], "werewolf": [285, 292], "west": [12, 13, 14, 28, 30, 43, 45, 53, 56, 86, 87, 118, 121, 128, 133, 136, 137, 139, 143, 146, 147, 150, 153], "west2": [35, 38, 39, 66, 69, 70, 71], "western": [285, 291, 292], "wget": [330, 332], "what": [2, 4, 6, 7, 10, 12, 14, 18, 35, 36, 66, 70, 80, 81, 103, 108, 121, 123, 137, 139, 156, 159, 160, 162, 163, 171, 172, 174, 175, 179, 199, 206, 212, 230, 252, 253, 257, 259, 262, 267, 283, 285, 288, 292, 332], "whatev": [337, 342, 343, 349, 350, 359], "when": [1, 2, 3, 7, 14, 28, 30, 33, 35, 39, 42, 43, 45, 46, 50, 53, 56, 58, 62, 66, 70, 78, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 104, 105, 110, 114, 115, 116, 118, 123, 128, 130, 132, 135, 137, 145, 147, 153, 159, 162, 163, 164, 165, 167, 169, 170, 175, 179, 180, 181, 183, 185, 186, 187, 189, 190, 197, 199, 202, 203, 207, 209, 210, 211, 212, 213, 214, 215, 219, 232, 235, 236, 238, 239, 245, 248, 251, 253, 257, 260, 266, 267, 273, 279, 282, 283, 285, 288, 292, 307, 311, 316, 317, 319, 323, 324, 329, 330, 334, 336, 337, 339, 343, 347, 349, 350, 352, 355, 357], "where": [2, 5, 6, 7, 8, 9, 12, 13, 14, 53, 56, 66, 71, 86, 87, 94, 95, 96, 98, 100, 101, 102, 104, 110, 112, 128, 132, 135, 137, 146, 147, 149, 151, 155, 158, 159, 162, 175, 177, 191, 196, 197, 198, 201, 224, 234, 235, 236, 238, 244, 250, 253, 255, 259, 263, 267, 283, 285, 288, 291, 292, 293, 298, 302, 307, 313, 325, 331, 332, 335, 338, 344, 348, 351, 353], "wherea": [8, 191, 194, 197], "wherev": [267, 283], "whether": [1, 5, 6, 7, 12, 13, 14, 79, 86, 87, 94, 95, 169, 170, 224, 227, 230, 231, 232, 253, 257, 259, 263, 293, 295, 330, 334, 337, 339, 343, 347], "which": [2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 20, 27, 28, 30, 43, 45, 53, 56, 57, 63, 66, 69, 84, 85, 92, 93, 110, 112, 116, 118, 121, 123, 126, 127, 128, 130, 131, 147, 151, 155, 158, 159, 163, 164, 167, 171, 174, 175, 180, 181, 182, 183, 184, 188, 189, 190, 191, 193, 196, 198, 201, 202, 203, 204, 206, 211, 212, 213, 214, 224, 226, 228, 232, 233, 234, 236, 238, 240, 253, 257, 259, 262, 266, 267, 279, 285, 288, 291, 292, 293, 295, 298, 299, 302, 307, 313, 317, 318, 319, 337, 338, 341, 343, 347, 349, 350, 351, 352, 355, 359], "while": [3, 4, 8, 11, 82, 83, 84, 85, 88, 89, 100, 110, 112, 118, 121, 128, 134, 135, 137, 139, 145, 147, 153, 154, 159, 162, 163, 164, 167, 171, 173, 181, 188, 189, 191, 192, 196, 218, 221, 224, 225, 227, 228, 238, 239, 240, 241, 285, 288, 291, 292, 317, 319, 321, 330, 331, 332, 337, 339, 343, 344, 345, 350, 351, 352, 357], "whilst": [285, 292], "white": [7, 14, 253, 255, 267, 283, 285, 292], "whl": [126, 127], "who": [17, 22, 79, 94, 95, 101, 102, 106, 267, 283, 285, 288, 291, 292, 293, 300], "whole": [4, 12, 171, 174, 337, 340], "whose": [285, 291, 292], "why": [10, 111, 117, 206, 207, 224, 225, 245, 246, 267, 283, 285, 291], "whyyyyyyi": [267, 283], "wichita": [267, 283], "wide": [8, 66, 73, 128, 130, 191, 195, 267, 283, 285, 292, 302, 307, 313], "widescreen": [285, 291], "width": [10, 15, 16, 159, 163, 206, 212, 317, 319], "wife": [285, 292], "wildlif": [307, 308, 309, 316], "wilki": [285, 291], "willam": [267, 281, 283], "william": [267, 283, 285, 291], "wilmer": [267, 283], "win": [8, 191, 196, 267, 281, 283], "wind": [285, 291], "window": [1, 35, 38, 82, 83, 110, 116, 155, 158, 169, 170, 245, 252, 285, 288, 344, 346, 349], "wire": [245, 252, 317, 321], "wise": [224, 237], "wish": [267, 283], "with_resourc": [7, 14, 253, 257], "within": [3, 8, 17, 19, 22, 24, 26, 82, 83, 86, 87, 90, 91, 94, 95, 96, 98, 128, 135, 155, 158, 181, 187, 191, 192, 324, 329, 330, 332], "without": [3, 8, 11, 17, 22, 82, 83, 84, 85, 100, 101, 102, 105, 118, 121, 122, 126, 127, 128, 130, 155, 158, 181, 183, 184, 187, 189, 191, 192, 218, 221, 224, 225, 245, 246, 248, 249, 285, 288, 292, 317, 318, 323, 324, 325, 330, 331, 332, 334, 335, 336, 337, 340, 341, 342, 343, 344, 345, 349, 350, 351, 358], "woman": [285, 291, 292], "women": [285, 288], "won": [3, 8, 128, 130, 181, 185, 191, 196, 285, 288], "wonder": [7, 24, 26, 253, 257, 267, 281, 283], "wood": [267, 283], "wooden": [285, 291], "word": [101, 102, 104, 285, 292], "work": [1, 2, 3, 5, 7, 9, 11, 13, 24, 26, 27, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 100, 101, 102, 104, 110, 113, 117, 118, 120, 126, 127, 128, 132, 135, 169, 170, 175, 180, 181, 183, 198, 203, 204, 207, 218, 221, 231, 238, 242, 245, 252, 253, 256, 266, 267, 273, 279, 282, 283, 285, 291, 292, 293, 299, 322, 324, 325, 330, 331, 332, 334, 337, 338, 339, 350, 351, 353, 358, 359], "worker": [1, 3, 4, 9, 10, 11, 13, 15, 17, 22, 24, 26, 43, 50, 53, 62, 80, 81, 92, 93, 126, 127, 128, 132, 134, 137, 143, 144, 147, 153, 159, 161, 163, 169, 170, 171, 174, 181, 183, 185, 186, 188, 190, 198, 200, 204, 206, 213, 215, 218, 221, 225, 227, 229, 230, 231, 232, 233, 234, 235, 237, 238, 241, 242, 243, 244, 245, 246, 248, 249, 267, 283, 285, 290, 295, 299, 300, 317, 318, 319, 320, 321, 323, 324, 325, 328, 329, 330, 331, 332, 334, 338, 339, 342, 343, 344, 345, 347, 349, 350, 351, 352, 353, 354, 355, 356, 357, 359], "worker_devic": [267, 275, 283], "worker_nod": [43, 50, 53, 62, 159, 163], "worker_rank": [4, 171, 174], "workernodegroupconfig": [43, 50, 53, 62], "workflow": [1, 8, 24, 26, 80, 81, 82, 83, 88, 89, 90, 91, 100, 118, 123, 128, 130, 147, 154, 169, 170, 191, 192, 194, 195, 196, 219, 224, 225, 234, 237, 245, 252, 266, 267, 277, 278, 279, 284, 285, 286, 287, 292, 293, 294, 299, 300, 302, 307, 312, 313, 324, 325, 337, 338, 339, 343, 344, 349, 350, 351], "working_dir": [90, 91, 101, 102, 108, 110, 115, 128, 129, 137, 138, 147, 148], "workload": [1, 3, 4, 5, 6, 8, 9, 10, 12, 15, 17, 22, 25, 27, 28, 34, 43, 52, 53, 65, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 99, 100, 101, 102, 106, 110, 112, 117, 126, 127, 128, 129, 130, 132, 134, 135, 137, 138, 139, 143, 144, 145, 146, 147, 154, 155, 158, 159, 162, 163, 165, 169, 170, 171, 173, 174, 181, 187, 191, 192, 195, 196, 198, 205, 206, 208, 224, 232, 238, 239, 245, 252, 259, 262, 302, 307, 313, 323, 329, 336, 349, 359], "workload_identity_pool_provid": [35, 39, 66, 70], "workloadidentitypool": [35, 39, 66, 70], "workloadserviceaccountnam": [43, 45, 48, 53, 56, 60, 66, 76], "workshop": [80, 81], "workspac": [17, 21, 22, 24, 26, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 110, 112, 116, 117, 126, 127, 128, 129, 135, 137, 138, 145, 147, 148, 153, 324, 325, 337, 342, 343, 349, 350, 351, 359, 360], "workspace_v2": [82, 83], "world": [5, 82, 83, 90, 91, 118, 125, 164, 167, 224, 228, 267, 283, 285, 292, 293, 298, 317, 321, 330, 331, 336, 350, 351], "world_rank": [5, 224, 233, 235], "world_siz": [5, 224, 228, 238, 240], "worri": [126, 127, 350, 351], "worth": [267, 283], "would": [5, 6, 9, 13, 110, 112, 117, 128, 133, 159, 163, 198, 202, 203, 204, 224, 230, 259, 262, 263, 267, 283, 285, 287, 289, 291, 292, 293, 298], "wound": [285, 291], "wrangl": [224, 226], "wrap": [3, 5, 13, 92, 93, 100, 128, 135, 137, 145, 147, 151, 181, 184, 225, 226, 227, 228, 232, 234, 235, 238, 244, 318, 325, 326, 332, 340, 344, 345, 351, 354], "wright": [267, 283], "write": [2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15, 17, 22, 82, 83, 86, 87, 128, 129, 130, 137, 139, 159, 163, 168, 171, 174, 175, 177, 180, 181, 190, 191, 192, 199, 200, 202, 206, 207, 209, 211, 212, 216, 224, 234, 238, 242, 245, 248, 253, 257, 259, 263, 317, 319, 330, 331, 343, 345, 350, 351, 352, 353, 355], "write_csv": [9, 198, 203], "write_parquet": [4, 9, 10, 15, 128, 133, 137, 139, 159, 163, 164, 166, 171, 174, 198, 203, 206, 211, 216, 317, 319, 343, 345], "write_t": [343, 345, 350, 352], "writefil": [164, 166], "writer": [5, 13], "writerow": [5, 13], "written": [1, 8, 155, 158, 164, 167, 169, 170, 191, 194, 224, 226, 234, 285, 291, 317, 319, 337, 339, 343, 345, 350, 355], "wrong": [267, 283], "wrote": [9, 198, 203, 267, 283, 317, 319, 330, 332, 337, 339, 350, 352], "wt": [267, 281, 283], "ww2": [285, 292], "wwe": [267, 281, 283], "wyom": [285, 291], "x": [2, 3, 6, 7, 8, 14, 88, 89, 128, 129, 137, 138, 147, 148, 150, 153, 159, 163, 168, 175, 180, 181, 183, 184, 185, 189, 191, 193, 224, 237, 253, 257, 259, 262, 267, 283, 317, 318, 320, 324, 327, 329, 337, 340, 343, 346, 349, 350, 351, 359], "x_": [317, 318, 324, 325], "x_0": [317, 318, 320, 324, 325], "x_t": [317, 318, 320, 324, 325], "x_test": [4, 171, 174], "x_train": [4, 171, 174], "xb": [343, 345, 350, 355], "xgb": [5, 6, 224, 225, 259, 261, 337, 338, 339, 340, 341, 342], "xgb_model": [337, 340], "xgb_param": [337, 340], "xgboost": [7, 12, 253, 257, 339, 341, 342], "xgboost_predict": [4, 171, 174], "xgboosterror": [4, 171, 172], "xgboosttrain": [4, 12, 171, 172, 174, 337, 339, 340], "xgboosttrainer_2024": 12, "xgboosttrainer_81312_00000terminated10": 12, "xgboosttrainer_81312_00001terminated10": 12, "xgboosttrainer_81312_00002terminated10": 12, "xgbpredictor": [337, 341, 342], "xing": [35, 39, 66, 70], "xlabel": [317, 321, 324, 328, 330, 332, 334, 337, 341, 343, 347, 349, 350, 357], "xxx": [35, 39, 53, 56, 66, 69, 70], "xxxx": [35, 39, 66, 70], "xxxxx": [28, 30, 35, 39, 40, 43, 47, 53, 59, 66, 70, 75], "xxxxxx": [28, 30, 43, 45, 53, 56], "xxxxxxx": [43, 45, 53, 56], "xxxxxxxx": [28, 30, 43, 45, 53, 56], "xxxxxxxxx": [28, 30], "xxxxxxxxxx": [28, 30], "xxxxxxxxxxxx": [28, 30, 43, 45, 53, 56], "y": [1, 3, 5, 7, 13, 14, 88, 89, 102, 108, 110, 114, 116, 118, 121, 122, 123, 169, 170, 181, 183, 253, 257, 267, 283, 337, 340, 350, 351], "y_pred": [137, 140, 143, 146], "y_prob": [137, 140], "y_test": [4, 171, 174], "y_train": [4, 171, 174], "y_true": [137, 143], "ya": [267, 283], "yaml": [11, 43, 46, 53, 58, 66, 73, 92, 93, 101, 102, 108, 110, 115, 118, 122, 126, 127, 128, 135, 137, 145, 147, 153, 159, 163, 168, 218, 223], "yanke": [267, 281, 283], "yann": [13, 14], "yara": [118, 121], "yard": [267, 283], "yb": [343, 345, 350, 355], "ye": [17, 22, 267, 283], "year": [4, 118, 121, 123, 171, 174, 267, 283, 285, 288, 292], "yellow": [4, 12, 101, 102, 105, 171, 174, 285, 288], "yellow_tripdata_": [4, 171, 174], "yellow_tripdata_2011": [9, 164, 166, 198, 201, 204], "yellow_tripdata_2021": [4, 171, 174], "yelp": [293, 295, 300], "yelp_review_ful": [293, 298], "yepo": [267, 283], "yesterdai": [267, 283], "yet": [285, 292, 330, 331, 332, 350, 351], "yield": [3, 181, 188], "ylabel": [317, 321, 324, 328, 330, 332, 334, 337, 339, 341, 343, 345, 347, 349, 350, 357], "yml": 0, "york": [4, 9, 171, 174, 198, 201, 343, 344], "you": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 100, 101, 102, 103, 105, 106, 107, 108, 110, 111, 114, 115, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 130, 131, 133, 134, 135, 136, 137, 139, 142, 143, 144, 145, 146, 147, 149, 151, 152, 153, 154, 155, 158, 159, 160, 161, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 179, 181, 182, 184, 185, 186, 187, 188, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 211, 212, 213, 214, 215, 216, 218, 219, 223, 226, 229, 230, 231, 232, 233, 234, 235, 236, 237, 244, 250, 251, 252, 253, 254, 257, 259, 260, 261, 263, 266, 267, 271, 273, 275, 279, 281, 282, 283, 285, 287, 288, 289, 290, 291, 292, 293, 295, 298, 299, 300, 302, 307, 309, 313, 316, 319, 321, 326, 327, 328, 332, 334, 339, 340, 345, 346, 347, 352, 353, 354, 357], "young": [285, 288, 292], "your": [0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 22, 25, 26, 28, 29, 30, 31, 32, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 54, 56, 58, 59, 60, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 98, 101, 102, 105, 106, 107, 108, 110, 113, 114, 115, 116, 118, 121, 122, 123, 124, 125, 126, 127, 128, 132, 135, 137, 139, 143, 147, 149, 150, 151, 154, 164, 166, 167, 168, 171, 174, 175, 178, 180, 181, 186, 188, 190, 198, 203, 206, 208, 209, 212, 213, 215, 216, 218, 220, 222, 224, 225, 226, 227, 228, 229, 232, 234, 235, 238, 239, 245, 246, 249, 251, 253, 257, 259, 263, 266, 267, 273, 279, 282, 283, 285, 287, 291, 292, 293, 295, 299, 300, 317, 318, 324, 325, 330, 331, 332, 334, 336, 337, 338, 340, 342, 343, 344, 345, 347, 349, 350, 351, 356, 357, 359], "your_anyscale_org_id": [35, 39], "your_gcp_project_nam": [66, 78], "your_project_id": [35, 38], "yourself": [3, 181, 186, 317, 318], "yr": [267, 283], "ytick": [337, 341], "yunikorn": [24, 26], "yy": [128, 129, 137, 138, 147, 148], "z": [137, 140, 143, 267, 283, 343, 345], "zentropa": [285, 291, 292], "zero": [8, 92, 93, 101, 102, 106, 107, 110, 117, 126, 127, 147, 151, 153, 164, 167, 191, 192, 293, 298, 307, 311, 316, 337, 339, 343, 346, 347, 349, 350, 355, 359], "zero_copy_onli": [337, 340], "zero_grad": [5, 7, 13, 14, 137, 143, 224, 228, 238, 240, 245, 247, 253, 256, 257, 293, 298, 330, 334, 343, 347, 350, 355], "zeros_lik": [343, 347], "zilliz": [8, 191, 192], "zip": [7, 14, 253, 255, 317, 319, 323, 330, 332, 336, 337, 341, 350, 352], "zip_ref": [330, 332], "zipfil": [330, 332], "zone": [9, 17, 22, 43, 45, 53, 56, 66, 71, 147, 151, 198, 201], "zprofil": [1, 169, 170], "zsh": [66, 69], "zshrc": [66, 69], "zuoma": [267, 283], "zz": [128, 129, 137, 138, 147, 148], "\u03b8": [324, 326, 329], "\u03c0": [88, 89, 324, 326], "\u03f5": [317, 320, 324, 327]}, "titles": ["Ray Enablement Content: Jupyter Book Publishing", "Introduction to Ray: Developer", "Introduction to Ray Core: Getting Started", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "Introduction to Ray Train + PyTorch", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "Introduction to Ray Tune", "Introduction to Ray Data: Industry Landscape", "Introduction to Ray Data: Ray Data + Structured Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "Introduction to Ray Serve with PyTorch", "Introduction to the Ray AI Libraries", "Introduction to Ray Train", "Intro to Ray Tune", "Intro to Ray Data", "Intro to Ray Serve", "Anyscale Administrator Overview", "Anyscale Administrator Overview", "1. What is an Anyscale Cloud?", "2. Cloud Deployment Types", "3. A Demonstrative Example of Resource Creation with AWS EC2", "3.1 IAM Role Definition", "4. Register Anyscale Cloud to Your Cloud Provider", "Deployment Options: Virtual Machines vs. Kubernetes", "Deployment Options: Virtual Machines vs. Kubernetes", "2. Virtual Machines (VM) vs. Kubernetes (K8s)", "3. (Optional) More Kubernetes Deployments Components", "Introduction: Deploy Anyscale Ray on AWS EC2 Instances", "Introduction: Deploy Anyscale Ray on AWS EC2 Instances", "1. Create Anyscale Resources with Terraform", "2. Register the Anyscale Cloud", "3. Test", "4. Cleanup", "5. Conclusion", "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)", "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)", "Prerequisites", "1. Installation", "2. Create Anyscale Resources with Terraform", "3. Register the Anyscale Cloud", "4. Test", "5. Cleanup", "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster", "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster", "1. Create Anyscale Resources with Terraform", "2. Install Kubernetes Components", "3. Register the Anyscale Cloud", "4. Install the Anyscale Operator", "5. Verify the Installation", "6. Test", "7. Clean up", "8. Conclusion", "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster", "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster", "Prerequisites", "1. Create Anyscale Resources with Terraform", "2. Attach Required IAM Policies to Your existing EKS\u2019s Node Role", "3. Install Kubernetes Components", "4. Register the Anyscale Cloud", "5. Install the Anyscale Operator", "6. Verify the Installation", "7. Test", "8. Troubleshooting", "9. Clean up", "10. Conclusion", "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster", "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster", "Prerequisites", "1. Installation", "2. Create Anyscale Resources with Terraform", "3. Troubleshooting GPU Availability", "4. kubectl Configuration", "5. Install NGINX Ingress Controller", "6. (Optional) Upgrade Anyscale Dependencies", "7. Register the Anyscale Cloud", "8. Install the Anyscale Operator", "8. Test", "9. Cleanup", "Welcome to Anyscale Administration", "101 \u2014 Introduction to Anyscale Workspaces", "101 \u2014 Introduction to Anyscale Workspaces", "101 \u2013 Developing Application with Anyscale", "101 \u2013 Developing Application with Anyscale", "101 \u2013 Compute Configs and Execution Environments in Anyscale", "101 \u2013 Compute Configs and Execution Environments in Anyscale", "101 \u2013 Storage Options in the Anyscale Platform", "101 \u2013 Storage Options in the Anyscale Platform", "101 \u2013 Debug and Monitor Your Anyscale Application", "101 \u2013 Debug and Monitor Your Anyscale Application", "101 \u2013 Introduction to Anyscale Jobs", "101 \u2013 Introduction to Anyscale Jobs", "101 \u2013  Introduction to Anyscale Services", "101 \u2013  Introduction to Anyscale Services", "101 \u2013 Collaboration on Anyscale", "101 \u2013 Collaboration on Anyscale", "101 - Anyscale Organization and Cloud Setup", "101 - Anyscale Organization and Cloud Setup", "\ud83d\udccc Overview of Structure", "\ud83e\udde0 Summary", "Last Updated 6/19", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "What is LLM Serving?", "Key Concepts and Optimizations", "Challenges in LLM Serving", "Ray Serve LLM + Anyscale Architecture", "Getting Started with Ray Serve LLM", "Key Takeaways", "Deploy a Medium-Sized LLM with Ray Serve LLM", "Deploy a Medium-Sized LLM with Ray Serve LLM", "Overview: Why Medium-Sized Models?", "Setting up Ray Serve LLM", "Local Deployment &amp; Inference", "Deploying to Anyscale Services", "Advanced Topics: Monitoring &amp; Optimization", "Summary &amp; Outlook", "Advanced LLM Features with Ray Serve LLM", "Advanced LLM Features with Ray Serve LLM", "Overview: Advanced Features Preview", "Example: Deploying LoRA Adapters", "Example: Getting Structured JSON Output", "Example: Setting up Tool Calling", "How to Choose an LLM?", "Conclusion: Next Steps", "Multi-modal AI pipeline", "Multi-modal AI pipeline", "Batch inference", "Batch inference", "Data ingestion", "Batch embeddings", "Ray Data", "Data storage", "Monitoring and Debugging", "Production jobs", "Similar images", "Distributed training", "Distributed training", "Preprocess", "Model", "Batching", "Model registry", "Training", "Ray Train", "Production Job", "Evaluation", "Online serving", "Online serving", "Deployments", "Application", "Ray Serve", "Observability", "Production services", "CI/CD", "Observability Introduction", "Observability Introduction", "Observability Overview", "Setting Up Local Ray Observability", "Ray and Anyscale Observability Introduction", "Ray and Anyscale Observability Introduction", "Ray Observability", "Anyscale Observability", "Example", "Ray and Anyscale Observability in Detail", "Ray and Anyscale Observability in Detail", "Data Pipeline Observability (Ray Data)", "Web Application Observability (Ray Serve)", "Multi-Actor Ray Serve Tracing Example", "Introduction to Ray: Developer", "Introduction to Ray: Developer", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "1. Overview of the Ray AI Libraries", "2. Quick end-to-end example", "Introduction to Ray Core: Getting Started", "Introduction to Ray Core: Getting Started", "0. Overview", "1. Creating Remote Functions", "2. Executing Remote Functions", "4. Putting It All Together", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "1. Object store", "2. Chaining Tasks and Passing Data", "3. Task retries", "4. Task Runtime Environments", "5. Resource allocation and management", "6. Nested Tasks", "7. Pattern: Pipeline data processing and waiting for results", "8. Ray Actors", "Introduction to Ray Data: Industry Landscape", "Introduction to Ray Data: Industry Landscape", "The Compute Layer", "The Orchestration Layer", "Distributed Computing Frameworks", "Data Processing with Ray Data", "Ray Serve", "Introduction to Ray Data: Ray Data + Structured Data", "Introduction to Ray Data: Ray Data + Structured Data", "0. What is Ray Data?", "2. Loading Data", "3. Transforming Data", "4. Writing Data", "5. Data Operations: Shuffling, Grouping and Aggregation", "6. When to use Ray Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "1. When to Consider Ray Data", "2. How to work with Ray Data", "3. Loading data", "3. Lazy execution mode", "4. Transforming data", "5. Stateful transformations with Ray Actors", "6. Materializing data", "7. Data Operations: grouping, aggregation, and shuffling", "8. Persisting data", "9. Ray Data in production", "Introduction to Ray Serve with PyTorch", "Introduction to Ray Serve with PyTorch", "1. When to Consider Ray Serve", "2. Overview of Ray Serve", "3. Implement an image classification service", "4. Development workflow", "\ud83d\udcda 01 \u00b7 Introduction to Ray Train", "\ud83d\udcda 01 \u00b7 Introduction to Ray Train", "01 \u00b7 Imports", "04 \u00b7 Define ResNet-18 Model for MNIST", "05 \u00b7 Define the Ray Train Loop (DDP per-worker)", "06 \u00b7 Define <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop_config</span></code>", "07 \u00b7 Configure Scaling with <code class=\"docutils literal notranslate\"><span class=\"pre\">ScalingConfig</span></code>", "08 \u00b7 Wrap the Model with <code class=\"docutils literal notranslate\"><span class=\"pre\">prepare_model()</span></code>", "09 \u00b7 Build the DataLoader with <code class=\"docutils literal notranslate\"><span class=\"pre\">prepare_data_loader()</span></code>", "10 \u00b7 Report Training Metrics", "11 \u00b7 Save Checkpoints and Report Metrics", "14 \u00b7 Create the <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code>", "16 \u00b7 Inspect the Training Results", "18 \u00b7 Load a Checkpoint for Inference", "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data", "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data", "01 \u00b7 Define Training Loop with Ray Data", "02 \u00b7 Build DataLoader from Ray Data", "03 \u00b7 Prepare Dataset for Ray Data", "05 \u00b7 Define Image Transformation", "07 \u00b7 Configure <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code> with Ray Data", "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train", "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train", "01 \u00b7 Modify Training Loop to Enable Checkpoint Loading", "02 \u00b7 Save Full Checkpoint with Extra State", "04 \u00b7 Launch Fault-Tolerant Training", "05 \u00b7 Manual Restoration from Checkpoints", "07 \u00b7 Clean Up Cluster Storage", "\ud83c\udf89 Wrapping Up &amp; Next Steps", "Introduction to Ray Tune", "Introduction to Ray Tune", "1. Loading the data", "2. Starting out with vanilla PyTorch", "3. Hyperparameter tuning with Ray Tune", "4. Ray Tune in Production", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "1. When to use Ray Train", "2. Single GPU Training with PyTorch Lightning", "3. Distributed Training with Ray Train and PyTorch Lightning", "4. Ray Train in Production", "Batch Inference with Ray Data\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb <strong>Launch Locally</strong>: You can run this notebook locally.\ud83d\ude80 <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)This example shows how to do batch inference with Ray Data.Batch inference with Ray Data enables you to efficiently generate predictions from machine learning models on large datasets by processing multiple data points at once. Instead of running inference on one row at a time, which can be slow and resource-inefficient, batch inference leverages vectorized computation and parallelism to maximize throughput. This is especially useful when working with modern deep learning models, which are optimized for batch processing on CPUs, GPUs, or Apple Silicon devices.The typical workflow begins by loading your dataset\u2014such as a public dataset from Hugging Face\u2014into a Ray Dataset. Ray Data can automatically partition the data for parallel processing, or you can repartition it explicitly to control the number of data blocks. Once the data is loaded, you define a callable class (such as a text embedding model) that loads the machine learning model in its constructor and implements a <code class=\"docutils literal notranslate\"><span class=\"pre\">__call__</span></code> method to process each batch. Ray Data\u2019s <code class=\"docutils literal notranslate\"><span class=\"pre\">map_batches</span></code> API is then used to apply this callable to each batch of data, with options to control concurrency and resource allocation (e.g., number of GPUs).This approach allows you to spin up multiple concurrent model instances, each processing different batches of data in parallel. The result is a significant speedup in inference time, especially for large datasets. After inference, you can materialize the results, inspect the output, and shut down the Ray cluster to free up resources. Batch inference with Ray Data is scalable, flexible, and integrates seamlessly with modern ML workflows, making it a powerful tool for production and research environments alike.", "Batch Inference with Ray Data", "Batch Inference with Ray Data", "Architecture", "Architecture Overview", "Load a datasetLoad a dataset from hugging face or local and convert into Ray Dataset. A Ray cluster automatically initialized on local or on Anyscale platform. You can also use <strong>ray.init()</strong> To explicitly create or connect to an existing Ray cluster.https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html#ray.init# load a Hugging Face datasethf_dataset = load_dataset(\u201ccardiffnlp/tweet_eval\u201d, \u201csentiment\u201d, split=\u201dtrain\u201d)# Convert the Hugging Face dataset to a Ray Datasetds = ray.data.from_huggingface(hf_dataset).repartition(2) # repartition to 2 blocks for parallel processing. Not necessary if already partitioned due to the size of the dataset.", "Loading a Dataset", "Batch Inference ClassMany machine learning models are optimized for processing a batch of inputs at once. When working with a large dataset, there could be many batches of data. Instead of loading machine learning models repeatedly to run each batch of data, you want to spin up a number of actor processes that are <strong>initialized once</strong> with your model <strong>and reused</strong> to process multiple batches. To implement this, you can use the <code class=\"docutils literal notranslate\"><span class=\"pre\">map_batches</span></code> API with a \u201cCallable\u201d class method that implements:- <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code>: Initialize any expensive state.- <code class=\"docutils literal notranslate\"><span class=\"pre\">__call__</span></code>: Perform the stateful transformation.In this example, a lightweight sentence transformer model, <strong>all-MiniLM-L6-v2</strong> is used to generate embeddings of text data.", "Defining the Batch Inference Class", "Create a batch data and call the modelDefine a Ray Data map_batches function to embed text using the SentenceTransformer model. This function will be applied to each batch of data in the Ray Data dataset. It will take a batch of sentences, encode them into embeddings, and return the batch with the embeddings added.Showcasing two options of to do batch inference based on if the ray cluster has have GPU nodes or if it has just CPU nodes. The second option also works on a local ray cluster on an Apple Silicon Mac with MPS.# setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPSworker_device = \u201ccpu\u201d # or \u201ccuda\u201d if you have a nvidia gpu on worker nodes# batch_size should be set based on VRAM if worker_device == \u201ccuda\u201d: # if you have a nvidia gpu on worker nodes    # adjust batch_size based on the VRAM available on the GPU    ds = ds.map_batches(TextEmbedder, num_gpus=1, concurrency=2, batch_size=64) # 2 nodes with 1 GPU eachelse:    ds = ds.map_batches(TextEmbedder, concurrency=2, batch_size=64) # either cpu or mps (on a mac)", "Creating a Data Batch and Calling the Model", "Run inference on the entire datasetExecute and materialize this dataset into object store memory. This operation will trigger execution of the lazy transformations performed on this dataset. The embedding model \u2018TextEmbedder\u2019 in map_batches() is called on the entire dataset.# Run inference on the entire dataset# Note that this does not mutate the original Dataset.materialized_ds = ds.materialize()# metadata after inferenceprint(\u2018** Original dataset:\u2019, ds)print(\u2018\\n** Materialized dataset:\u2019, materialized_ds)# Show a few rows of the materialized dataset with embeddingsmaterialized_ds.show(3)", "Running inference on the entire dataset", "Data Processing and ML examples with Ray", "Batch Inference with Ray Data", "Architecture", "Load a dataset", "Batch Inference Class", "Create a batch data and call the model", "Run inference on the entire dataset", "Data Processing with Ray Data", "Data Processing and ML examples with Ray", "Data Processing with Ray Data", "Library Imports", "Convert to Ray Dataset", "Filter Ray Dataset", "Join Two Ray Datasets", "Preprocessing with a Tokenizer", "Distributed training with Ray Train, PyTorch and Hugging Face", "Data Processing and ML examples with Ray", "Distributed training with Ray Train, PyTorch and Hugging Face", "1. Architecture", "3. Metrics Setup", "4. Training function per worker", "5. Main Training Function", "6. Start Training", "Online Model Serving with Ray Serve\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb <strong>Launch Locally</strong>: You can run this notebook locally.\ud83d\ude80 <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)Model serving is the process of deploying machine learning models to production so that they can be accessed and used by applications or users. It involves creating an API or interface that allows users to send requests to the modeland receive predictions in response. There are several libraries and frameworks available for model serving, each with its own features and capabilities. In this notebook, we showcase Ray Serve and FastAPI to deploy a sentiment analysismachine learning (ML) model.", "Online Model Serving with Ray Serve", "Architecture### Import librariesIn addition to ray and serve, we also import FastAPI to create webservice and Hugging Face transformers to download ML models.# Import ray serve and FastAPI librariesimport rayfrom ray import servefrom fastapi import FastAPI# library for pre-trained modelsfrom transformers import pipeline", "Architecture Overview", "FastAPI webservice and deploy a modelFastAPI is used to create a webservice \u2018app\u2019 to accept HTTP requests.MySentimentModel class loads the ML model and defines <em>predict</em> function for online inference. &#64;serve.deployment decorator defines the Ray Serve deployment.<em>&#64;app.get()</em> is used to create a GET \u2018/predict\u2019 route. Similarly, &#64;app.post() can be used POST requests. See https://docs.ray.io/en/latest/serve/http-guide.html for more details.In this example, <em>application_logic()</em> function is used to define a sample transformation or business logic that can be applied before sending the input to the ML model for inference. See inline comments for further explanation.### Scaling deployment<em>num_replicas</em> parameter sets the number of instances of the deployment. FastAPI and RayServe automatically load balances to send requests to each instance. There are more options to set the <em>accelerator_type</em> to GPU and even use fractional GPUs. See configuration options here: https://docs.ray.io/en/latest/serve/configure-serve-deployment.html .", "Building a FastAPI Web Service and Deploying a Model", "Online Model Serving with Ray Serve", "Deploy the modelserve.run(MySentimentModel.bind()) # Bind the deployment to the Ray Serve runtimeDeploymentHandle(deployment=\u2019MySentimentModel\u2019)", "Deploying Our Model and Testing it", "Shutdown the Ray Serve instances and Ray Cluster# stop ray serveserve.shutdown()  # Shutdown Ray Serve when done, ray cluster will still be runningray.shutdown()  # Shutdown Ray cluster", "Shutdown and Summary", "Data Processing and ML examples with Ray", "Online Model Serving with Ray Serve", "Architecture", "FastAPI webservice and deploy a model", "Simulate Client: Send test requests", "04-d1 Generative computer-vision pattern with Ray Train", "04-d1 Generative computer-vision pattern with Ray Train", "1. Imports and setup", "8. Pixel diffusion LightningModule", "9. Ray Train <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop</span></code> (Lightning + Ray integration)", "12. Resume from latest checkpoint", "13. Reverse diffusion sampler", "04-d2 Diffusion-Policy Pattern with Ray Train", "04-d2 Diffusion-Policy Pattern with Ray Train", "1. Imports and setup", "4. DiffusionPolicy LightningModule", "5. Distributed Train loop with checkpointing", "8. Reverse diffusion helper", "04e Recommendation system pattern with Ray Train", "04e Recommendation system pattern with Ray Train", "1. Imports", "7. Define matrix factorization model", "8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)", "11. Resume training from checkpoint", "12. Inference: recommend top-N items for a user", "04b Tabular workload pattern with Ray Train", "04b Tabular workload pattern with Ray Train", "1. Imports", "8. Define the Ray Train worker loop (Arrow-based, memory-efficient)", "12. Confusion matrix visualization", "15. Continue training from the latest checkpoint", "04c Time-Series workload pattern with Ray Train", "04c Time-Series workload pattern with Ray Train", "1. Imports", "9. PositionalEncoding and Transformer model", "10. Ray Train training loop (with teacher forcing)", "13. Resume training from checkpoint", "14. Inference helper \u2014 Ray Data batch predictor on GPU", "04a Computer-vision pattern with Ray Train", "04a Computer-vision pattern with Ray Train", "1. Imports", "6. Custom <code class=\"docutils literal notranslate\"><span class=\"pre\">Food101Dataset</span></code> for Parquet", "10. Helper: Ray-prepared DataLoaders", "11. <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop_per_worker</span></code>", "12. Launch distributed training with <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code>", "13. Plot training and validation loss curves", "14. Demonstrate fault-tolerant resumption", "15. Batch inference with Ray Data", "Ray Enablement Content"], "titleterms": {"": [53, 57, 84, 85, 155, 158, 265, 274, 276, 324, 325, 337, 338, 343, 344], "0": [2, 9, 17, 18, 175, 177, 198, 200, 224, 234, 308], "01": [224, 225, 226, 238, 240, 245, 247, 252], "02": [224, 226, 238, 239, 241, 245, 248, 252], "03": [224, 226, 238, 242, 245, 246, 248, 252], "04": [224, 227, 238, 242, 245, 249, 317, 318, 324, 325], "04a": [350, 351], "04b": [337, 338], "04c": [343, 344], "04e": [330, 331], "05": [224, 228, 238, 243, 245, 250], "06": [224, 229, 238, 243, 245, 250], "07": [224, 230, 238, 244, 245, 251], "08": [224, 231, 238, 244], "09": [224, 232], "1": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 22, 24, 25, 26, 28, 30, 35, 38, 39, 43, 45, 46, 53, 56, 58, 66, 69, 70, 86, 87, 90, 91, 92, 93, 100, 101, 102, 105, 106, 107, 108, 110, 112, 116, 118, 124, 155, 158, 168, 169, 170, 171, 173, 174, 175, 178, 180, 181, 183, 186, 187, 189, 198, 200, 206, 208, 212, 213, 215, 218, 220, 253, 255, 259, 261, 262, 263, 274, 293, 296, 317, 319, 324, 325, 326, 330, 332, 337, 339, 343, 345, 350, 352], "10": [5, 53, 65, 224, 233, 317, 319, 321, 324, 329, 330, 334, 337, 340, 343, 347, 350, 352, 354], "100k": [330, 332], "101": [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 100, 317, 319, 350, 351, 352], "11": [224, 234, 317, 321, 330, 335, 337, 340, 343, 347, 350, 355], "12": [224, 234, 317, 322, 330, 336, 337, 341, 343, 347, 350, 356], "128": [274, 275], "13": [224, 234, 317, 323, 330, 336, 337, 341, 343, 348, 350, 357], "14": [224, 235, 317, 323, 330, 336, 337, 341, 343, 349, 350, 358], "15": [224, 235, 317, 323, 337, 342, 343, 349, 350, 359], "16": [224, 236, 337, 342, 343, 349, 350, 359], "17": [224, 236, 337, 342, 350, 359], "18": [224, 227, 237], "19": [100, 224, 237], "2": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 22, 24, 26, 28, 31, 35, 38, 39, 43, 46, 53, 57, 58, 66, 69, 70, 86, 87, 90, 91, 100, 101, 102, 105, 106, 107, 108, 110, 116, 118, 124, 155, 158, 168, 169, 170, 171, 174, 175, 179, 180, 181, 184, 187, 189, 198, 201, 206, 209, 210, 212, 213, 215, 218, 221, 253, 256, 259, 262, 263, 270, 274, 293, 296, 317, 319, 324, 325, 326, 330, 332, 337, 339, 343, 345, 350, 352], "20": [224, 237], "2025": [265, 301], "3": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 24, 26, 27, 28, 32, 35, 40, 43, 46, 47, 53, 58, 66, 71, 86, 87, 100, 101, 102, 105, 106, 107, 108, 110, 112, 116, 118, 124, 155, 158, 169, 170, 171, 174, 175, 179, 181, 185, 187, 198, 202, 206, 210, 211, 215, 218, 222, 253, 257, 259, 262, 263, 276, 293, 297, 317, 319, 324, 325, 326, 330, 332, 337, 339, 343, 345, 350, 352], "30": [343, 345], "4": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 22, 23, 24, 26, 27, 28, 33, 35, 41, 43, 46, 48, 53, 58, 59, 66, 72, 86, 87, 101, 102, 105, 106, 108, 110, 116, 118, 124, 155, 158, 169, 170, 171, 174, 175, 180, 181, 186, 198, 203, 206, 212, 218, 223, 253, 258, 259, 262, 263, 264, 293, 298, 317, 319, 324, 327, 330, 332, 337, 339, 343, 345, 350, 352], "5": [1, 3, 4, 5, 6, 9, 10, 13, 14, 15, 17, 22, 28, 34, 35, 42, 43, 49, 53, 60, 66, 73, 110, 116, 169, 170, 171, 174, 181, 187, 198, 204, 206, 213, 259, 263, 293, 299, 317, 319, 324, 328, 330, 332, 337, 339, 343, 345, 350, 352], "6": [1, 3, 4, 5, 6, 9, 10, 13, 17, 22, 43, 50, 53, 61, 66, 74, 100, 169, 170, 171, 174, 181, 188, 198, 205, 206, 214, 259, 263, 293, 300, 317, 319, 324, 328, 330, 332, 337, 339, 343, 345, 350, 353], "64": 274, "7": [1, 3, 5, 6, 9, 10, 13, 17, 22, 43, 51, 53, 62, 66, 75, 169, 170, 181, 189, 198, 205, 206, 215, 259, 263, 293, 300, 317, 319, 324, 328, 330, 333, 337, 339, 343, 345, 350, 353], "70b": [110, 112], "8": [1, 3, 5, 9, 10, 13, 17, 22, 43, 52, 53, 63, 66, 76, 77, 169, 170, 181, 190, 198, 205, 206, 216, 293, 300, 317, 320, 324, 329, 330, 334, 337, 340, 343, 345, 347, 350, 353], "8000": 308, "9": [5, 10, 53, 64, 66, 78, 206, 217, 317, 321, 324, 329, 330, 334, 337, 340, 343, 346, 350, 353], "9998507499694824": 308, "A": [17, 21, 43, 44, 66, 67, 270], "For": 360, "If": [274, 276], "In": [1, 8, 169, 170, 191, 192, 265, 272, 276, 301, 305], "It": [1, 2, 169, 170, 175, 180, 274, 276, 301], "No": [126, 127], "Not": 270, "On": [8, 10, 191, 196, 206, 212], "The": [8, 90, 91, 101, 102, 104, 191, 192, 193, 194, 265, 274, 276], "There": [301, 305, 310], "These": [118, 120], "To": [24, 26, 270, 272], "With": 301, "__call__": [265, 272], "__init__": 272, "abl": 308, "about": [2, 3, 15, 175, 180, 181, 186, 265, 301], "accelerator_typ": 305, "accept": [301, 305], "access": [5, 6, 13, 24, 26, 259, 263, 301], "accomplish": [110, 117, 118, 125], "across": 301, "action": [324, 325, 329], "activ": [1, 5, 6, 13, 169, 170, 259, 263], "actor": [3, 10, 15, 168, 181, 182, 190, 206, 213, 224, 237, 265, 272], "ad": [0, 274, 317, 318], "adapt": [118, 121], "add": [1, 169, 170], "addit": [126, 127, 155, 158, 301, 303], "adjust": 274, "admin": 360, "administr": [17, 18, 79], "advanc": [3, 16, 110, 116, 118, 119, 120, 125, 181, 182], "after": [265, 276, 277], "aggreg": [9, 10, 15, 198, 204, 206, 215], "ai": [4, 8, 12, 126, 127, 171, 172, 173, 191, 193, 360], "alert": [164, 167], "align": [118, 124], "alik": 265, "all": [2, 9, 10, 175, 180, 198, 204, 206, 215, 265, 272, 301, 343, 349], "alloc": [3, 181, 187, 265], "allow": [265, 301], "alreadi": 270, "also": [270, 274, 276, 301, 303], "altern": [101, 102, 105], "an": [3, 4, 11, 12, 16, 17, 19, 53, 54, 84, 85, 110, 115, 118, 124, 171, 172, 174, 181, 183, 218, 222, 270, 274, 301, 324, 329], "analysi": [308, 310], "analysismachin": 301, "ani": [272, 308], "annot": 14, "anti": [2, 175, 180], "anyscal": [17, 18, 19, 22, 23, 24, 25, 26, 28, 29, 30, 31, 35, 36, 39, 40, 43, 44, 45, 47, 48, 53, 54, 56, 59, 60, 66, 67, 70, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 100, 101, 102, 107, 110, 115, 159, 160, 162, 164, 165, 167, 265, 270, 301, 317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351, 360], "apach": [8, 191, 192], "api": [35, 38, 66, 69, 168, 265, 270, 272, 276, 301], "app": [301, 305], "appl": [265, 274], "appli": [238, 243, 265, 274, 276, 305], "applic": [8, 11, 82, 83, 88, 89, 100, 147, 150, 164, 167, 191, 195, 218, 221, 301], "application_log": 305, "approach": [118, 120, 265], "ar": [17, 20, 265, 272, 274, 301, 305, 310, 317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351], "architectur": [17, 22, 101, 102, 107, 168, 265, 267, 269, 280, 293, 296, 301, 303, 304, 307, 314], "architecturearchitectur": 268, "area": 13, "argument": [3, 181, 183], "arm": [1, 169, 170], "arrow": [8, 191, 192, 337, 340], "artifact": [343, 349], "assist": [118, 121, 123], "assumpt": [155, 158], "attach": [53, 57], "auroc": 13, "authent": [35, 38, 66, 69], "autom": [90, 91], "automat": [245, 248, 265, 270, 305], "autosc": [10, 16, 206, 213, 301], "autoscal": [43, 46, 53, 58, 310], "avail": [3, 66, 71, 168, 181, 187, 274, 276, 301], "avoid": [276, 277], "aw": [17, 21, 28, 29, 43, 44, 46, 53, 54, 58], "awai": [224, 225, 238, 239, 245, 246, 317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351], "b": 265, "backend": [24, 26], "balanc": [43, 46, 53, 58, 301, 305, 337, 339], "base": [9, 10, 15, 198, 204, 206, 215, 274, 275, 330, 331, 337, 340], "batch": [3, 4, 8, 9, 10, 15, 101, 102, 105, 128, 129, 131, 137, 141, 171, 174, 181, 189, 191, 195, 198, 204, 206, 215, 265, 266, 267, 272, 273, 274, 275, 276, 279, 282, 283, 337, 339, 341, 343, 345, 349, 350, 353, 359, 360], "batch_siz": [274, 275, 276], "befor": 305, "begin": 265, "being": 308, "benchmark": [118, 124], "benefit": [118, 121, 122, 123], "best": [276, 317, 323], "better": 301, "bind": 308, "block": [9, 10, 15, 198, 204, 206, 210, 215, 265, 270], "book": 0, "both": [274, 275], "bound": [3, 181, 187], "breakdown": [110, 113], "build": [0, 5, 13, 224, 232, 238, 241, 306, 337, 340], "built": 301, "busi": 305, "cach": [101, 102, 105], "california": [337, 339], "call": [2, 118, 123, 175, 180, 267, 274, 275, 276, 283], "callabl": [265, 272, 276], "caller": 274, "can": [265, 270, 272, 274, 276, 301, 305, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "capabl": 301, "car": [118, 122], "cardiffnlp": 270, "case": [1, 118, 124, 169, 170, 276], "caus": [3, 181, 189], "cd": [147, 154], "chain": [3, 181, 184], "challeng": [8, 101, 102, 106, 191, 195], "characterist": [92, 93], "check": [317, 319, 343, 345, 350, 352], "checkpoint": [5, 6, 13, 224, 234, 237, 245, 247, 248, 250, 259, 263, 317, 322, 323, 324, 328, 330, 334, 335, 337, 342, 343, 348], "choic": 301, "choos": [110, 112, 118, 124], "ci": [147, 154], "class": [265, 267, 272, 273, 276, 282, 305, 337, 339], "classif": [11, 218, 222, 337, 338, 350, 351], "classifi": 16, "classmani": 272, "clean": [1, 4, 16, 43, 51, 53, 64, 88, 89, 90, 91, 92, 93, 169, 170, 171, 174, 224, 237, 245, 251, 317, 323, 324, 329, 330, 336, 337, 342, 350, 359], "cleanup": [28, 33, 35, 42, 66, 78, 343, 349], "cli": 100, "click": [265, 301], "client": [301, 307, 308, 309, 316], "clone": [94, 95, 100], "cloud": [17, 19, 20, 23, 28, 31, 35, 38, 40, 43, 47, 53, 59, 66, 69, 75, 86, 87, 96, 97, 98, 265, 301], "cluster": [3, 43, 44, 46, 53, 54, 58, 66, 67, 86, 87, 155, 158, 181, 187, 245, 251, 265, 267, 270, 274, 275, 276, 277, 284, 293, 300, 301, 307, 308, 310, 311, 316], "code": [4, 118, 121, 171, 174, 274, 275, 301], "collabor": [94, 95, 100], "collison": [276, 277], "command": [35, 39], "comment": 305, "comparison": [110, 112], "compon": [24, 27, 43, 46, 53, 58, 110, 113], "compos": 16, "comput": [1, 8, 13, 35, 36, 84, 85, 169, 170, 191, 193, 195, 265, 301, 317, 318, 350, 351], "concept": [5, 6, 7, 14, 101, 102, 105, 253, 257, 259, 263], "conclus": [28, 34, 43, 52, 53, 65, 118, 125], "concurr": [10, 110, 116, 206, 212, 265, 274], "conda": [1, 169, 170], "config": [84, 85], "configur": [3, 5, 6, 13, 35, 38, 66, 69, 72, 84, 85, 101, 102, 108, 110, 113, 115, 118, 121, 164, 167, 168, 181, 187, 224, 230, 234, 238, 244, 245, 248, 259, 263, 305, 337, 340], "confus": [337, 341], "connect": 270, "consid": [10, 11, 206, 208, 218, 220], "consider": [101, 102, 105, 118, 124], "constraint": 276, "constructor": 265, "contain": [84, 85], "content": [0, 360], "context": [101, 102, 105, 118, 124], "continu": [101, 102, 105, 337, 342], "control": [17, 22, 24, 26, 43, 46, 53, 58, 66, 73, 265, 274], "convert": [270, 271, 276, 285, 289, 292], "core": [2, 3, 8, 175, 176, 181, 182, 191, 196, 270, 360], "correct": 274, "cost": [101, 102, 106, 118, 124], "could": [272, 301], "count": [343, 345], "cours": [0, 1, 169, 170, 245, 252], "cover": [90, 91, 92, 93, 118, 120, 276, 337, 338, 339], "cpu": [265, 274, 275, 276, 337, 341], "creat": [1, 2, 5, 6, 17, 22, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 84, 85, 90, 91, 169, 170, 175, 178, 224, 235, 259, 262, 263, 265, 267, 270, 274, 275, 283, 285, 289, 301, 303, 305, 330, 332], "creation": [17, 21], "cuda": 274, "cursor": [82, 83], "curv": [13, 317, 321, 330, 334, 350, 357], "custom": [0, 9, 10, 15, 16, 17, 22, 198, 204, 206, 215, 350, 353], "cv": 360, "d": [274, 276], "d1": [317, 318], "d2": [324, 325], "dashboard": [88, 89, 164, 166], "data": [3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 24, 26, 128, 130, 132, 133, 164, 166, 171, 172, 174, 181, 184, 189, 191, 192, 193, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 214, 215, 216, 217, 224, 225, 234, 238, 239, 240, 241, 242, 243, 244, 245, 252, 253, 255, 259, 263, 265, 266, 267, 270, 272, 274, 275, 276, 278, 279, 283, 285, 286, 287, 294, 312, 317, 319, 330, 332, 337, 341, 343, 349, 350, 359, 360], "databas": [8, 191, 192], "datafram": [224, 236, 285, 292], "dataload": [5, 6, 224, 232, 238, 241, 259, 262, 293, 298, 343, 345, 350, 353, 354], "dataset": [5, 9, 12, 13, 15, 198, 201, 224, 226, 238, 242, 265, 267, 268, 270, 271, 272, 274, 276, 277, 281, 284, 285, 288, 289, 290, 291, 324, 325, 326, 330, 332, 337, 339, 343, 345], "datasetd": 270, "datasetexecut": 276, "datasethf_dataset": 270, "datasetload": 270, "ddp": [224, 225, 228], "de": [317, 318], "debug": [88, 89, 128, 134], "decod": [101, 102, 104, 317, 319], "decor": 305, "deep": 265, "deeper": [7, 14, 253, 257], "default": 14, "defin": [6, 13, 17, 20, 224, 227, 228, 229, 238, 240, 243, 259, 262, 265, 273, 276, 305, 330, 333, 334, 337, 340], "definit": [17, 22], "demand": [101, 102, 106, 343, 344], "demonstr": [17, 21, 276, 350, 358], "depend": [1, 3, 24, 26, 66, 74, 126, 127, 169, 170, 181, 186], "deploi": [24, 25, 27, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 70, 100, 110, 111, 115, 118, 121, 265, 267, 274, 275, 283, 301, 305, 306, 307, 308, 309, 310, 315], "deploy": [11, 16, 17, 20, 24, 25, 27, 79, 101, 102, 108, 110, 114, 147, 149, 168, 218, 221, 301, 305, 306, 307, 308, 315], "deploymentnum_replica": 305, "descript": [118, 122], "design": 301, "detail": [164, 165, 305], "develop": [1, 11, 82, 83, 100, 126, 127, 168, 169, 170, 218, 223], "devic": [43, 46, 53, 58, 265, 274], "diagnost": [337, 341], "diagram": 268, "dictimport": 268, "differ": [110, 117, 265], "diffus": [6, 259, 262, 317, 318, 320, 323, 324, 325, 329], "diffusionpolici": [324, 327], "digit": [224, 226], "directori": 13, "disabl": 0, "displai": [317, 323], "distribut": [4, 5, 6, 8, 13, 137, 138, 171, 174, 191, 195, 224, 225, 234, 259, 263, 276, 278, 286, 293, 294, 295, 301, 312, 317, 318, 321, 324, 328, 330, 331, 334, 337, 338, 340, 343, 344, 349, 350, 351, 356, 360], "div": [265, 301], "dive": [7, 14, 253, 257], "do": [265, 274], "doc": [270, 301, 305], "doe": [276, 277, 350, 351], "domain": [118, 124], "done": 310, "down": [1, 110, 114, 115, 169, 170, 265], "download": [224, 226, 303], "downscal": 310, "dual": [17, 22], "due": [270, 308], "duplic": [94, 95], "dure": 276, "e": 265, "each": [24, 26, 265, 272, 274, 301, 305], "eachels": 274, "easi": 301, "easili": [265, 301], "easilydeploi": 301, "ec2": [17, 21, 28, 29], "ef": [17, 22], "effici": [265, 276, 337, 340], "either": 274, "ek": [43, 44, 53, 54, 57], "emb": 274, "embed": [128, 131, 265, 272, 274, 276, 277, 330, 331], "embeddingsmaterialized_d": 276, "en": [270, 301, 305], "enabl": [0, 35, 38, 66, 69, 110, 116, 245, 247, 265, 276, 360], "encod": [274, 317, 319, 330, 332, 350, 352], "encount": 308, "end": [4, 12, 171, 174, 308], "endpoint": 168, "engin": [8, 35, 36, 66, 67, 101, 102, 107, 191, 193], "ensembl": [4, 171, 174], "ensur": 274, "entir": [265, 267, 276, 277, 284], "environ": [1, 3, 84, 85, 169, 170, 181, 186, 265, 301, 324, 325], "error": [265, 267, 276, 277, 284, 308], "errorsgpu": 276, "especi": 265, "evalu": [137, 146, 337, 340], "even": 305, "exampl": [0, 1, 4, 12, 13, 17, 21, 24, 27, 79, 110, 112, 117, 118, 121, 122, 123, 159, 163, 168, 169, 170, 171, 172, 174, 265, 272, 274, 278, 286, 294, 305, 312], "execut": [0, 2, 9, 10, 15, 24, 26, 84, 85, 175, 179, 198, 202, 206, 211, 274, 276], "exercis": [7, 14, 253, 257], "exist": [17, 22, 53, 54, 57, 270, 301], "expect": [118, 122], "expens": 272, "experi": [12, 14], "explan": 305, "explicitli": [265, 270], "explor": [88, 89], "extern": [24, 26], "extra": [245, 248], "face": [265, 270, 271, 276, 293, 295, 303, 310], "factor": [330, 331, 333], "failur": [3, 181, 189], "failureconfig": [245, 248], "fastapi": [16, 301, 302, 303, 304, 305, 306, 307, 310, 313, 315], "fault": [245, 246, 249, 252, 350, 358], "featur": [0, 9, 12, 16, 118, 119, 120, 198, 205, 301, 337, 341], "fetch": [3, 181, 189], "few": [276, 277], "file": [9, 10, 15, 86, 87, 110, 115, 198, 204, 206, 215, 337, 339], "filter": [285, 290], "first": [90, 91, 92, 93], "fit": [6, 224, 235, 259, 263], "flask": [301, 302, 307, 313], "flexibl": 265, "flow": [8, 168, 191, 197], "follow": [90, 91, 92, 93], "food": [317, 319, 350, 351, 352], "food101dataset": [350, 353], "forc": [343, 347], "forecast": [343, 344], "forest": [337, 338], "format": [8, 191, 192], "forward": [317, 318], "foundat": [101, 102, 103, 360], "fraction": [3, 16, 181, 187, 305], "framework": [8, 118, 124, 191, 195, 301], "frameworkthat": 301, "free": 265, "from": [238, 241, 245, 250, 265, 270, 276, 310, 317, 322, 323, 324, 329, 330, 332, 335, 337, 342, 343, 348], "from_huggingfac": 270, "full": [245, 248], "function": [2, 8, 17, 19, 175, 178, 179, 191, 193, 274, 293, 298, 299, 305], "further": 305, "g": 265, "gce": [35, 36], "gcp": [35, 36], "gener": [0, 5, 6, 8, 101, 102, 104, 191, 195, 259, 263, 265, 272, 317, 318, 323, 324, 326, 360], "get": [2, 3, 7, 14, 100, 101, 102, 108, 118, 122, 168, 175, 176, 179, 180, 181, 189, 253, 257, 305, 308, 360], "gke": [66, 67], "global": [9, 10, 15, 198, 204, 206, 215], "go": [245, 252, 265], "googl": [35, 38, 66, 67, 69], "gpu": [5, 6, 13, 16, 66, 71, 259, 262, 263, 265, 274, 275, 276, 305, 343, 347, 349], "gracefulli": 308, "grafana": [155, 158], "group": [9, 10, 15, 17, 22, 198, 204, 206, 215], "groupbi": [9, 10, 15, 198, 204, 206, 215], "guid": 305, "ha": [274, 276], "handl": [276, 301], "hardwar": [110, 116, 118, 124], "harm": [2, 175, 180], "have": [274, 301], "head": [155, 158], "headach": [126, 127], "helper": [324, 329, 343, 349, 350, 354], "here": [265, 301, 305], "hf_dataset": 270, "high": [276, 301, 310], "hourli": [343, 345], "how": [0, 9, 10, 12, 17, 20, 24, 26, 110, 117, 118, 124, 198, 200, 206, 209, 224, 225, 265, 274, 276, 308, 317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351], "html": [270, 301, 305], "http": [270, 301, 305, 308], "hug": [265, 270, 271, 276, 293, 295, 303, 310], "huggingfac": [268, 278, 286, 294, 312], "hyperparamet": [4, 7, 14, 171, 174, 253, 257], "i": [8, 9, 17, 19, 101, 102, 104, 110, 115, 191, 196, 197, 198, 200, 265, 272, 274, 276, 301, 302, 305, 307, 308, 310, 313], "iam": [17, 22, 53, 57], "id": [2, 17, 22, 82, 83, 175, 180, 330, 332, 336], "imag": [11, 84, 85, 128, 136, 218, 222, 238, 243, 317, 318, 319, 350, 351, 352, 353], "implement": [4, 11, 16, 171, 172, 218, 222, 265, 272], "import": [224, 226, 265, 267, 268, 269, 280, 285, 288, 293, 296, 301, 303, 304, 307, 308, 314, 317, 319, 324, 326, 330, 332, 337, 339, 341, 343, 345, 350, 352], "improv": [110, 116], "includ": 301, "increas": 310, "index": 301, "industri": [8, 191, 192], "ineffici": 265, "infer": [4, 101, 102, 104, 107, 110, 114, 115, 128, 129, 171, 174, 224, 237, 265, 266, 267, 272, 273, 274, 275, 276, 277, 279, 282, 284, 305, 330, 331, 336, 337, 341, 342, 343, 349, 350, 359, 360], "inferenceprint": 276, "inform": 301, "infrastructur": [17, 20, 24, 27, 66, 70, 101, 102, 107, 126, 127], "ingest": [128, 130], "ingress": [43, 46, 53, 58, 66, 73], "init": 270, "initi": [270, 272, 285, 288], "inlin": 305, "input": [272, 305, 317, 318, 330, 331, 350, 351], "inspect": [12, 224, 236, 265, 337, 339, 343, 345, 350, 353], "instal": [0, 1, 35, 38, 43, 46, 48, 49, 53, 58, 60, 61, 66, 69, 73, 76, 100, 155, 158, 168, 169, 170, 278, 286, 294, 312], "instanc": [17, 22, 28, 29, 35, 36, 265, 305, 307, 308, 310, 311, 316], "instead": [265, 272], "instruct": [90, 91], "integr": [16, 238, 239, 245, 252, 265, 301, 317, 321], "interfac": 301, "intro": [7, 10, 14, 15, 16, 206, 207, 253, 257], "introduct": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 100, 101, 102, 103, 155, 156, 159, 160, 169, 170, 171, 172, 175, 176, 181, 182, 191, 192, 198, 199, 218, 219, 224, 225, 245, 252, 253, 254, 259, 260], "introductori": 13, "invert": [324, 325], "involv": 301, "io": [3, 181, 187, 270, 301, 305], "irvin": [337, 339], "item": [330, 331, 332, 336], "its": [265, 301], "job": [5, 13, 90, 91, 100, 128, 135, 137, 145, 276, 277], "join": [285, 291, 330, 336], "json": [118, 122, 308], "jupyt": [0, 1, 169, 170], "just": [274, 301, 302, 307, 313], "jvm": [8, 191, 195], "k8": [24, 26], "keep": 276, "kei": [5, 6, 13, 16, 17, 19, 92, 93, 101, 102, 105, 109, 110, 113, 117, 118, 121, 122, 123, 125, 259, 263], "kubectl": [66, 72], "kubernet": [24, 25, 26, 27, 43, 46, 53, 58, 66, 67, 101, 102, 106], "kv": [101, 102, 105], "l6": 272, "label": [308, 350, 351], "lake": [8, 191, 192], "lakehous": [8, 191, 192], "landscap": [8, 191, 192], "languag": [101, 102, 103], "larg": [3, 101, 102, 103, 181, 187, 265, 272, 276, 301], "last": [100, 245, 250], "latenc": [101, 102, 106], "latest": [270, 301, 305, 317, 322, 337, 342], "launch": [1, 5, 13, 80, 81, 110, 114, 115, 155, 158, 164, 167, 169, 170, 224, 235, 238, 244, 245, 249, 265, 301, 317, 321, 324, 328, 330, 334, 343, 347, 350, 356], "layer": [8, 24, 26, 191, 192, 193, 194], "lazi": [10, 206, 211, 274, 276], "learn": [8, 79, 100, 118, 120, 121, 122, 123, 191, 193, 224, 225, 238, 239, 245, 246, 265, 272, 276, 301, 317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351, 360], "legend": [1, 169, 170], "level": [3, 181, 183], "leverag": [265, 276], "lib": 360, "librari": [4, 12, 171, 172, 173, 265, 267, 269, 280, 285, 288, 293, 296, 301, 303, 304, 307, 308, 314], "librariesimport": [268, 303], "librariesin": 303, "lifecycl": [5, 224, 234], "lightn": [6, 259, 260, 262, 263, 317, 321, 360], "lightningmodul": [317, 320, 324, 327], "lightweight": 272, "like": [301, 308], "limit": [10, 206, 212], "lite": [350, 351], "ll": [79, 118, 120, 224, 225, 238, 239, 245, 246], "llama": [110, 112], "llm": [101, 102, 103, 104, 106, 107, 108, 110, 111, 113, 116, 118, 119, 121, 124, 360], "load": [5, 6, 7, 9, 10, 13, 14, 15, 43, 46, 53, 58, 198, 201, 206, 210, 224, 237, 238, 242, 245, 247, 253, 255, 259, 263, 265, 267, 270, 271, 272, 276, 281, 285, 288, 301, 305, 317, 319, 330, 332, 337, 339, 343, 345, 350, 352], "load_dataset": [268, 270], "loader": 13, "local": [0, 1, 13, 82, 83, 86, 87, 110, 114, 155, 158, 168, 169, 170, 265, 270, 274, 275, 301], "localhost": 308, "log": [88, 89, 164, 166, 167], "logic": 305, "loop": [2, 5, 6, 13, 175, 180, 224, 228, 238, 240, 245, 247, 259, 262, 324, 328, 330, 334, 337, 340, 343, 347], "lora": [118, 121], "loss": [317, 321, 324, 328, 330, 334, 343, 347, 350, 357], "love": 308, "mac": [1, 169, 170, 274, 275], "machin": [8, 24, 25, 26, 191, 193, 265, 272, 274, 276, 277, 301], "machinerai": 276, "mai": 301, "main": [293, 299], "make": [265, 301], "manag": [1, 3, 24, 26, 101, 102, 106, 169, 170, 181, 187, 301, 330, 334], "mani": [3, 181, 189, 272, 274, 310], "manual": [245, 250, 274, 275], "map_batch": [265, 272, 274, 276], "materi": [10, 15, 206, 214, 265, 276, 277], "materialized_d": 276, "matrix": [330, 331, 333, 337, 341], "matter": [118, 120, 122, 123], "max_model_len": [110, 116], "maxim": 265, "medium": [110, 111, 112, 113], "memori": [8, 101, 102, 106, 191, 192, 265, 267, 274, 276, 277, 284, 337, 340], "memorydb": [17, 22], "metadata": [276, 277], "method": [265, 272], "metric": [5, 13, 88, 89, 164, 166, 167, 224, 233, 234, 236, 293, 297, 330, 334], "migrat": [5, 6, 13, 259, 263, 317, 318, 330, 331, 337, 338, 343, 344, 350, 351], "min": [343, 345], "mini": [337, 339], "miniforg": [1, 169, 170], "minilm": 272, "ml": [265, 276, 278, 286, 294, 301, 303, 305, 312], "mnist": [13, 224, 226, 227], "modal": [126, 127], "mode": [9, 10, 15, 198, 202, 206, 211], "model": [4, 5, 6, 7, 13, 14, 101, 102, 103, 105, 110, 112, 113, 116, 118, 124, 137, 140, 142, 171, 172, 174, 224, 227, 231, 253, 257, 259, 262, 263, 265, 267, 272, 274, 275, 276, 283, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 313, 315, 330, 331, 333, 337, 340, 343, 346, 350, 351], "modeland": 301, "modeldefin": 274, "modelfastapi": 305, "modelrespons": 308, "modelserv": 308, "modelsfrom": 303, "modern": [265, 276], "modifi": [245, 247], "modul": [245, 252], "monitor": [88, 89, 110, 116, 128, 134], "more": [5, 6, 15, 24, 27, 110, 116, 118, 121, 122, 123, 125, 259, 263, 301, 305, 310], "most": 308, "move": 265, "movi": [330, 336], "movielen": [330, 332], "mp": [274, 275, 276], "mpsworker_devic": 274, "multi": [126, 127, 168, 343, 344], "multimod": 360, "multipl": [13, 265, 272, 301], "mutat": [276, 277], "mysentimentmodel": [305, 308], "n": [276, 330, 336], "navig": 0, "necessari": 270, "need": [24, 26, 274, 301], "nest": [3, 181, 188], "new": [0, 1, 12, 17, 22, 43, 44, 66, 67, 168, 169, 170, 317, 318], "next": [101, 102, 109, 110, 117, 118, 125, 155, 158, 245, 252, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "nginx": [43, 46, 53, 58, 66, 73], "node": [53, 57, 84, 85, 155, 158, 274, 343, 344], "nois": [317, 318], "normal": [324, 326, 343, 345], "note": [2, 3, 5, 6, 10, 175, 180, 181, 186, 187, 189, 206, 210, 213, 224, 234, 259, 263, 276, 277, 308], "notebook": [0, 1, 12, 82, 83, 90, 91, 92, 93, 169, 170, 265, 276, 277, 285, 287, 301, 308, 310], "now": [155, 158], "npfrom": 268, "num_gpu": 274, "num_replica": 310, "number": [265, 272, 305, 310], "numpi": 268, "nvidia": [43, 46, 53, 58, 274], "nyc": [12, 343, 344, 345], "o": [1, 169, 170], "object": [3, 24, 26, 86, 87, 100, 181, 182, 183, 189, 265, 276, 317, 318, 324, 325, 330, 331], "observ": [147, 152, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 360], "onc": [3, 181, 189, 265, 272], "one": [265, 343, 345], "ones": 301, "onli": [164, 167, 224, 234], "onlin": [147, 148, 301, 302, 305, 307, 313, 360], "oper": [9, 10, 15, 24, 25, 43, 48, 53, 60, 66, 76, 198, 204, 206, 215, 276], "optim": [101, 102, 105, 106, 110, 116, 265, 272], "option": [1, 17, 22, 24, 25, 27, 43, 46, 53, 58, 66, 74, 86, 87, 100, 155, 158, 169, 170, 265, 274, 305, 310, 337, 339], "orchestr": [8, 101, 102, 107, 191, 194], "order": [9, 10, 15, 198, 204, 206, 215], "organ": [96, 97, 98], "origin": [276, 277], "other": [110, 117, 276, 277, 301], "our": [13, 110, 112, 309], "out": [7, 253, 256, 265, 267, 276, 277, 284], "outlin": [265, 266, 267, 279, 285, 287, 293, 295, 301, 302, 307, 313], "outlook": [15, 17, 18, 24, 27, 110, 117], "output": [0, 118, 122, 265], "over": [8, 191, 196, 343, 345], "overview": [1, 2, 4, 5, 6, 11, 12, 13, 16, 17, 18, 96, 98, 100, 110, 112, 118, 120, 155, 157, 168, 169, 170, 171, 173, 175, 177, 218, 221, 259, 262, 269, 304], "own": 301, "packag": [155, 158], "panda": [285, 292], "parallel": [2, 5, 6, 13, 101, 102, 105, 110, 116, 175, 180, 224, 225, 234, 259, 263, 265, 270, 274, 276, 301], "param": 308, "paramet": [274, 305], "parquet": [317, 319, 330, 332, 337, 339, 343, 345, 350, 352, 353], "part": [90, 91, 92, 93], "partit": [265, 270], "pass": [3, 181, 183, 184], "passeng": [343, 345], "path": 100, "pattern": [2, 3, 175, 180, 181, 183, 189, 317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351, 360], "pendulum": [324, 325, 326], "per": [224, 228, 293, 298, 330, 331], "perform": [272, 274, 276], "persist": [10, 13, 15, 206, 216, 224, 234, 317, 319, 350, 352], "phase": [101, 102, 104], "pip": [3, 181, 186], "pipelin": [3, 100, 110, 116, 126, 127, 164, 166, 181, 189, 303], "pixel": [317, 320], "plane": [17, 22], "platform": [86, 87, 270], "plot": [317, 321, 324, 328, 330, 334, 343, 347, 350, 357], "plugin": [43, 46, 53, 58], "point": [13, 265, 330, 332], "polici": [53, 57, 317, 318, 324, 325, 329, 360], "posit": 308, "positionalencod": [343, 346], "post": [305, 337, 342], "power": 265, "practic": [118, 124, 276], "pre": [303, 304], "predict": [5, 6, 12, 224, 237, 259, 263, 265, 301, 305, 308], "predictor": [343, 349], "prefil": [101, 102, 104], "prepar": [238, 242, 343, 345, 350, 354], "prepare_data_load": [224, 232], "prepare_model": [224, 231], "preprocess": [137, 139, 285, 292], "prerequisit": [1, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 79, 110, 114, 155, 158, 164, 165, 168, 169, 170], "preview": [118, 120], "previou": 308, "print": [276, 308], "problem": [317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351], "process": [3, 8, 101, 102, 104, 118, 124, 181, 189, 191, 195, 196, 265, 270, 272, 274, 276, 278, 285, 286, 287, 294, 301, 312, 317, 318, 360], "product": [5, 6, 7, 9, 10, 13, 15, 16, 126, 127, 128, 135, 137, 145, 147, 153, 198, 205, 206, 217, 253, 258, 259, 264, 265, 301], "profil": 168, "project": [94, 95, 96, 98], "prometheu": [155, 158], "properli": 308, "provid": [17, 23, 43, 44, 301], "public": [265, 276], "publish": 0, "purpos": [8, 17, 19, 191, 195], "put": [2, 175, 180], "python": 301, "pytorch": [5, 6, 7, 11, 13, 14, 218, 219, 253, 256, 257, 259, 260, 262, 263, 278, 286, 293, 294, 295, 301, 312, 343, 345, 360], "qualiti": [118, 124], "quantiz": [110, 116], "queri": [101, 102, 108, 308, 309], "quick": [4, 171, 174, 343, 345], "rai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 24, 26, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 88, 89, 100, 101, 102, 103, 107, 108, 110, 111, 113, 114, 118, 119, 121, 128, 132, 137, 144, 147, 151, 155, 158, 159, 160, 161, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 180, 181, 182, 189, 190, 191, 192, 196, 197, 198, 199, 200, 205, 206, 207, 208, 209, 213, 217, 218, 219, 220, 221, 224, 225, 228, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 252, 253, 254, 257, 258, 259, 260, 261, 263, 264, 265, 266, 267, 270, 271, 274, 275, 276, 277, 278, 279, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 300, 301, 302, 303, 304, 305, 307, 308, 310, 311, 312, 313, 316, 317, 318, 319, 321, 324, 325, 328, 330, 331, 332, 334, 337, 338, 339, 340, 341, 343, 344, 345, 347, 349, 350, 351, 354, 359, 360], "random": [343, 345], "rang": 301, "rank": [224, 234, 330, 331], "rate": [330, 331, 332], "rayfrom": 303, "rayimport": 268, "rayserv": 305, "read": [9, 10, 15, 198, 204, 206, 215], "real": [324, 326], "rec": 360, "recap": 12, "receiv": 301, "recommend": [1, 100, 118, 124, 169, 170, 330, 331, 336], "reduc": [110, 116, 276], "regist": [17, 23, 28, 31, 35, 40, 43, 47, 53, 59, 66, 75, 168], "registr": 168, "registri": [137, 142], "regress": [4, 171, 172], "relat": [110, 112, 117], "remot": [2, 5, 175, 178, 179], "remov": [343, 349], "repartit": [265, 270], "repeatedli": 272, "replica": [11, 110, 116, 218, 221, 310], "report": [5, 13, 224, 233, 234], "repositori": 100, "request": [3, 110, 114, 168, 181, 187, 301, 305, 307, 308, 309, 316], "requestsw": 308, "requir": [1, 35, 38, 43, 44, 53, 57, 66, 69, 101, 102, 106, 118, 124, 155, 158, 169, 170, 274], "resampl": [343, 345], "research": 265, "reserv": [265, 301], "resiz": [317, 319, 350, 352], "resnet": [224, 227], "resourc": [3, 10, 17, 20, 21, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 94, 95, 101, 102, 109, 110, 117, 118, 125, 181, 187, 206, 212, 213, 265], "respons": [301, 308], "rest": 301, "restart": 308, "restor": [245, 250], "result": [2, 3, 5, 6, 13, 175, 179, 181, 189, 224, 236, 259, 263, 265, 308, 343, 349], "resum": [245, 250, 317, 322, 330, 335, 343, 348], "resumpt": [350, 358], "retri": [3, 181, 185, 245, 248], "retriev": 168, "return": 274, "reus": 272, "revers": [317, 318, 323, 324, 325, 329], "right": [265, 301], "roc": 13, "role": [17, 22, 53, 57, 96, 98], "rout": 305, "row": [9, 10, 15, 198, 204, 206, 215, 265, 274, 275, 276, 277], "run": [5, 6, 12, 24, 26, 35, 39, 90, 91, 110, 115, 164, 166, 224, 237, 259, 263, 265, 267, 272, 274, 275, 276, 277, 284, 301, 308, 343, 349, 350, 359], "runconfig": [224, 234], "runningrai": 310, "runtim": [3, 181, 186], "runtimedeploymenthandl": 308, "s3": [17, 22], "same": [276, 277], "sampl": [168, 224, 226, 305, 317, 318, 323, 324, 325, 329], "sampler": [317, 323], "saniti": [317, 319, 343, 345, 350, 352], "save": [13, 224, 234, 245, 248], "scalabl": [101, 102, 106, 265, 276, 301], "scale": [5, 6, 13, 110, 116, 224, 230, 259, 263, 265, 267, 274, 275, 276, 283, 301, 305, 306, 307, 310, 315, 324, 325], "scalingconfig": [224, 230], "schedul": [13, 90, 91], "scikit": 301, "score": 308, "seamlessli": 265, "second": [274, 285, 289], "secur": [17, 22], "see": [305, 308], "select": [118, 124], "send": [110, 114, 301, 305, 307, 308, 309, 316], "sentenc": [265, 268, 272, 274], "sentence_transform": 268, "sentencetransform": [265, 268, 274, 276], "sentiment": [270, 301, 308, 310], "sequenc": [343, 344], "seri": [343, 344, 360], "serv": [0, 4, 8, 11, 16, 101, 102, 103, 104, 106, 107, 108, 110, 111, 113, 114, 118, 119, 121, 147, 148, 151, 164, 167, 168, 171, 172, 174, 191, 197, 218, 219, 220, 221, 301, 302, 303, 304, 305, 307, 308, 310, 311, 313, 316, 360], "servefrom": 303, "server": [1, 169, 170], "serveserv": 310, "servic": [11, 16, 92, 93, 100, 110, 115, 147, 153, 218, 222, 301, 306], "set": [1, 14, 110, 113, 115, 118, 123, 155, 158, 169, 170, 274, 275, 305], "setup": [96, 97, 155, 158, 168, 293, 297, 317, 318, 319, 324, 326, 330, 331, 337, 338, 343, 344, 350, 351], "sever": 301, "share": [86, 87, 317, 323, 330, 336], "should": [274, 275, 308], "show": [265, 274, 276, 277], "showcas": [274, 301], "shuffl": [9, 10, 15, 198, 204, 206, 215, 317, 319], "shut": [1, 110, 114, 115, 169, 170, 265], "shutdown": [102, 108, 267, 276, 277, 284, 285, 292, 293, 300, 301, 307, 308, 310, 311, 316], "sign": 100, "signific": 265, "silicon": [265, 274], "similar": [128, 136], "similarli": 305, "simpl": [1, 164, 166, 169, 170, 301], "simpli": 301, "simul": [301, 307, 308, 309, 316], "singl": [5, 6, 13, 259, 262], "size": [110, 111, 112, 113, 117, 270, 274, 337, 339], "slide": [343, 345], "slow": 265, "so": [274, 275, 301], "solv": [317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351], "spark": [8, 191, 196], "specif": [2, 10, 175, 180, 206, 212, 213], "speedup": 265, "spin": [265, 272], "split": [0, 270, 317, 319, 324, 326, 330, 332, 337, 339, 350, 353], "stabl": [6, 259, 262], "start": [2, 7, 14, 92, 93, 100, 101, 102, 108, 155, 158, 175, 176, 253, 256, 257, 265, 293, 300, 301, 308, 337, 340, 360], "state": [10, 15, 206, 213, 245, 248, 272, 324, 325], "step": [12, 101, 102, 108, 109, 110, 117, 118, 125, 155, 158, 245, 252, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "still": 310, "stop": [310, 311], "storag": [5, 13, 24, 26, 86, 87, 128, 133, 224, 234, 245, 251, 317, 323, 330, 336], "store": [3, 24, 26, 86, 87, 181, 182, 183, 265, 276], "strategi": [110, 116], "stream": [8, 191, 195], "structur": [8, 9, 79, 96, 98, 118, 122, 168, 191, 192, 198, 199], "style": [324, 325], "submit": [90, 91], "subnet": [17, 22], "summar": 276, "summari": [17, 22, 96, 99, 110, 117, 265, 267, 277, 284, 285, 292, 293, 300, 307, 311, 316], "summaryin": 310, "summarythi": 276, "support": [17, 20, 301], "sy": 360, "system": [330, 331], "tab": [88, 89], "tabl": 14, "tabular": [337, 338, 360], "take": [224, 225, 238, 239, 245, 246, 274, 317, 318, 323, 324, 325, 329, 330, 331, 336, 337, 338, 342, 343, 344, 349, 350, 351, 359], "take_batch": 274, "takeawai": [101, 102, 109, 110, 117, 118, 125], "task": [3, 12, 118, 124, 181, 182, 184, 185, 186, 187, 188], "taxi": [12, 343, 344, 345], "teacher": [343, 347], "team": 100, "templat": [110, 117], "tensor": [317, 318], "tensorflow": 301, "termin": [155, 158], "terraform": [28, 30, 35, 39, 43, 45, 53, 56, 66, 70], "test": [0, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 84, 85, 274, 275, 301, 307, 308, 309, 316], "text": [101, 102, 104, 265, 272, 274, 276, 308], "textembedd": [274, 276], "tfvar": [35, 39, 66, 70], "thatcan": 301, "thei": 301, "them": 274, "thi": [1, 12, 90, 91, 92, 93, 169, 170, 265, 272, 274, 276, 277, 301, 305, 310, 317, 318, 323, 324, 325, 329, 330, 331, 336, 337, 338, 342, 343, 344, 349, 350, 351, 359], "think": [265, 301], "through": [90, 91, 265, 276], "throughput": [265, 276], "throw": 276, "time": [265, 343, 344, 360], "tip": 12, "titl": [330, 336], "togeth": [2, 175, 180], "token": [285, 292, 293, 298], "toler": [245, 246, 249, 252, 350, 358], "too": [3, 181, 189, 276], "tool": [118, 123, 265], "top": [3, 181, 183, 301, 330, 336], "topic": [110, 116, 118, 125], "torch": [6, 259, 262], "torchfrom": 268, "torchtrain": [6, 13, 224, 235, 238, 244, 259, 263, 317, 321, 324, 328, 350, 356], "toward": 308, "trace": [164, 167, 168], "traffic": [301, 310], "train": [4, 5, 6, 13, 137, 138, 143, 144, 171, 172, 174, 224, 225, 228, 233, 234, 235, 236, 238, 239, 240, 244, 245, 246, 247, 249, 250, 252, 259, 260, 261, 262, 263, 264, 270, 278, 286, 293, 294, 295, 298, 299, 300, 303, 304, 312, 317, 318, 319, 321, 324, 325, 328, 329, 330, 331, 332, 334, 335, 337, 338, 339, 340, 342, 343, 344, 347, 348, 349, 350, 351, 353, 356, 357, 360], "train_loop": [317, 321], "train_loop_config": [224, 229], "train_loop_per_work": [350, 355], "trainer": [224, 235, 337, 340], "transform": [9, 10, 15, 198, 202, 206, 212, 213, 238, 243, 272, 276, 303, 305, 343, 344, 346, 350, 353], "transformersfrom": 268, "trigger": 276, "tripl": [330, 331], "troubleshoot": [53, 63, 66, 71], "tune": [4, 7, 14, 171, 172, 174, 253, 254, 257, 258, 360], "tupl": [324, 325], "tutori": [245, 252], "tweet_ev": 270, "two": [101, 102, 104, 274, 285, 291], "type": [17, 20, 118, 122, 268, 337, 339], "typic": 265, "uci": [337, 339], "ul": [265, 301], "under": 13, "univers": [337, 339], "unstructur": [10, 206, 207], "until": 274, "up": [1, 4, 14, 16, 43, 51, 53, 64, 88, 89, 90, 91, 92, 93, 100, 110, 113, 115, 118, 123, 155, 158, 169, 170, 171, 174, 224, 237, 245, 251, 252, 265, 272, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "upcom": [9, 198, 205], "updat": [13, 100, 301], "upgrad": [66, 74, 110, 116], "uri": [330, 332], "us": [1, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 22, 24, 26, 110, 116, 118, 121, 122, 123, 124, 164, 167, 169, 170, 171, 172, 191, 196, 198, 200, 204, 205, 206, 215, 224, 225, 253, 257, 259, 261, 265, 270, 272, 274, 276, 301, 302, 305, 307, 308, 310, 313, 317, 318, 324, 325, 330, 331, 332, 337, 338, 343, 344, 350, 351], "usag": 0, "user": [96, 98, 168, 301, 330, 331, 332, 336], "uv": [1, 169, 170], "v": [8, 24, 25, 26, 191, 196, 197], "v2": 272, "val": [317, 319, 324, 328], "valid": [330, 332, 334, 337, 339, 343, 347, 350, 353, 357], "valu": [101, 102, 105], "vanilla": [4, 7, 171, 174, 253, 256], "vector": 265, "verifi": [1, 43, 49, 53, 61, 169, 170, 337, 342], "version": 301, "view": [224, 236], "viewer": [88, 89], "virtual": [24, 25, 26], "vision": [317, 318, 350, 351, 360], "visual": [13, 14, 224, 226, 237, 317, 319, 330, 332, 337, 339, 341, 343, 345, 349, 350, 352, 359], "vllm": [101, 102, 107], "vm": [24, 26], "vpc": [17, 22], "vram": [274, 275], "vscode": [82, 83], "wai": 301, "wait": [3, 181, 189], "walk": [90, 91, 276], "want": 272, "warehous": [8, 191, 192], "we": [1, 12, 110, 117, 118, 120, 125, 169, 170, 265, 301, 303, 310], "weather": [118, 123], "web": [164, 167, 301, 306], "webservic": [303, 305, 306, 307, 315], "welcom": [1, 79, 169, 170], "well": 301, "what": [8, 9, 17, 19, 24, 26, 79, 101, 102, 104, 110, 115, 117, 118, 120, 125, 155, 158, 191, 196, 197, 198, 200, 224, 225, 238, 239, 245, 246, 301, 302, 307, 313, 317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351], "when": [5, 6, 8, 9, 10, 11, 15, 16, 24, 26, 191, 196, 198, 205, 206, 208, 218, 220, 224, 225, 259, 261, 265, 272, 310], "where": [245, 252, 301, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "which": [24, 26, 265, 301], "why": [1, 8, 100, 101, 102, 106, 110, 112, 118, 120, 121, 122, 123, 169, 170, 191, 196, 197, 301, 302, 307, 313, 317, 318], "wide": 301, "window": [101, 102, 105, 118, 124, 343, 345], "work": [0, 10, 206, 209, 224, 225, 265, 272, 274, 275, 317, 318], "worker": [5, 6, 84, 85, 155, 158, 224, 228, 259, 263, 274, 275, 293, 298, 337, 340], "worker_devic": 274, "workflow": [0, 11, 84, 85, 218, 223, 265, 276, 301], "workload": [24, 26, 96, 98, 164, 166, 301, 317, 318, 324, 325, 330, 331, 337, 338, 343, 344, 350, 351, 360], "workspac": [80, 81, 100], "wrap": [224, 231, 245, 252, 317, 323, 324, 329, 330, 336, 337, 342, 343, 349, 350, 359], "write": [9, 198, 203, 337, 339], "xgboost": [4, 171, 172, 174, 337, 338, 340], "york": 12, "you": [79, 90, 91, 224, 225, 238, 239, 245, 246, 265, 270, 272, 274, 301, 308, 317, 318, 323, 324, 325, 329, 330, 331, 336, 337, 338, 342, 343, 344, 349, 350, 351, 359], "your": [1, 17, 23, 24, 27, 53, 57, 88, 89, 90, 91, 92, 93, 100, 169, 170, 265, 272], "zero": 310}})