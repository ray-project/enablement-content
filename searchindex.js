Search.setIndex({"alltitles": {"0. Overview": [[1, "overview"], [13, null], [100, "overview"], [101, "overview"]], "0. What is Ray Data?": [[8, "what-is-ray-data"], [50, null]], "01 Examples": [[343, "examples"]], "01 \u00b7 Define Training Loop with Ray Data": [[273, "define-training-loop-with-ray-data"], [294, null]], "01 \u00b7 Imports": [[273, "imports"], [281, null]], "01 \u00b7 Modify Training Loop to Enable Checkpoint Loading": [[273, "modify-training-loop-to-enable-checkpoint-loading"], [300, null]], "02 Anyscale Admin": [[343, "anyscale-admin"]], "02 \u00b7 Build DataLoader from Ray Data": [[273, "build-dataloader-from-ray-data"], [295, null]], "02 \u00b7 Download MNIST Dataset": [[273, "download-mnist-dataset"], [281, "download-mnist-dataset"]], "02 \u00b7 Save Full Checkpoint with Extra State": [[273, "save-full-checkpoint-with-extra-state"], [301, null]], "03 Observability": [[343, "observability"]], "03 \u00b7 Configure Automatic Retries with FailureConfig": [[273, "configure-automatic-retries-with-failureconfig"], [301, "configure-automatic-retries-with-failureconfig"]], "03 \u00b7 Prepare Dataset for Ray Data": [[273, "prepare-dataset-for-ray-data"], [296, null]], "03 \u00b7 Visualize Sample Digits": [[273, "visualize-sample-digits"], [281, "visualize-sample-digits"]], "04 \u00b7 Define ResNet-18 Model for MNIST": [[273, "define-resnet-18-model-for-mnist"], [282, null]], "04 \u00b7 Launch Fault-Tolerant Training": [[273, "launch-fault-tolerant-training"], [302, null]], "04 \u00b7 Load Dataset into Ray Data": [[273, "load-dataset-into-ray-data"], [296, "load-dataset-into-ray-data"]], "04-d1 Generative computer-vision pattern with Ray Train": [[277, null], [326, null]], "04-d2 Diffusion-Policy Pattern with Ray Train": [[278, null], [332, null]], "04a Computer-vision pattern with Ray Train": [[274, null], [306, null]], "04b Tabular workload pattern with Ray Train": [[275, null], [315, null]], "04c Time-Series workload pattern with Ray Train": [[276, null], [320, null]], "04e Recommendation system pattern with Ray Train": [[279, null], [337, null]], "05 \u00b7 Define Image Transformation": [[273, "define-image-transformation"], [297, null]], "05 \u00b7 Define the Ray Train Loop (DDP per-worker)": [[273, "define-the-ray-train-loop-ddp-per-worker"], [283, null]], "05 \u00b7 Manual Restoration from Checkpoints": [[273, "manual-restoration-from-checkpoints"], [303, null]], "06 \u00b7 Apply Transformations with Ray Data": [[273, "apply-transformations-with-ray-data"], [297, "apply-transformations-with-ray-data"]], "06 \u00b7 Define train_loop_config": [[273, "define-train-loop-config"], [284, null]], "06 \u00b7 Resume Training from the Last Checkpoint": [[273, "resume-training-from-the-last-checkpoint"], [303, "resume-training-from-the-last-checkpoint"]], "07 \u00b7 Clean Up Cluster Storage": [[273, "clean-up-cluster-storage"], [304, null]], "07 \u00b7 Configure Scaling with ScalingConfig": [[273, "configure-scaling-with-scalingconfig"], [285, null]], "07 \u00b7 Configure TorchTrainer with Ray Data": [[273, "configure-torchtrainer-with-ray-data"], [298, null]], "08 \u00b7 Launch Training with Ray Data": [[273, "launch-training-with-ray-data"], [298, "launch-training-with-ray-data"]], "08 \u00b7 Wrap the Model with prepare_model()": [[273, "wrap-the-model-with-prepare-model"], [286, null]], "09 \u00b7 Build the DataLoader with prepare_data_loader()": [[273, "build-the-dataloader-with-prepare-data-loader"], [287, null]], "1. Architecture": [[76, "architecture"], [95, null]], "1. Cloud Object Store": [[180, "cloud-object-store"], [192, "cloud-object-store"]], "1. Create Anyscale Resources with Terraform": [[111, "create-anyscale-resources-with-terraform"], [113, null], [126, "create-anyscale-resources-with-terraform"], [128, null], [136, "create-anyscale-resources-with-terraform"], [139, null]], "1. Creating Remote Functions": [[1, "creating-remote-functions"], [14, null]], "1. Dataset tuples": [[278, "dataset-tuples"], [332, "dataset-tuples"]], "1. Deploy to Kubernetes with Anyscale Operator": [[107, "deploy-to-kubernetes-with-anyscale-operator"], [108, "deploy-to-kubernetes-with-anyscale-operator"]], "1. How to Use Ray Data?": [[8, "how-to-use-ray-data"], [50, "how-to-use-ray-data"]], "1. Imports": [[274, "imports"], [275, "imports"], [276, "imports"], [279, "imports"], [307, null], [316, null], [321, null], [338, null]], "1. Imports and setup": [[277, "imports-and-setup"], [278, "imports-and-setup"], [327, null], [333, null]], "1. In the Anyscale Console, open (or create) a Workspace.": [[186, "in-the-anyscale-console-open-or-create-a-workspace"], [201, "in-the-anyscale-console-open-or-create-a-workspace"], [205, "in-the-anyscale-console-open-or-create-a-workspace"], [208, "in-the-anyscale-console-open-or-create-a-workspace"]], "1. Installation": [[118, "installation"], [121, null], [149, "installation"], [152, null]], "1. Key-Value (KV) Caching": [[212, "key-value-kv-caching"], [213, "key-value-kv-caching"], [216, "key-value-kv-caching"]], "1. Loading and visualizing data": [[239, "loading-and-visualizing-data"], [256, null]], "1. Loading the data": [[6, "loading-the-data"], [39, null]], "1. Memory Management": [[212, "memory-management"], [213, "memory-management"], [217, "memory-management"]], "1. Model Quality Benchmarks": [[229, "model-quality-benchmarks"], [235, "model-quality-benchmarks"]], "1. Object store": [[2, "object-store"], [18, null]], "1. Overview of Ray Serve": [[241, "overview-of-ray-serve"], [268, null]], "1. Overview of the Ray AI Libraries": [[3, "overview-of-the-ray-ai-libraries"], [27, null], [237, "overview-of-the-ray-ai-libraries"], [243, null]], "1. PyTorch introductory example (single GPU)": [[238, "pytorch-introductory-example-single-gpu"], [247, null]], "1. Ray Serve for Orchestration": [[212, "ray-serve-for-orchestration"], [213, "ray-serve-for-orchestration"], [218, "ray-serve-for-orchestration"]], "1. Reduce max_model_len": [[221, "reduce-max-model-len"], [227, "reduce-max-model-len"]], "1. Sign Up for Anyscale": [[188, "sign-up-for-anyscale"]], "1. Spin up a Anyscale Workspace, we will use this as the environment to develop and publish the Anyscale Service. Give this workspace a name, check the Auto-Select Worker Nodes and leave everything else as default.": [[187, "spin-up-a-anyscale-workspace-we-will-use-this-as-the-environment-to-develop-and-publish-the-anyscale-service-give-this-workspace-a-name-check-the-auto-select-worker-nodes-and-leave-everything-else-as-default"], [204, "spin-up-a-anyscale-workspace-we-will-use-this-as-the-environment-to-develop-and-publish-the-anyscale-service-give-this-workspace-a-name-check-the-auto-select-worker-nodes-and-leave-everything-else-as-default"], [206, "spin-up-a-anyscale-workspace-we-will-use-this-as-the-environment-to-develop-and-publish-the-anyscale-service-give-this-workspace-a-name-check-the-auto-select-worker-nodes-and-leave-everything-else-as-default"], [211, "spin-up-a-anyscale-workspace-we-will-use-this-as-the-environment-to-develop-and-publish-the-anyscale-service-give-this-workspace-a-name-check-the-auto-select-worker-nodes-and-leave-everything-else-as-default"]], "1. Split Notebooks and Generate Navigation": [[0, "split-notebooks-and-generate-navigation"]], "1. User Profile Retrieval": [[173, "user-profile-retrieval"]], "1. Using the same workspace, create a notebook folder": [[186, "using-the-same-workspace-create-a-notebook-folder"], [202, "using-the-same-workspace-create-a-notebook-folder"], [205, "using-the-same-workspace-create-a-notebook-folder"], [209, "using-the-same-workspace-create-a-notebook-folder"]], "1. What is an Anyscale Cloud?": [[100, "what-is-an-anyscale-cloud"], [102, null]], "1. When to Consider Ray Data": [[9, "when-to-consider-ray-data"], [57, null]], "1. When to Consider Ray Serve": [[10, "when-to-consider-ray-serve"], [68, null]], "1. When to use Ray Data": [[240, "when-to-use-ray-data"], [262, null]], "1. When to use Ray Train": [[4, "when-to-use-ray-train"], [5, "when-to-use-ray-train"], [30, null], [34, null]], "1.1 Configure Google Cloud Authentication": [[118, "configure-google-cloud-authentication"], [121, "configure-google-cloud-authentication"]], "1.1. Configure Google Cloud Authentication": [[149, "configure-google-cloud-authentication"], [152, "configure-google-cloud-authentication"]], "1.1. Pattern: pass an object as a top-level argument": [[2, "pattern-pass-an-object-as-a-top-level-argument"], [18, "pattern-pass-an-object-as-a-top-level-argument"]], "1.2 Enable Required APIs": [[118, "enable-required-apis"], [121, "enable-required-apis"]], "1.2: Enable Required APIs": [[149, "enable-required-apis"], [152, "enable-required-apis"]], "10 \u00b7 Report Training Metrics": [[273, "report-training-metrics"], [288, null]], "10. Clean up": [[278, "clean-up"], [336, "clean-up"]], "10. Conclusion": [[136, "conclusion"], [148, null]], "10. Helper: Ray-prepared DataLoaders": [[274, "helper-ray-prepared-dataloaders"], [309, null]], "10. Launch distributed Training with TorchTrainer": [[277, "launch-distributed-training-with-torchtrainer"], [329, "launch-distributed-training-with-torchtrainer"]], "10. Plot train and validation loss curves": [[279, "plot-train-and-validation-loss-curves"], [340, "plot-train-and-validation-loss-curves"]], "10. Ray Train training loop (with teacher forcing)": [[276, "ray-train-training-loop-with-teacher-forcing"], [323, null]], "10. Start distributed training": [[275, "start-distributed-training"], [317, "start-distributed-training"]], "101 - Anyscale Organization and Cloud Setup": [[185, null], [197, null]], "101 Introduction to Anyscale Services": [[186, "introduction-to-anyscale-services"], [187, "introduction-to-anyscale-services"], [200, "introduction-to-anyscale-services"], [203, "introduction-to-anyscale-services"], [205, "introduction-to-anyscale-services"], [206, "introduction-to-anyscale-services"], [207, "introduction-to-anyscale-services"], [210, "introduction-to-anyscale-services"]], "101 \u2013  Introduction to Anyscale Services": [[183, null], [195, null]], "101 \u2013 Collaboration on Anyscale": [[184, null], [196, null]], "101 \u2013 Compute Configs and Execution Environments in Anyscale": [[179, null], [191, null]], "101 \u2013 Debug and Monitor Your Anyscale Application": [[181, null], [193, null]], "101 \u2013 Developing Application with Anyscale": [[178, null], [190, null]], "101 \u2013 Introduction to Anyscale Jobs": [[182, null], [194, null]], "101 \u2013 Storage Options in the Anyscale Platform": [[180, null], [192, null]], "101 \u2014 Introduction to Anyscale Workspaces": [[177, null], [189, null]], "11 \u00b7 Save Checkpoints and Report Metrics": [[273, "save-checkpoints-and-report-metrics"], [289, null]], "11. Evaluate the trained model": [[275, "evaluate-the-trained-model"], [317, "evaluate-the-trained-model"]], "11. Launch training on 8 GPUs": [[276, "launch-training-on-8-gpus"], [323, "launch-training-on-8-gpus"]], "11. Plot loss curves": [[277, "plot-loss-curves"], [329, "plot-loss-curves"]], "11. Resume training from checkpoint": [[279, "resume-training-from-checkpoint"], [341, null]], "11. train_loop_per_worker": [[274, "train-loop-per-worker"], [310, null]], "12 \u00b7 Save Checkpoints on Rank-0 Only": [[273, "save-checkpoints-on-rank-0-only"], [289, "save-checkpoints-on-rank-0-only"]], "12. Confusion matrix visualization": [[275, "confusion-matrix-visualization"], [318, null]], "12. Inference: recommend top-N items for a user": [[279, "inference-recommend-top-n-items-for-a-user"], [342, null]], "12. Launch distributed training with TorchTrainer": [[274, "launch-distributed-training-with-torchtrainer"], [311, null]], "12. Plot training and validation loss": [[276, "plot-training-and-validation-loss"], [323, "plot-training-and-validation-loss"]], "12. Resume from latest checkpoint": [[277, "resume-from-latest-checkpoint"], [330, null]], "13 \u00b7 Configure Persistent Storage with RunConfig": [[273, "configure-persistent-storage-with-runconfig"], [289, "configure-persistent-storage-with-runconfig"]], "13. CPU batch inference with Ray Data": [[275, "cpu-batch-inference-with-ray-data"], [318, "cpu-batch-inference-with-ray-data"]], "13. Join top-N item IDs with movie titles": [[279, "join-top-n-item-ids-with-movie-titles"], [342, "join-top-n-item-ids-with-movie-titles"]], "13. Plot training and validation loss curves": [[274, "plot-training-and-validation-loss-curves"], [312, null]], "13. Resume training from checkpoint": [[276, "resume-training-from-checkpoint"], [324, null]], "13. Reverse diffusion sampler": [[277, "reverse-diffusion-sampler"], [331, null]], "14 \u00b7 Create the TorchTrainer": [[273, "create-the-torchtrainer"], [290, null]], "14. Clean up shared storage": [[279, "clean-up-shared-storage"], [342, "clean-up-shared-storage"]], "14. Demonstrate fault-tolerant resumption": [[274, "demonstrate-fault-tolerant-resumption"], [313, null]], "14. Feature-importance diagnostics": [[275, "feature-importance-diagnostics"], [318, "feature-importance-diagnostics"]], "14. Generate and display samples from the best checkpoint": [[277, "generate-and-display-samples-from-the-best-checkpoint"], [331, "generate-and-display-samples-from-the-best-checkpoint"]], "14. Inference helper \u2014 Ray Data batch predictor on GPU": [[276, "inference-helper-ray-data-batch-predictor-on-gpu"], [325, null]], "15 \u00b7 Launch Training with trainer.fit()": [[273, "launch-training-with-trainer-fit"], [290, "launch-training-with-trainer-fit"]], "15. Batch inference with Ray Data": [[274, "batch-inference-with-ray-data"], [314, null]], "15. Clean up shared storage": [[277, "clean-up-shared-storage"], [331, "clean-up-shared-storage"]], "15. Continue training from the latest checkpoint": [[275, "continue-training-from-the-latest-checkpoint"], [319, null]], "15. Run distributed inference and visualize results": [[276, "run-distributed-inference-and-visualize-results"], [325, "run-distributed-inference-and-visualize-results"]], "16 \u00b7 Inspect the Training Results": [[273, "inspect-the-training-results"], [291, null]], "16. Cleanup: remove all training artifacts": [[276, "cleanup-remove-all-training-artifacts"], [325, "cleanup-remove-all-training-artifacts"]], "16. Run and visualize Ray Data inference": [[274, "run-and-visualize-ray-data-inference"], [314, "run-and-visualize-ray-data-inference"]], "16. Verify post-training inference": [[275, "verify-post-training-inference"], [319, "verify-post-training-inference"]], "17 \u00b7 View Metrics as a DataFrame": [[273, "view-metrics-as-a-dataframe"], [291, "view-metrics-as-a-dataframe"]], "17. Clean up": [[274, "clean-up"], [275, "clean-up"], [314, "clean-up"], [319, "clean-up"]], "18 \u00b7 Load a Checkpoint for Inference": [[273, "load-a-checkpoint-for-inference"], [292, null]], "19 \u00b7 Run Inference and Visualize Predictions": [[273, "run-inference-and-visualize-predictions"], [292, "run-inference-and-visualize-predictions"]], "2. Attach Required IAM Policies to Your existing EKS\u2019s Node Role": [[136, "attach-required-iam-policies-to-your-existing-eks-s-node-role"], [140, null]], "2. Build the Book": [[0, "build-the-book"]], "2. Chaining Tasks and Passing Data": [[2, "chaining-tasks-and-passing-data"], [19, null]], "2. Clone the Repository (Optional)": [[188, "clone-the-repository-optional"]], "2. Cloud Deployment Types": [[100, "cloud-deployment-types"], [103, null]], "2. Continuous Batching": [[212, "continuous-batching"], [213, "continuous-batching"], [216, "continuous-batching"]], "2. Create Anyscale Resources with Terraform": [[118, "create-anyscale-resources-with-terraform"], [122, null], [149, "create-anyscale-resources-with-terraform"], [153, null]], "2. Distributed Data Parallel Training with Ray Train and PyTorch (multiple GPUs)": [[238, "distributed-data-parallel-training-with-ray-train-and-pytorch-multiple-gpus"], [248, null]], "2. End-to-end example: predicting taxi tips in New York": [[237, "end-to-end-example-predicting-taxi-tips-in-new-york"], [244, null]], "2. Executing Remote Functions": [[1, "executing-remote-functions"], [15, null]], "2. Generate a real pendulum dataset": [[278, "generate-a-real-pendulum-dataset"], [333, "generate-a-real-pendulum-dataset"]], "2. How to work with Ray Data": [[9, "how-to-work-with-ray-data"], [58, null]], "2. Implement an Classifier service": [[241, "implement-an-classifier-service"], [269, null]], "2. Install Kubernetes Components": [[126, "install-kubernetes-components"], [129, null]], "2. Latency Requirements": [[212, "latency-requirements"], [213, "latency-requirements"], [217, "latency-requirements"]], "2. Library Imports": [[76, "library-imports"], [95, "library-imports"]], "2. Load 10 % of Food-101": [[274, "load-10-of-food-101"], [277, "load-10-of-food-101"], [307, "load-10-of-food-101"], [327, "load-10-of-food-101"]], "2. Load MovieLens 100K dataset": [[279, "load-movielens-100k-dataset"], [338, "load-movielens-100k-dataset"]], "2. Load NYC taxi passenger counts (30-min)": [[276, "load-nyc-taxi-passenger-counts-30-min"], [321, "load-nyc-taxi-passenger-counts-30-min"]], "2. Load the University of California, Irvine (UCI) Cover type dataset": [[275, "load-the-university-of-california-irvine-uci-cover-type-dataset"], [316, "load-the-university-of-california-irvine-uci-cover-type-dataset"]], "2. Loading Data": [[8, "loading-data"], [51, null], [240, "loading-data"], [263, null]], "2. Once in the workspace, navigate to the VS Code Editor": [[186, "once-in-the-workspace-navigate-to-the-vs-code-editor"], [201, "once-in-the-workspace-navigate-to-the-vs-code-editor"], [205, "once-in-the-workspace-navigate-to-the-vs-code-editor"], [208, "once-in-the-workspace-navigate-to-the-vs-code-editor"]], "2. Overview of Ray Serve": [[10, "overview-of-ray-serve"], [69, null]], "2. Quick end-to-end example": [[3, "quick-end-to-end-example"], [28, null]], "2. Register the Anyscale Cloud": [[111, "register-the-anyscale-cloud"], [114, null]], "2. Setting up a PyTorch model": [[239, "setting-up-a-pytorch-model"], [257, null]], "2. Shared File Storage": [[180, "shared-file-storage"], [192, "shared-file-storage"]], "2. Single GPU Training with PyTorch": [[4, "single-gpu-training-with-pytorch"], [31, null]], "2. Single GPU Training with PyTorch Lightning": [[5, "single-gpu-training-with-pytorch-lightning"], [35, null]], "2. Starting out with vanilla PyTorch": [[6, "starting-out-with-vanilla-pytorch"], [40, null]], "2. Task and Domain Alignment": [[229, "task-and-domain-alignment"], [235, "task-and-domain-alignment"]], "2. Training objective": [[278, "training-objective"], [332, "training-objective"]], "2. Use Quantized Models": [[221, "use-quantized-models"], [227, "use-quantized-models"]], "2. Use the Anyscale CLI to submit the Anyscale Job. For a full list of all available arguments, check out the Anyscale Job CLI documentation.": [[186, "use-the-anyscale-cli-to-submit-the-anyscale-job-for-a-full-list-of-all-available-arguments-check-out-the-anyscale-job-cli-documentation"], [202, "use-the-anyscale-cli-to-submit-the-anyscale-job-for-a-full-list-of-all-available-arguments-check-out-the-anyscale-job-cli-documentation"], [205, "use-the-anyscale-cli-to-submit-the-anyscale-job-for-a-full-list-of-all-available-arguments-check-out-the-anyscale-job-cli-documentation"], [209, "use-the-anyscale-cli-to-submit-the-anyscale-job-for-a-full-list-of-all-available-arguments-check-out-the-anyscale-job-cli-documentation"]], "2. User Registration": [[173, "user-registration"]], "2. Virtual Machines (VM) vs. Kubernetes (K8s)": [[107, "virtual-machines-vm-vs-kubernetes-k8s"], [109, null]], "2. vLLM as the inference engine": [[212, "vllm-as-the-inference-engine"], [213, "vllm-as-the-inference-engine"], [218, "vllm-as-the-inference-engine"]], "2.1 Control Layer: What Anyscale Manages or Needs Access To": [[107, "control-layer-what-anyscale-manages-or-needs-access-to"], [109, "control-layer-what-anyscale-manages-or-needs-access-to"]], "2.1 Create terraform.tfvars": [[118, "create-terraform-tfvars"], [122, "create-terraform-tfvars"]], "2.1 Install the Cluster Autoscaler": [[126, "install-the-cluster-autoscaler"], [129, "install-the-cluster-autoscaler"]], "2.1 Overview": [[5, "overview"], [35, "overview"]], "2.1 Vanilla XGboost code": [[3, "vanilla-xgboost-code"], [28, "vanilla-xgboost-code"]], "2.1. Overview": [[4, "overview"], [31, "overview"]], "2.1: Create terraform.tfvars": [[149, "create-terraform-tfvars"], [153, "create-terraform-tfvars"]], "2.2 Data Layer: Storage, Object Stores, and External Dependencies": [[107, "data-layer-storage-object-stores-and-external-dependencies"], [109, "data-layer-storage-object-stores-and-external-dependencies"]], "2.2 Hyperparameter tuning with Ray Tune": [[3, "hyperparameter-tuning-with-ray-tune"], [28, "hyperparameter-tuning-with-ray-tune"]], "2.2 Install the AWS Load Balancer Controller": [[126, "install-the-aws-load-balancer-controller"], [129, "install-the-aws-load-balancer-controller"]], "2.2 Note on blocks": [[9, "note-on-blocks"], [59, "note-on-blocks"]], "2.2 Run Terraform Commands": [[118, "run-terraform-commands"], [122, "run-terraform-commands"]], "2.2. Build model and load it on the GPU": [[4, "build-model-and-load-it-on-the-gpu"], [31, "build-model-and-load-it-on-the-gpu"]], "2.2. Create a torch dataloader": [[5, "create-a-torch-dataloader"], [35, "create-a-torch-dataloader"]], "2.2: Deploy Infrastructure": [[149, "deploy-infrastructure"], [153, "deploy-infrastructure"]], "2.3 Define a stable diffusion model": [[5, "define-a-stable-diffusion-model"], [35, "define-a-stable-diffusion-model"]], "2.3 Install the Nginx Ingress Controller": [[126, "install-the-nginx-ingress-controller"], [129, "install-the-nginx-ingress-controller"]], "2.3 Workload Execution Layer: How Ray Runs on Each Backend": [[107, "workload-execution-layer-how-ray-runs-on-each-backend"], [109, "workload-execution-layer-how-ray-runs-on-each-backend"]], "2.3. Create Dataset and DataLoader": [[4, "create-dataset-and-dataloader"], [31, "create-dataset-and-dataloader"]], "2.3. Distributed training with Ray Train": [[3, "distributed-training-with-ray-train"], [28, "distributed-training-with-ray-train"]], "2.4 (Optional) Install the Nvidia Device Plugin": [[126, "optional-install-the-nvidia-device-plugin"], [129, "optional-install-the-nvidia-device-plugin"]], "2.4 Serving an ensemble model with Ray Serve": [[3, "serving-an-ensemble-model-with-ray-serve"], [28, "serving-an-ensemble-model-with-ray-serve"]], "2.4 When to use which": [[107, "when-to-use-which"], [109, "when-to-use-which"]], "2.4. Create metrics and checkpointing": [[4, "create-metrics-and-checkpointing"], [31, "create-metrics-and-checkpointing"]], "2.4. Define a PyTorch Lightning training loop": [[5, "define-a-pytorch-lightning-training-loop"], [35, "define-a-pytorch-lightning-training-loop"]], "2.5 Batch inference with Ray Data": [[3, "batch-inference-with-ray-data"], [28, "batch-inference-with-ray-data"]], "2.5. Run the training loop": [[4, "run-the-training-loop"], [31, "run-the-training-loop"]], "2.6 Clean up": [[3, "clean-up"], [28, "clean-up"]], "2.6. Use checkpointed model to generate predictions": [[4, "use-checkpointed-model-to-generate-predictions"], [31, "use-checkpointed-model-to-generate-predictions"]], "2.Download starter template. Clone a github repository containing the files needed to deploy a Anyscale Service. Head over to the VSCode Tab (In Anyscale Workspace) and enter the following command into the terminal.": [[187, "download-starter-template-clone-a-github-repository-containing-the-files-needed-to-deploy-a-anyscale-service-head-over-to-the-vscode-tab-in-anyscale-workspace-and-enter-the-following-command-into-the-terminal"], [204, "download-starter-template-clone-a-github-repository-containing-the-files-needed-to-deploy-a-anyscale-service-head-over-to-the-vscode-tab-in-anyscale-workspace-and-enter-the-following-command-into-the-terminal"], [206, "download-starter-template-clone-a-github-repository-containing-the-files-needed-to-deploy-a-anyscale-service-head-over-to-the-vscode-tab-in-anyscale-workspace-and-enter-the-following-command-into-the-terminal"], [211, "download-starter-template-clone-a-github-repository-containing-the-files-needed-to-deploy-a-anyscale-service-head-over-to-the-vscode-tab-in-anyscale-workspace-and-enter-the-following-command-into-the-terminal"]], "20 \u00b7 Clean Up the Ray Actor": [[273, "clean-up-the-ray-actor"], [292, "clean-up-the-ray-actor"]], "3. (Optional) More Kubernetes Deployments Components": [[107, "optional-more-kubernetes-deployments-components"], [110, null]], "3. A Demonstrative Example of Resource Creation with AWS EC2": [[100, "a-demonstrative-example-of-resource-creation-with-aws-ec2"], [104, null]], "3. Advanced features of Ray Serve": [[241, "advanced-features-of-ray-serve"], [270, null]], "3. Anyscale for Infrastructure": [[212, "anyscale-for-infrastructure"], [213, "anyscale-for-infrastructure"], [218, "anyscale-for-infrastructure"]], "3. Context Window Requirements": [[229, "context-window-requirements"], [235, "context-window-requirements"]], "3. Distributed Data Parallel Training with Ray Train and PyTorch": [[4, "distributed-data-parallel-training-with-ray-train-and-pytorch"], [32, null]], "3. Distributed Training with Ray Train and PyTorch Lightning": [[5, "distributed-training-with-ray-train-and-pytorch-lightning"], [36, null]], "3. Enable Pipeline Parallelism": [[221, "enable-pipeline-parallelism"], [227, "enable-pipeline-parallelism"]], "3. Getting Results": [[1, "getting-results"], [15, "getting-results"]], "3. Hyperparameter tuning with Ray Tune": [[6, "hyperparameter-tuning-with-ray-tune"], [41, null]], "3. Implement an image classification service": [[10, "implement-an-image-classification-service"], [70, null]], "3. Inspect the code for the Service Endpoint (./examples/02_service_hello_world/main.py)": [[187, "inspect-the-code-for-the-service-endpoint-examples-02-service-hello-world-main-py"], [204, "inspect-the-code-for-the-service-endpoint-examples-02-service-hello-world-main-py"], [206, "inspect-the-code-for-the-service-endpoint-examples-02-service-hello-world-main-py"], [211, "inspect-the-code-for-the-service-endpoint-examples-02-service-hello-world-main-py"]], "3. Install Kubernetes Components": [[136, "install-kubernetes-components"], [141, null]], "3. Install Ray and the Anyscale CLI (Recommended)": [[188, "install-ray-and-the-anyscale-cli-recommended"]], "3. Introduction to Ray Tune": [[239, "introduction-to-ray-tune"], [258, null]], "3. Lazy execution mode": [[9, "lazy-execution-mode"], [60, null]], "3. Loading data": [[9, "loading-data"], [59, null]], "3. Local Cluster Storage": [[180, "local-cluster-storage"], [192, "local-cluster-storage"]], "3. Metrics Setup": [[76, "metrics-setup"], [96, null]], "3. Model parallelization or alternatives": [[212, "model-parallelization-or-alternatives"], [213, "model-parallelization-or-alternatives"], [216, "model-parallelization-or-alternatives"]], "3. Next, create a new file. You can name it hello_world.py": [[186, "next-create-a-new-file-you-can-name-it-hello-world-py"], [201, "next-create-a-new-file-you-can-name-it-hello-world-py"], [205, "next-create-a-new-file-you-can-name-it-hello-world-py"], [208, "next-create-a-new-file-you-can-name-it-hello-world-py"]], "3. Normalize and split": [[278, "normalize-and-split"], [333, "normalize-and-split"]], "3. Overview of the training loop in Ray Train": [[238, "overview-of-the-training-loop-in-ray-train"], [249, null]], "3. Point to Parquet dataset URI": [[279, "point-to-parquet-dataset-uri"], [338, "point-to-parquet-dataset-uri"]], "3. Register the Anyscale Cloud": [[118, "register-the-anyscale-cloud"], [123, null], [126, "register-the-anyscale-cloud"], [130, null]], "3. Resample to hourly, then normalize": [[276, "resample-to-hourly-then-normalize"], [321, "resample-to-hourly-then-normalize"]], "3. Resize and encode images": [[274, "resize-and-encode-images"], [277, "resize-and-encode-images"], [307, "resize-and-encode-images"], [327, "resize-and-encode-images"]], "3. Reverse diffusion (sampling)": [[278, "reverse-diffusion-sampling"], [332, "reverse-diffusion-sampling"]], "3. Running an experiment with Ray AI libraries": [[237, "running-an-experiment-with-ray-ai-libraries"], [244, "running-an-experiment-with-ray-ai-libraries"], [245, null]], "3. Scalability Demands": [[212, "scalability-demands"], [213, "scalability-demands"], [217, "scalability-demands"]], "3. Serve Locally for Testing": [[0, "serve-locally-for-testing"]], "3. Submit the job again using the Anyscale Python SDK": [[186, "submit-the-job-again-using-the-anyscale-python-sdk"], [202, "submit-the-job-again-using-the-anyscale-python-sdk"], [205, "submit-the-job-again-using-the-anyscale-python-sdk"], [209, "submit-the-job-again-using-the-anyscale-python-sdk"]], "3. Task retries": [[2, "task-retries"], [20, null]], "3. Test": [[111, "test"], [115, null]], "3. Transforming Data": [[8, "transforming-data"], [52, null], [240, "transforming-data"], [264, null]], "3. Troubleshooting GPU Availability": [[149, "troubleshooting-gpu-availability"], [154, null]], "3. Visualize class balance": [[275, "visualize-class-balance"], [316, "visualize-class-balance"]], "3.1 Distributed Data Parallel Training": [[5, "distributed-data-parallel-training"], [36, "distributed-data-parallel-training"]], "3.1 IAM Role Definition": [[100, "iam-role-definition"], [105, null]], "3.1 Install the Cluster Autoscaler": [[136, "install-the-cluster-autoscaler"], [141, "install-the-cluster-autoscaler"]], "3.1. Overview of the training loop in Ray Train": [[4, "overview-of-the-training-loop-in-ray-train"], [32, "overview-of-the-training-loop-in-ray-train"]], "3.1.1\u202f\u202fAnyscale Control Plane Role (anyscale-iam-role-id)": [[100, "anyscale-control-plane-role-anyscale-iam-role-id"], [105, "anyscale-control-plane-role-anyscale-iam-role-id"]], "3.1.2\u202f\u202fInstance Role (instance-iam-role-id)": [[100, "instance-role-instance-iam-role-id"], [105, "instance-role-instance-iam-role-id"]], "3.10. Activity: Run the distributed training with more workers": [[4, "activity-run-the-distributed-training-with-more-workers"], [32, "activity-run-the-distributed-training-with-more-workers"]], "3.2 Install the AWS Load Balancer Controller": [[136, "install-the-aws-load-balancer-controller"], [141, "install-the-aws-load-balancer-controller"]], "3.2 Ray Train Migration": [[5, "ray-train-migration"], [36, "ray-train-migration"]], "3.2. Configure scale and GPUs": [[4, "configure-scale-and-gpus"], [32, "configure-scale-and-gpus"]], "3.2.1. Note on Ray Train key concepts": [[4, "note-on-ray-train-key-concepts"], [32, "note-on-ray-train-key-concepts"]], "3.2\u202fVPC": [[100, "vpc"], [105, "vpc"]], "3.3 Install the Nginx Ingress Controller": [[136, "install-the-nginx-ingress-controller"], [141, "install-the-nginx-ingress-controller"]], "3.3 Subnets": [[100, "subnets"], [105, "subnets"]], "3.3. Configure scale and GPUs": [[5, "configure-scale-and-gpus"], [36, "configure-scale-and-gpus"]], "3.3. Migrating the model to Ray Train": [[4, "migrating-the-model-to-ray-train"], [32, "migrating-the-model-to-ray-train"]], "3.3.1. Note on Ray Train key concepts": [[5, "note-on-ray-train-key-concepts"], [36, "note-on-ray-train-key-concepts"]], "3.4 (Optional) Install the Nvidia Device Plugin": [[136, "optional-install-the-nvidia-device-plugin"], [141, "optional-install-the-nvidia-device-plugin"]], "3.4 Create and fit a Ray Train TorchTrainer": [[5, "create-and-fit-a-ray-train-torchtrainer"], [36, "create-and-fit-a-ray-train-torchtrainer"]], "3.4. Migrating the dataset to Ray Train": [[4, "migrating-the-dataset-to-ray-train"], [32, "migrating-the-dataset-to-ray-train"]], "3.4\u202fSecurity Groups": [[100, "security-groups"], [105, "security-groups"]], "3.5. Access the training results": [[5, "access-the-training-results"], [36, "access-the-training-results"]], "3.5. Reporting checkpoints and metrics": [[4, "reporting-checkpoints-and-metrics"], [32, "reporting-checkpoints-and-metrics"]], "3.5.1. Note on the checkpoint lifecycle": [[4, "note-on-the-checkpoint-lifecycle"], [32, "note-on-the-checkpoint-lifecycle"]], "3.5\u202fS3": [[100, "s3"], [105, "s3"]], "3.6. Configure remote storage": [[4, "configure-remote-storage"], [32, "configure-remote-storage"]], "3.6. Load the checkpointed model to generate predictions": [[5, "load-the-checkpointed-model-to-generate-predictions"], [36, "load-the-checkpointed-model-to-generate-predictions"]], "3.6\u202fEFS (Optional)": [[100, "efs-optional"], [105, "efs-optional"]], "3.7. Activity: Run the distributed training with more workers": [[5, "activity-run-the-distributed-training-with-more-workers"], [36, "activity-run-the-distributed-training-with-more-workers"]], "3.7. Launching the distributed training job": [[4, "launching-the-distributed-training-job"], [32, "launching-the-distributed-training-job"]], "3.7\u202fMemoryDB (Optional)": [[100, "memorydb-optional"], [105, "memorydb-optional"]], "3.8 Summary": [[100, "summary"], [105, "summary"]], "3.8. Access the training results": [[4, "access-the-training-results"], [32, "access-the-training-results"]], "3.9. Use checkpointed model to generate predictions": [[4, "id1"], [32, "use-checkpointed-model-to-generate-predictions"]], "4. Checkout the service.yaml file.": [[187, "checkout-the-service-yaml-file"], [204, "checkout-the-service-yaml-file"], [206, "checkout-the-service-yaml-file"], [211, "checkout-the-service-yaml-file"]], "4. Cleanup": [[111, "cleanup"], [116, null]], "4. Context Window Considerations": [[212, "context-window-considerations"], [213, "context-window-considerations"], [216, "context-window-considerations"]], "4. Cost Optimization": [[212, "cost-optimization"], [213, "cost-optimization"], [217, "cost-optimization"]], "4. Data Operations: Grouping, Aggregation, and Shuffling": [[240, "data-operations-grouping-aggregation-and-shuffling"], [265, null]], "4. Development workflow": [[10, "development-workflow"], [71, null]], "4. DiffusionPolicy LightningModule": [[278, "diffusionpolicy-lightningmodule"], [334, null]], "4. Diving deeper into Ray Tune concepts": [[239, "diving-deeper-into-ray-tune-concepts"], [259, null]], "4. Examples Outlook: Deploying to Your Infrastructure": [[107, "examples-outlook-deploying-to-your-infrastructure"], [110, "examples-outlook-deploying-to-your-infrastructure"]], "4. Hardware and Cost Considerations": [[229, "hardware-and-cost-considerations"], [235, "hardware-and-cost-considerations"]], "4. Install the Anyscale Operator": [[126, "install-the-anyscale-operator"], [131, null]], "4. Local File Store": [[180, "local-file-store"], [192, "local-file-store"]], "4. Migrating the model and dataset to Ray Train": [[238, "migrating-the-model-and-dataset-to-ray-train"], [250, null]], "4. Paste the basic Ray example below into the file.": [[186, "paste-the-basic-ray-example-below-into-the-file"], [201, "paste-the-basic-ray-example-below-into-the-file"], [205, "paste-the-basic-ray-example-below-into-the-file"], [208, "paste-the-basic-ray-example-below-into-the-file"]], "4. Putting It All Together": [[1, "putting-it-all-together"], [16, null]], "4. Quick visual sanity-check": [[276, "quick-visual-sanity-check"], [321, "quick-visual-sanity-check"]], "4. Ray Serve in Production": [[241, "ray-serve-in-production"], [271, null]], "4. Ray Train in Production": [[4, "ray-train-in-production"], [5, "ray-train-in-production"], [32, "ray-train-in-production"], [37, null]], "4. Ray Tune in Production": [[6, "ray-tune-in-production"], [42, null]], "4. Register Anyscale Cloud to Your Cloud Provider": [[100, "register-anyscale-cloud-to-your-cloud-provider"], [106, null]], "4. Register the Anyscale Cloud": [[136, "register-the-anyscale-cloud"], [142, null]], "4. Scale with More Replicas": [[221, "scale-with-more-replicas"], [227, "scale-with-more-replicas"]], "4. Task Runtime Environments": [[2, "task-runtime-environments"], [21, null]], "4. Test": [[118, "test"], [124, null]], "4. Training function per worker": [[76, "training-function-per-worker"], [97, null]], "4. Transforming data": [[9, "transforming-data"], [61, null]], "4. Visual sanity check": [[274, "visual-sanity-check"], [277, "visual-sanity-check"], [307, "visual-sanity-check"], [327, "visual-sanity-check"]], "4. Visualize dataset: ratings, users, and items": [[279, "visualize-dataset-ratings-users-and-items"], [338, "visualize-dataset-ratings-users-and-items"]], "4. Write train / validation Parquet files": [[275, "write-train-validation-parquet-files"], [316, "write-train-validation-parquet-files"]], "4. Writing Data": [[8, "writing-data"], [53, null]], "4. kubectl Configuration": [[149, "kubectl-configuration"], [155, null]], "4.1 On resource specification": [[9, "on-resource-specification"], [61, "on-resource-specification"]], "4.1. Note about Ray ID Specification": [[1, "note-about-ray-id-specification"], [16, "note-about-ray-id-specification"]], "4.1. Note about pip dependencies": [[2, "note-about-pip-dependencies"], [21, "note-about-pip-dependencies"]], "4.2 On concurrency limiting": [[9, "on-concurrency-limiting"], [61, "on-concurrency-limiting"]], "4.2. Anti-pattern: Calling ray.get in a loop harms parallelism": [[1, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"], [16, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"]], "5. Cleanup": [[118, "cleanup"], [125, null]], "5. Conclusion": [[111, "conclusion"], [117, null]], "5. Create Ray Dataset from Parquet and encode IDs": [[279, "create-ray-dataset-from-parquet-and-encode-ids"], [338, "create-ray-dataset-from-parquet-and-encode-ids"]], "5. Data Operations: Shuffling, Grouping and Aggregation": [[8, "data-operations-shuffling-grouping-and-aggregation"], [54, null]], "5. Deploy the Anyscale Service by running the command below in the terminal and passing in the service configuration yaml file.": [[187, "deploy-the-anyscale-service-by-running-the-command-below-in-the-terminal-and-passing-in-the-service-configuration-yaml-file"], [204, "deploy-the-anyscale-service-by-running-the-command-below-in-the-terminal-and-passing-in-the-service-configuration-yaml-file"], [206, "deploy-the-anyscale-service-by-running-the-command-below-in-the-terminal-and-passing-in-the-service-configuration-yaml-file"], [211, "deploy-the-anyscale-service-by-running-the-command-below-in-the-terminal-and-passing-in-the-service-configuration-yaml-file"]], "5. Distributed Train loop with checkpointing": [[278, "distributed-train-loop-with-checkpointing"], [335, null]], "5. Hyperparameter tuning the PyTorch model using Ray Tune": [[239, "hyperparameter-tuning-the-pytorch-model-using-ray-tune"], [260, null]], "5. Install NGINX Ingress Controller": [[149, "install-nginx-ingress-controller"], [156, null]], "5. Install the Anyscale Operator": [[136, "install-the-anyscale-operator"], [143, null]], "5. Load the train and validation splits as Ray Datasets": [[275, "load-the-train-and-validation-splits-as-ray-datasets"], [316, "load-the-train-and-validation-splits-as-ray-datasets"]], "5. Main Training Function": [[76, "main-training-function"], [98, null]], "5. Open the terminal and run the following command to submit the Ray workflow as an Anyscale Job.": [[186, "open-the-terminal-and-run-the-following-command-to-submit-the-ray-workflow-as-an-anyscale-job"], [201, "open-the-terminal-and-run-the-following-command-to-submit-the-ray-workflow-as-an-anyscale-job"], [205, "open-the-terminal-and-run-the-following-command-to-submit-the-ray-workflow-as-an-anyscale-job"], [208, "open-the-terminal-and-run-the-following-command-to-submit-the-ray-workflow-as-an-anyscale-job"]], "5. Persist to Parquet": [[274, "persist-to-parquet"], [277, "persist-to-parquet"], [307, "persist-to-parquet"], [327, "persist-to-parquet"]], "5. Persisting Data": [[240, "persisting-data"], [266, null]], "5. Reporting checkpoints and metrics": [[238, "reporting-checkpoints-and-metrics"], [251, null]], "5. Resource allocation and management": [[2, "resource-allocation-and-management"], [22, null]], "5. Sliding-window dataset to Parquet": [[276, "sliding-window-dataset-to-parquet"], [321, "sliding-window-dataset-to-parquet"]], "5. Stateful transformations with Ray Actors": [[9, "stateful-transformations-with-ray-actors"], [62, null]], "5. Upgrade Hardware": [[221, "upgrade-hardware"], [227, "upgrade-hardware"]], "5. Verify the Installation": [[126, "verify-the-installation"], [132, null]], "5.1 Resource specification for stateful transformations": [[9, "resource-specification-for-stateful-transformations"], [62, "resource-specification-for-stateful-transformations"]], "5.1. Note on resources requests, available resources, configuring large clusters": [[2, "note-on-resources-requests-available-resources-configuring-large-clusters"], [22, "note-on-resources-requests-available-resources-configuring-large-clusters"]], "5.2 Note on autoscaling for stateful transformations": [[9, "note-on-autoscaling-for-stateful-transformations"], [62, "note-on-autoscaling-for-stateful-transformations"]], "5.2. Fractional resources": [[2, "fractional-resources"], [22, "fractional-resources"]], "5.3. IO bound tasks and fractional resources": [[2, "io-bound-tasks-and-fractional-resources"], [22, "io-bound-tasks-and-fractional-resources"]], "6. (Optional) Upgrade Anyscale Dependencies": [[149, "optional-upgrade-anyscale-dependencies"], [157, null]], "6. Custom Food101Dataset for Parquet": [[274, "custom-food101dataset-for-parquet"], [308, null]], "6. Inspect dataset sizes (optional)": [[275, "inspect-dataset-sizes-optional"], [316, "inspect-dataset-sizes-optional"]], "6. Launch Ray TorchTrainer": [[278, "launch-ray-torchtrainer"], [335, "launch-ray-torchtrainer"]], "6. Launching the distributed training job": [[238, "launching-the-distributed-training-job"], [252, null]], "6. Load and decode with Ray Data": [[277, "load-and-decode-with-ray-data"], [327, "load-and-decode-with-ray-data"]], "6. Materializing data": [[9, "materializing-data"], [63, null]], "6. Nested Tasks": [[2, "nested-tasks"], [23, null]], "6. PyTorch Dataset over Parquet": [[276, "pytorch-dataset-over-parquet"], [321, "pytorch-dataset-over-parquet"]], "6. Start Training": [[76, "start-training"], [99, null]], "6. Test": [[126, "test"], [133, null]], "6. Track the status of the job, head over to the Jobs tab and find the submitted Anyscale Job. The url is also displayed in the terminal.": [[186, "track-the-status-of-the-job-head-over-to-the-jobs-tab-and-find-the-submitted-anyscale-job-the-url-is-also-displayed-in-the-terminal"], [201, "track-the-status-of-the-job-head-over-to-the-jobs-tab-and-find-the-submitted-anyscale-job-the-url-is-also-displayed-in-the-terminal"], [205, "track-the-status-of-the-job-head-over-to-the-jobs-tab-and-find-the-submitted-anyscale-job-the-url-is-also-displayed-in-the-terminal"], [208, "track-the-status-of-the-job-head-over-to-the-jobs-tab-and-find-the-submitted-anyscale-job-the-url-is-also-displayed-in-the-terminal"]], "6. Train/validation split using Ray Data": [[279, "train-validation-split-using-ray-data"], [338, "train-validation-split-using-ray-data"]], "6. Verify the Installation": [[136, "verify-the-installation"], [144, null]], "6. When to use Ray Data": [[8, "when-to-use-ray-data"], [55, null]], "7. Accessing the training results": [[238, "accessing-the-training-results"], [253, null]], "7. Clean up": [[126, "clean-up"], [134, null]], "7. Data Operations: grouping, aggregation, and shuffling": [[9, "data-operations-grouping-aggregation-and-shuffling"], [64, null]], "7. Define matrix factorization model": [[279, "define-matrix-factorization-model"], [339, null]], "7. Image transform": [[274, "image-transform"], [308, "image-transform"]], "7. In the Anyscale Jobs console, we can check out the status of the submitted job. From the logs, we can verify that our job was successfully executed and Anyscale will now handle the cleanup.": [[186, "in-the-anyscale-jobs-console-we-can-check-out-the-status-of-the-submitted-job-from-the-logs-we-can-verify-that-our-job-was-successfully-executed-and-anyscale-will-now-handle-the-cleanup"], [201, "in-the-anyscale-jobs-console-we-can-check-out-the-status-of-the-submitted-job-from-the-logs-we-can-verify-that-our-job-was-successfully-executed-and-anyscale-will-now-handle-the-cleanup"], [205, "in-the-anyscale-jobs-console-we-can-check-out-the-status-of-the-submitted-job-from-the-logs-we-can-verify-that-our-job-was-successfully-executed-and-anyscale-will-now-handle-the-cleanup"], [208, "in-the-anyscale-jobs-console-we-can-check-out-the-status-of-the-submitted-job-from-the-logs-we-can-verify-that-our-job-was-successfully-executed-and-anyscale-will-now-handle-the-cleanup"]], "7. Inspect a mini-batch": [[275, "inspect-a-mini-batch"], [316, "inspect-a-mini-batch"]], "7. Inspect one random batch": [[276, "inspect-one-random-batch"], [321, "inspect-one-random-batch"]], "7. Pattern: Pipeline data processing and waiting for results": [[2, "pattern-pipeline-data-processing-and-waiting-for-results"], [24, null]], "7. Plot train / val loss": [[278, "plot-train-val-loss"], [335, "plot-train-val-loss"]], "7. Ray Data in Production": [[8, "ray-data-in-production"], [55, "ray-data-in-production"]], "7. Register the Anyscale Cloud": [[149, "register-the-anyscale-cloud"], [158, null]], "7. Shuffle and Train/Val split": [[277, "shuffle-and-train-val-split"], [327, "shuffle-and-train-val-split"]], "7. Shutdown Ray Cluster": [[76, "shutdown-ray-cluster"], [99, "shutdown-ray-cluster"]], "7. Test": [[136, "test"], [145, null]], "7.1 Batch Processing Pattern": [[2, "batch-processing-pattern"], [24, "batch-processing-pattern"]], "7.1. Custom batching using groupby.": [[9, "custom-batching-using-groupby"], [64, "custom-batching-using-groupby"]], "7.2 Note on fetching too many objects at once with ray.get causes failure": [[2, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"], [24, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"]], "7.2. Aggregations": [[9, "aggregations"], [64, "aggregations"]], "7.3. Shuffling data": [[9, "shuffling-data"], [64, "shuffling-data"]], "7.3.1. File based shuffle on read": [[9, "file-based-shuffle-on-read"], [64, "file-based-shuffle-on-read"]], "7.3.2. Shuffling block order": [[9, "shuffling-block-order"], [64, "shuffling-block-order"]], "7.3.3. Shuffle all rows globally": [[9, "shuffle-all-rows-globally"], [64, "shuffle-all-rows-globally"]], "8. Conclusion": [[126, "conclusion"], [135, null]], "8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)": [[279, "define-ray-train-loop-with-validation-checkpointing-and-ray-managed-metrics"], [340, null]], "8. Define the Ray Train worker loop (Arrow-based, memory-efficient)": [[275, "define-the-ray-train-worker-loop-arrow-based-memory-efficient"], [317, null]], "8. Install the Anyscale Operator": [[149, "install-the-anyscale-operator"], [159, null]], "8. Persisting data": [[9, "persisting-data"], [65, null]], "8. Pixel diffusion LightningModule": [[277, "pixel-diffusion-lightningmodule"], [328, null]], "8. Ray Actors": [[2, "ray-actors"], [25, null]], "8. Ray Train in Production": [[238, "ray-train-in-production"], [254, null]], "8. Ray-prepared DataLoader": [[276, "ray-prepared-dataloader"], [321, "ray-prepared-dataloader"]], "8. Reverse diffusion helper": [[278, "reverse-diffusion-helper"], [336, null]], "8. Summary": [[76, "summary"], [99, "summary"]], "8. Test": [[149, "test"], [160, null]], "8. Train/validation split": [[274, "train-validation-split"], [308, "train-validation-split"]], "8. Troubleshooting": [[136, "troubleshooting"], [146, null]], "8. Upcoming Features in Ray Data": [[8, "upcoming-features-in-ray-data"], [55, "upcoming-features-in-ray-data"]], "9. Clean up": [[136, "clean-up"], [147, null]], "9. Cleanup": [[149, "cleanup"], [161, null]], "9. Configure XGBoost and build the Trainer": [[275, "configure-xgboost-and-build-the-trainer"], [317, "configure-xgboost-and-build-the-trainer"]], "9. Inspect a DataLoader batch": [[274, "inspect-a-dataloader-batch"], [308, "inspect-a-dataloader-batch"]], "9. Launch distributed training with Ray Train": [[279, "launch-distributed-training-with-ray-train"], [340, "launch-distributed-training-with-ray-train"]], "9. PositionalEncoding and Transformer model": [[276, "positionalencoding-and-transformer-model"], [322, null]], "9. Ray Data in production": [[9, "ray-data-in-production"], [66, null]], "9. Ray Train train_loop (Lightning + Ray integration)": [[277, "ray-train-train-loop-lightning-ray-integration"], [329, null]], "9. Sample an action from the trained policy": [[278, "sample-an-action-from-the-trained-policy"], [336, "sample-an-action-from-the-trained-policy"]], "API Endpoints": [[173, "api-endpoints"]], "Activity: Update the training loop to compute the area under the curve of ROC (AUROC)": [[238, "activity-update-the-training-loop-to-compute-the-area-under-the-curve-of-roc-auroc"], [253, "activity-update-the-training-loop-to-compute-the-area-under-the-curve-of-roc-auroc"]], "Adding New Notebooks or Courses": [[0, "adding-new-notebooks-or-courses"]], "Additional Setup (Optional)": [[163, "additional-setup-optional"], [176, "additional-setup-optional"]], "Advanced LLM Features with Ray Serve LLM": [[229, null], [230, null]], "Advanced Topics: Monitoring & Optimization": [[221, "advanced-topics-monitoring-optimization"], [227, null]], "After navigating to a specific Anyscale Workspace, you can submit your main python script as a Anyscale Job.": [[186, "after-navigating-to-a-specific-anyscale-workspace-you-can-submit-your-main-python-script-as-a-anyscale-job"], [201, "after-navigating-to-a-specific-anyscale-workspace-you-can-submit-your-main-python-script-as-a-anyscale-job"], [205, "after-navigating-to-a-specific-anyscale-workspace-you-can-submit-your-main-python-script-as-a-anyscale-job"], [208, "after-navigating-to-a-specific-anyscale-workspace-you-can-submit-your-main-python-script-as-a-anyscale-job"]], "Aggregations": [[240, "aggregations"], [265, "aggregations"]], "Annotated experiment table": [[239, "annotated-experiment-table"], [259, "annotated-experiment-table"]], "Anyscale 101": [[343, "anyscale-101"]], "Anyscale 101 Learning Path": [[188, "anyscale-101-learning-path"]], "Anyscale Administrator Overview": [[100, null], [101, null]], "Anyscale Observability": [[164, "anyscale-observability"], [167, null]], "Anyscale Projects": [[184, "anyscale-projects"], [196, "anyscale-projects"]], "Anyscale Ray Serve Observability": [[169, "anyscale-ray-serve-observability"], [172, "anyscale-ray-serve-observability"]], "Anyscale Service Lifecycle": [[187, "anyscale-service-lifecycle"], [203, "anyscale-service-lifecycle"], [206, "anyscale-service-lifecycle"], [210, "anyscale-service-lifecycle"]], "Apache Arrow": [[7, "apache-arrow"], [43, "apache-arrow"]], "Applications": [[10, "applications"], [69, "applications"]], "Architecture": [[73, "architecture"], [75, "architecture"], [79, null], [91, null], [173, "architecture"]], "Available Endpoints": [[173, "available-endpoints"]], "Batch Inference Class": [[73, "batch-inference-class"], [81, null]], "Batch Inference with Ray Data": [[73, null], [78, null]], "Batch Processing": [[7, "batch-processing"], [46, "batch-processing"]], "Benefits of using Anyscale Services": [[187, "benefits-of-using-anyscale-services"], [203, "benefits-of-using-anyscale-services"], [206, "benefits-of-using-anyscale-services"], [210, "benefits-of-using-anyscale-services"]], "Build and load our model on a single GPU": [[238, "build-and-load-our-model-on-a-single-gpu"], [247, "build-and-load-our-model-on-a-single-gpu"]], "Challenges in LLM Serving": [[212, "challenges-in-llm-serving"], [213, "challenges-in-llm-serving"], [217, null]], "Challenges with JVM": [[7, "challenges-with-jvm"], [46, "challenges-with-jvm"]], "Clean Up": [[181, "clean-up"], [182, "clean-up"], [183, "clean-up"], [193, "clean-up"], [194, "clean-up"], [195, "clean-up"]], "Clean up": [[241, "clean-up"], [272, null]], "Cloning/Duplicating Resources": [[184, "cloning-duplicating-resources"], [196, "cloning-duplicating-resources"]], "Cloud": [[185, "cloud"], [198, "cloud"]], "Collaborating with Your Team": [[188, "collaborating-with-your-team"]], "Composing Deployments": [[241, "composing-deployments"], [270, "composing-deployments"]], "Compute by Function": [[7, "compute-by-function"], [44, "compute-by-function"]], "Conclusion: Next Steps": [[229, "conclusion-next-steps"], [236, null]], "Concurrency Optimization Strategies": [[221, "concurrency-optimization-strategies"], [227, "concurrency-optimization-strategies"]], "Configuration Breakdown": [[221, "configuration-breakdown"], [224, "configuration-breakdown"]], "Configuration for Medium-Sized Models": [[221, "configuration-for-medium-sized-models"], [224, "configuration-for-medium-sized-models"]], "Configure Ray Serve LLM with LoRA": [[229, "configure-ray-serve-llm-with-lora"], [232, "configure-ray-serve-llm-with-lora"]], "Configure persistent storage": [[238, "configure-persistent-storage"], [252, "configure-persistent-storage"]], "Configure scale and GPUs": [[238, "configure-scale-and-gpus"], [249, "configure-scale-and-gpus"]], "Configure the Worker Node(s)": [[179, "configure-the-worker-node-s"], [191, "configure-the-worker-node-s"]], "Content Used": [[186, null], [200, null], [205, null], [207, null]], "Convert to Pandas DataFrame": [[74, "convert-to-pandas-dataframe"], [89, "convert-to-pandas-dataframe"]], "Convert to Ray Dataset": [[74, "convert-to-ray-dataset"], [86, null]], "Course Welcome and Overview": [[11, "course-welcome-and-overview"], [72, "course-welcome-and-overview"]], "Create a Second Dataset": [[74, "create-a-second-dataset"], [86, "create-a-second-dataset"]], "Create a batch data and call the model": [[73, "create-a-batch-data-and-call-the-model"], [82, null]], "Create custom security group": [[100, "create-custom-security-group"], [105, "create-custom-security-group"]], "Creating Anyscale Resources": [[111, "creating-anyscale-resources"], [113, "creating-anyscale-resources"], [118, "creating-anyscale-resources"], [122, "creating-anyscale-resources"], [126, "creating-anyscale-resources"], [128, "creating-anyscale-resources"]], "Creating a Compute Config": [[179, "creating-a-compute-config"], [191, "creating-a-compute-config"]], "Creating a Container Image": [[179, "creating-a-container-image"], [191, "creating-a-container-image"]], "Custom batching using groupby": [[240, "custom-batching-using-groupby"], [265, "custom-batching-using-groupby"]], "Custom batching using groupby and aggregations": [[8, "custom-batching-using-groupby-and-aggregations"], [54, "custom-batching-using-groupby-and-aggregations"]], "Customization": [[0, "customization"]], "Customizing autoscaling": [[241, "customizing-autoscaling"], [270, "customizing-autoscaling"]], "Data Engineering Compute": [[7, "data-engineering-compute"], [44, "data-engineering-compute"]], "Data Pipeline Observability (Ray Data)": [[169, "data-pipeline-observability-ray-data"], [171, null]], "Data Processing and ML examples with Ray": [[77, null]], "Data Processing with Ray Data": [[7, "data-processing-with-ray-data"], [47, null], [74, null], [84, null]], "Data flow": [[7, "data-flow"], [48, "data-flow"]], "Data lakes": [[7, "data-lakes"], [43, "data-lakes"]], "Data warehouses": [[7, "data-warehouses"], [43, "data-warehouses"]], "Databases": [[7, "databases"], [43, "databases"]], "Dataloaders": [[76, "dataloaders"], [97, "dataloaders"]], "Dataset": [[8, "dataset"], [51, "dataset"]], "Decode Phase": [[212, "decode-phase"], [213, "decode-phase"], [215, "decode-phase"]], "Default settings for Ray Tune": [[239, "default-settings-for-ray-tune"], [259, "default-settings-for-ray-tune"]], "Defining a data loader": [[238, "defining-a-data-loader"], [247, "defining-a-data-loader"]], "Deploy a Medium-Sized LLM with Ray Serve LLM": [[221, null], [222, null]], "Deploy the model": [[75, "deploy-the-model"], [92, "deploy-the-model"]], "Deploying Applications with Services": [[188, "deploying-applications-with-services"]], "Deploying Pipelines with Jobs": [[188, "deploying-pipelines-with-jobs"]], "Deploying at scale": [[73, "deploying-at-scale"], [82, "deploying-at-scale"]], "Deploying to Anyscale Services": [[221, "deploying-to-anyscale-services"], [226, null]], "Deployment": [[173, "deployment"]], "Deployment Example Structure": [[162, "deployment-example-structure"]], "Deployment Options: Virtual Machines vs. Kubernetes": [[107, null], [108, null]], "Deployments": [[10, "deployments"], [69, "deployments"], [241, "deployments"], [268, "deployments"]], "Developer Intro to Ray": [[343, "developer-intro-to-ray"]], "Developing in Anyscale Workspaces": [[188, "developing-in-anyscale-workspaces"]], "Disabling Notebook Execution and Outputs": [[0, "disabling-notebook-execution-and-outputs"]], "Distributed Computing Frameworks": [[7, "distributed-computing-frameworks"], [46, null]], "Distributed Data-Parallel Training with Ray Train": [[273, "distributed-data-parallel-training-with-ray-train"], [289, "distributed-data-parallel-training-with-ray-train"]], "Distributed training with Ray Train, PyTorch and Hugging Face": [[76, null], [94, null]], "Distributed training with Ray Train, PyTorch and HuggingFace": [[77, "distributed-training-with-ray-train-pytorch-and-huggingface"]], "Diving deeper into Ray Tune concepts": [[6, "diving-deeper-into-ray-tune-concepts"], [41, "diving-deeper-into-ray-tune-concepts"]], "Dual-Subnet Architecture": [[100, "dual-subnet-architecture"], [105, "dual-subnet-architecture"]], "Enabling LLM Monitoring": [[221, "enabling-llm-monitoring"], [227, "enabling-llm-monitoring"]], "End of Module 01 \u00b7 Introduction to Ray Train": [[273, "end-of-module-01-introduction-to-ray-train"], [292, "end-of-module-01-introduction-to-ray-train"]], "Environment state and action": [[278, "environment-state-and-action"], [332, "environment-state-and-action"]], "Example": [[164, "example"], [168, null]], "Example Workflow": [[0, "example-workflow"]], "Example: Car type description": [[229, "example-car-type-description"], [233, "example-car-type-description"]], "Example: Code Assistant LoRA": [[229, "example-code-assistant-lora"], [232, "example-code-assistant-lora"]], "Example: Deploying LoRA Adapters": [[229, "example-deploying-lora-adapters"], [232, null]], "Example: Getting Structured JSON Output": [[229, "example-getting-structured-json-output"], [233, null]], "Example: Setting up Tool Calling": [[229, "example-setting-up-tool-calling"], [234, null]], "Example: Weather Assistant with Tool Calling": [[229, "example-weather-assistant-with-tool-calling"], [234, "example-weather-assistant-with-tool-calling"]], "Execution mode": [[8, "execution-mode"], [52, "execution-mode"], [240, "execution-mode"], [264, "execution-mode"]], "Exercise": [[6, "exercise"], [41, "exercise"], [239, "exercise"], [259, "exercise"]], "Expected Output": [[229, "expected-output"], [233, "expected-output"]], "Exploring the Anyscale Log Viewer": [[181, "exploring-the-anyscale-log-viewer"], [193, "exploring-the-anyscale-log-viewer"]], "Exploring the Anyscale Metrics Tab": [[181, "exploring-the-anyscale-metrics-tab"], [193, "exploring-the-anyscale-metrics-tab"]], "Exploring the Ray Dashboard": [[181, "exploring-the-ray-dashboard"], [193, "exploring-the-ray-dashboard"]], "FastAPI webservice and deploy a model": [[75, "fastapi-webservice-and-deploy-a-model"], [92, null]], "Features": [[0, "features"]], "File based shuffle on read": [[8, "file-based-shuffle-on-read"], [54, "file-based-shuffle-on-read"], [240, "file-based-shuffle-on-read"], [265, "file-based-shuffle-on-read"]], "Filter Ray Dataset": [[74, "filter-ray-dataset"], [87, null]], "Forward process: adding noise": [[277, "forward-process-adding-noise"], [326, "forward-process-adding-noise"]], "General-Purpose Distributed Computing": [[7, "general-purpose-distributed-computing"], [46, "general-purpose-distributed-computing"]], "Get User Profile": [[173, "get-user-profile"]], "Getting Started": [[188, "getting-started"]], "Getting Started with Ray Serve LLM": [[212, "getting-started-with-ray-serve-llm"], [213, "getting-started-with-ray-serve-llm"], [219, null]], "Getting started": [[6, "getting-started"], [41, "getting-started"], [239, "getting-started"], [258, "getting-started"]], "Gettingstarted": [[343, "gettingstarted"]], "How Navigation Works": [[0, "how-navigation-works"]], "How Other Sizes Differ": [[221, "how-other-sizes-differ"], [228, "how-other-sizes-differ"]], "How Resources are defined": [[100, "how-resources-are-defined"], [103, "how-resources-are-defined"]], "How to Choose an LLM?": [[229, "how-to-choose-an-llm"], [235, null]], "How to migrate this computer vision workload to a distributed setup using Ray on Anyscale": [[274, "how-to-migrate-this-computer-vision-workload-to-a-distributed-setup-using-ray-on-anyscale"], [306, "how-to-migrate-this-computer-vision-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this diffusion-policy workload to a distributed setup using Ray on Anyscale": [[277, "how-to-migrate-this-diffusion-policy-workload-to-a-distributed-setup-using-ray-on-anyscale"], [326, "how-to-migrate-this-diffusion-policy-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this recommendation system workload to a distributed setup using Ray on Anyscale": [[279, "how-to-migrate-this-recommendation-system-workload-to-a-distributed-setup-using-ray-on-anyscale"], [337, "how-to-migrate-this-recommendation-system-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this tabular workload to a distributed setup using Ray on Anyscale": [[275, "how-to-migrate-this-tabular-workload-to-a-distributed-setup-using-ray-on-anyscale"], [315, "how-to-migrate-this-tabular-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this time-series workload to a distributed multi-node setup using Ray on Anyscale": [[276, "how-to-migrate-this-time-series-workload-to-a-distributed-multi-node-setup-using-ray-on-anyscale"], [320, "how-to-migrate-this-time-series-workload-to-a-distributed-multi-node-setup-using-ray-on-anyscale"]], "How to scale this policy learning workload using Ray on Anyscale": [[278, "how-to-scale-this-policy-learning-workload-using-ray-on-anyscale"], [332, "how-to-scale-this-policy-learning-workload-using-ray-on-anyscale"]], "How we use Ray AI Libraries for this task": [[237, "how-we-use-ray-ai-libraries-for-this-task"], [244, "how-we-use-ray-ai-libraries-for-this-task"]], "Hyperparameter tune the PyTorch model using Ray Tune": [[6, "hyperparameter-tune-the-pytorch-model-using-ray-tune"], [41, "hyperparameter-tune-the-pytorch-model-using-ray-tune"]], "Import Libraries": [[73, "import-libraries"], [79, "import-libraries"]], "Import libraries": [[75, "import-libraries"], [91, "import-libraries"]], "Improving Concurrency": [[221, "improving-concurrency"], [227, "improving-concurrency"]], "In-memory data formats": [[7, "in-memory-data-formats"], [43, "in-memory-data-formats"]], "Inference: ranking items per user": [[279, "inference-ranking-items-per-user"], [337, "inference-ranking-items-per-user"]], "Initialize Ray and Load a Dataset": [[74, "initialize-ray-and-load-a-dataset"], [85, "initialize-ray-and-load-a-dataset"]], "Input: Images as tensors": [[277, "input-images-as-tensors"], [326, "input-images-as-tensors"]], "Input: user\u2013item\u2013rating triples": [[279, "input-useritemrating-triples"], [337, "input-useritemrating-triples"]], "Inputs": [[274, "inputs"], [306, "inputs"]], "Inspecting the features of the NYC taxi dataset": [[237, "inspecting-the-features-of-the-nyc-taxi-dataset"], [244, "inspecting-the-features-of-the-nyc-taxi-dataset"]], "Installation": [[0, "installation"], [77, "installation"]], "Integrating with FastAPI": [[241, "integrating-with-fastapi"], [270, "integrating-with-fastapi"]], "Intro to Ray Data": [[240, null], [261, null]], "Intro to Ray Data:  Ray Data + Unstructured Data": [[9, null], [56, null]], "Intro to Ray Serve": [[241, null], [267, null]], "Intro to Ray Tune": [[6, "intro-to-ray-tune"], [41, "intro-to-ray-tune"], [239, null], [255, null]], "Introduction": [[178, "introduction"], [179, "introduction"], [180, "introduction"], [181, "introduction"], [190, "introduction"], [191, "introduction"], [192, "introduction"], [193, "introduction"]], "Introduction to Ray Core (Advancement): Object store, Tasks, Actors": [[2, null], [17, null]], "Introduction to Ray Core: Getting Started": [[1, null], [12, null]], "Introduction to Ray Data: Industry Landscape": [[7, null], [43, null]], "Introduction to Ray Data: Ray Data + Structured Data": [[8, null], [49, null]], "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving": [[212, null], [213, null], [214, null]], "Introduction to Ray Serve with PyTorch": [[10, null], [67, null]], "Introduction to Ray Train": [[238, null], [246, null]], "Introduction to Ray Train + PyTorch": [[4, null], [29, null]], "Introduction to Ray Train: Ray Train + PyTorch Lightning": [[5, null], [33, null]], "Introduction to Ray Tune": [[6, null], [38, null]], "Introduction to Ray: Developer": [[11, null], [72, null]], "Introduction to the Ray AI Libraries": [[237, null], [242, null]], "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model": [[3, null], [26, null]], "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster": [[126, null], [127, null]], "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster": [[149, null], [150, null]], "Introduction: Deploy Anyscale Ray on AWS EC2 Instances": [[111, null], [112, null]], "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster": [[136, null], [137, null]], "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)": [[118, null], [119, null]], "Introduction: Why Anyscale?": [[188, "introduction-why-anyscale"]], "Join Two Ray Datasets": [[74, "join-two-ray-datasets"], [88, null]], "Key Benefits": [[229, "key-benefits"], [229, "id1"], [229, "id3"], [232, "key-benefits"], [233, "key-benefits"], [234, "key-benefits"]], "Key Components": [[221, "key-components"], [224, "key-components"]], "Key Concepts and Optimizations": [[212, "key-concepts-and-optimizations"], [213, "key-concepts-and-optimizations"], [216, null]], "Key Features": [[186, "key-features"], [200, "key-features"], [205, "key-features"], [207, "key-features"]], "Key Functions": [[100, "key-functions"], [102, "key-functions"]], "Key Ray Serve Features": [[241, "key-ray-serve-features"], [268, "key-ray-serve-features"]], "Key Takeaways": [[212, "key-takeaways"], [213, "key-takeaways"], [220, null], [221, "key-takeaways"], [228, "key-takeaways"], [229, "key-takeaways"], [236, "key-takeaways"]], "Key characteristics of Anyscale Services": [[183, "key-characteristics-of-anyscale-services"], [195, "key-characteristics-of-anyscale-services"]], "Key points": [[238, "key-points"], [249, "key-points"]], "Labels": [[274, "labels"], [306, "labels"]], "Lakehouses": [[7, "lakehouses"], [43, "lakehouses"]], "Last Updated 6/19": [[188, null]], "Launch Grafana": [[163, "launch-grafana"], [176, "launch-grafana"]], "Launching Ray Serve": [[221, "launching-ray-serve"], [225, "launching-ray-serve"]], "Launching a Anyscale Workspace": [[177, "launching-a-anyscale-workspace"], [189, "launching-a-anyscale-workspace"]], "Launching a Web Application using Ray Serve": [[169, "launching-a-web-application-using-ray-serve"], [172, "launching-a-web-application-using-ray-serve"]], "Launching a distributed training job with a TorchTrainer.": [[238, "launching-a-distributed-training-job-with-a-torchtrainer"], [252, "launching-a-distributed-training-job-with-a-torchtrainer"]], "Launching the Service": [[221, "launching-the-service"], [226, "launching-the-service"]], "Learn More": [[229, "learn-more"], [229, "id2"], [229, "id4"], [232, "learn-more"], [233, "learn-more"], [234, "learn-more"]], "Learning Approach": [[229, "learning-approach"], [231, "learning-approach"]], "Learning Path Overview and Objectives": [[188, "learning-path-overview-and-objectives"]], "Library Imports": [[74, "library-imports"], [85, null]], "Llm Serving": [[343, "llm-serving"]], "Load a dataset": [[73, "load-a-dataset"], [80, null]], "Loading and visualizing MNIST data": [[238, "loading-and-visualizing-mnist-data"], [247, "loading-and-visualizing-mnist-data"]], "Local Deployment & Inference": [[221, "local-deployment-inference"], [225, null]], "Local Development": [[173, "local-development"]], "Local IDE (VSCode / Cursor)": [[178, "local-ide-vscode-cursor"], [190, "local-ide-vscode-cursor"]], "Logging Configuration": [[169, "logging-configuration"], [172, "logging-configuration"]], "Machine Learning and AI Compute": [[7, "machine-learning-and-ai-compute"], [44, "machine-learning-and-ai-compute"]], "Managing Dependencies": [[11, "managing-dependencies"], [72, "managing-dependencies"]], "Materializing Data": [[240, "materializing-data"], [264, "materializing-data"]], "Model Recommendations by Use Case": [[229, "model-recommendations-by-use-case"], [235, "model-recommendations-by-use-case"]], "Model Selection Framework": [[229, "model-selection-framework"], [235, "model-selection-framework"]], "Model Size Comparison": [[221, "model-size-comparison"], [223, "model-size-comparison"]], "Model: embedding-based matrix factorization": [[279, "model-embedding-based-matrix-factorization"], [337, "model-embedding-based-matrix-factorization"]], "More Advanced Topics": [[229, "more-advanced-topics"], [236, "more-advanced-topics"]], "More about Datasets": [[240, "more-about-datasets"], [263, "more-about-datasets"]], "Multi-Actor Ray Serve Tracing Example": [[173, null]], "Next Steps": [[212, "next-steps"], [213, "next-steps"], [220, "next-steps"], [221, "next-steps"], [228, "next-steps"], [229, "next-steps"], [236, "next-steps"]], "Note on the Checkpoint Lifecycle": [[273, "note-on-the-checkpoint-lifecycle"], [289, "note-on-the-checkpoint-lifecycle"]], "Notebook": [[178, "notebook"], [190, "notebook"]], "Now launch a Ray worker node in the terminal:": [[163, "now-launch-a-ray-worker-node-in-the-terminal"], [176, "now-launch-a-ray-worker-node-in-the-terminal"]], "Observability Introduction": [[163, null], [174, null]], "Observability Overview": [[163, "observability-overview"], [175, null]], "On Ray Data vs Spark": [[7, "on-ray-data-vs-spark"], [47, "on-ray-data-vs-spark"]], "Online Model Serving with Ray Serve": [[75, null], [90, null]], "Option 1: Create a New VPC": [[100, "option-1-create-a-new-vpc"], [105, "option-1-create-a-new-vpc"]], "Option 2: Use Existing VPC": [[100, "option-2-use-existing-vpc"], [105, "option-2-use-existing-vpc"]], "Organization": [[185, "organization"], [198, "organization"]], "Our Example: Llama-3.1-70B": [[221, "our-example-llama-3-1-70b"], [223, "our-example-llama-3-1-70b"]], "Out of memory errors": [[73, "out-of-memory-errors"], [83, "out-of-memory-errors"]], "Outline": [[73, "outline"], [75, "outline"], [76, "outline"], [78, "outline"], [90, "outline"], [94, "outline"]], "Outline of the notebook": [[74, "outline-of-the-notebook"], [84, "outline-of-the-notebook"]], "Outlook": [[100, "outlook"], [101, "outlook"]], "Outlook:  Ray Data in Production": [[240, "outlook-ray-data-in-production"], [266, "outlook-ray-data-in-production"]], "Overview": [[173, "overview"]], "Overview: Advanced Features Preview": [[229, "overview-advanced-features-preview"], [231, null]], "Overview: Why Medium-Sized Models?": [[221, "overview-why-medium-sized-models"], [223, null]], "Part 1. Creating and Submitting your first job": [[182, "part-1-creating-and-submitting-your-first-job"], [186, "part-1-creating-and-submitting-your-first-job"], [194, "part-1-creating-and-submitting-your-first-job"], [201, null], [205, "part-1-creating-and-submitting-your-first-job"], [208, null]], "Part 1: Starting your first Anyscale Service": [[183, "part-1-starting-your-first-anyscale-service"], [187, "part-1-starting-your-first-anyscale-service"], [195, "part-1-starting-your-first-anyscale-service"], [204, null], [206, "part-1-starting-your-first-anyscale-service"], [211, null]], "Part 2. Automation and Scheduling": [[182, "part-2-automation-and-scheduling"], [186, "part-2-automation-and-scheduling"], [194, "part-2-automation-and-scheduling"], [202, null], [205, "part-2-automation-and-scheduling"], [209, null]], "Practical Selection Process": [[229, "practical-selection-process"], [235, "practical-selection-process"]], "Prefill Phase": [[212, "prefill-phase"], [213, "prefill-phase"], [215, "prefill-phase"]], "Preprocessing with a Tokenizer": [[74, "preprocessing-with-a-tokenizer"], [89, null]], "Prerequisites": [[11, "prerequisites"], [72, "prerequisites"], [111, "prerequisites"], [112, "prerequisites"], [118, "prerequisites"], [120, null], [126, "prerequisites"], [127, "prerequisites"], [136, "prerequisites"], [138, null], [149, "prerequisites"], [151, null], [162, "prerequisites"], [169, "prerequisites"], [170, "prerequisites"], [173, "prerequisites"], [221, "prerequisites"], [225, "prerequisites"]], "Prerequisites and Assumptions": [[163, "prerequisites-and-assumptions"], [176, "prerequisites-and-assumptions"]], "Projects": [[185, "projects"], [198, "projects"]], "Providers": [[126, "providers"], [127, "providers"]], "Publishing": [[0, "publishing"]], "Purpose": [[100, "purpose"], [102, "purpose"]], "Ray 101": [[343, "ray-101"]], "Ray Data Logs": [[169, "ray-data-logs"], [171, "ray-data-logs"]], "Ray Data Metrics": [[169, "ray-data-metrics"], [171, "ray-data-metrics"]], "Ray Enablement Content": [[343, null]], "Ray Enablement Content: Jupyter Book Publishing": [[0, null]], "Ray Observability": [[164, "ray-observability"], [166, null]], "Ray Serve": [[7, "ray-serve"], [48, null]], "Ray Serve Alerts": [[169, "ray-serve-alerts"], [172, "ray-serve-alerts"]], "Ray Serve LLM + Anyscale Architecture": [[212, "ray-serve-llm-anyscale-architecture"], [213, "ray-serve-llm-anyscale-architecture"], [218, null]], "Ray Serve Logs": [[169, "ray-serve-logs"], [172, "ray-serve-logs"]], "Ray Serve Metrics": [[169, "ray-serve-metrics"], [172, "ray-serve-metrics"]], "Ray Serve Tracing (Anyscale Only)": [[169, "ray-serve-tracing-anyscale-only"], [172, "ray-serve-tracing-anyscale-only"]], "Ray Serve vs Ray Data": [[7, "ray-serve-vs-ray-data"], [48, "ray-serve-vs-ray-data"]], "Ray Train 201": [[343, "ray-train-201"]], "Ray Workloads Data Dashboard": [[169, "ray-workloads-data-dashboard"], [171, "ray-workloads-data-dashboard"]], "Ray and Anyscale Observability Introduction": [[164, null], [165, null]], "Ray and Anyscale Observability in Detail": [[169, null], [170, null]], "Recap": [[237, "recap"], [244, "recap"], [245, "recap"]], "Register New User": [[173, "register-new-user"]], "Related Examples": [[221, "related-examples"], [223, "related-examples"]], "Related Examples & Templates": [[221, "related-examples-templates"], [228, "related-examples-templates"]], "Replicas": [[10, "replicas"], [69, "replicas"]], "Reporting metrics": [[238, "reporting-metrics"], [247, "reporting-metrics"]], "Request Flow": [[173, "request-flow"]], "Requirements": [[126, "requirements"], [127, "requirements"]], "Resources": [[212, "resources"], [213, "resources"], [220, "resources"], [221, "resources"], [228, "resources"], [229, "resources"], [236, "resources"]], "Reverse diffusion: sampling new images": [[277, "reverse-diffusion-sampling-new-images"], [326, "reverse-diffusion-sampling-new-images"]], "Run a simple Data Pipeline": [[169, "run-a-simple-data-pipeline"], [171, "run-a-simple-data-pipeline"]], "Run inference on the entire dataset": [[73, "run-inference-on-the-entire-dataset"], [83, null]], "Running Inference on Anyscale": [[221, "running-inference-on-anyscale"], [226, "running-inference-on-anyscale"]], "Running this notebook": [[237, "running-this-notebook"], [242, "running-this-notebook"]], "Sample Requests": [[173, "sample-requests"]], "Saving a checkpoint in a local directory": [[238, "saving-a-checkpoint-in-a-local-directory"], [247, "saving-a-checkpoint-in-a-local-directory"]], "Scaling deployment": [[75, "scaling-deployment"], [92, "scaling-deployment"]], "Scheduling the training loop on a single GPU": [[238, "scheduling-the-training-loop-on-a-single-gpu"], [247, "scheduling-the-training-loop-on-a-single-gpu"]], "Sending Requests": [[221, "sending-requests"], [225, "sending-requests"]], "Setting Up Local Ray Observability": [[163, "setting-up-local-ray-observability"], [176, null]], "Setting Up a Local Ray server using Jupyter Notebook": [[11, "setting-up-a-local-ray-server-using-jupyter-notebook"], [72, "setting-up-a-local-ray-server-using-jupyter-notebook"]], "Setting up Ray Serve LLM": [[221, "setting-up-ray-serve-llm"], [224, null]], "Setting up the Configuration File": [[221, "setting-up-the-configuration-file"], [226, "setting-up-the-configuration-file"]], "Setup and Installation": [[173, "setup-and-installation"]], "Shuffle all rows globally": [[8, "shuffle-all-rows-globally"], [54, "shuffle-all-rows-globally"]], "Shuffle rows globally": [[240, "shuffle-rows-globally"], [265, "shuffle-rows-globally"]], "Shuffling block order": [[8, "shuffling-block-order"], [54, "shuffling-block-order"], [240, "shuffling-block-order"], [265, "shuffling-block-order"]], "Shuffling data": [[8, "shuffling-data"], [54, "shuffling-data"], [240, "shuffling-data"], [265, "shuffling-data"]], "Shutdown Ray": [[74, "shutdown-ray"], [89, "shutdown-ray"]], "Shutdown Ray cluster": [[73, "shutdown-ray-cluster"], [83, "shutdown-ray-cluster"]], "Shutdown the Ray Serve instances and Ray Cluster": [[75, "shutdown-the-ray-serve-instances-and-ray-cluster"], [93, "shutdown-the-ray-serve-instances-and-ray-cluster"]], "Shutting Down": [[221, "shutting-down"], [225, "shutting-down"]], "Shutting Down the Service": [[221, "shutting-down-the-service"], [226, "shutting-down-the-service"]], "Simulate Client: Send test requests": [[75, "simulate-client-send-test-requests"], [93, null]], "Sources": [[187, null], [203, null], [206, null], [210, null]], "Start by launching the Ray head node in the terminal:": [[163, "start-by-launching-the-ray-head-node-in-the-terminal"], [176, "start-by-launching-the-ray-head-node-in-the-terminal"]], "Stateful transformations with actors": [[240, "stateful-transformations-with-actors"], [264, "stateful-transformations-with-actors"]], "Step 1: Configuration": [[212, "step-1-configuration"], [213, "step-1-configuration"], [219, "step-1-configuration"]], "Step 1: Install Required Packages": [[163, "step-1-install-required-packages"], [176, "step-1-install-required-packages"]], "Step 2: Deployment": [[212, "step-2-deployment"], [213, "step-2-deployment"], [219, "step-2-deployment"]], "Step 2: Launch Prometheus": [[163, "step-2-launch-prometheus"], [176, "step-2-launch-prometheus"]], "Step 3: Launch Ray Cluster": [[163, "step-3-launch-ray-cluster"], [176, "step-3-launch-ray-cluster"]], "Step 3: Querying": [[212, "step-3-querying"], [213, "step-3-querying"], [219, "step-3-querying"]], "Step 4: Install and Launch Grafana": [[163, "step-4-install-and-launch-grafana"], [176, "step-4-install-and-launch-grafana"]], "Step 4: Shutdown": [[213, "step-4-shutdown"], [219, "step-4-shutdown"]], "Steps to run:": [[237, "steps-to-run"], [244, "steps-to-run"], [245, "steps-to-run"]], "Streaming Applications": [[7, "streaming-applications"], [46, "streaming-applications"]], "Structure of a data lake": [[7, "structure-of-a-data-lake"], [43, "structure-of-a-data-lake"]], "Summary": [[73, "summary"], [74, "summary"], [75, "summary"], [83, "summary"], [89, "summary"], [93, "summary"]], "Summary & Outlook": [[221, "summary-outlook"], [228, null]], "Supported Infrastructure Types": [[100, "supported-infrastructure-types"], [103, "supported-infrastructure-types"]], "Testing the Container Image and Compute Config with an Anyscale Workflow": [[179, "testing-the-container-image-and-compute-config-with-an-anyscale-workflow"], [191, "testing-the-container-image-and-compute-config-with-an-anyscale-workflow"]], "The Compute Layer": [[7, "the-compute-layer"], [44, null]], "The LLM Text Generation Process": [[212, "the-llm-text-generation-process"], [213, "the-llm-text-generation-process"], [215, "the-llm-text-generation-process"]], "The Orchestration Layer": [[7, "the-orchestration-layer"], [45, null]], "The data layer": [[7, "the-data-layer"], [43, "the-data-layer"]], "The following instructions will walk you through running your first job.": [[186, "the-following-instructions-will-walk-you-through-running-your-first-job"], [200, "the-following-instructions-will-walk-you-through-running-your-first-job"], [205, "the-following-instructions-will-walk-you-through-running-your-first-job"], [207, "the-following-instructions-will-walk-you-through-running-your-first-job"]], "The following instructions will walk you through running your first job. This notebook covers the following:": [[182, "the-following-instructions-will-walk-you-through-running-your-first-job-this-notebook-covers-the-following"], [194, "the-following-instructions-will-walk-you-through-running-your-first-job-this-notebook-covers-the-following"]], "This guide will walk you through deploying, updating, and managing Anyscale Services": [[187, "this-guide-will-walk-you-through-deploying-updating-and-managing-anyscale-services"], [203, "this-guide-will-walk-you-through-deploying-updating-and-managing-anyscale-services"], [206, "this-guide-will-walk-you-through-deploying-updating-and-managing-anyscale-services"], [210, "this-guide-will-walk-you-through-deploying-updating-and-managing-anyscale-services"]], "This notebook covers the following:": [[183, "this-notebook-covers-the-following"], [195, "this-notebook-covers-the-following"]], "Tokenizer": [[76, "tokenizer"], [97, "tokenizer"]], "Trace Structure": [[173, "trace-structure"]], "Tracing Configuration": [[173, "tracing-configuration"]], "Training objective": [[277, "training-objective"], [279, "training-objective"], [326, "training-objective"], [337, "training-objective"]], "Two Phases of LLM Inference": [[212, "two-phases-of-llm-inference"], [213, "two-phases-of-llm-inference"], [215, "two-phases-of-llm-inference"]], "Usage": [[0, "usage"]], "Users and Roles": [[185, "users-and-roles"], [198, "users-and-roles"]], "Using LoRA Adapters": [[229, "using-lora-adapters"], [232, "using-lora-adapters"]], "Using Structured Output": [[229, "using-structured-output"], [233, "using-structured-output"]], "Using Tool Calling": [[229, "using-tool-calling"], [234, "using-tool-calling"]], "Using fractions of a GPU": [[241, "using-fractions-of-a-gpu"], [270, "using-fractions-of-a-gpu"]], "VSCode": [[178, "vscode"], [190, "vscode"]], "Web Application Observability (Ray Serve)": [[169, "web-application-observability-ray-serve"], [172, null]], "Welcome to Anyscale Administration": [[162, null]], "What We Accomplished": [[221, "what-we-accomplished"], [228, "what-we-accomplished"], [229, "what-we-accomplished"], [236, "what-we-accomplished"]], "What We\u2019ll Cover": [[229, "what-we-ll-cover"], [231, "what-we-ll-cover"]], "What You\u2019ll Learn": [[162, "what-you-ll-learn"]], "What does the model learn?": [[274, "what-does-the-model-learn"], [306, "what-does-the-model-learn"]], "What is LLM Serving?": [[212, "what-is-llm-serving"], [213, "what-is-llm-serving"], [215, null]], "What is Ray Data ?": [[7, "what-is-ray-data"], [47, "what-is-ray-data"]], "What is Ray Serve ?": [[7, "what-is-ray-serve"], [48, "what-is-ray-serve"]], "What is Ray Serve?": [[75, "what-is-ray-serve"], [90, "what-is-ray-serve"]], "What is an Anyscale Service?": [[221, "what-is-an-anyscale-service"], [226, "what-is-an-anyscale-service"]], "What problem are you solving? (Diffusion as image de-noising)": [[277, "what-problem-are-you-solving-diffusion-as-image-de-noising"], [326, "what-problem-are-you-solving-diffusion-as-image-de-noising"]], "What problem are you solving? (Forest cover classification with XGBoost)": [[275, "what-problem-are-you-solving-forest-cover-classification-with-xgboost"], [315, "what-problem-are-you-solving-forest-cover-classification-with-xgboost"]], "What problem are you solving? (Inverted Pendulum, Diffusion-Style)": [[278, "what-problem-are-you-solving-inverted-pendulum-diffusion-style"], [332, "what-problem-are-you-solving-inverted-pendulum-diffusion-style"]], "What problem are you solving? (NYC taxi demand forecasting with a Transformer)": [[276, "what-problem-are-you-solving-nyc-taxi-demand-forecasting-with-a-transformer"], [320, "what-problem-are-you-solving-nyc-taxi-demand-forecasting-with-a-transformer"]], "What problem are you solving? (image classification with Food-101-Lite)": [[274, "what-problem-are-you-solving-image-classification-with-food-101-lite"], [306, "what-problem-are-you-solving-image-classification-with-food-101-lite"]], "What problem are you solving? (matrix factorization for recommendations)": [[279, "what-problem-are-you-solving-matrix-factorization-for-recommendations"], [337, "what-problem-are-you-solving-matrix-factorization-for-recommendations"]], "What you learn and take away": [[274, "what-you-learn-and-take-away"], [275, "what-you-learn-and-take-away"], [276, "what-you-learn-and-take-away"], [277, "what-you-learn-and-take-away"], [278, "what-you-learn-and-take-away"], [279, "what-you-learn-and-take-away"], [306, "what-you-learn-and-take-away"], [315, "what-you-learn-and-take-away"], [320, "what-you-learn-and-take-away"], [326, "what-you-learn-and-take-away"], [332, "what-you-learn-and-take-away"], [337, "what-you-learn-and-take-away"]], "What you\u2019ll learn & take away": [[273, "what-youll-learn-take-away"], [273, "id1"], [273, "id3"], [280, "what-youll-learn-take-away"], [293, "what-youll-learn-take-away"], [299, "what-youll-learn-take-away"]], "What\u2019s Next": [[163, "what-s-next"], [176, "what-s-next"]], "What\u2019s XGBoost?": [[275, "what-s-xgboost"], [315, "what-s-xgboost"]], "What\u2019s a policy?": [[278, "what-s-a-policy"], [332, "what-s-a-policy"]], "What\u2019s a sequence-to-sequence Transformer?": [[276, "what-s-a-sequence-to-sequence-transformer"], [320, "what-s-a-sequence-to-sequence-transformer"]], "When to use Ray Core over Ray Data ?": [[7, "when-to-use-ray-core-over-ray-data"], [47, "when-to-use-ray-core-over-ray-data"]], "When to use Ray Serve?": [[241, "when-to-use-ray-serve"], [268, "when-to-use-ray-serve"]], "Where can you take this next?": [[274, "where-can-you-take-this-next"], [275, "where-can-you-take-this-next"], [276, "where-can-you-take-this-next"], [277, "where-can-you-take-this-next"], [278, "where-can-you-take-this-next"], [279, "where-can-you-take-this-next"], [314, "where-can-you-take-this-next"], [319, "where-can-you-take-this-next"], [325, "where-can-you-take-this-next"], [331, "where-can-you-take-this-next"], [336, "where-can-you-take-this-next"], [342, "where-can-you-take-this-next"]], "Why Choose Medium-Sized Models?": [[221, "why-choose-medium-sized-models"], [223, "why-choose-medium-sized-models"]], "Why Ray Data ?": [[7, "why-ray-data"], [47, "why-ray-data"]], "Why Ray Serve ?": [[7, "why-ray-serve"], [48, "why-ray-serve"]], "Why Ray?": [[11, "why-ray"], [72, "why-ray"]], "Why Structured Output Matters": [[229, "why-structured-output-matters"], [233, "why-structured-output-matters"]], "Why These Features Matter": [[229, "why-these-features-matter"], [231, "why-these-features-matter"]], "Why Tool Calling Matters": [[229, "why-tool-calling-matters"], [234, "why-tool-calling-matters"]], "Why Use LoRA Adapters?": [[229, "why-use-lora-adapters"], [232, "why-use-lora-adapters"]], "Why not Kubernetes ?": [[212, "why-not-kubernetes"], [213, "why-not-kubernetes"], [217, "why-not-kubernetes"]], "Why not use just FastAPI or Flask?": [[75, "why-not-use-just-fastapi-or-flask"], [90, "why-not-use-just-fastapi-or-flask"]], "Why this works": [[277, "why-this-works"], [326, "why-this-works"]], "Workloads": [[185, "workloads"], [198, "workloads"]], "Wrap up and next steps": [[274, "wrap-up-and-next-steps"], [275, "wrap-up-and-next-steps"], [276, "wrap-up-and-next-steps"], [277, "wrap-up-and-next-steps"], [278, "wrap-up-and-next-steps"], [279, "wrap-up-and-next-steps"], [314, "wrap-up-and-next-steps"], [319, "wrap-up-and-next-steps"], [325, "wrap-up-and-next-steps"], [331, "wrap-up-and-next-steps"], [336, "wrap-up-and-next-steps"], [342, "wrap-up-and-next-steps"]], "\u25b6\ufe0f 3. Activate the Environment": [[11, "activate-the-environment"], [72, "activate-the-environment"]], "\u2705 1. Install Conda": [[11, "install-conda"], [72, "install-conda"]], "\u2705 7. Verify Ray Installation with a Simple Example": [[11, "verify-ray-installation-with-a-simple-example"], [72, "verify-ray-installation-with-a-simple-example"]], "\u2705 Module 01 \u00b7 Introduction to Ray Train": [[273, "module-01-introduction-to-ray-train"], [305, "module-01-introduction-to-ray-train"]], "\u2705 Module 02 \u00b7 Integrating Ray Train with Ray Data": [[273, "module-02-integrating-ray-train-with-ray-data"], [305, "module-02-integrating-ray-train-with-ray-data"]], "\u2705 Module 03 \u00b7 Fault Tolerance in Ray Train": [[273, "module-03-fault-tolerance-in-ray-train"], [305, "module-03-fault-tolerance-in-ray-train"]], "\ud83c\udf89 Wrapping Up & Next Steps": [[273, "wrapping-up-next-steps"], [305, null]], "\ud83d\udccb Notebook Compute Requirements Legend": [[11, "notebook-compute-requirements-legend"], [72, "notebook-compute-requirements-legend"]], "\ud83d\udccc Overview of Structure": [[185, "overview-of-structure"], [198, null]], "\ud83d\udcda 01 \u00b7 Introduction to Ray Train": [[273, null], [280, null]], "\ud83d\udcda Next Tutorials in the Course": [[273, "next-tutorials-in-the-course"], [305, "next-tutorials-in-the-course"]], "\ud83d\udce6 4. Install UV and Dependencies": [[11, "install-uv-and-dependencies"], [72, "install-uv-and-dependencies"]], "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data": [[273, "integrating-ray-train-with-ray-data"], [293, null]], "\ud83d\udd0e Integrating Ray Train with Ray Data": [[273, "id2"], [293, "id1"]], "\ud83d\udd0e When to use Ray Train": [[273, "when-to-use-ray-train"], [280, "when-to-use-ray-train"]], "\ud83d\udda5\ufe0f 5. (Optional but Recommended) Add Your Conda Environment to Jupyter": [[11, "optional-but-recommended-add-your-conda-environment-to-jupyter"], [72, "optional-but-recommended-add-your-conda-environment-to-jupyter"]], "\ud83d\udda5\ufe0f How Distributed Data Parallel (DDP) Works": [[273, "how-distributed-data-parallel-ddp-works"], [280, "how-distributed-data-parallel-ddp-works"]], "\ud83d\ude80 6. Launch Jupyter Notebook": [[11, "launch-jupyter-notebook"], [72, "launch-jupyter-notebook"]], "\ud83d\ude80 Where to go next": [[273, "where-to-go-next"], [305, "where-to-go-next"]], "\ud83d\udee0\ufe0f 2. Create a New Conda Environment": [[11, "create-a-new-conda-environment"], [72, "create-a-new-conda-environment"]], "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train": [[273, "fault-tolerance-in-ray-train"], [299, null]], "\ud83e\udde0 Summary": [[185, "summary"], [199, null]], "\ud83e\udde9 Miniforge Installation (It depends on your OS. In this case, we use ARM Macs)": [[11, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"], [72, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"]], "\ud83e\uddf9 8. Shut Down and Clean Up": [[11, "shut-down-and-clean-up"], [72, "shut-down-and-clean-up"]]}, "docnames": ["README", "courses/00_Developer_Intro_to_Ray/00_Intro_Ray_Core_Basics", "courses/00_Developer_Intro_to_Ray/00a_Intro_Ray_Core_Advancement", "courses/00_Developer_Intro_to_Ray/01_Intro_Ray_AI_Libs_Overview", "courses/00_Developer_Intro_to_Ray/02a_Intro_Ray_Train_with_PyTorch", "courses/00_Developer_Intro_to_Ray/02b_Intro_Ray_Train_with_PyTorch_Lightning", "courses/00_Developer_Intro_to_Ray/03_Intro_Ray_Tune", "courses/00_Developer_Intro_to_Ray/04a_Intro_Ray_Data_Industry_Landscape", "courses/00_Developer_Intro_to_Ray/04b_Intro_Ray_Data_Structured", "courses/00_Developer_Intro_to_Ray/04c_Intro_Ray_Data_Unstructured", "courses/00_Developer_Intro_to_Ray/05_Intro_Ray_Serve_PyTorch", "courses/00_Developer_Intro_to_Ray/README", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_01", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_02", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_03", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_04", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_05", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_01", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_02", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_03", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_04", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_05", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_06", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_07", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_08", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_09", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_01", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_02", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_03", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_01", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_02", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_03", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_04", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_01", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_02", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_03", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_04", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_05", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_01", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_02", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_03", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_04", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_05", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_06", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_01", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_02", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_03", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_04", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_05", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_06", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_07", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_01", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_02", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_03", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_04", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_05", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_06", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_07", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_08", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_09", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_10", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_11", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_01", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_02", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_03", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_04", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_05", "courses/00_Developer_Intro_to_Ray/output/README_01", "courses/01_examples/01_Ray_Data_batch_inference", "courses/01_examples/02_Ray_Data_data_processing", "courses/01_examples/03_Ray_Serve_online_serving", "courses/01_examples/04_Ray_Train_distributed_training", "courses/01_examples/README", "courses/01_examples/output/01_Ray_Data_batch_inference_01", "courses/01_examples/output/01_Ray_Data_batch_inference_02", "courses/01_examples/output/01_Ray_Data_batch_inference_03", "courses/01_examples/output/01_Ray_Data_batch_inference_04", "courses/01_examples/output/01_Ray_Data_batch_inference_05", "courses/01_examples/output/01_Ray_Data_batch_inference_06", "courses/01_examples/output/02_Ray_Data_data_processing_01", "courses/01_examples/output/02_Ray_Data_data_processing_02", "courses/01_examples/output/02_Ray_Data_data_processing_03", "courses/01_examples/output/02_Ray_Data_data_processing_04", "courses/01_examples/output/02_Ray_Data_data_processing_05", "courses/01_examples/output/02_Ray_Data_data_processing_06", "courses/01_examples/output/03_Ray_Serve_online_serving_01", "courses/01_examples/output/03_Ray_Serve_online_serving_02", "courses/01_examples/output/03_Ray_Serve_online_serving_03", "courses/01_examples/output/03_Ray_Serve_online_serving_04", "courses/01_examples/output/04_Ray_Train_distributed_training_01", "courses/01_examples/output/04_Ray_Train_distributed_training_02", "courses/01_examples/output/04_Ray_Train_distributed_training_03", "courses/01_examples/output/04_Ray_Train_distributed_training_04", "courses/01_examples/output/04_Ray_Train_distributed_training_05", "courses/01_examples/output/04_Ray_Train_distributed_training_06", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/anyscale_administrator_overview", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/anyscale_vm_vs_k8s", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/deploy_to_ec2", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/deploy_to_GCE", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/deploy_to_a_new_EKS", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/deploy_to_an_existing_EKS", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/deploy_to_new_GKE", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12", "courses/02_Anyscale_Admin/README", "courses/03_Observability/01_general_intro_and_setup", "courses/03_Observability/02_Ray_Anyscale_Introduction/2_Ray_Anyscale_Observability_Overview", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/03_Ray_Anyscale_Observability_in_Details", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/tracing_example/README", "courses/03_Observability/output/01_general_intro_and_setup_01", "courses/03_Observability/output/01_general_intro_and_setup_02", "courses/03_Observability/output/01_general_intro_and_setup_03", "courses/GettingStarted/101_01_anyscale_intro_workspace", "courses/GettingStarted/101_02_anyscale_development_intro", "courses/GettingStarted/101_03_anyscale_compute_runtime_intro", "courses/GettingStarted/101_04_anyscale_storage_options", "courses/GettingStarted/101_05_anyscale_logging_metrics", "courses/GettingStarted/101_06_anyscale_intro_jobs", "courses/GettingStarted/101_07_anyscale_intro_services", "courses/GettingStarted/101_08_anyscale_collaboration", "courses/GettingStarted/101_09_anyscale_org_setup", "courses/GettingStarted/101_anyscale_intro_jobs", "courses/GettingStarted/101_anyscale_intro_services", "courses/GettingStarted/README", "courses/GettingStarted/output/101_01_anyscale_intro_workspace_01", "courses/GettingStarted/output/101_02_anyscale_development_intro_01", "courses/GettingStarted/output/101_03_anyscale_compute_runtime_intro_01", "courses/GettingStarted/output/101_04_anyscale_storage_options_01", "courses/GettingStarted/output/101_05_anyscale_logging_metrics_01", "courses/GettingStarted/output/101_06_anyscale_intro_jobs_01", "courses/GettingStarted/output/101_07_anyscale_intro_services_01", "courses/GettingStarted/output/101_08_anyscale_collaboration_01", "courses/GettingStarted/output/101_09_anyscale_org_setup_01", "courses/GettingStarted/output/101_09_anyscale_org_setup_02", "courses/GettingStarted/output/101_09_anyscale_org_setup_03", "courses/GettingStarted/output/101_anyscale_intro_jobs_01", "courses/GettingStarted/output/101_anyscale_intro_jobs_02", "courses/GettingStarted/output/101_anyscale_intro_jobs_03", "courses/GettingStarted/output/101_anyscale_intro_services_01", "courses/GettingStarted/output/101_anyscale_intro_services_02", "courses/anyscale_101/101_anyscale_intro_jobs", "courses/anyscale_101/101_anyscale_intro_services", "courses/anyscale_101/output/101_anyscale_intro_jobs_01", "courses/anyscale_101/output/101_anyscale_intro_jobs_02", "courses/anyscale_101/output/101_anyscale_intro_jobs_03", "courses/anyscale_101/output/101_anyscale_intro_services_01", "courses/anyscale_101/output/101_anyscale_intro_services_02", "courses/llm_serving/00_intro_serve_llm/README", "courses/llm_serving/00_intro_serve_llm/notebook", "courses/llm_serving/00_intro_serve_llm/output/notebook_01", "courses/llm_serving/00_intro_serve_llm/output/notebook_02", "courses/llm_serving/00_intro_serve_llm/output/notebook_03", "courses/llm_serving/00_intro_serve_llm/output/notebook_04", "courses/llm_serving/00_intro_serve_llm/output/notebook_05", "courses/llm_serving/00_intro_serve_llm/output/notebook_06", "courses/llm_serving/00_intro_serve_llm/output/notebook_07", "courses/llm_serving/01_deploy_medium_llm/notebook", "courses/llm_serving/01_deploy_medium_llm/output/notebook_01", "courses/llm_serving/01_deploy_medium_llm/output/notebook_02", "courses/llm_serving/01_deploy_medium_llm/output/notebook_03", "courses/llm_serving/01_deploy_medium_llm/output/notebook_04", "courses/llm_serving/01_deploy_medium_llm/output/notebook_05", "courses/llm_serving/01_deploy_medium_llm/output/notebook_06", "courses/llm_serving/01_deploy_medium_llm/output/notebook_07", "courses/llm_serving/02_advanced_llm_features/notebook", "courses/llm_serving/02_advanced_llm_features/output/notebook_01", "courses/llm_serving/02_advanced_llm_features/output/notebook_02", "courses/llm_serving/02_advanced_llm_features/output/notebook_03", "courses/llm_serving/02_advanced_llm_features/output/notebook_04", "courses/llm_serving/02_advanced_llm_features/output/notebook_05", "courses/llm_serving/02_advanced_llm_features/output/notebook_06", "courses/llm_serving/02_advanced_llm_features/output/notebook_07", "courses/ray-101/1_AI_Libs_Intro", "courses/ray-101/2_Intro_Train", "courses/ray-101/3_Intro_Tune", "courses/ray-101/4_Intro_Data", "courses/ray-101/5_Intro_Serve", "courses/ray-101/output/1_AI_Libs_Intro_01", "courses/ray-101/output/1_AI_Libs_Intro_02", "courses/ray-101/output/1_AI_Libs_Intro_03", "courses/ray-101/output/1_AI_Libs_Intro_04", "courses/ray-101/output/2_Intro_Train_01", "courses/ray-101/output/2_Intro_Train_02", "courses/ray-101/output/2_Intro_Train_03", "courses/ray-101/output/2_Intro_Train_04", "courses/ray-101/output/2_Intro_Train_05", "courses/ray-101/output/2_Intro_Train_06", "courses/ray-101/output/2_Intro_Train_07", "courses/ray-101/output/2_Intro_Train_08", "courses/ray-101/output/2_Intro_Train_09", "courses/ray-101/output/3_Intro_Tune_01", "courses/ray-101/output/3_Intro_Tune_02", "courses/ray-101/output/3_Intro_Tune_03", "courses/ray-101/output/3_Intro_Tune_04", "courses/ray-101/output/3_Intro_Tune_05", "courses/ray-101/output/3_Intro_Tune_06", "courses/ray-101/output/4_Intro_Data_01", "courses/ray-101/output/4_Intro_Data_02", "courses/ray-101/output/4_Intro_Data_03", "courses/ray-101/output/4_Intro_Data_04", "courses/ray-101/output/4_Intro_Data_05", "courses/ray-101/output/4_Intro_Data_06", "courses/ray-101/output/5_Intro_Serve_01", "courses/ray-101/output/5_Intro_Serve_02", "courses/ray-101/output/5_Intro_Serve_03", "courses/ray-101/output/5_Intro_Serve_04", "courses/ray-101/output/5_Intro_Serve_05", "courses/ray-101/output/5_Intro_Serve_06", "courses/ray-train-201/01_02_03_intro_to_ray_train", "courses/ray-train-201/04a_vision_pattern", "courses/ray-train-201/04b_tabular_workload_pattern", "courses/ray-train-201/04c_time_series_workload_pattern", "courses/ray-train-201/04d1_generative_cv_pattern", "courses/ray-train-201/04d2_policy_learning_pattern", "courses/ray-train-201/04e_rec_sys_workload_pattern", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_01", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_02", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_03", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_04", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_05", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_06", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_07", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_08", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_09", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_10", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_11", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_12", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_13", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_14", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_15", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_16", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_17", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_18", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_19", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_20", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_21", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_22", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_23", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_24", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_25", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_26", "courses/ray-train-201/output/04a_vision_pattern_01", "courses/ray-train-201/output/04a_vision_pattern_02", "courses/ray-train-201/output/04a_vision_pattern_03", "courses/ray-train-201/output/04a_vision_pattern_04", "courses/ray-train-201/output/04a_vision_pattern_05", "courses/ray-train-201/output/04a_vision_pattern_06", "courses/ray-train-201/output/04a_vision_pattern_07", "courses/ray-train-201/output/04a_vision_pattern_08", "courses/ray-train-201/output/04a_vision_pattern_09", "courses/ray-train-201/output/04b_tabular_workload_pattern_01", "courses/ray-train-201/output/04b_tabular_workload_pattern_02", "courses/ray-train-201/output/04b_tabular_workload_pattern_03", "courses/ray-train-201/output/04b_tabular_workload_pattern_04", "courses/ray-train-201/output/04b_tabular_workload_pattern_05", "courses/ray-train-201/output/04c_time_series_workload_pattern_01", "courses/ray-train-201/output/04c_time_series_workload_pattern_02", "courses/ray-train-201/output/04c_time_series_workload_pattern_03", "courses/ray-train-201/output/04c_time_series_workload_pattern_04", "courses/ray-train-201/output/04c_time_series_workload_pattern_05", "courses/ray-train-201/output/04c_time_series_workload_pattern_06", "courses/ray-train-201/output/04d1_generative_cv_pattern_01", "courses/ray-train-201/output/04d1_generative_cv_pattern_02", "courses/ray-train-201/output/04d1_generative_cv_pattern_03", "courses/ray-train-201/output/04d1_generative_cv_pattern_04", "courses/ray-train-201/output/04d1_generative_cv_pattern_05", "courses/ray-train-201/output/04d1_generative_cv_pattern_06", "courses/ray-train-201/output/04d2_policy_learning_pattern_01", "courses/ray-train-201/output/04d2_policy_learning_pattern_02", "courses/ray-train-201/output/04d2_policy_learning_pattern_03", "courses/ray-train-201/output/04d2_policy_learning_pattern_04", "courses/ray-train-201/output/04d2_policy_learning_pattern_05", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_01", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_02", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_03", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_04", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_05", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_06", "index"], "envversion": {"sphinx": 62, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["README.md", "courses/00_Developer_Intro_to_Ray/00_Intro_Ray_Core_Basics.ipynb", "courses/00_Developer_Intro_to_Ray/00a_Intro_Ray_Core_Advancement.ipynb", "courses/00_Developer_Intro_to_Ray/01_Intro_Ray_AI_Libs_Overview.ipynb", "courses/00_Developer_Intro_to_Ray/02a_Intro_Ray_Train_with_PyTorch.ipynb", "courses/00_Developer_Intro_to_Ray/02b_Intro_Ray_Train_with_PyTorch_Lightning.ipynb", "courses/00_Developer_Intro_to_Ray/03_Intro_Ray_Tune.ipynb", "courses/00_Developer_Intro_to_Ray/04a_Intro_Ray_Data_Industry_Landscape.ipynb", "courses/00_Developer_Intro_to_Ray/04b_Intro_Ray_Data_Structured.ipynb", "courses/00_Developer_Intro_to_Ray/04c_Intro_Ray_Data_Unstructured.ipynb", "courses/00_Developer_Intro_to_Ray/05_Intro_Ray_Serve_PyTorch.ipynb", "courses/00_Developer_Intro_to_Ray/README.ipynb", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_06.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_07.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_08.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_09.ipynb", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_06.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_06.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_07.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_06.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_07.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_08.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_09.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_10.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_11.ipynb", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/README_01.ipynb", "courses/01_examples/01_Ray_Data_batch_inference.ipynb", "courses/01_examples/02_Ray_Data_data_processing.ipynb", "courses/01_examples/03_Ray_Serve_online_serving.ipynb", "courses/01_examples/04_Ray_Train_distributed_training.ipynb", "courses/01_examples/README.md", "courses/01_examples/output/01_Ray_Data_batch_inference_01.ipynb", "courses/01_examples/output/01_Ray_Data_batch_inference_02.ipynb", "courses/01_examples/output/01_Ray_Data_batch_inference_03.ipynb", "courses/01_examples/output/01_Ray_Data_batch_inference_04.ipynb", "courses/01_examples/output/01_Ray_Data_batch_inference_05.ipynb", "courses/01_examples/output/01_Ray_Data_batch_inference_06.ipynb", "courses/01_examples/output/02_Ray_Data_data_processing_01.ipynb", "courses/01_examples/output/02_Ray_Data_data_processing_02.ipynb", "courses/01_examples/output/02_Ray_Data_data_processing_03.ipynb", "courses/01_examples/output/02_Ray_Data_data_processing_04.ipynb", "courses/01_examples/output/02_Ray_Data_data_processing_05.ipynb", "courses/01_examples/output/02_Ray_Data_data_processing_06.ipynb", "courses/01_examples/output/03_Ray_Serve_online_serving_01.ipynb", "courses/01_examples/output/03_Ray_Serve_online_serving_02.ipynb", "courses/01_examples/output/03_Ray_Serve_online_serving_03.ipynb", "courses/01_examples/output/03_Ray_Serve_online_serving_04.ipynb", "courses/01_examples/output/04_Ray_Train_distributed_training_01.ipynb", "courses/01_examples/output/04_Ray_Train_distributed_training_02.ipynb", "courses/01_examples/output/04_Ray_Train_distributed_training_03.ipynb", "courses/01_examples/output/04_Ray_Train_distributed_training_04.ipynb", "courses/01_examples/output/04_Ray_Train_distributed_training_05.ipynb", "courses/01_examples/output/04_Ray_Train_distributed_training_06.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/anyscale_administrator_overview.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06.ipynb", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/anyscale_vm_vs_k8s.ipynb", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01.ipynb", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02.ipynb", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/deploy_to_ec2.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/deploy_to_GCE.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/deploy_to_a_new_EKS.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/deploy_to_an_existing_EKS.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/deploy_to_new_GKE.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12.ipynb", "courses/02_Anyscale_Admin/README.md", "courses/03_Observability/01_general_intro_and_setup.ipynb", "courses/03_Observability/02_Ray_Anyscale_Introduction/2_Ray_Anyscale_Observability_Overview.ipynb", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01.ipynb", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02.ipynb", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03.ipynb", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04.ipynb", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/03_Ray_Anyscale_Observability_in_Details.ipynb", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01.ipynb", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02.ipynb", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03.ipynb", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/tracing_example/README.md", "courses/03_Observability/output/01_general_intro_and_setup_01.ipynb", "courses/03_Observability/output/01_general_intro_and_setup_02.ipynb", "courses/03_Observability/output/01_general_intro_and_setup_03.ipynb", "courses/GettingStarted/101_01_anyscale_intro_workspace.ipynb", "courses/GettingStarted/101_02_anyscale_development_intro.ipynb", "courses/GettingStarted/101_03_anyscale_compute_runtime_intro.ipynb", "courses/GettingStarted/101_04_anyscale_storage_options.ipynb", "courses/GettingStarted/101_05_anyscale_logging_metrics.ipynb", "courses/GettingStarted/101_06_anyscale_intro_jobs.ipynb", "courses/GettingStarted/101_07_anyscale_intro_services.ipynb", "courses/GettingStarted/101_08_anyscale_collaboration.ipynb", "courses/GettingStarted/101_09_anyscale_org_setup.ipynb", "courses/GettingStarted/101_anyscale_intro_jobs.ipynb", "courses/GettingStarted/101_anyscale_intro_services.ipynb", "courses/GettingStarted/README.md", "courses/GettingStarted/output/101_01_anyscale_intro_workspace_01.ipynb", "courses/GettingStarted/output/101_02_anyscale_development_intro_01.ipynb", "courses/GettingStarted/output/101_03_anyscale_compute_runtime_intro_01.ipynb", "courses/GettingStarted/output/101_04_anyscale_storage_options_01.ipynb", "courses/GettingStarted/output/101_05_anyscale_logging_metrics_01.ipynb", "courses/GettingStarted/output/101_06_anyscale_intro_jobs_01.ipynb", "courses/GettingStarted/output/101_07_anyscale_intro_services_01.ipynb", "courses/GettingStarted/output/101_08_anyscale_collaboration_01.ipynb", "courses/GettingStarted/output/101_09_anyscale_org_setup_01.ipynb", "courses/GettingStarted/output/101_09_anyscale_org_setup_02.ipynb", "courses/GettingStarted/output/101_09_anyscale_org_setup_03.ipynb", "courses/GettingStarted/output/101_anyscale_intro_jobs_01.ipynb", "courses/GettingStarted/output/101_anyscale_intro_jobs_02.ipynb", "courses/GettingStarted/output/101_anyscale_intro_jobs_03.ipynb", "courses/GettingStarted/output/101_anyscale_intro_services_01.ipynb", "courses/GettingStarted/output/101_anyscale_intro_services_02.ipynb", "courses/anyscale_101/101_anyscale_intro_jobs.ipynb", "courses/anyscale_101/101_anyscale_intro_services.ipynb", "courses/anyscale_101/output/101_anyscale_intro_jobs_01.ipynb", "courses/anyscale_101/output/101_anyscale_intro_jobs_02.ipynb", "courses/anyscale_101/output/101_anyscale_intro_jobs_03.ipynb", "courses/anyscale_101/output/101_anyscale_intro_services_01.ipynb", "courses/anyscale_101/output/101_anyscale_intro_services_02.ipynb", "courses/llm_serving/00_intro_serve_llm/README.md", "courses/llm_serving/00_intro_serve_llm/notebook.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_01.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_02.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_03.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_04.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_05.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_06.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_07.ipynb", "courses/llm_serving/01_deploy_medium_llm/notebook.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_01.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_02.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_03.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_04.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_05.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_06.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_07.ipynb", "courses/llm_serving/02_advanced_llm_features/notebook.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_01.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_02.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_03.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_04.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_05.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_06.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_07.ipynb", "courses/ray-101/1_AI_Libs_Intro.ipynb", "courses/ray-101/2_Intro_Train.ipynb", "courses/ray-101/3_Intro_Tune.ipynb", "courses/ray-101/4_Intro_Data.ipynb", "courses/ray-101/5_Intro_Serve.ipynb", "courses/ray-101/output/1_AI_Libs_Intro_01.ipynb", "courses/ray-101/output/1_AI_Libs_Intro_02.ipynb", "courses/ray-101/output/1_AI_Libs_Intro_03.ipynb", "courses/ray-101/output/1_AI_Libs_Intro_04.ipynb", "courses/ray-101/output/2_Intro_Train_01.ipynb", "courses/ray-101/output/2_Intro_Train_02.ipynb", "courses/ray-101/output/2_Intro_Train_03.ipynb", "courses/ray-101/output/2_Intro_Train_04.ipynb", "courses/ray-101/output/2_Intro_Train_05.ipynb", "courses/ray-101/output/2_Intro_Train_06.ipynb", "courses/ray-101/output/2_Intro_Train_07.ipynb", "courses/ray-101/output/2_Intro_Train_08.ipynb", "courses/ray-101/output/2_Intro_Train_09.ipynb", "courses/ray-101/output/3_Intro_Tune_01.ipynb", "courses/ray-101/output/3_Intro_Tune_02.ipynb", "courses/ray-101/output/3_Intro_Tune_03.ipynb", "courses/ray-101/output/3_Intro_Tune_04.ipynb", "courses/ray-101/output/3_Intro_Tune_05.ipynb", "courses/ray-101/output/3_Intro_Tune_06.ipynb", "courses/ray-101/output/4_Intro_Data_01.ipynb", "courses/ray-101/output/4_Intro_Data_02.ipynb", "courses/ray-101/output/4_Intro_Data_03.ipynb", "courses/ray-101/output/4_Intro_Data_04.ipynb", "courses/ray-101/output/4_Intro_Data_05.ipynb", "courses/ray-101/output/4_Intro_Data_06.ipynb", "courses/ray-101/output/5_Intro_Serve_01.ipynb", "courses/ray-101/output/5_Intro_Serve_02.ipynb", "courses/ray-101/output/5_Intro_Serve_03.ipynb", "courses/ray-101/output/5_Intro_Serve_04.ipynb", "courses/ray-101/output/5_Intro_Serve_05.ipynb", "courses/ray-101/output/5_Intro_Serve_06.ipynb", "courses/ray-train-201/01_02_03_intro_to_ray_train.ipynb", "courses/ray-train-201/04a_vision_pattern.ipynb", "courses/ray-train-201/04b_tabular_workload_pattern.ipynb", "courses/ray-train-201/04c_time_series_workload_pattern.ipynb", "courses/ray-train-201/04d1_generative_cv_pattern.ipynb", "courses/ray-train-201/04d2_policy_learning_pattern.ipynb", "courses/ray-train-201/04e_rec_sys_workload_pattern.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_01.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_02.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_03.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_04.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_05.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_06.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_07.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_08.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_09.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_10.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_11.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_12.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_13.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_14.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_15.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_16.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_17.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_18.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_19.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_20.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_21.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_22.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_23.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_24.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_25.ipynb", "courses/ray-train-201/output/01_02_03_intro_to_ray_train_26.ipynb", "courses/ray-train-201/output/04a_vision_pattern_01.ipynb", "courses/ray-train-201/output/04a_vision_pattern_02.ipynb", "courses/ray-train-201/output/04a_vision_pattern_03.ipynb", "courses/ray-train-201/output/04a_vision_pattern_04.ipynb", "courses/ray-train-201/output/04a_vision_pattern_05.ipynb", "courses/ray-train-201/output/04a_vision_pattern_06.ipynb", "courses/ray-train-201/output/04a_vision_pattern_07.ipynb", "courses/ray-train-201/output/04a_vision_pattern_08.ipynb", "courses/ray-train-201/output/04a_vision_pattern_09.ipynb", "courses/ray-train-201/output/04b_tabular_workload_pattern_01.ipynb", "courses/ray-train-201/output/04b_tabular_workload_pattern_02.ipynb", "courses/ray-train-201/output/04b_tabular_workload_pattern_03.ipynb", "courses/ray-train-201/output/04b_tabular_workload_pattern_04.ipynb", "courses/ray-train-201/output/04b_tabular_workload_pattern_05.ipynb", "courses/ray-train-201/output/04c_time_series_workload_pattern_01.ipynb", "courses/ray-train-201/output/04c_time_series_workload_pattern_02.ipynb", "courses/ray-train-201/output/04c_time_series_workload_pattern_03.ipynb", "courses/ray-train-201/output/04c_time_series_workload_pattern_04.ipynb", "courses/ray-train-201/output/04c_time_series_workload_pattern_05.ipynb", "courses/ray-train-201/output/04c_time_series_workload_pattern_06.ipynb", "courses/ray-train-201/output/04d1_generative_cv_pattern_01.ipynb", "courses/ray-train-201/output/04d1_generative_cv_pattern_02.ipynb", "courses/ray-train-201/output/04d1_generative_cv_pattern_03.ipynb", "courses/ray-train-201/output/04d1_generative_cv_pattern_04.ipynb", "courses/ray-train-201/output/04d1_generative_cv_pattern_05.ipynb", "courses/ray-train-201/output/04d1_generative_cv_pattern_06.ipynb", "courses/ray-train-201/output/04d2_policy_learning_pattern_01.ipynb", "courses/ray-train-201/output/04d2_policy_learning_pattern_02.ipynb", "courses/ray-train-201/output/04d2_policy_learning_pattern_03.ipynb", "courses/ray-train-201/output/04d2_policy_learning_pattern_04.ipynb", "courses/ray-train-201/output/04d2_policy_learning_pattern_05.ipynb", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_01.ipynb", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_02.ipynb", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_03.ipynb", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_04.ipynb", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_05.ipynb", "courses/ray-train-201/output/04e_rec_sys_workload_pattern_06.ipynb", "index.md"], "indexentries": {}, "objects": {}, "objnames": {}, "objtypes": {}, "terms": {"": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 16, 18, 19, 20, 22, 24, 25, 26, 28, 30, 31, 32, 34, 35, 36, 39, 40, 41, 43, 47, 48, 51, 52, 53, 54, 57, 59, 60, 62, 64, 68, 69, 70, 72, 73, 74, 76, 78, 80, 82, 83, 85, 88, 89, 94, 96, 97, 98, 100, 105, 111, 113, 114, 117, 118, 122, 123, 126, 128, 129, 130, 139, 141, 142, 149, 158, 164, 167, 168, 169, 171, 172, 178, 180, 181, 183, 184, 188, 190, 192, 193, 195, 196, 212, 213, 215, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 252, 256, 257, 258, 260, 263, 265, 269, 270, 272, 273, 274, 277, 279, 280, 281, 282, 283, 287, 288, 289, 292, 293, 295, 305, 306, 307, 308, 309, 310, 312, 314, 316, 317, 318, 319, 321, 323, 325, 326, 327, 329, 331, 335, 336, 337, 338, 339, 340, 341, 342], "0": [2, 3, 4, 5, 6, 9, 11, 20, 22, 24, 25, 28, 31, 32, 35, 36, 39, 40, 41, 49, 53, 60, 61, 62, 72, 73, 74, 75, 76, 80, 82, 85, 86, 89, 93, 97, 99, 105, 111, 112, 113, 118, 120, 123, 126, 127, 128, 129, 130, 136, 138, 139, 141, 142, 149, 151, 158, 163, 164, 168, 173, 176, 177, 179, 181, 189, 191, 193, 212, 213, 219, 221, 225, 226, 229, 232, 233, 234, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260, 261, 263, 264, 265, 267, 269, 270, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 287, 288, 290, 292, 296, 297, 300, 301, 305, 306, 307, 308, 310, 314, 315, 316, 317, 321, 322, 323, 325, 326, 327, 328, 329, 331, 332, 333, 335, 336, 338, 340, 342], "00": [73, 76, 82, 99, 173, 180, 192, 237, 238, 239, 240, 244, 245, 252, 258, 260, 265, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338, 343], "000": [6, 39, 239, 256, 273, 275, 278, 279, 281, 316, 333, 338], "0000000000000": [111, 113], "00000000000000000": [111, 113], "0001": [5, 9, 36, 62], "0003573892": [239, 258], "0003590581": [239, 258], "0003788471": [239, 258], "0003824231": [239, 258], "0004189012": [239, 258], "00046369e": [73, 83], "00087994e": [73, 83], "00129196e": [73, 83], "00217544e": [73, 83], "00403815e": [73, 83], "0059": [237, 244, 245], "00596860e": [73, 83], "00641076e": [73, 83], "006742": [238, 253], "00719017e": [73, 83], "00724374e": [73, 83], "00728178e": [73, 83], "00749106": [73, 82], "00753223": [73, 82], "007877049646500664": [239, 260], "00787705": [239, 260], "00844238": [73, 82], "00926834e": [73, 83], "0092816": [73, 82], "00958297e": [73, 83], "00974117e": [73, 83], "00982723e": [73, 83], "00994138e": [73, 83], "00_developer_intro_to_rai": 77, "00a": 343, "00z": 173, "01": [3, 5, 28, 36, 73, 83, 238, 252, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338], "01000227e": [73, 83], "01103884e": [73, 83], "01104600e": [73, 83], "01141734e": [73, 83], "01148352e": [73, 83], "01190887": [73, 82], "01222771": [73, 82], "01231135": [73, 82], "01273207e": [73, 83], "01347007e": [73, 83], "01351717e": [73, 83], "01387227e": [73, 83], "01402104e": [73, 83], "01455652": [73, 82], "01504247": [73, 82], "01505721e": [73, 83], "01544438e": [73, 83], "01616676e": [73, 83], "01646197e": [73, 83], "01848297e": [73, 83], "01875377e": [73, 83], "01946776e": [73, 83], "01951000e": [73, 83], "01958193e": [73, 83], "02": [3, 28, 73, 83, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338], "02095584": [73, 82], "02202111": [73, 82], "02316421e": [73, 83], "02338964e": [73, 83], "02352677e": [73, 83], "0242": [237, 244, 245], "02448604e": [73, 83], "02480531e": [73, 83], "02481507e": [73, 83], "02496293": [73, 82], "02508835e": [73, 83], "02556132": [73, 82], "02750473e": [73, 83], "02791084": [73, 82], "02842702e": [73, 83], "02_service_hello_world": [183, 195], "02a": 343, "02b": 343, "02d": [279, 338], "03": [3, 28, 73, 83, 237, 239, 244, 245, 258, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338], "03000000000000000": [111, 113], "03162946e": [73, 83], "03241184e": [73, 83], "03302041": [73, 82], "03319024e": [73, 83], "03347346e": [73, 83], "03351809e": [73, 83], "03357503": [73, 82], "03620186e": [73, 83], "03722222e": [73, 83], "03901269e": [73, 83], "03915609e": [73, 83], "03924675": [73, 82], "03974594e": [73, 83], "03d": [277, 278, 329, 335], "04": [73, 83, 274, 275, 276, 279, 307, 316, 321, 327, 334, 338, 343], "04023737e": [73, 83], "04127836e": [73, 83], "04148921": [73, 82], "04188204e": [73, 83], "04267104e": [73, 83], "04279362e": [73, 83], "04313433e": [73, 83], "04401015e": [73, 83], "04419766e": [73, 83], "04514116e": [73, 83], "04760937e": [73, 83], "04781413e": [73, 83], "04831458": [73, 82], "04871886e": [73, 83], "04a": 343, "04b": 343, "04c": 343, "04d1": 343, "04d2": 343, "04e": 343, "05": [5, 8, 35, 51, 54, 76, 99, 169, 171, 237, 238, 244, 245, 247, 252, 253, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 335, 338, 343], "050227": [73, 82], "05029851e": [73, 83], "05031021e": [73, 83], "05048873e": [73, 83], "05110919e": [73, 83], "05117615e": [73, 83], "05286286": [73, 82], "05403318e": [73, 83], "05412337e": [73, 83], "0564279": [239, 260], "05699580e": [73, 83], "0582891": [239, 260], "05838008e": [73, 83], "05897461e": [73, 83], "05951829e": [73, 83], "05955295e": [73, 83], "05962829": [73, 82], "05964907e": [73, 83], "05_069833_2422": [238, 252], "06": [11, 72, 180, 192, 238, 252, 274, 276, 277, 278, 279, 308, 321, 327, 335, 338, 343], "06012297e": [73, 83], "06039613e": [73, 83], "06096685": [73, 82], "06099542": [73, 82], "06128380e": [73, 83], "06155156e": [73, 83], "06164196e": [73, 83], "06194988": [73, 82], "06202352e": [73, 83], "06234232e": [73, 83], "06241195e": [73, 83], "06251295": [73, 82], "06282867e": [73, 83], "06317782e": [73, 83], "06359579e": [73, 83], "06367093324661255": [238, 252, 253], "063671": [238, 253], "06383444e": [73, 83], "06465332e": [73, 83], "06659506e": [73, 83], "06678507e": [73, 83], "06857569e": [73, 83], "06872221": [73, 82], "06887527e": [73, 83], "07": [76, 99, 229, 234, 274, 275, 276, 277, 278, 279, 308, 316, 321, 327, 335, 339, 343], "07005756e": [73, 83], "07039157e": [73, 83], "07176238": [73, 82], "07316985e": [73, 83], "07334603e": [73, 83], "07348490e": [73, 83], "07420641e": [73, 83], "07510021": [73, 82], "07512747e": [73, 83], "07565679e": [73, 83], "07582638e": [73, 83], "07590961e": [73, 83], "07614997e": [73, 83], "07735191e": [73, 83], "07769895e": [73, 83], "07796153e": [73, 83], "07813133e": [73, 83], "07829855": [73, 82], "08": [73, 83, 173, 221, 227, 274, 276, 277, 278, 279, 308, 321, 328, 336, 340, 343], "08080895": [73, 82], "08113792": [73, 82], "08117312e": [73, 83], "08142687": [73, 82], "08161136e": [73, 83], "08306534e": [73, 83], "08318681e": [73, 83], "08386130e": [73, 83], "08393911e": [73, 83], "08423311e": [73, 83], "08562492e": [73, 83], "08593434e": [73, 83], "08834168e": [73, 83], "08847059e": [73, 83], "08849846e": [73, 83], "08855490e": [73, 83], "08866049e": [73, 83], "08888834e": [73, 83], "08900222e": [73, 83], "08926150e": [73, 83], "08967713e": [73, 83], "09": [238, 239, 247, 253, 258, 260, 274, 275, 276, 277, 278, 279, 308, 317, 322, 329, 336, 340, 343], "09058516e": [73, 83], "09158831e": [73, 83], "09305708e": [73, 83], "09318195e": [73, 83], "09376505e": [73, 83], "09640113e": [73, 83], "09668531e": [73, 83], "09694359e": [73, 83], "09729558e": [73, 83], "09788750e": [73, 83], "09841380e": [73, 83], "09954223e": [73, 83], "09_200164_18044": [76, 99], "0a000000000000000": [111, 113], "0bd7bde3f2c914b3": [111, 113], "0f8bb12ddf9a451e9": [111, 113, 126, 128, 136, 139], "0m": [76, 99, 238, 239, 241, 252, 258, 260, 269], "0x72c6d85fc9d0": [241, 270], "0x72c6d85fcf90": [241, 270], "0x72c6d85fd3d0": [241, 270], "0x72c6d85fd550": [241, 270], "0x72c6d85fe590": [241, 270], "0x72c6d85ff250": [241, 270], "0x72c6d85ff3d0": [241, 270], "0x72c6d8608750": [241, 270], "0x72c6d8609050": [241, 270], "0x72c6d860b6d0": [241, 270], "0x72c6d860b7d0": [241, 270], "0x72c6d8610490": [241, 270], "0x72c6d8610a50": [241, 270], "0x72c6d8611310": [241, 270], "0x72c6d8611ad0": [241, 270], "0x72c6d8611b90": [241, 270], "0x72c6d8612050": [241, 270], "0x72c6d8613690": [241, 270], "0x72c6d8620a90": [241, 270], "0x72c6d8620e10": [241, 270], "0x72c6d86218d0": [241, 270], "0x72c6d8621b90": [241, 270], "0x72c6d8622ad0": [241, 270], "0x72c6d8623590": [241, 270], "0x72c6d86281d0": [241, 270], "0x72c6d8628710": [241, 270], "0x72c6d862a1d0": [241, 270], "0x72c6d862ac50": [241, 270], "0x72c6d862b790": [241, 270], "0x72c6d862b7d0": [241, 270], "0x72c6d862c690": [241, 270], "0x72c6d872db50": [241, 270], "0x72c6d8747390": [241, 270], "0x72c6d87500d0": [241, 270], "0x72c6d8752290": [241, 270], "0x72c6d8752e50": [241, 270], "0x72c6d8757dd0": [241, 270], "0x72c6d87793d0": [241, 270], "0x72c6d8779b90": [241, 270], "0x72c6d877a010": [241, 270], "0x72c6d877a6d0": [241, 270], "0x72c6d877b010": [241, 270], "0x72c6d877bc10": [241, 270], "0x72c6d8785e50": [241, 270], "0x72c6d8785fd0": [241, 270], "0x72c6d8786a50": [241, 270], "0x72c6d8787c90": [241, 270], "0x72c6d8794350": [241, 270], "0x72c6d8795110": [241, 270], "0x72c6d8796b50": [241, 270], "0x72c6d8797150": [241, 270], "0x72c6d8797ed0": [241, 270], "0x72c6d87a0690": [241, 270], "0x72c6d87a14d0": [241, 270], "0x72c6d87a1b50": [241, 270], "0x72c6d87a1f50": [241, 270], "0x72c6d87a2c10": [241, 270], "0x72c6d87a3d50": [241, 270], "0x72c6d87a3ed0": [241, 270], "0x72c6d87a8690": [241, 270], "0x72c6d87a96d0": [241, 270], "0x72c6d87a9cd0": [241, 270], "0x72c6d87aa0d0": [241, 270], "0x72c6d87aa4d0": [241, 270], "0x72c6d87aad50": [241, 270], "0x72c6d87ab690": [241, 270], "0x72c6d87ac4d0": [241, 270], "0x72c6d87adb90": [241, 270], "0x72c6d87ae4d0": [241, 270], "0x72c6d87ae710": [241, 270], "0x72c6d87c0110": [241, 270], "0x72c6d87c1110": [241, 270], "0x72c6d87c1250": [241, 270], "0x72c6d87c18d0": [241, 270], "0x72c6d87c2350": [241, 270], "0x72c6d87c3a50": [241, 270], "0x72c6d87d0690": [241, 270], "0x72c6d87d0d90": [241, 270], "0x72c6d87d1c50": [241, 270], "0x72c6d87d1cd0": [241, 270], "0x72c6d87d3190": [241, 270], "0x72c6d87d3fd0": [241, 270], "0x72c6d87d8250": [241, 270], "0x72c6d87d8e90": [241, 270], "0x72c6d87d96d0": [241, 270], "0x72c6d87da1d0": [241, 270], "0x72c6d87e4810": [241, 270], "0x72c6d87e4f90": [241, 270], "0x72c6d87e62d0": [241, 270], "0x72c6d87e64d0": [241, 270], "0x72c6e01cbd50": [241, 270], "0x72c6e034a510": [241, 270], "0x72c6e034b950": [241, 270], "0x72c6e0351e10": [241, 270], "0x72c6e0353410": [241, 270], "0x72c6e035ca50": [241, 270], "0x72c6e035d5d0": [241, 270], "0x72c6e03660d0": [241, 270], "0x72c72000ddd0": [241, 270], "0x72c73032a850": [241, 270], "0xxxxxxxx": [111, 113], "0xxxxxxxxx": [111, 113], "0xxxxxxxxxx": [111, 113], "1": [15, 19, 23, 38, 40, 41, 49, 67, 70, 73, 74, 80, 82, 83, 85, 87, 88, 89, 96, 98, 99, 112, 120, 125, 127, 133, 138, 145, 151, 156, 161, 164, 168, 169, 170, 172, 181, 193, 224, 225, 226, 228, 232, 233, 234, 242, 244, 245, 246, 252, 253, 255, 257, 258, 259, 260, 261, 263, 264, 265, 267, 269, 270, 273, 280, 281, 282, 285, 286, 290, 291, 292, 297, 298, 300, 301, 303, 306, 308, 310, 311, 314, 315, 317, 318, 319, 320, 322, 323, 325, 326, 328, 329, 331, 334, 335, 336, 337, 339, 340, 342, 343], "10": [2, 3, 6, 9, 10, 11, 18, 20, 24, 25, 28, 31, 39, 41, 57, 61, 70, 72, 73, 74, 76, 80, 82, 88, 89, 99, 100, 105, 118, 122, 126, 127, 128, 138, 139, 149, 151, 153, 164, 168, 169, 171, 173, 180, 181, 192, 193, 237, 238, 239, 240, 241, 244, 245, 247, 249, 252, 253, 256, 258, 260, 264, 269, 270, 282, 306, 315, 316, 319, 321, 325, 326, 330, 331, 333, 335, 338, 342, 343], "100": [2, 6, 9, 25, 41, 61, 62, 74, 76, 86, 97, 173, 181, 193, 229, 232, 239, 240, 241, 259, 264, 269, 270, 273, 278, 279, 286, 336, 338], "1000": [8, 9, 51, 59, 164, 168, 181, 193, 240, 263, 277, 278, 328, 333, 334], "10000": [276, 322], "100000000000": [5, 35], "100k": [337, 342], "100th": [74, 86], "101": [100, 105, 310, 314, 326, 331], "1010": [74, 89], "10129036e": [73, 83], "101_01_anyscale_intro_workspac": 188, "101_02_anyscale_development_intro": 188, "101_03_anycale_compute_runtime_intro": 188, "101_04_anyscale_storage_opt": 188, "101_05_anyscale_logging_and_metr": 188, "101_06_anyscale_intro_job": 188, "101_07_anyscale_intro_servic": 188, "101_08_anyscale_collaboration_intro": 188, "101_09_anyscale_org_setup": 188, "102": [100, 105], "1024": [2, 5, 8, 9, 18, 35, 51, 52, 61, 118, 122, 276, 322], "10279503e": [73, 83], "10307": [239, 260], "10526211e": [73, 83], "10536157": [73, 82], "10537948e": [73, 83], "105m": [169, 172, 173], "10776436e": [73, 83], "10807291e": [73, 83], "10863163e": [73, 83], "10879738e": [73, 83], "108934": [238, 247], "10893423855304718": [238, 247], "10954670e": [73, 83], "10956261e": [73, 83], "10_000": [5, 35, 36, 278, 333], "10am": [73, 82], "10m": [279, 342], "11": [11, 72, 73, 76, 82, 99, 237, 238, 239, 244, 245, 252, 253, 258, 260, 343], "11016287e": [73, 83], "11058047e": [73, 83], "11085677e": [73, 83], "110m": [169, 172, 173], "11493243e": [73, 83], "11712754e": [73, 83], "11721872": [73, 82], "11745796e": [73, 83], "11788076e": [73, 83], "11897744e": [73, 83], "11_10": [76, 99], "11th": [73, 82], "12": [3, 11, 28, 72, 77, 126, 129, 136, 141, 149, 156, 180, 192, 237, 238, 244, 245, 252, 321, 338, 343], "12014441e": [73, 83], "12174596e": [73, 83], "12183236e": [73, 83], "12234001e": [73, 83], "123": 173, "123456": [118, 121], "12468980e": [73, 83], "12480514e": [73, 83], "12500": [74, 85, 88, 89], "12501": [74, 89], "12502": [74, 89], "12503": [74, 89], "12504": [74, 89], "12587933e": [73, 83], "12685782e": [73, 83], "127": [11, 72, 163, 176], "128": [4, 6, 31, 32, 40, 41, 73, 82, 238, 239, 247, 252, 253, 257, 260, 273, 276, 278, 284, 323, 325, 334], "12821269e": [73, 83], "12832280e": [73, 83], "12841654e": [73, 83], "128k": [212, 213, 216, 229, 235], "12907687e": [73, 83], "12912727e": [73, 83], "12939501e": [73, 83], "129887": [238, 252], "12th": [73, 82], "12x": [239, 260], "12xlarg": [164, 168], "13": [126, 129, 136, 141, 237, 239, 244, 245, 258, 316, 343], "13000": [74, 88, 89], "13086134e": [73, 83], "13095595e": [73, 83], "13100": [74, 88, 89], "13238472e": [73, 83], "13547181e": [73, 83], "13586960e": [73, 83], "13600": [74, 88, 89], "13700": [74, 88, 89], "138": [238, 252, 253], "13803817e": [73, 83], "13828215e": [73, 83], "13841531e": [73, 83], "13b": [221, 223, 228, 229, 235], "14": [73, 76, 82, 99, 221, 223, 278, 307, 316, 321, 327, 333, 338, 343], "140": [221, 223], "14019522e": [73, 83], "140gb": [221, 223, 225], "14159100e": [73, 83], "14268738e": [73, 83], "14443852e": [73, 83], "14533243e": [73, 83], "14656349e": [73, 83], "14703774e": [73, 83], "14777484e": [73, 83], "14787792e": [73, 83], "14892137e": [73, 83], "14971709e": [73, 83], "14gb": [212, 213, 217], "14th": [73, 82], "15": [2, 4, 24, 31, 73, 76, 80, 82, 99, 111, 112, 118, 122, 126, 127, 128, 136, 138, 139, 149, 153, 163, 176, 229, 234, 237, 238, 244, 245, 252, 318, 343], "150": [73, 80, 82], "15072963e": [73, 83], "150m": 173, "15157820e": [73, 83], "15247765e": [73, 83], "15391724e": [73, 83], "15394783": [73, 82], "15428728e": [73, 83], "15451038e": [73, 83], "155": [8, 51], "15531293e": [73, 83], "15553670e": [73, 83], "15556864e": [73, 83], "15585802e": [73, 83], "155m": 173, "156": [237, 244, 245], "15658525e": [73, 83], "15747452e": [73, 83], "15786707e": [73, 83], "15844142e": [73, 83], "15874708e": [73, 83], "15_08": [237, 244, 245], "15_15": [237, 244, 245], "15x": [238, 252], "16": [73, 76, 82, 99, 221, 227, 237, 238, 239, 244, 245, 252, 258, 260, 277, 284, 307, 308, 315, 317, 327, 343], "160": [5, 35, 221, 223], "16129310e": [73, 83], "16142738e": [73, 83], "16249922e": [73, 83], "16273381e": [73, 83], "16315296e": [73, 83], "163491": [237, 244, 245], "163492": [237, 244, 245], "16707636e": [73, 83], "168": [276, 320, 321], "16848189e": [73, 83], "1693310400": 173, "16946062e": [73, 83], "16970104e": [73, 83], "16th": [73, 80, 82], "16xlarg": [164, 168], "17": [73, 74, 82, 85, 100, 105, 126, 129, 130, 136, 141, 142, 149, 158, 180, 192, 221, 227, 237, 244, 245, 343], "17145060e": [73, 83], "172": [100, 105], "17218718e": [73, 83], "17278847e": [73, 83], "17306670e": [73, 83], "1732276209": [238, 253], "1732276227": [238, 253], "17342269e": [73, 83], "17458829e": [73, 83], "17503238e": [73, 83], "17549804e": [73, 83], "175m": [169, 172, 173], "17605399e": [73, 83], "17623755e": [73, 83], "17677706e": [73, 83], "17716263e": [73, 83], "17952654e": [73, 83], "17982033e": [73, 83], "17th": [73, 82], "18": [73, 82, 118, 123, 126, 130, 136, 142, 149, 158, 180, 192, 221, 227, 238, 253, 274, 280, 281, 286, 306, 343], "18025970e": [73, 83], "180m": [169, 172, 173], "18165373e": [73, 83], "18200418e": [73, 83], "18252768e": [73, 83], "18264270e": [73, 83], "18500029e": [73, 83], "18707759e": [73, 83], "1879": [11, 72], "18899436e": [73, 83], "18926680e": [73, 83], "18932852e": [73, 83], "18th": [73, 82], "19": [73, 82, 180, 192, 221, 227, 237, 239, 244, 245, 258, 260, 274, 276, 277, 307, 321, 327, 343], "19172417e": [73, 83], "19192507e": [73, 83], "19254841e": [73, 83], "19275388e": [73, 83], "19408388e": [73, 83], "19601890e": [73, 83], "19630387e": [73, 83], "1967": [74, 85], "19670653e": [73, 83], "19684739e": [73, 83], "19716156e": [73, 83], "19767813e": [73, 83], "19797611e": [73, 83], "19835320e": [73, 83], "19861914e": [73, 83], "19884178e": [73, 83], "1995": [73, 82], "19986786e": [73, 83], "1_000_000": [2, 24], "1d": [278, 334], "1e": [4, 5, 6, 31, 32, 35, 40, 41, 76, 98, 238, 239, 247, 249, 253, 257, 259, 260, 273, 274, 276, 278, 279, 283, 294, 300, 311, 323, 334, 340], "1f": [276, 321], "1gb": [8, 51], "1h34m20": [238, 252], "1m": [279, 342], "1m10": [241, 269], "1mb": [164, 168], "1pb": [164, 168], "1st": [2, 19, 73, 80, 82], "1xt4": [237, 238, 239, 244, 245, 252, 258, 260], "2": [20, 23, 38, 39, 41, 49, 53, 67, 70, 73, 74, 75, 80, 82, 83, 87, 88, 89, 92, 98, 99, 112, 113, 125, 127, 128, 138, 139, 161, 164, 168, 169, 170, 172, 177, 183, 189, 195, 223, 226, 228, 232, 233, 234, 242, 245, 246, 247, 249, 252, 253, 255, 256, 258, 259, 260, 261, 265, 267, 270, 273, 280, 282, 284, 292, 295, 306, 309, 310, 314, 317, 322, 323, 325, 326, 331, 337, 340, 342, 343], "20": [6, 11, 39, 41, 72, 76, 99, 100, 105, 180, 192, 221, 227, 237, 238, 239, 244, 245, 247, 252, 253, 256, 258, 259, 260, 275, 276, 277, 278, 279, 316, 323, 327, 333, 338, 340, 343], "200": [73, 74, 80, 82, 86], "20093006e": [73, 83], "200m": 173, "20103974e": [73, 83], "20113872e": [73, 83], "2012": [73, 82, 100, 105], "2014": [276, 320, 321], "2015": [73, 82], "20152864e": [73, 83], "2017": [73, 80, 82], "20175812e": [73, 83], "2021": [3, 28, 237, 244], "2023": [8, 55], "2024": [8, 9, 55, 66, 173, 237, 238, 239, 240, 244, 245, 252, 253, 258, 260, 266], "2025": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 17, 26, 29, 33, 38, 43, 49, 56, 72, 73, 74, 75, 76, 78, 84, 90, 94, 99, 111, 112, 118, 119, 126, 127, 136, 137, 149, 150, 152, 163, 164, 165, 169, 170, 174, 180, 186, 187, 188, 192, 200, 203, 205, 206, 207, 210, 212, 213, 214, 221, 222, 229, 230, 234], "20302368700504303": [239, 257], "20312032e": [73, 83], "20370263e": [73, 83], "20407227e": [73, 83], "20567256e": [73, 83], "20587808e": [73, 83], "205m": 173, "20716389e": [73, 83], "20yr": [73, 82], "21": [76, 99, 100, 105, 238, 252, 343], "210m": 173, "21206143e": [73, 83], "21334969e": [73, 83], "21377383e": [73, 83], "21380231e": [73, 83], "21405767e": [73, 83], "2147483648": [2, 22], "21503563e": [73, 83], "21508265e": [73, 83], "21584712e": [73, 83], "21600205e": [73, 83], "21637220e": [73, 83], "21745682e": [73, 83], "21776196e": [73, 83], "21796946e": [73, 83], "21971425e": [73, 83], "21st": [73, 82], "22": [73, 76, 82, 99, 238, 247, 252, 343], "22192677e": [73, 83], "222": [239, 258, 260], "22234142e": [73, 83], "22277103e": [73, 83], "22332841e": [73, 83], "224": [274, 277, 306, 307, 308, 327, 331], "22458421e": [73, 83], "225": [274, 308], "22552105e": [73, 83], "22804798e": [73, 83], "22867932e": [73, 83], "22891478e": [73, 83], "229": [274, 308], "22918031e": [73, 83], "22967193e": [73, 83], "22_06": [238, 252], "22_11": [238, 252, 253], "22nd": [73, 82], "23": [73, 82, 237, 244, 274, 277, 307, 327, 343], "23053056e": [73, 83], "23069037e": [73, 83], "23083012e": [73, 83], "23107583e": [73, 83], "23113478e": [73, 83], "23204021e": [73, 83], "23302741e": [73, 83], "23314434e": [73, 83], "23478852e": [73, 83], "235m": [169, 172, 173], "23639209e": [73, 83], "23694149e": [73, 83], "23694418e": [73, 83], "23702966e": [73, 83], "23793101e": [73, 83], "23815991e": [73, 83], "23960292e": [73, 83], "23982316e": [73, 83], "23x": [76, 99], "24": [2, 22, 100, 105, 237, 244, 245, 276, 320, 321, 343], "24085984e": [73, 83], "240m": [169, 172, 173], "24192730e": [73, 83], "24219281e": [73, 83], "24473451e": [73, 83], "245m": [169, 172, 173], "24615501e": [73, 83], "24854468e": [73, 83], "24885803e": [73, 83], "24984046e": [73, 83], "24xlarg": [164, 168], "25": [76, 99, 126, 127, 136, 138, 149, 151, 237, 238, 239, 244, 245, 252, 260, 343], "250": [74, 86], "25000": [74, 85, 86, 87], "25175510e": [73, 83], "25248020e": [73, 83], "25258800e": [73, 83], "25268223e": [73, 83], "25294994e": [73, 83], "25387060e": [73, 83], "25401214e": [73, 83], "255": [6, 39, 239, 240, 256, 264, 277, 327], "256": [5, 35, 36, 164, 168, 238, 247, 253, 274, 277, 307, 327], "25669474e": [73, 83], "25707304e": [73, 83], "25723777e": [73, 83], "25795130e": [73, 83], "25806283e": [73, 83], "25912409e": [73, 83], "25934454e": [73, 83], "25_924022_2383": [237, 244, 245], "25th": [73, 82], "26": [76, 99, 221, 223, 238, 239, 247, 252, 258, 343], "26000811e": [73, 83], "26009388e": [73, 83], "26156314e": [73, 83], "26208720e": [73, 83], "26244992e": [73, 83], "26258478e": [73, 83], "26309279e": [73, 83], "26314560e": [73, 83], "26320729e": [73, 83], "26351237e": [73, 83], "26371822e": [73, 83], "264131": [239, 260], "26429361e": [73, 83], "26502474e": [73, 83], "26816351e": [73, 83], "26948217e": [73, 83], "26990714e": [73, 83], "27": [73, 80, 82, 237, 238, 244, 245, 253], "27106623e": [73, 83], "27159020e": [73, 83], "27211": [239, 258], "27212": [239, 258], "27213278e": [73, 83], "27219909e": [73, 83], "27394149e": [73, 83], "27486494e": [73, 83], "27514724e": [73, 83], "27598614e": [73, 83], "27818017e": [73, 83], "2784426808357239": [238, 247], "278443": [238, 247], "27902825e": [73, 83], "27904241e": [73, 83], "27935463e": [73, 83], "27x": [76, 99], "28": [9, 10, 61, 70, 76, 99, 169, 172, 238, 239, 240, 241, 252, 258, 264, 269, 270, 273, 281], "28050": [239, 260], "28076579e": [73, 83], "28086493e": [73, 83], "28125295e": [73, 83], "28131025e": [73, 83], "28160176e": [73, 83], "28330866e": [73, 83], "28415197e": [73, 83], "28529142e": [73, 83], "28545947e": [73, 83], "28572544e": [73, 83], "28628640e": [73, 83], "28712449e": [73, 83], "28749549e": [73, 83], "28796948e": [73, 83], "28858958e": [73, 83], "28869668e": [73, 83], "28947487e": [73, 83], "28947702e": [73, 83], "28th": [73, 82], "28x28": [6, 39, 239, 256], "29": [229, 234, 237, 238, 239, 244, 245, 252, 253, 258, 260], "29297644e": [73, 83], "29473785e": [73, 83], "29535252e": [73, 83], "29715446e": [73, 83], "29792884e": [73, 83], "29807210e": [73, 83], "29825398e": [73, 83], "29964035e": [73, 83], "29_09": [239, 258], "29t10": 173, "2a": [126, 128, 136, 139], "2b": [126, 128, 136, 139], "2cpu": [126, 133, 136, 145], "2d": [6, 39, 239, 256, 279, 342], "2e": [277, 328], "2f": [2, 18, 181, 193, 275, 279, 318, 342], "2nd": [2, 19, 73, 82], "2ykut_ijz8q8gwt5vphvitzshksddol6msszjxzwe5a": [221, 226], "3": [16, 24, 37, 38, 40, 49, 61, 62, 67, 73, 74, 77, 82, 83, 85, 89, 98, 99, 114, 125, 127, 134, 138, 147, 151, 161, 164, 168, 169, 172, 173, 179, 191, 224, 225, 226, 231, 232, 233, 236, 242, 246, 247, 253, 254, 255, 257, 259, 260, 261, 263, 265, 267, 273, 280, 281, 282, 292, 294, 300, 301, 306, 311, 317, 323, 325, 326, 328, 331, 334, 335, 336, 337, 340, 342, 343], "30": [73, 76, 82, 99, 229, 234, 238, 252, 279, 320, 325, 338], "30478994e": [73, 83], "30517557e": [73, 83], "30540405e": [73, 83], "30551": [238, 253], "30557770e": [73, 83], "30565623e": [73, 83], "30582720e": [73, 83], "30603": [238, 252], "30618355e": [73, 83], "30638674e": [73, 83], "30715715e": [73, 83], "30746688e": [73, 83], "30834863e": [73, 83], "308870": [238, 253], "30887049436569214": [238, 252], "30980236e": [73, 83], "30981060e": [73, 83], "30999158e": [73, 83], "30min": [276, 321], "30th": [73, 82], "31": [11, 72, 76, 99, 237, 238, 239, 244, 245, 252, 258, 260], "31019164e": [73, 83], "31141504e": [73, 83], "31172134e": [73, 83], "31209707e": [73, 83], "31449399e": [73, 83], "31499174e": [73, 83], "31562141e": [73, 83], "31659403e": [73, 83], "31913936e": [73, 83], "31973000e": [73, 83], "31st": [73, 82], "32": [2, 5, 9, 18, 25, 35, 61, 221, 227, 229, 232, 275, 277, 278, 317, 328, 329, 335], "320": [5, 35], "32145682e": [73, 83], "32192443e": [73, 83], "32244647e": [73, 83], "32266051e": [73, 83], "32296453e": [73, 83], "32373542e": [73, 83], "32401919e": [73, 83], "32454751e": [73, 83], "32564947e": [73, 83], "326001912355423": [239, 257], "32635012e": [73, 83], "32654747e": [73, 83], "3266499161421599": [239, 258], "32665": [239, 258], "32722983e": [73, 83], "32756231e": [73, 83], "32768": [221, 224, 227, 229, 234], "32768166e": [73, 83], "32890965e": [73, 83], "32902160e": [73, 83], "32918817e": [73, 83], "32b": [229, 234, 235], "32gb": [239, 258, 260], "32k": [212, 213, 216, 229, 235], "32m": [76, 99, 238, 239, 252, 260], "33": [73, 80, 82, 83], "33023707e": [73, 83], "33163792e": [73, 83], "33167297e": [73, 83], "33281359e": [73, 83], "33300245e": [73, 83], "33315668e": [73, 83], "33572224e": [73, 83], "33688403e": [73, 83], "33728483e": [73, 83], "33760041e": [73, 83], "33768886e": [73, 83], "33827804e": [73, 83], "33832851e": [73, 83], "33951919e": [73, 83], "34": [73, 83, 238, 239, 240, 252, 258, 265], "34206163e": [73, 83], "34323069e": [73, 83], "34348310e": [73, 83], "34449054e": [73, 83], "34561165e": [73, 83], "34613437e": [73, 83], "34668782e": [73, 83], "34740751e": [73, 83], "34842591e": [73, 83], "34999743e": [73, 83], "35": [73, 76, 83, 99, 238, 239, 252, 253, 258, 260], "35016027e": [73, 83], "35024523e": [73, 83], "35026570e": [73, 83], "35185423e": [73, 83], "35189386e": [73, 83], "35268092e": [73, 83], "35481167e": [73, 83], "35665376e": [73, 83], "35665385e": [73, 83], "35833579e": [73, 83], "35873899e": [73, 83], "35890651e": [73, 83], "36": [76, 99, 221, 227, 239, 260], "36150215e": [73, 83], "36240765e": [73, 83], "365191": [238, 253], "36595646e": [73, 83], "36710434e": [73, 83], "36786141e": [73, 83], "36829392e": [73, 83], "36855166e": [73, 83], "36857450e": [73, 83], "36868265e": [73, 83], "36873224e": [73, 83], "36m": [76, 99, 238, 239, 241, 252, 258, 260, 269], "37": [221, 227, 238, 239, 252, 260], "37080820e": [73, 83], "37142141e": [73, 83], "37153175e": [73, 83], "37196398e": [73, 83], "37391533e": [73, 83], "37605290e": [73, 83], "37751494e": [73, 83], "37764162e": [73, 83], "37784477e": [73, 83], "37850042e": [73, 83], "37964366e": [73, 83], "38": [76, 99], "38109175e": [73, 83], "38115362e": [73, 83], "38281021e": [73, 83], "384": [73, 82, 83, 221, 227], "38448": [239, 258, 260], "384480": [239, 260], "38451725e": [73, 83], "38496622e": [73, 83], "38554320e": [73, 83], "38563488e": [73, 83], "38687068e": [73, 83], "38798335e": [73, 83], "38858718e": [73, 83], "38910706e": [73, 83], "39053045e": [73, 83], "39066431e": [73, 83], "39268537e": [73, 83], "39287962e": [73, 83], "39421923e": [73, 83], "39532143e": [73, 83], "39561000e": [73, 83], "39590132e": [73, 83], "39660065e": [73, 83], "39745891e": [73, 83], "39747020e": [73, 83], "39796034e": [73, 83], "39855982e": [73, 83], "39905545e": [73, 83], "3b": [229, 233], "3d": [278, 334], "3f": [275, 317, 318, 319], "3rd": [73, 82], "3x3": [273, 281, 292], "4": [20, 26, 29, 33, 38, 41, 49, 62, 67, 73, 74, 82, 83, 89, 125, 156, 161, 169, 172, 173, 181, 193, 222, 223, 224, 225, 232, 233, 234, 237, 244, 245, 246, 252, 253, 255, 258, 260, 261, 263, 267, 270, 273, 280, 308, 317, 323, 325, 328, 329, 335, 337, 340, 342, 343], "40": [73, 74, 82, 85, 173, 238, 252], "4000": [2, 22], "40064341e": [73, 83], "40084913e": [73, 83], "400b": [221, 223, 228, 229, 235], "40222309e": [73, 83], "40240113e": [73, 83], "40254933e": [73, 83], "403": [238, 239, 252, 258, 260], "40336857e": [73, 83], "40409318e": [73, 83], "40456108e": [73, 83], "40510444e": [73, 83], "40537590e": [73, 83], "406": [274, 308], "40600796e": [73, 83], "40880044e": [73, 83], "40937243e": [73, 83], "40g": [221, 224, 225], "41": [11, 72, 76, 99], "41169238e": [73, 83], "41280317e": [73, 83], "41516277e": [73, 83], "41520910e": [73, 83], "41526775e": [73, 83], "41575071e": [73, 83], "41598": [76, 99], "41599": [76, 99], "415m": 173, "41709536e": [73, 83], "41786465e": [73, 83], "41920993e": [73, 83], "41922843e": [73, 83], "41933542e": [73, 83], "41968962e": [73, 83], "41985670e": [73, 83], "42": [3, 28, 274, 275, 279, 308, 316, 338], "420m": 173, "42177847e": [73, 83], "422321": [76, 99], "42241838e": [73, 83], "42471355e": [73, 83], "42482564e": [73, 83], "42548003e": [73, 83], "42604055e": [73, 83], "42662169e": [73, 83], "42837034e": [73, 83], "42856956e": [73, 83], "42857780e": [73, 83], "42904809e": [73, 83], "42943636e": [73, 83], "43057": [237, 244, 245], "43168001e": [73, 83], "43248991e": [73, 83], "43267226e": [73, 83], "43328887e": [73, 83], "43732": [238, 247], "43747482e": [73, 83], "43779554e": [73, 83], "43806068e": [73, 83], "43821533e": [73, 83], "43902507e": [73, 83], "43916206e": [73, 83], "43940079e": [73, 83], "43996522e": [73, 83], "44": [237, 239, 244, 245, 260], "44072895e": [73, 83], "44087312e": [73, 83], "44237953e": [73, 83], "44269560e": [73, 83], "44299744e": [73, 83], "443": [100, 105], "44326049e": [73, 83], "44500265e": [73, 83], "44628939e": [73, 83], "44769201e": [73, 83], "44773570": [238, 247], "44875658e": [73, 83], "44877301e": [73, 83], "44888324e": [73, 83], "44945610e": [73, 83], "45": [73, 82, 111, 114, 238, 252], "45237213e": [73, 83], "45387161e": [73, 83], "45463008e": [73, 83], "45558545e": [73, 83], "45565206e": [73, 83], "45596695e": [73, 83], "456": [173, 274, 308], "45615": [73, 80, 83], "45630976e": [73, 83], "45667797e": [73, 83], "45803327e": [73, 83], "45804777e": [73, 83], "45845979e": [73, 83], "45928478e": [73, 83], "45977615e": [73, 83], "45981687e": [73, 83], "46": [76, 99, 126, 129, 136, 141], "46038486e": [73, 83], "46128924e": [73, 83], "46165411e": [73, 83], "46190545e": [73, 83], "46210968e": [73, 83], "46212946e": [73, 83], "46281177e": [73, 83], "46350823e": [73, 83], "46354072e": [73, 83], "46371266e": [73, 83], "46490431e": [73, 83], "46654081e": [73, 83], "46736982e": [73, 83], "46787928e": [73, 83], "46938870e": [73, 83], "46954274e": [73, 83], "47110615e": [73, 83], "47293536e": [73, 83], "47399181e": [73, 83], "47439016e": [73, 83], "47477984e": [73, 83], "475m": 173, "47602344e": [73, 83], "47635157e": [73, 83], "47678024e": [73, 83], "47783903e": [73, 83], "47834991e": [73, 83], "47896763e": [73, 83], "47925606e": [73, 83], "47997355e": [73, 83], "48": [164, 168, 221, 227, 238, 253, 276, 320, 321], "48048115e": [73, 83], "480m": 173, "485": [274, 308], "485m": 173, "48663167e": [73, 83], "48813944e": [73, 83], "48855758e": [73, 83], "48856053e": [73, 83], "49": [76, 99, 212, 213, 219, 221, 226, 238, 252, 253], "49026504e": [73, 83], "49106100e": [73, 83], "49187856e": [73, 83], "49249397e": [73, 83], "49257514e": [73, 83], "49307863e": [73, 83], "49331436e": [73, 83], "49361154e": [73, 83], "49425897e": [73, 83], "49631714e": [73, 83], "49642932e": [73, 83], "49779003e": [73, 83], "49808554e": [73, 83], "49876371e": [73, 83], "49959813e": [73, 83], "4f": [279, 340], "4k": [212, 213, 216, 229, 235], "4m37": [239, 258], "4th": [73, 82], "5": [1, 6, 16, 19, 20, 25, 39, 40, 41, 49, 61, 73, 74, 82, 83, 88, 89, 97, 112, 120, 127, 138, 151, 169, 172, 173, 229, 232, 233, 235, 237, 241, 244, 245, 246, 247, 249, 250, 253, 255, 256, 257, 258, 259, 261, 263, 264, 265, 270, 273, 283, 287, 292, 297, 311, 312, 314, 323, 325, 329, 331, 333, 337, 340, 343], "50": [9, 59, 74, 75, 88, 92, 93, 111, 113, 221, 227, 238, 240, 252, 253, 263, 275, 277, 278, 317, 319, 331, 336], "500": [6, 41, 164, 168, 274, 277, 307, 308, 327], "50099332e": [73, 83], "50164117e": [73, 83], "50424745e": [73, 83], "50576949e": [73, 83], "50653918e": [73, 83], "50785267e": [73, 83], "50892793e": [73, 83], "50946071e": [73, 83], "50_per_index": [9, 59, 61, 64, 240, 241, 263, 265, 270], "51002133e": [73, 83], "51042950e": [73, 83], "51119480e": [73, 83], "512": [238, 247, 253, 273, 279, 298, 301, 303, 340], "51281480e": [73, 83], "51315774e": [73, 83], "51320172e": [73, 83], "51383309e": [73, 83], "51503164e": [73, 83], "51518283e": [73, 83], "51617597e": [73, 83], "51739728e": [73, 83], "51786283e": [73, 83], "51865722e": [73, 83], "52096841e": [73, 83], "52135583e": [73, 83], "52154651e": [73, 83], "52324782e": [73, 83], "525325868955": [100, 105], "52539432e": [73, 83], "52647242e": [73, 83], "53": [238, 252], "53193595e": [73, 83], "53377999e": [73, 83], "53381238e": [73, 83], "53534813e": [73, 83], "53647423e": [73, 83], "53716086e": [73, 83], "53754605e": [73, 83], "53959617e": [73, 83], "53998228e": [73, 83], "54": [76, 99, 237, 244, 245, 275, 315, 316], "5404948": [237, 244, 245], "54113623e": [73, 83], "54230055e": [73, 83], "54291149e": [73, 83], "54304842e": [73, 83], "54450669e": [73, 83], "54546804e": [73, 83], "54573391e": [73, 83], "54621774e": [73, 83], "54645248e": [73, 83], "54685497e": [73, 83], "55": [238, 252], "55025972e": [73, 83], "55099240e": [73, 83], "550_000": [5, 36], "5529555": [237, 244, 245], "5552995": [237, 244, 245], "55601753e": [73, 83], "55613178e": [73, 83], "55635225e": [73, 83], "55699139e": [73, 83], "5588189": [237, 244, 245], "55900936e": [73, 83], "55923614e": [73, 83], "56011016e": [73, 83], "56041365e": [73, 83], "56115811e": [73, 83], "56188577e": [73, 83], "56274483e": [73, 83], "56389399e": [73, 83], "56662523e": [73, 83], "56688479e": [73, 83], "56718080e": [73, 83], "56896329e": [73, 83], "56896693e": [73, 83], "56920916e": [73, 83], "57": [221, 227], "57008413e": [73, 83], "57156566e": [73, 83], "57160601e": [73, 83], "57175567e": [73, 83], "57226282e": [73, 83], "57393692e": [73, 83], "57440994e": [73, 83], "57502690e": [73, 83], "57536250e": [73, 83], "57737350e": [73, 83], "57877821e": [73, 83], "57970206e": [73, 83], "58": [237, 244, 245], "580": [164, 168, 275, 315, 316], "58003160e": [73, 83], "58086956e": [73, 83], "580k": [275, 316], "580x580x3": [164, 168], "58155808e": [73, 83], "58189368e": [73, 83], "58261052e": [73, 83], "58301930e": [73, 83], "58452298e": [73, 83], "58531007e": [73, 83], "58614498e": [73, 83], "58806413e": [73, 83], "58818898e": [73, 83], "58824509e": [73, 83], "58904049e": [73, 83], "59": [229, 234, 237, 244, 245], "59060508e": [73, 83], "59105931e": [73, 83], "59260547e": [73, 83], "59349391e": [73, 83], "59438765e": [73, 83], "59552705e": [73, 83], "59555081e": [73, 83], "59728657e": [73, 83], "59733031e": [73, 83], "59831755e": [73, 83], "59980466e": [73, 83], "5_anyscalejob": [186, 200, 205, 207], "6": [49, 73, 82, 83, 163, 169, 172, 173, 176, 239, 240, 241, 246, 258, 260, 263, 265, 269, 270, 307, 315, 323, 333], "60": [6, 39, 169, 172, 239, 256, 273, 281], "60254669e": [73, 83], "60417205": [237, 244, 245], "60559933e": [73, 83], "60700288e": [73, 83], "60769555e": [73, 83], "60900429e": [73, 83], "60926636e": [73, 83], "60929009e": [73, 83], "61124960e": [73, 83], "61173157e": [73, 83], "61210120e": [73, 83], "61377022e": [73, 83], "61495838e": [73, 83], "61626707e": [73, 83], "61644816e": [73, 83], "61783993e": [73, 83], "61808310e": [73, 83], "62014505e": [73, 83], "62131321e": [73, 83], "62209135e": [73, 83], "62317707e": [73, 83], "62408248e": [73, 83], "62470581e": [73, 83], "62676229e": [73, 83], "62782574e": [73, 83], "6290559": [237, 244, 245], "629055917263031": [237, 244, 245], "63": [76, 99], "63008086e": [73, 83], "63077107e": [73, 83], "63363028e": [73, 83], "63391770e": [73, 83], "63410094e": [73, 83], "63496330e": [73, 83], "63529071e": [73, 83], "63559180e": [73, 83], "63716504e": [73, 83], "63769671e": [73, 83], "6379": [163, 176], "64": [4, 5, 6, 31, 35, 40, 41, 73, 82, 238, 239, 247, 253, 257, 260, 273, 274, 276, 279, 282, 311, 314, 322, 339, 340], "640": [5, 35], "64042781e": [73, 83], "641551": [238, 253], "64353992e": [73, 83], "64574068e": [73, 83], "64824829e": [73, 83], "64927173e": [73, 83], "65101083e": [73, 83], "65288996e": [73, 83], "65297012e": [73, 83], "65683129e": [73, 83], "65831": [239, 258], "65908886e": [73, 83], "66008": [237, 244, 245], "66033891e": [73, 83], "66034375e": [73, 83], "660569": [237, 244, 245], "662196": [237, 244, 245], "66246349e": [73, 83], "662499": [237, 244, 245], "66510823e": [73, 83], "66672035e": [73, 83], "66712171e": [73, 83], "66808441e": [73, 83], "66871315e": [73, 83], "66888866e": [73, 83], "67074795e": [73, 83], "67096058e": [73, 83], "67168414e": [73, 83], "67200039e": [73, 83], "67249476e": [73, 83], "67394597e": [73, 83], "67530221e": [73, 83], "67625724e": [73, 83], "67702921e": [73, 83], "68044616e": [73, 83], "682": [279, 338], "68261507e": [73, 83], "68279577e": [73, 83], "68410710e": [73, 83], "68623075e": [73, 83], "68863142e": [73, 83], "68928802e": [73, 83], "68990344e": [73, 83], "69230062e": [73, 83], "69485952e": [73, 83], "69655108e": [73, 83], "69689246e": [73, 83], "69690510e": [73, 83], "69849765e": [73, 83], "69882979e": [73, 83], "69974936e": [73, 83], "6_000108_000000": [5, 35], "6_2024": [239, 258], "6_anyscaleservic": [187, 203, 206, 210], "6f": [76, 97, 181, 193], "6th": [73, 82], "6x": [239, 260], "7": [6, 31, 40, 41, 49, 73, 82, 83, 169, 172, 173, 237, 239, 240, 244, 246, 247, 257, 258, 260, 263, 265, 273, 282, 307, 317, 323, 329, 340], "70": [221, 223], "70078446e": [73, 83], "70130879e": [73, 83], "70160577e": [73, 83], "70176884e": [73, 83], "70207208e": [73, 83], "70480572e": [73, 83], "70483862e": [73, 83], "70502144e": [73, 83], "70606668e": [73, 83], "70623609e": [73, 83], "70640138e": [73, 83], "70758316e": [73, 83], "70849383e": [73, 83], "70991046e": [73, 83], "70b": [212, 213, 216, 224, 225, 226, 227, 228, 229, 235], "71": [237, 244, 245], "71107422e": [73, 83], "71276686e": [73, 83], "71327189e": [73, 83], "71361454e": [73, 83], "71367359e": [73, 83], "71599907e": [73, 83], "71614686e": [73, 83], "71677093e": [73, 83], "71802861e": [73, 83], "71831726e": [73, 83], "72043703e": [73, 83], "72060728e": [73, 83], "72087287e": [73, 83], "72232352e": [73, 83], "72505680e": [73, 83], "72602186e": [73, 83], "72664893e": [73, 83], "72793153e": [73, 83], "72918749e": [73, 83], "72964428e": [73, 83], "73005784e": [73, 83], "73056117e": [73, 83], "73160497e": [73, 83], "73338448e": [73, 83], "73339425e": [73, 83], "73386657e": [73, 83], "73471044e": [73, 83], "73475703e": [73, 83], "73528048e": [73, 83], "73541475e": [73, 83], "73842654e": [73, 83], "73909385e": [73, 83], "73954112e": [73, 83], "73962507e": [73, 83], "74006203e": [73, 83], "74092592e": [73, 83], "74106744e": [73, 83], "74198601e": [73, 83], "74435990e": [73, 83], "74639919e": [73, 83], "74725649e": [73, 83], "75": [221, 227, 274, 314], "75087003e": [73, 83], "75092961e": [73, 83], "75342406e": [73, 83], "75359203e": [73, 83], "75361482e": [73, 83], "75378935e": [73, 83], "75653571e": [73, 83], "75755769e": [73, 83], "75899668e": [73, 83], "76055871e": [73, 83], "76067518e": [73, 83], "76138058e": [73, 83], "76263633e": [73, 83], "76323075e": [73, 83], "76346910e": [73, 83], "76389585e": [73, 83], "76404205e": [73, 83], "76448804e": [73, 83], "76463524e": [73, 83], "764641": [76, 99], "76568236e": [73, 83], "76702512e": [73, 83], "76715726e": [73, 83], "76780378e": [73, 83], "76795331e": [73, 83], "768": [221, 227], "76875392e": [73, 83], "76900255e": [73, 83], "76936167e": [73, 83], "76972622e": [73, 83], "77": [5, 35], "77095975e": [73, 83], "7721": [239, 258], "7722": [239, 258], "7725": [239, 258], "77374096e": [73, 83], "774": [11, 72], "77459675e": [73, 83], "77555919e": [73, 83], "77606642e": [73, 83], "77784879e": [73, 83], "77830246e": [73, 83], "78041089e": [73, 83], "78072055e": [73, 83], "78197911e": [73, 83], "78284839e": [73, 83], "7834368348121643": [238, 252], "783437": [238, 253], "78439620e": [73, 83], "78474542e": [73, 83], "78715137e": [73, 83], "78778227e": [73, 83], "78793629e": [73, 83], "78836194e": [73, 83], "78927866e": [73, 83], "79144512e": [73, 83], "79223316e": [73, 83], "79288665e": [73, 83], "79409343e": [73, 83], "79455946e": [73, 83], "79656491e": [73, 83], "79875319e": [73, 83], "79880509e": [73, 83], "79888725e": [73, 83], "799808": [76, 99], "79x": [221, 227], "7am": [73, 82], "7b": [212, 213, 217, 221, 223, 228, 229, 235], "7th": [73, 80, 82, 83], "7x": [4, 5, 32, 37, 238, 252, 254], "8": [5, 18, 22, 31, 35, 36, 49, 54, 73, 74, 82, 83, 88, 98, 173, 221, 222, 223, 224, 225, 227, 228, 229, 235, 237, 240, 242, 263, 265, 273, 281, 284, 285, 292, 306, 307, 311, 312, 320, 322, 326, 327, 329, 332, 333, 335, 337, 338], "80": [221, 227, 275, 277, 278, 279, 316, 327, 333, 338], "800": [221, 223], "8000": [0, 3, 10, 28, 70, 75, 93, 169, 172, 212, 213, 219, 221, 225, 229, 232, 233, 234, 237, 241, 244, 245, 269, 270], "80058852e": [73, 83], "80134752e": [73, 83], "80346647e": [73, 83], "80356482e": [73, 83], "80584863e": [73, 83], "80600139e": [73, 83], "80770779e": [73, 83], "80772168e": [73, 83], "80778313e": [73, 83], "80779386e": [73, 83], "8080": [163, 176], "80b": [221, 223, 229, 235], "81101489e": [73, 83], "81150190e": [73, 83], "81153491e": [73, 83], "81209888e": [73, 83], "81385726e": [73, 83], "8147535920143127": [238, 247], "814754": [238, 247], "81605020e": [73, 83], "81611199e": [73, 83], "81677291e": [73, 83], "81750199e": [73, 83], "8192": [212, 213, 219, 229, 232, 233], "81969379e": [73, 83], "81974494e": [73, 83], "81992236e": [73, 83], "82237032e": [73, 83], "82248098e": [73, 83], "82377563e": [73, 83], "82634446e": [73, 83], "8265": [11, 72, 163, 176], "82670507e": [73, 83], "82704625e": [73, 83], "82709087e": [73, 83], "82714777e": [73, 83], "82736081e": [73, 83], "82819171e": [73, 83], "82876396e": [73, 83], "82900697e": [73, 83], "82929088e": [73, 83], "82934299e": [73, 83], "82985464e": [73, 83], "83": [238, 247], "83045536e": [73, 83], "83186028e": [73, 83], "83414386e": [73, 83], "83462293e": [73, 83], "83483610e": [73, 83], "83486040e": [73, 83], "837": [221, 227], "83987024e": [73, 83], "84016372e": [73, 83], "84025085e": [73, 83], "84072098e": [73, 83], "84077434e": [73, 83], "84100178e": [73, 83], "84200376e": [73, 83], "84204574e": [73, 83], "84257627e": [73, 83], "84342591e": [73, 83], "84438897e": [73, 83], "84560393e": [73, 83], "84609653e": [73, 83], "84649059e": [73, 83], "84724203e": [73, 83], "84787306e": [73, 83], "84826868e": [73, 83], "85": [229, 232], "85011083e": [73, 83], "85030222e": [73, 83], "85491417e": [73, 83], "85630648e": [73, 83], "85674404e": [73, 83], "85712914e": [73, 83], "85737485e": [73, 83], "86": [111, 112, 118, 120, 126, 127, 136, 138, 149, 151, 229, 234], "86025219e": [73, 83], "86067504e": [73, 83], "86110076e": [73, 83], "86166140e": [73, 83], "86302094e": [73, 83], "86359452e": [73, 83], "86543170e": [73, 83], "86763499e": [73, 83], "87068461e": [73, 83], "87136489e": [73, 83], "87145798e": [73, 83], "87155861e": [73, 83], "87222108e": [73, 83], "87272584e": [73, 83], "87305135e": [73, 83], "87395632e": [73, 83], "87503409e": [73, 83], "87622940e": [73, 83], "87687856e": [73, 83], "87722724e": [73, 83], "87823978e": [73, 83], "87826851e": [73, 83], "88060308e": [73, 83], "88125470e": [73, 83], "88136353e": [73, 83], "88193573e": [73, 83], "88231084e": [73, 83], "88282757e": [73, 83], "88339034e": [73, 83], "88483773e": [73, 83], "88547347e": [73, 83], "88624009e": [73, 83], "88834351e": [73, 83], "88852262e": [73, 83], "89050034e": [73, 83], "89146360e": [73, 83], "89227986e": [73, 83], "89326443e": [73, 83], "89384127e": [73, 83], "89393270e": [73, 83], "89414667e": [73, 83], "89564747e": [73, 83], "89613281e": [73, 83], "89739510e": [73, 83], "89740060e": [73, 83], "89891556e": [73, 83], "89910729e": [73, 83], "8b": [212, 213, 219, 229, 232, 235], "8cpu": [239, 258, 260], "8gb": [126, 133, 136, 145], "8k": [212, 213, 216, 229, 235], "8th": [73, 80, 82], "8xl40": [221, 227], "9": [2, 6, 25, 31, 39, 40, 73, 76, 83, 97, 111, 112, 118, 120, 126, 127, 129, 130, 138, 141, 142, 151, 158, 164, 168, 173, 237, 239, 240, 244, 245, 256, 257, 263, 265, 273, 281, 282, 296, 306, 307, 321, 327, 331], "90": [73, 82, 126, 127, 164, 168, 229, 233], "90151963e": [73, 83], "90165711e": [73, 83], "90290013e": [73, 83], "90392175e": [73, 83], "90476887e": [73, 83], "90511677e": [73, 83], "90528610e": [73, 83], "90640": [239, 260], "90656148e": [73, 83], "90659517e": [73, 83], "90668219e": [73, 83], "90833239e": [73, 83], "91": [238, 252], "91010717e": [73, 83], "91011913e": [73, 83], "91205712e": [73, 83], "91437262e": [73, 83], "91462375e": [73, 83], "91614705e": [73, 83], "91802080e": [73, 83], "91938744e": [73, 83], "92106171e": [73, 83], "92230538e": [73, 83], "92250460e": [73, 83], "92267558e": [73, 83], "92452052e": [73, 83], "92608377e": [73, 83], "92633970e": [73, 83], "92704949e": [73, 83], "92848936e": [73, 83], "92968845e": [73, 83], "93108886e": [73, 83], "93218452e": [73, 83], "93256009e": [73, 83], "93267390e": [73, 83], "93274853e": [73, 83], "93358018e": [73, 83], "93396618e": [73, 83], "93505753e": [73, 83], "93599756e": [73, 83], "93615": [237, 244, 245], "93653901e": [73, 83], "93655512e": [73, 83], "93872128e": [73, 83], "93877090e": [73, 83], "93991698e": [73, 83], "94008095e": [73, 83], "94048835e": [73, 83], "94202918e": [73, 83], "942508": [238, 253], "9425080418586731": [238, 252, 253], "943": [279, 338], "94449548e": [73, 83], "94474315e": [73, 83], "94507216e": [73, 83], "94511395e": [73, 83], "94559568e": [73, 83], "94806529e": [73, 83], "94919703e": [73, 83], "94921815e": [73, 83], "94926137e": [73, 83], "949393": [76, 99], "94980036e": [73, 83], "950654": [238, 247], "9506543874740601": [238, 247], "95128240e": [73, 83], "95309842e": [73, 83], "95409912e": [73, 83], "95493992e": [73, 83], "95530374e": [73, 83], "95612733e": [73, 83], "95642184e": [73, 83], "95678936e": [73, 83], "95717700e": [73, 83], "95755831e": [73, 83], "95777296e": [73, 83], "95807119e": [73, 83], "9590334296226501": [75, 93], "959243851260": [111, 113], "96": [240, 265], "96016713e": [73, 83], "96078059e": [73, 83], "96112816e": [73, 83], "96223371e": [73, 83], "96230914e": [73, 83], "96423475e": [73, 83], "96519734e": [73, 83], "96568063e": [73, 83], "96672040e": [73, 83], "96707787e": [73, 83], "96917129e": [73, 83], "97122377e": [73, 83], "97128675e": [73, 83], "97161290e": [73, 83], "97234564e": [73, 83], "97312343e": [73, 83], "97468323e": [73, 83], "97478577e": [73, 83], "97603959e": [73, 83], "97606084e": [73, 83], "97651729e": [73, 83], "97699012e": [73, 83], "97741865e": [73, 83], "97773625e": [73, 83], "97899076e": [73, 83], "97899440e": [73, 83], "97910169e": [73, 83], "97926176e": [73, 83], "97952076e": [73, 83], "97973699e": [73, 83], "98": [240, 265], "98070179e": [73, 83], "98221380e": [73, 83], "98234466e": [73, 83], "98250581e": [73, 83], "98440845e": [73, 83], "98496": [239, 258], "98530062e": [73, 83], "98645657e": [73, 83], "98712543e": [73, 83], "988": [240, 265], "99006638e": [73, 83], "99074054e": [73, 83], "99094741e": [73, 83], "99340957e": [73, 83], "99445761e": [73, 83], "99487356e": [73, 83], "99530393e": [73, 83], "99537045e": [73, 83], "999": [278, 333], "99955577e": [73, 83], "9996353387832642": [75, 93], "9998507499694824": [75, 93], "A": [1, 2, 3, 4, 5, 6, 8, 9, 10, 14, 18, 22, 25, 26, 28, 29, 32, 33, 36, 38, 41, 49, 51, 56, 59, 67, 69, 73, 74, 76, 80, 81, 82, 88, 89, 94, 107, 109, 118, 120, 124, 136, 145, 173, 178, 179, 185, 190, 191, 198, 212, 213, 214, 216, 217, 221, 222, 223, 229, 230, 232, 235, 238, 239, 240, 241, 249, 258, 259, 263, 268, 273, 275, 276, 277, 278, 279, 282, 289, 291, 316, 320, 323, 328, 331, 332, 334, 336, 338, 339, 340, 342], "AND": [183, 195], "And": [73, 74, 82, 85], "As": [11, 72, 74, 85, 89, 164, 168, 178, 182, 186, 190, 194, 202, 205, 209, 229, 232, 240, 265, 276, 325], "At": [1, 2, 4, 13, 20, 31, 73, 74, 82, 89, 100, 105, 169, 172, 238, 247, 274, 277, 278, 306, 326, 332], "Be": [73, 82], "But": [6, 40, 73, 74, 82, 85, 88, 89, 239, 257], "By": [2, 7, 9, 22, 46, 59, 61, 169, 172, 182, 186, 188, 194, 200, 205, 207, 273, 274, 275, 276, 280, 281, 289, 306, 307, 308, 312, 314, 317, 320], "For": [1, 2, 3, 7, 8, 9, 10, 11, 13, 21, 23, 28, 48, 52, 53, 54, 55, 62, 64, 71, 72, 73, 74, 76, 77, 82, 84, 89, 95, 97, 100, 101, 103, 107, 108, 109, 110, 126, 130, 136, 142, 149, 152, 158, 163, 169, 171, 172, 176, 177, 178, 179, 180, 181, 182, 183, 187, 189, 190, 191, 192, 193, 194, 195, 204, 206, 211, 212, 213, 217, 221, 224, 225, 226, 229, 232, 233, 234, 235, 237, 238, 240, 243, 251, 264, 265, 273, 274, 275, 278, 279, 289, 293, 306, 318, 332, 337], "IN": [169, 172], "IT": [73, 82], "If": [1, 2, 3, 4, 6, 7, 8, 11, 15, 21, 22, 26, 31, 32, 41, 44, 47, 48, 53, 72, 73, 74, 76, 82, 83, 85, 87, 88, 89, 98, 111, 112, 118, 125, 126, 127, 133, 136, 138, 145, 146, 149, 151, 154, 161, 178, 179, 180, 181, 188, 190, 191, 192, 193, 229, 232, 239, 259, 273, 274, 276, 277, 279, 285, 288, 300, 302, 303, 306, 312, 313, 323, 331, 340, 342], "In": [2, 3, 4, 8, 9, 10, 19, 22, 23, 28, 31, 47, 48, 54, 60, 61, 62, 64, 71, 73, 74, 75, 76, 78, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 97, 107, 110, 111, 113, 126, 128, 136, 139, 164, 166, 177, 178, 179, 180, 181, 182, 183, 185, 188, 189, 190, 191, 192, 193, 194, 195, 198, 212, 213, 220, 221, 223, 229, 231, 232, 240, 264, 265, 273, 274, 275, 276, 277, 278, 279, 280, 281, 284, 289, 291, 293, 296, 299, 305, 306, 315, 317, 320, 323, 331, 332, 336, 342], "It": [0, 3, 4, 5, 6, 7, 8, 9, 12, 28, 31, 32, 35, 41, 47, 50, 51, 58, 61, 62, 73, 74, 75, 76, 82, 83, 85, 89, 90, 94, 97, 98, 99, 100, 102, 105, 111, 112, 126, 127, 130, 136, 137, 142, 149, 150, 158, 169, 172, 178, 179, 185, 190, 191, 197, 221, 223, 229, 233, 240, 241, 263, 270, 273, 275, 276, 278, 279, 283, 284, 287, 315, 322, 334, 339], "Its": [7, 47, 74, 89], "NOT": [73, 82], "No": [0, 8, 53, 73, 82, 107, 109, 163, 176, 221, 228, 273, 274, 277, 278, 279, 294, 312, 314, 331, 332, 336, 337], "Not": [2, 4, 5, 25, 29, 33, 35, 73, 80, 82, 100, 101, 273, 274, 304, 314], "OR": [118, 121, 149, 152], "Of": [74, 88], "On": [100, 103, 107, 110, 149, 152, 179, 184, 191, 196, 212, 213, 216, 221, 227, 238, 251, 274, 279, 306, 340], "One": [74, 85, 89, 178, 185, 190, 198, 229, 232, 274, 311], "Or": [0, 180, 192, 212, 221, 224, 229, 232], "Such": [74, 89], "THE": [73, 74, 82, 88], "TO": [73, 82], "That": [2, 18, 73, 74, 82, 88, 89, 188], "The": [0, 1, 2, 3, 4, 5, 6, 8, 9, 14, 15, 19, 25, 27, 28, 31, 32, 36, 39, 41, 42, 46, 48, 51, 52, 54, 60, 61, 63, 73, 74, 76, 78, 82, 83, 85, 88, 89, 94, 96, 97, 98, 99, 100, 103, 105, 106, 107, 108, 109, 110, 111, 114, 118, 122, 123, 126, 129, 130, 136, 141, 142, 146, 149, 152, 158, 163, 164, 168, 169, 171, 172, 173, 175, 176, 178, 179, 180, 181, 183, 184, 185, 187, 188, 190, 191, 192, 193, 195, 196, 199, 202, 204, 206, 209, 211, 216, 217, 218, 219, 221, 224, 226, 227, 229, 232, 233, 234, 237, 238, 239, 240, 244, 249, 256, 260, 263, 273, 274, 275, 276, 277, 278, 279, 280, 282, 284, 285, 289, 290, 291, 292, 293, 294, 296, 297, 300, 303, 306, 314, 315, 316, 317, 322, 325, 326, 332, 336, 337, 338, 339, 340, 342], "Their": [73, 82], "Then": [0, 7, 10, 11, 47, 70, 72, 73, 74, 80, 82, 83, 88, 111, 114, 118, 123, 126, 130, 136, 140, 142, 149, 158, 279, 342], "There": [8, 9, 51, 53, 54, 64, 73, 74, 75, 82, 88, 89, 90, 92, 93, 182, 186, 194, 202, 205, 209, 240, 265], "These": [0, 2, 3, 7, 9, 18, 22, 27, 46, 60, 74, 77, 89, 100, 105, 118, 122, 163, 175, 177, 179, 181, 189, 191, 193, 212, 213, 216, 221, 223, 240, 265, 273, 279, 281, 290, 338], "To": [2, 3, 4, 6, 7, 8, 9, 10, 11, 19, 23, 28, 32, 41, 45, 52, 54, 60, 61, 62, 63, 64, 71, 72, 73, 76, 80, 81, 82, 95, 98, 100, 105, 108, 111, 113, 126, 128, 136, 139, 163, 169, 171, 172, 175, 176, 178, 180, 181, 188, 190, 192, 193, 212, 213, 219, 238, 239, 240, 241, 250, 251, 252, 253, 258, 259, 262, 264, 265, 269, 270, 273, 274, 279, 289, 291, 297, 300, 301, 308, 313, 342], "With": [4, 11, 32, 72, 74, 75, 84, 89, 90, 164, 168, 185, 199, 212, 213, 216, 229, 235, 241, 270, 273, 274, 275, 279, 280, 292, 293, 298, 299, 302, 305, 306, 315, 337], "_": [2, 6, 20, 41, 73, 82, 181, 193, 229, 232, 239, 241, 259, 269, 270, 274, 276, 277, 278, 279, 306, 314, 320, 326, 328, 331, 332, 333, 337], "_2": [277, 278, 326, 332], "__call__": [3, 9, 10, 28, 62, 70, 73, 78, 81, 169, 172, 237, 240, 241, 244, 245, 264, 269, 270, 274, 275, 276, 314, 318, 325], "__file__": [180, 192], "__getitem__": [5, 35, 274, 276, 308, 321], "__index_level_0__": [275, 317], "__init__": [2, 3, 5, 9, 10, 25, 28, 35, 62, 70, 73, 75, 81, 92, 180, 192, 237, 240, 241, 244, 245, 264, 269, 270, 273, 274, 275, 276, 277, 278, 279, 292, 308, 314, 318, 321, 322, 325, 328, 334, 339], "__len__": [5, 35, 274, 276, 308, 321], "__main__": [164, 168, 181, 193], "__name__": [164, 168, 181, 193], "_arrow_table_from_shard": [275, 317], "_build": 0, "_class_nam": [5, 35], "_config": 0, "_diffusers_vers": [5, 35], "_dmat_from_arrow": [275, 317], "_k": [278, 332], "_model": [3, 28, 237, 244, 245], "_sample_timestep": [5, 35], "_shared_step": [277, 278, 328, 334], "_static": 0, "_toc": 0, "a10": [274, 306], "a100": [221, 224, 225, 227, 274, 306], "a10g": [278, 332, 335, 336], "a_random_job_nam": [118, 124, 136, 145], "abandon": [74, 89], "abil": [2, 4, 5, 23, 30, 34, 74, 85, 164, 166, 177, 179, 189, 191, 273, 280], "abl": [4, 7, 8, 32, 47, 54, 75, 93, 111, 112, 126, 127, 136, 138, 146, 149, 151, 163, 176], "abortmultipartupload": [100, 105], "about": [4, 6, 12, 13, 17, 19, 22, 23, 24, 31, 32, 41, 73, 74, 75, 76, 78, 82, 84, 85, 88, 89, 90, 94, 107, 109, 111, 117, 164, 166, 168, 179, 180, 181, 191, 192, 193, 212, 213, 217, 221, 226, 229, 232, 234, 237, 238, 239, 241, 244, 245, 250, 253, 259, 270, 273, 274, 281, 283, 291, 306], "abov": [2, 4, 5, 22, 32, 36, 73, 74, 82, 89, 163, 164, 168, 169, 172, 176, 212, 213, 217, 218, 229, 232, 273, 276, 280, 281, 282, 289, 291, 323], "absenc": [73, 82, 169, 172, 275, 319], "absent": [74, 89], "absolut": [4, 32, 276, 321], "abstract": [3, 7, 27, 43, 100, 102, 188, 212, 213, 217, 218, 221, 224, 240, 262, 275, 276, 278, 315, 325, 332], "absurd": [74, 89], "academ": [229, 235, 276, 320], "academi": [74, 89], "acc": [2, 25, 238, 247, 249, 253, 275, 317], "acc_metr": [274, 310], "acceler": [1, 2, 5, 7, 9, 10, 11, 13, 22, 35, 36, 43, 62, 68, 72, 111, 117, 149, 154, 179, 188, 191, 277, 278, 329, 335], "accelerator_shap": [237, 238, 239, 244, 245, 252, 258, 260], "accelerator_typ": [9, 62, 75, 92, 212, 213, 219, 221, 224, 227, 229, 232, 233, 234, 237, 238, 239, 244, 245, 252, 258, 260], "accept": [1, 6, 9, 10, 11, 16, 41, 61, 62, 68, 72, 75, 90, 92, 239, 240, 258, 260, 264, 273, 276, 282, 292, 322], "access": [2, 7, 18, 25, 31, 43, 75, 76, 90, 97, 99, 100, 103, 104, 105, 110, 136, 146, 163, 164, 168, 169, 172, 173, 176, 178, 180, 181, 184, 185, 188, 190, 192, 193, 196, 197, 198, 221, 222, 224, 225, 229, 230, 232, 233, 234, 246, 273, 274, 275, 284, 289, 291, 298, 306, 307, 316], "accident": [275, 317], "acclaim": [74, 89], "accomplish": [273, 305], "accord": [1, 8, 9, 16, 54, 64, 179, 191, 240, 265, 273, 290, 298], "accordingli": [76, 97, 229, 235, 278, 336], "account": [2, 7, 25, 47, 74, 85, 100, 104, 105, 107, 109, 111, 112, 113, 118, 119, 122, 126, 127, 128, 136, 138, 139, 149, 151, 153, 161, 188], "account_id": [100, 105], "accross": [212, 213, 216, 217], "accumul": [4, 32, 238, 249, 273, 283], "accur": [212, 213, 216, 276, 320], "accuraci": [6, 9, 40, 64, 76, 94, 96, 98, 221, 223, 229, 235, 238, 239, 240, 247, 249, 251, 252, 253, 257, 265, 274, 275, 310, 314, 315, 317, 318, 319], "accuracy_scor": [275, 316, 317], "achiev": [2, 4, 5, 8, 9, 10, 19, 24, 30, 34, 54, 62, 64, 68, 212, 213, 216, 240, 265, 273, 280], "acid": [7, 43], "acl": [185, 198], "across": [1, 2, 3, 4, 5, 7, 8, 9, 11, 13, 18, 21, 23, 28, 32, 36, 43, 46, 54, 55, 57, 59, 63, 64, 66, 72, 74, 75, 76, 86, 89, 90, 94, 98, 99, 100, 102, 105, 107, 109, 162, 164, 166, 168, 173, 179, 180, 181, 185, 191, 192, 193, 199, 212, 213, 217, 218, 221, 224, 227, 228, 229, 232, 235, 237, 238, 239, 240, 242, 249, 251, 252, 260, 262, 264, 265, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 289, 290, 291, 293, 296, 297, 298, 305, 306, 310, 312, 314, 315, 316, 317, 318, 319, 320, 321, 323, 325, 326, 327, 331, 332, 333, 336, 337, 338, 340, 342], "act": [74, 89, 100, 105, 273, 284], "act_dim": [278, 334, 336], "act_fn": [5, 35], "action": [74, 88, 89, 100, 105, 164, 168, 229, 232, 234, 241, 270, 333, 334], "action_spac": [278, 333], "activ": [0, 1, 2, 6, 8, 9, 16, 25, 41, 54, 61, 173, 177, 182, 186, 189, 194, 200, 205, 207, 212, 213, 217, 239, 259, 279, 338], "actor": [4, 5, 7, 8, 10, 18, 22, 32, 36, 47, 51, 56, 68, 69, 73, 78, 81, 164, 166, 212, 213, 217, 238, 241, 249, 263, 268, 274, 275, 276, 282, 305, 314, 318, 319, 320, 325], "actorpoolmapoper": [9, 63], "actorpoolstrategi": [275, 316, 318, 319], "actress": [74, 85], "actual": [1, 6, 9, 15, 41, 62, 73, 74, 82, 89, 111, 113, 118, 121, 126, 128, 131, 136, 139, 143, 149, 152, 155, 159, 229, 234, 239, 240, 258, 264, 273, 278, 279, 289, 333, 337], "ad": [5, 36, 73, 82, 100, 105, 111, 113, 117, 179, 184, 188, 191, 196, 241, 269, 273, 275, 278, 293, 305, 317, 318, 332], "adam": [4, 6, 29, 31, 32, 38, 40, 41, 238, 239, 247, 249, 253, 257, 260, 273, 274, 276, 277, 278, 279, 281, 283, 294, 300, 310, 323, 328, 334, 340], "adamw": [5, 35], "adapt": [76, 98, 212, 213, 216, 230, 231, 236, 273, 274, 275, 276, 277, 278, 279, 281, 286, 314, 319, 325, 331, 336, 342], "adapter_config": [229, 232], "adapter_model": [229, 232], "adapter_nam": [229, 232], "adaptiveavgpool2d": [238, 247, 253], "add": [1, 2, 4, 5, 7, 8, 9, 10, 14, 15, 16, 18, 19, 21, 25, 32, 35, 36, 43, 52, 53, 61, 70, 74, 77, 86, 100, 105, 118, 122, 126, 129, 131, 134, 136, 141, 143, 146, 147, 149, 152, 156, 159, 164, 168, 169, 171, 177, 178, 179, 183, 187, 189, 190, 191, 195, 204, 206, 211, 212, 213, 218, 229, 232, 234, 238, 241, 250, 253, 269, 273, 274, 275, 276, 277, 278, 279, 281, 282, 284, 287, 292, 305, 314, 319, 323, 325, 326, 331, 333, 336, 342], "add_label": [9, 61, 64, 240, 265], "add_nois": [5, 35], "add_ref": [2, 23], "add_subplot": [4, 31, 32, 273, 281, 292], "addit": [4, 8, 32, 52, 75, 90, 91, 100, 103, 164, 167, 177, 178, 179, 181, 188, 189, 190, 191, 193, 229, 232, 236, 273, 274, 275, 279, 286, 299, 300, 314, 319, 337, 342], "addition": [8, 9, 54, 61, 76, 98, 100, 105, 111, 117, 164, 168, 241, 270, 279, 337], "addr": [277, 329], "address": [1, 7, 13, 43, 73, 82, 100, 105, 111, 113, 126, 128, 136, 139, 163, 176, 273, 290], "adebayor": [73, 82], "adher": [74, 89], "adjust": [5, 8, 35, 53, 73, 74, 76, 82, 88, 99, 229, 235, 273, 274, 275, 282, 314, 317], "adjust_total_amount": [8, 52], "adjusted_data": [8, 53, 55], "adjusted_data_rai": [8, 53, 55], "adjusted_total_amount": [8, 52, 169, 171], "admin": [100, 101, 105, 185, 198], "administr": [74, 89, 343], "admittedli": [74, 85], "adopt": [8, 51], "adv": [73, 80, 82], "advanc": [5, 6, 7, 35, 40, 43, 47, 107, 109, 110, 111, 117, 169, 172, 179, 191, 212, 213, 220, 222, 228, 232, 239, 257, 267, 274, 314, 343], "advantag": [7, 47, 221, 223], "adventureland": [73, 82], "adversari": [277, 326], "affect": [10, 69], "affin": [238, 247, 253], "afford": [73, 82, 221, 223], "after": [1, 2, 3, 4, 5, 10, 11, 16, 18, 24, 28, 31, 32, 35, 71, 72, 73, 74, 76, 78, 82, 83, 88, 94, 99, 126, 130, 133, 136, 140, 142, 145, 149, 158, 164, 168, 169, 172, 173, 178, 179, 181, 187, 190, 191, 193, 203, 206, 210, 212, 213, 216, 238, 241, 253, 270, 273, 274, 275, 276, 277, 278, 279, 281, 283, 289, 292, 306, 310, 312, 316, 319, 321, 323, 324, 326, 331, 332, 337, 338, 340, 342], "afterward": [274, 306], "again": [2, 3, 8, 22, 26, 52, 74, 89, 111, 116, 179, 182, 191, 194, 274, 275, 277, 279, 313, 315, 330, 341], "against": [73, 74, 80, 82, 88, 89, 221, 226, 229, 232, 273, 274, 276, 278, 279, 302, 312, 325, 336, 337, 340, 342], "agent": [212, 213, 216, 229, 235, 278, 332, 336], "aggreg": [7, 46, 49, 56, 60, 238, 249, 261, 273, 274, 275, 277, 280, 310, 318, 319, 328, 329], "aggress": [9, 61, 169, 172], "agil": [278, 332], "agnost": [276, 321], "ago": [74, 85], "agre": [74, 88], "aguero": [73, 82], "ahead": [74, 88, 221, 228, 229, 236], "ai": [2, 8, 9, 10, 11, 25, 55, 59, 61, 62, 64, 70, 72, 180, 182, 184, 186, 187, 188, 192, 194, 196, 200, 203, 205, 206, 207, 210, 212, 213, 219, 221, 228, 229, 231, 232, 234, 240, 241, 263, 264, 265, 266, 269, 270, 343], "air": [237, 244, 245], "aj": [73, 80, 82], "ak": [100, 103], "ako": [73, 82], "alaska": [74, 88], "alb": [107, 110], "alberta": [74, 88], "album": [73, 82], "alciato": [73, 80, 82], "ald": [73, 82], "alert": [170, 183, 186, 187, 195, 200, 203, 205, 206, 207, 210, 273, 305], "algorithm": [6, 41, 76, 99, 107, 110, 188, 212, 213, 218, 237, 238, 239, 244, 245, 252, 258, 259, 260, 275, 315], "alic": 173, "align": [7, 46, 273, 274, 277, 281, 307, 310, 329], "alik": [73, 78], "all": [0, 2, 3, 4, 5, 6, 7, 11, 12, 17, 18, 23, 24, 26, 27, 29, 32, 33, 36, 38, 41, 43, 49, 51, 55, 56, 61, 72, 73, 74, 75, 76, 77, 78, 81, 82, 84, 85, 88, 89, 90, 94, 98, 100, 103, 105, 107, 109, 111, 112, 118, 119, 121, 126, 127, 132, 133, 134, 136, 137, 144, 145, 147, 149, 150, 152, 154, 163, 164, 165, 168, 169, 170, 173, 174, 176, 178, 179, 180, 181, 182, 184, 185, 188, 190, 191, 192, 193, 194, 196, 198, 212, 213, 214, 215, 216, 221, 222, 229, 230, 232, 233, 236, 237, 238, 239, 240, 243, 251, 259, 260, 265, 273, 274, 275, 277, 278, 279, 280, 281, 283, 284, 288, 289, 291, 298, 301, 302, 303, 306, 309, 310, 312, 314, 316, 317, 318, 319, 320, 321, 329, 333, 336, 337, 338, 340, 342], "all_fil": [180, 192], "all_results_at_onc": [2, 24], "alleg": [74, 85], "allegori": [74, 89], "alli": [74, 88], "allobjectact": [100, 105], "alloc": [6, 7, 9, 17, 41, 48, 57, 61, 73, 78, 100, 105, 107, 108, 111, 113, 126, 128, 136, 139, 239, 259, 273, 275, 276, 285, 292, 317, 320], "allow": [2, 3, 4, 5, 7, 18, 22, 27, 28, 30, 32, 34, 48, 73, 74, 75, 76, 78, 82, 89, 90, 92, 94, 97, 98, 99, 100, 105, 136, 146, 169, 172, 177, 179, 181, 184, 185, 189, 191, 193, 196, 198, 221, 227, 229, 231, 232, 240, 241, 263, 264, 270, 273, 274, 275, 276, 277, 279, 280, 285, 286, 289, 298, 301, 303, 306, 317, 323, 325, 329, 341], "allowedhead": [100, 105, 136, 146], "allowedmethod": [100, 105, 136, 146], "allowedorigin": [100, 105, 136, 146], "allreduc": [273, 280], "alltoallapi": [9, 64], "allus": [74, 89], "almost": [74, 89, 275, 316], "alon": [8, 9, 54, 64, 240, 265], "along": [73, 82, 179, 191, 273, 274, 288, 306], "alongsid": [273, 274, 275, 299, 305, 314, 316], "alonso": [73, 82], "alphas_cumprod": [5, 35], "alreadi": [8, 9, 10, 11, 51, 64, 68, 72, 73, 80, 82, 126, 129, 136, 141, 164, 168, 178, 180, 188, 190, 192, 240, 265, 273, 274, 275, 276, 279, 281, 283, 292, 303, 313, 318, 321, 338, 342], "also": [2, 4, 8, 9, 10, 19, 31, 32, 55, 64, 70, 73, 74, 75, 80, 82, 83, 84, 85, 88, 89, 90, 91, 111, 112, 118, 120, 126, 127, 133, 136, 138, 145, 149, 151, 164, 168, 169, 170, 172, 178, 180, 181, 182, 183, 185, 187, 190, 192, 193, 194, 195, 197, 203, 206, 210, 212, 213, 216, 238, 240, 241, 247, 253, 265, 269, 270, 273, 275, 276, 277, 279, 281, 291, 301, 316, 323, 329, 338, 339, 340], "altern": [11, 72, 76, 97, 111, 112, 126, 127, 136, 138, 149, 151, 275, 319], "alwai": [0, 2, 22, 164, 167, 229, 232, 233, 275, 276, 319, 321], "am": [74, 85], "amaz": [73, 74, 82, 88, 229, 236], "amazon": [7, 43, 73, 82, 100, 103, 111, 112, 126, 127, 136, 138, 180, 181, 192, 193], "amazonaw": [100, 105, 238, 239, 252, 258, 260], "amazonelasticfilesystemclientreadwriteaccess": [136, 140, 146], "ambush": [74, 88], "america": [74, 85], "american": [74, 85, 88, 89], "ami": [73, 82, 107, 109], "amnt": [73, 82], "among": [8, 9, 54, 64, 74, 88, 89, 221, 224, 240, 265], "amount": [2, 3, 4, 5, 7, 8, 9, 25, 28, 30, 34, 43, 51, 61, 180, 192, 237, 244, 273, 274, 276, 280, 306, 323, 325], "amp": [73, 82, 274, 277, 314, 331], "amus": [74, 89], "an": [0, 1, 4, 7, 8, 9, 13, 15, 17, 19, 20, 21, 25, 27, 31, 43, 47, 48, 49, 53, 54, 55, 56, 58, 59, 60, 61, 62, 67, 69, 71, 73, 74, 75, 76, 77, 80, 81, 82, 85, 87, 88, 89, 90, 92, 98, 101, 104, 105, 106, 107, 110, 111, 112, 118, 124, 126, 127, 135, 138, 148, 163, 164, 165, 168, 169, 171, 172, 176, 178, 180, 182, 183, 184, 185, 188, 190, 192, 194, 195, 196, 197, 198, 200, 207, 212, 213, 216, 218, 219, 224, 225, 227, 230, 232, 238, 240, 242, 247, 251, 261, 263, 264, 268, 270, 273, 274, 275, 276, 277, 279, 280, 281, 306, 308, 315, 317, 318, 319, 320, 326, 331, 332, 337, 343], "anal": [74, 89], "analys": [74, 89], "analysi": [7, 46, 74, 75, 89, 90, 92, 93, 162, 181, 193, 212, 213, 216, 229, 232, 235], "analyt": [7, 43, 44, 46], "analyz": [7, 43, 163, 175, 229, 232, 273, 291], "anatom": [74, 85], "anatomi": [73, 82], "angel": [74, 88, 89], "angelbr": [74, 89], "anger": [73, 82], "anggrek": [73, 82], "angl": [278, 332], "angular": [278, 332], "ani": [1, 2, 3, 6, 8, 9, 10, 11, 13, 14, 22, 28, 38, 40, 41, 51, 62, 65, 67, 70, 72, 73, 74, 75, 81, 84, 85, 88, 89, 92, 93, 111, 113, 118, 122, 125, 126, 128, 136, 139, 149, 161, 163, 164, 168, 169, 171, 176, 177, 178, 184, 188, 189, 190, 196, 229, 232, 239, 240, 241, 257, 258, 263, 264, 266, 269, 270, 273, 274, 275, 276, 277, 278, 279, 284, 297, 304, 306, 314, 315, 316, 317, 319, 325, 326, 329, 336, 337, 340, 342], "anniversari": [73, 82], "annot": [6, 41, 107, 110, 275, 318], "anon": [5, 35], "anonym": [169, 171, 237, 244, 245], "anoth": [8, 9, 52, 60, 73, 74, 80, 82, 86, 88, 89, 107, 110, 179, 182, 186, 191, 194, 202, 205, 209, 277, 331], "answer": [73, 74, 82, 85, 212, 213, 216], "ant": [73, 82], "anthoni": [74, 88, 89], "anti": [2, 12, 19, 24, 73, 74, 82, 89], "antiwoman": [74, 89], "anymor": [277, 279, 331, 342], "anyon": [73, 74, 82, 85], "anyscal": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 17, 26, 28, 29, 31, 32, 33, 37, 38, 43, 49, 51, 54, 56, 59, 61, 62, 64, 67, 70, 72, 73, 74, 75, 76, 78, 80, 84, 90, 94, 103, 104, 110, 115, 116, 117, 120, 121, 124, 125, 129, 132, 133, 134, 135, 138, 140, 141, 144, 145, 146, 147, 148, 151, 152, 155, 160, 161, 163, 168, 171, 173, 174, 175, 176, 198, 199, 214, 219, 220, 222, 223, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 252, 254, 258, 260, 263, 264, 265, 269, 270, 273, 280, 281, 305, 311, 319, 325, 331, 336, 342], "anyscale_101": [186, 187, 200, 203, 205, 206, 207, 210], "anyscale_artifact_storag": [5, 35, 164, 168, 180, 192], "anyscale_cloud_id": [118, 122], "anyscale_cloud_nam": [100, 106, 111, 112, 113, 114, 115, 116, 118, 119, 122, 123, 124, 125, 126, 127, 128, 130, 133, 134, 136, 137, 139, 142, 145, 147, 149, 150, 152, 153, 158, 160, 161], "anyscale_cloud_storage_bucket": [180, 192], "anyscale_cloud_storage_bucket_region": [180, 192], "anyscale_iam_rol": [136, 140], "anyscale_iam_role_arn": [100, 106], "anyscale_iam_s3_policy_arn": [136, 140], "anyscale_registration_command": [111, 113, 118, 122, 126, 128, 136, 139, 149, 153], "anyscale_s3_bucket_nam": [111, 113, 116, 126, 128, 134, 136, 139, 147], "anyscale_security_group": [100, 105], "anyscale_vpc": [100, 105], "anyscale_vpc_nam": [100, 105], "anyscalerai": [126, 134, 136, 147], "anyscaleuserdata": [221, 226], "anyth": [74, 85, 89, 107, 110, 180, 192, 273, 274, 284, 313], "anywher": [2, 18, 182, 194], "apach": 46, "aperitif": [73, 82], "api": [2, 7, 8, 9, 10, 25, 45, 46, 47, 48, 50, 55, 61, 62, 68, 73, 74, 75, 78, 80, 81, 83, 87, 90, 107, 108, 109, 110, 120, 151, 161, 182, 183, 185, 186, 187, 188, 194, 195, 198, 200, 202, 204, 205, 206, 207, 209, 211, 212, 213, 218, 219, 221, 224, 229, 231, 233, 234, 236, 238, 240, 241, 250, 264, 268, 274, 275, 276, 277, 279, 306, 314, 319, 325, 329, 338], "api_kei": [212, 213, 219, 221, 225, 226, 229, 232, 233, 234], "apigatewai": [169, 172, 173], "app": [1, 3, 10, 13, 28, 71, 75, 90, 92, 136, 146, 169, 172, 173, 181, 183, 187, 193, 195, 203, 204, 206, 210, 211, 212, 213, 217, 219, 221, 224, 225, 226, 227, 229, 232, 233, 234, 241, 270, 277, 331], "app1": [10, 71, 169, 172], "app_build": [10, 71], "apparatu": [278, 336], "appear": [73, 74, 82, 85, 89, 212, 213, 217], "append": [1, 2, 9, 16, 18, 24, 61, 180, 192, 229, 234, 274, 276, 277, 278, 279, 307, 314, 321, 327, 333, 340], "appl": [11, 72, 73, 76, 78, 82, 94, 97], "appli": [0, 2, 3, 4, 5, 8, 9, 10, 25, 28, 31, 35, 50, 51, 52, 54, 58, 62, 64, 70, 73, 74, 75, 76, 78, 82, 83, 84, 87, 89, 92, 97, 111, 113, 114, 118, 122, 123, 125, 126, 128, 130, 136, 139, 142, 149, 153, 158, 161, 237, 240, 241, 244, 245, 264, 265, 269, 274, 276, 277, 278, 279, 280, 281, 283, 287, 290, 292, 306, 308, 323, 327, 332, 333, 337, 338], "applic": [2, 3, 8, 11, 20, 27, 43, 47, 48, 50, 67, 70, 71, 72, 75, 90, 92, 93, 107, 110, 111, 117, 118, 121, 149, 152, 163, 164, 166, 167, 170, 173, 175, 177, 179, 182, 183, 184, 186, 187, 189, 191, 194, 195, 196, 200, 203, 204, 205, 206, 207, 210, 211, 212, 213, 217, 219, 221, 223, 224, 226, 228, 229, 231, 233, 234, 236, 237, 240, 241, 243, 262, 267, 268, 269, 270, 272, 279, 342], "application_log": [75, 92], "approach": [2, 7, 19, 48, 73, 74, 76, 78, 84, 89, 94, 98, 162, 274, 275, 277, 279, 306, 316, 326, 337, 338], "appropri": [126, 133, 136, 145, 212, 213, 217, 221, 224], "approv": [111, 113, 116, 118, 122, 125, 126, 128, 134, 136, 147, 149, 153, 161], "approx": [2, 18, 278, 332], "approxim": [212, 213, 217, 279, 337, 338], "april": [73, 82], "ar": [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 22, 24, 25, 31, 32, 36, 37, 41, 42, 43, 45, 46, 47, 48, 51, 52, 53, 54, 55, 59, 60, 61, 62, 64, 68, 69, 72, 73, 74, 75, 76, 77, 78, 81, 82, 85, 88, 89, 90, 92, 93, 97, 105, 107, 109, 110, 111, 112, 113, 116, 118, 121, 122, 126, 127, 128, 132, 133, 134, 136, 138, 139, 140, 144, 145, 146, 147, 149, 151, 152, 154, 161, 163, 164, 168, 169, 170, 171, 172, 173, 176, 177, 179, 180, 181, 182, 184, 185, 186, 189, 191, 192, 193, 194, 196, 198, 202, 205, 209, 212, 213, 217, 221, 223, 224, 226, 227, 228, 229, 232, 233, 234, 236, 238, 239, 240, 241, 249, 250, 254, 260, 262, 264, 265, 268, 270, 273, 280, 281, 282, 283, 284, 286, 287, 289, 290, 291, 292, 294, 297, 299, 303, 307, 310, 314, 319, 325, 327, 331, 333, 336, 340, 342], "arang": [276, 322, 325], "arbitrari": [1, 13, 229, 233], "architectur": [4, 5, 10, 32, 35, 36, 69, 78, 90, 94, 104, 107, 109, 162, 214, 220, 238, 241, 249, 268, 273, 274, 276, 277, 279, 282, 314, 320, 322, 331, 339], "archuleta": [73, 82], "area": [8, 54, 229, 236], "aree": 212, "aren": [136, 146, 276, 323], "arena": [229, 235], "arg": [229, 233, 274, 276, 314, 322], "argmax": [4, 9, 10, 31, 32, 62, 70, 76, 96, 238, 240, 241, 247, 253, 264, 269, 273, 274, 275, 292, 310, 314, 317, 318], "argu": [74, 89], "arguabl": [74, 85], "argument": [1, 4, 5, 6, 9, 10, 16, 17, 19, 22, 32, 36, 41, 62, 70, 73, 74, 76, 82, 89, 97, 182, 194, 229, 232, 234, 239, 240, 241, 258, 260, 264, 269, 273, 284, 286], "arm": [74, 88], "arm64": [11, 72], "arn": [100, 105, 111, 113, 126, 128, 136, 139, 140], "around": [4, 5, 32, 36, 74, 85, 88, 89, 183, 187, 195, 203, 206, 210, 229, 232, 273, 274, 282, 314], "arr": [273, 277, 292, 327], "arrai": [6, 8, 9, 10, 39, 41, 52, 61, 70, 73, 82, 83, 229, 233, 237, 239, 241, 244, 245, 256, 258, 259, 269, 270, 273, 281, 292, 296, 297], "arrang": [2, 23], "array_equ": [2, 18], "array_split": [279, 338], "arriv": [7, 46, 278, 336], "arrow_ref": [275, 317], "arsen": [73, 82], "art": [74, 88, 212, 213, 218], "articl": [241, 271], "artifact": [4, 32, 76, 99, 180, 192, 237, 238, 244, 245, 252, 273, 274, 275, 281, 289, 290, 304, 314, 319], "artifact_dir": [275, 319], "artifact_storag": [180, 192], "artifact_storage_path": [5, 35], "artifici": [278, 332], "artist": [74, 85], "as_directori": [4, 5, 32, 36, 238, 253, 273, 274, 275, 276, 277, 278, 279, 292, 300, 310, 314, 317, 323, 325, 329, 331, 335, 336, 340, 342], "as_fram": [275, 316], "as_index": [274, 276, 279, 312, 323, 340], "as_tensor": [5, 35, 273, 292], "asarrai": [277, 327], "asgi": 173, "ask": [2, 25, 74, 85, 274, 277, 311, 329], "aspen": [275, 315], "assert": [2, 9, 24, 61, 240, 241, 264, 270, 275, 277, 278, 316, 331, 336], "assess": [274, 276, 312, 323, 325], "assign": [8, 9, 53, 59, 149, 156, 184, 196, 240, 263, 273, 274, 275, 279, 285, 290, 295, 306, 317, 337], "assist": [73, 82, 235], "associ": [7, 48, 164, 166, 178, 190, 273, 279, 291, 342], "assum": [2, 18, 100, 105, 107, 109, 163, 176, 273, 275, 281, 318], "assumerol": [100, 105], "astral": [11, 72], "astyp": [5, 10, 35, 70, 274, 275, 276, 278, 279, 314, 318, 325, 333, 338], "async": [3, 10, 28, 70, 169, 172, 237, 241, 244, 245, 269, 270], "asynchron": [6, 40], "asyncio": [3, 7, 26, 28, 48], "athen": [73, 82], "atom": [7, 43, 73, 82], "attach": [107, 109, 110, 146, 273, 274, 276, 279, 289, 292, 301, 310, 323, 340], "attempt": [9, 60, 61, 273, 276, 300, 323], "attend": [74, 89], "attent": [74, 76, 85, 97, 212, 213, 220, 276, 320, 325], "attention_head_dim": [5, 35], "attribut": [275, 277, 319, 329], "audienc": [74, 88, 89, 279, 342], "audio": [73, 82], "audit": [212, 213, 218], "augment": [273, 274, 293, 305, 314], "august": [73, 82], "auschwitz": [74, 89], "auth": [118, 121, 149, 152, 155, 276, 321], "authent": [100, 105, 107, 110, 119, 120, 151, 178, 180, 190, 192, 221, 225, 226], "author": [173, 221, 225, 229, 232], "auto": [5, 35, 36, 107, 109, 111, 113, 116, 118, 122, 125, 126, 128, 134, 136, 147, 149, 153, 161, 169, 171, 183, 195, 229, 234, 241, 270, 274, 277, 278, 279, 306, 314, 329, 332, 335, 340], "auto_select_worker_config": [212, 213, 219, 221, 226], "autocal": [107, 109], "autocomplet": [229, 235], "autodiscoveri": [126, 129, 136, 141], "autograd": [273, 292], "autom": [107, 108, 109, 178, 190, 200, 207, 229, 234, 236, 275, 276, 315, 325], "automat": [0, 2, 4, 5, 8, 9, 20, 30, 32, 34, 35, 51, 57, 62, 73, 74, 75, 76, 78, 80, 81, 85, 92, 94, 97, 100, 105, 107, 109, 110, 118, 122, 178, 182, 183, 186, 187, 190, 194, 195, 200, 203, 205, 206, 207, 210, 212, 213, 218, 221, 226, 238, 241, 250, 270, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 287, 289, 291, 294, 295, 296, 298, 299, 302, 305, 306, 309, 310, 311, 312, 313, 314, 315, 317, 319, 320, 323, 324, 325, 326, 329, 331, 332, 333, 335, 336, 337, 340, 341, 342], "automodelforsequenceclassif": [76, 95, 97], "autosc": [7, 10, 48, 57, 69, 75, 90, 107, 109, 111, 117, 118, 125, 179, 181, 183, 187, 191, 193, 195, 203, 204, 206, 210, 211, 212, 213, 217, 219, 273, 280], "autoscal": [2, 4, 5, 9, 22, 30, 34, 62, 75, 93, 107, 109, 110, 111, 117, 132, 133, 134, 135, 144, 145, 146, 147, 148, 164, 166, 177, 179, 189, 191, 238, 239, 241, 252, 258, 269, 270], "autoscaling_config": [212, 213, 219, 221, 224, 227, 229, 234, 241, 270], "autotoken": [76, 95, 97], "autotun": [6, 42], "auxiliari": [278, 336], "avail": [4, 5, 7, 8, 9, 11, 17, 18, 19, 23, 24, 31, 36, 48, 54, 61, 62, 72, 73, 74, 75, 76, 82, 83, 87, 90, 94, 97, 100, 105, 111, 113, 126, 128, 136, 139, 163, 164, 167, 168, 169, 172, 176, 178, 182, 183, 187, 190, 194, 195, 203, 206, 210, 212, 213, 215, 218, 221, 225, 229, 232, 235, 273, 274, 276, 277, 278, 279, 281, 282, 284, 289, 298, 300, 310, 314, 324, 325, 331, 335, 338, 340], "available_resourc": [2, 22], "availi": [177, 183, 187, 189, 195, 203, 206, 210], "avalanch": [74, 88], "averag": [4, 5, 8, 32, 36, 54, 74, 85, 273, 274, 275, 279, 280, 283, 310, 318, 340], "avg_loss": [4, 31], "avg_train_loss": [276, 279, 323, 340], "avg_val_loss": [276, 279, 323, 340], "avgpool": [238, 247, 253], "avoid": [1, 2, 4, 5, 7, 8, 9, 10, 16, 23, 30, 34, 35, 46, 52, 53, 57, 61, 68, 73, 74, 83, 85, 88, 169, 172, 181, 182, 183, 193, 194, 195, 212, 213, 219, 273, 274, 275, 276, 278, 279, 280, 289, 292, 301, 304, 314, 316, 317, 318, 321, 332, 336, 338], "aw": [7, 8, 9, 10, 43, 51, 59, 62, 70, 74, 89, 101, 102, 103, 105, 106, 107, 109, 110, 113, 116, 117, 128, 131, 133, 134, 135, 138, 139, 140, 143, 146, 147, 148, 149, 159, 162, 180, 184, 185, 192, 196, 198, 229, 232, 237, 238, 239, 240, 241, 244, 245, 252, 258, 260, 263, 264, 269], "awai": [9, 60, 73, 74, 82, 85, 89, 178, 188, 190, 325], "await": [3, 10, 28, 70, 237, 241, 244, 245, 269, 270], "awak": [74, 89], "awar": [276, 278, 279, 321, 332, 342], "award": [73, 74, 82, 89], "aws_region": [100, 106, 111, 113, 126, 128, 129, 131, 136, 139, 141, 143, 229, 232], "aws_role_nam": [118, 122], "awsregion": [126, 129, 136, 141], "ax": [6, 39, 238, 239, 247, 253, 256, 273, 274, 277, 281, 307, 327, 331], "axi": [4, 6, 9, 10, 31, 32, 39, 60, 62, 70, 76, 96, 238, 239, 240, 241, 247, 253, 256, 264, 269, 273, 274, 275, 276, 277, 281, 292, 307, 314, 317, 318, 325, 327, 331], "axvlin": [276, 325], "aydin": [229, 232], "azur": [100, 103, 273, 274, 289, 305, 314], "b": [1, 2, 10, 14, 16, 19, 22, 70, 73, 82, 273, 274, 276, 277, 279, 292, 314, 322, 323, 325, 327, 328, 342], "babi": [73, 80, 82], "back": [2, 3, 25, 28, 73, 74, 80, 82, 83, 88, 89, 169, 172, 183, 195, 237, 241, 244, 245, 270, 273, 274, 275, 277, 278, 279, 295, 296, 314, 317, 318, 331, 332, 336, 338], "backbon": [273, 277, 282, 331], "backdrop": [74, 89], "backend": [76, 97, 98, 164, 167, 168, 169, 170, 221, 227, 229, 232, 274, 314], "background": [1, 15, 74, 89], "backpressur": [7, 47, 48, 169, 171, 212, 213, 217], "backpropag": [76, 97], "backward": [4, 5, 6, 31, 32, 36, 40, 41, 74, 76, 89, 97, 238, 239, 247, 249, 253, 257, 260, 273, 274, 276, 277, 279, 280, 283, 294, 300, 310, 323, 326, 340], "bad": [73, 74, 82, 89], "bai": [74, 89], "bake": [2, 21], "balanc": [2, 10, 25, 68, 75, 90, 92, 100, 105, 107, 110, 111, 117, 134, 135, 147, 148, 183, 187, 195, 203, 204, 206, 210, 211, 212, 213, 218, 221, 223, 224, 226, 229, 235, 278, 332], "bale": [73, 82], "ball": [73, 82], "band": [73, 82], "bandwidth": [212, 213, 215], "bank": [74, 89], "bar": [274, 275, 279, 307, 316, 338], "barca": [73, 82], "barcelona": [73, 82, 229, 232], "barh": [275, 318], "barr": [74, 89], "barrier": [277, 278, 329, 335], "base": [3, 4, 5, 6, 7, 11, 28, 30, 34, 35, 36, 41, 43, 45, 46, 48, 53, 58, 59, 62, 72, 73, 74, 75, 76, 82, 84, 88, 92, 97, 99, 107, 108, 109, 110, 126, 127, 136, 137, 149, 150, 162, 164, 168, 169, 172, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195, 198, 203, 206, 210, 212, 213, 217, 218, 229, 231, 232, 235, 236, 239, 241, 259, 263, 264, 270, 273, 274, 276, 277, 278, 280, 289, 292, 306, 312, 314, 315, 316, 318, 325, 326, 327, 332, 333, 342], "base_dir": [274, 314], "base_model_id": [229, 232], "base_s3_path": [229, 232], "base_url": [183, 195, 212, 213, 219, 221, 225, 226, 229, 232, 233, 234], "baselin": [6, 40, 239, 257, 273, 278, 279, 281, 336, 339], "basemodel": [3, 26, 28, 229, 233], "bash": [11, 72, 163, 176], "bash_profil": [11, 72], "basic": [1, 4, 6, 9, 10, 12, 29, 31, 38, 40, 41, 64, 71, 100, 104, 107, 110, 162, 163, 169, 170, 176, 182, 188, 194, 229, 230, 231, 236, 238, 239, 240, 246, 255, 257, 259, 265, 276, 321, 343], "basicblock": [238, 247, 253], "basicvariantgener": [6, 41, 239, 259, 260], "bastion": [100, 105], "batch": [4, 5, 6, 10, 17, 22, 26, 31, 32, 35, 36, 39, 47, 48, 52, 55, 57, 60, 61, 62, 68, 70, 76, 77, 83, 94, 95, 97, 98, 169, 171, 181, 182, 185, 186, 193, 194, 198, 200, 205, 207, 217, 218, 220, 237, 238, 239, 241, 244, 245, 249, 250, 256, 264, 268, 269, 270, 273, 277, 278, 279, 280, 283, 284, 287, 289, 292, 293, 294, 295, 298, 300, 305, 306, 315, 317, 319, 320, 327, 328, 333, 334, 335, 336, 337, 338, 340, 343], "batch_df": [277, 327], "batch_first": [276, 322], "batch_format": [8, 52, 274, 275, 276, 277, 278, 279, 314, 316, 318, 319, 325, 327, 333, 338], "batch_idx": [5, 35, 277, 278, 328, 334], "batch_pr": [9, 62, 240, 264], "batch_siz": [2, 4, 5, 6, 8, 9, 24, 31, 32, 35, 36, 39, 40, 41, 52, 60, 61, 62, 73, 76, 82, 83, 97, 238, 239, 240, 241, 247, 249, 250, 253, 256, 257, 260, 264, 269, 273, 274, 275, 276, 277, 278, 279, 283, 287, 294, 295, 300, 308, 309, 310, 311, 314, 316, 321, 323, 325, 329, 335, 340], "batch_size_per_work": [5, 36, 76, 97, 98], "batchnorm2d": [238, 247, 253], "bathtub": [74, 89], "bathtuby": [74, 89], "batman": [73, 82], "batteri": [237, 242], "battl": [73, 74, 80, 82, 83, 88, 89], "battleship": [74, 89], "bayesian": [6, 41, 239, 259], "bbc": [73, 82], "bc": [278, 336], "bd1": [73, 82], "beach": [229, 232], "bearer": 173, "beast": [73, 82], "beauti": [73, 74, 75, 82, 88, 89, 93], "bebr": [74, 89], "becaus": [2, 6, 8, 20, 41, 53, 74, 85, 88, 89, 107, 109, 111, 116, 126, 134, 136, 146, 147, 164, 168, 178, 190, 212, 213, 216, 219, 239, 258, 274, 275, 277, 307, 312, 319, 327, 329, 330], "becom": [2, 7, 8, 18, 24, 48, 51, 73, 82, 164, 168, 273, 277, 284, 292, 326], "bee": [73, 80, 82], "been": [7, 47, 74, 88, 89, 178, 179, 184, 190, 191, 196, 275, 317], "befor": [2, 3, 4, 5, 6, 8, 9, 24, 26, 28, 32, 36, 40, 41, 54, 64, 74, 75, 85, 92, 111, 112, 118, 120, 126, 127, 136, 138, 149, 151, 152, 161, 169, 170, 177, 188, 189, 212, 213, 216, 237, 238, 239, 240, 244, 245, 251, 257, 260, 265, 273, 274, 275, 276, 277, 279, 281, 289, 293, 294, 297, 298, 300, 301, 303, 306, 307, 308, 316, 321, 327, 338], "beforehand": [178, 190, 229, 232], "begin": [73, 74, 78, 89, 111, 112, 118, 120, 126, 127, 136, 138, 149, 151, 169, 170, 178, 188, 190, 229, 235, 279, 340], "beginn": 77, "behalf": [163, 175], "behavior": [2, 19, 163, 164, 166, 169, 172, 175, 179, 191, 229, 231, 232, 273, 274, 278, 279, 299, 307, 310, 312, 336, 338], "behaviour": [275, 277, 318, 327], "behind": [74, 88, 89, 107, 110, 212, 213, 217, 279, 337], "being": [4, 7, 32, 47, 74, 75, 85, 88, 89, 93, 179, 181, 191, 193, 238, 241, 251, 270, 274, 277, 307, 327], "believ": [74, 88], "belong": [274, 306], "below": [1, 2, 4, 6, 7, 16, 20, 25, 31, 32, 41, 43, 107, 109, 169, 171, 178, 179, 181, 182, 183, 188, 190, 191, 193, 194, 195, 202, 209, 212, 213, 217, 221, 227, 239, 259, 273, 274, 275, 277, 283, 291, 314, 319, 331], "ben": [73, 74, 80, 82, 83, 88], "benchmark": [233, 274, 275, 306, 319], "benefit": [2, 3, 6, 9, 11, 19, 27, 40, 61, 72, 100, 105, 236, 237, 239, 243, 257], "bergman": [74, 85], "berni": [73, 82], "bert": [76, 77, 94, 97, 98, 99], "besok": [73, 82], "best": [3, 4, 6, 8, 9, 28, 32, 41, 54, 64, 73, 74, 76, 82, 83, 88, 89, 94, 100, 103, 118, 122, 164, 167, 179, 191, 212, 229, 232, 235, 237, 239, 240, 244, 245, 258, 260, 265, 273, 274, 275, 276, 278, 279, 280, 289, 291, 296, 305, 306, 312, 314, 317, 319, 323, 325, 336, 342], "best_ckpt": [274, 275, 276, 277, 278, 311, 317, 318, 319, 323, 329, 331, 335, 336], "best_ckpt_path": [274, 276, 314, 325], "best_result": [6, 41, 239, 258, 260], "better": [2, 6, 7, 9, 22, 40, 46, 61, 73, 74, 75, 82, 85, 87, 88, 90, 181, 183, 193, 195, 229, 231, 239, 257, 273, 275, 277, 278, 279, 305, 318, 331, 336, 342], "between": [2, 6, 7, 8, 9, 10, 19, 39, 41, 43, 45, 46, 51, 54, 57, 61, 64, 68, 74, 85, 86, 100, 101, 102, 105, 107, 109, 136, 146, 162, 164, 167, 168, 180, 192, 212, 213, 217, 221, 223, 229, 231, 232, 239, 240, 256, 259, 263, 265, 273, 275, 276, 279, 284, 316, 321, 337], "beyonc": [73, 82], "beyond": [73, 74, 82, 88, 184, 196, 229, 230, 234], "bf16": [5, 35, 36], "bfloat16": [274, 314], "bia": [4, 6, 31, 40, 41, 238, 239, 247, 253, 257, 260, 273, 275, 282, 316], "bias": [273, 274, 275, 276, 277, 278, 279, 305, 314, 319, 325, 331, 336, 342], "bidder": [74, 88], "bieber": [73, 82], "big": [7, 43, 47, 74, 76, 88, 94, 277, 278, 279, 326, 332, 337], "bigger": [73, 82], "bigl": [274, 306], "bigqueri": [7, 43], "bigr": [274, 306], "bill": [118, 120, 149, 151, 185, 198], "billion": [164, 168, 221, 223], "bin": [0, 149, 152, 279, 338], "binari": [237, 244, 245, 275, 315, 319], "bind": [3, 7, 10, 28, 43, 70, 75, 92, 169, 172, 183, 187, 195, 204, 206, 211, 237, 241, 244, 245, 269, 270], "birthdai": [73, 82], "bit": [2, 24, 74, 88, 89], "bitten": [74, 88], "bjork": [74, 89], "bjp": [73, 82], "black": [6, 39, 73, 74, 82, 89, 239, 256, 279, 338], "blair": [73, 82], "blank": [177, 189], "bless": [73, 82], "blind": [74, 89], "blob": [273, 274, 279, 289, 314, 338], "block": [1, 2, 4, 6, 10, 15, 16, 17, 23, 24, 32, 40, 51, 52, 53, 57, 60, 61, 62, 69, 70, 71, 73, 74, 78, 80, 87, 126, 134, 136, 147, 169, 171, 172, 212, 213, 219, 221, 225, 227, 229, 232, 233, 234, 238, 241, 252, 263, 269, 270, 273, 275, 277, 279, 290, 293, 294, 317, 327, 337, 338], "block_out_channel": [5, 35], "blockbust": [74, 89], "blog": [4, 5, 6, 32, 37, 42, 238, 254], "blow": [73, 82], "blue": [73, 80, 82, 212, 213, 216, 274, 277, 306, 326], "bn1": [238, 247, 253], "bn2": [238, 247, 253], "board": [74, 89], "boat": [74, 88], "bob": [73, 82], "bodi": [10, 70, 74, 85, 89, 241, 269], "boi": [74, 85, 89], "boilerpl": [273, 274, 275, 277, 278, 286, 289, 314, 315, 326, 332], "bomb": [74, 89], "bon": [73, 82], "book": [73, 74, 80, 82, 83, 88], "bookkeep": [274, 309], "bool": [274, 314], "boolean": [237, 244], "boost": [3, 28, 237, 244, 275, 315, 316, 317, 319], "booster": [3, 28, 237, 244, 245, 275, 317, 318, 319], "boot": [73, 82], "booth": [73, 82], "bootstrap": [107, 109], "border": [74, 88], "bore": [74, 88], "both": [0, 7, 9, 43, 48, 57, 73, 76, 82, 94, 98, 100, 105, 107, 109, 136, 140, 163, 164, 166, 169, 172, 175, 185, 188, 198, 221, 222, 229, 233, 273, 275, 277, 289, 291, 293, 296, 299, 303, 316, 317, 327], "boto3": [180, 192, 229, 232], "bottleneck": [163, 164, 168, 175, 275, 316], "bottom": [2, 24, 184, 196], "bound": [17, 100, 105], "boundari": [7, 46], "bouquet": [73, 82], "bout": [73, 80, 82, 83], "box": [4, 32, 181, 184, 193, 196, 273, 280], "br": [74, 85, 88, 89], "brain": [74, 88], "branch": 0, "brand": [229, 233], "braun": [73, 82], "break": [5, 6, 35, 39, 74, 89, 181, 193, 239, 256, 274, 308], "breakdown": [7, 43], "breakneck": [74, 89], "breakpoint": [76, 97], "breez": [74, 85], "brennan": [74, 88], "brew": [3, 26, 111, 112, 126, 127, 136, 138, 149, 151, 163, 176], "bridg": [7, 43, 74, 89, 240, 241, 262, 271, 273, 284], "brief": [229, 232], "bring": [3, 27, 73, 74, 80, 82, 88, 89, 273, 275, 290, 316], "brit": [73, 82], "british": [73, 74, 82, 89], "broadcast": [273, 280], "broader": [7, 47], "brock": [73, 82], "brought": [74, 88], "brown": [73, 74, 82, 85], "brows": 173, "browser": [0, 11, 72, 118, 121, 178, 190], "bryant": [73, 82], "bst": [3, 28], "bubbl": [73, 82], "bucket": [100, 104, 105, 106, 111, 113, 116, 117, 118, 119, 122, 125, 126, 128, 134, 136, 139, 146, 147, 149, 153, 161, 180, 192, 229, 232], "bucket_nam": [118, 122, 229, 232], "budget": [229, 235], "buf": [274, 277, 307, 327], "buffalo": [73, 82], "buffer": [7, 43, 212, 213, 217, 273, 295], "bug": [163, 175], "bui": [73, 74, 82, 88], "build": [1, 2, 3, 5, 7, 8, 9, 10, 11, 16, 17, 27, 28, 32, 35, 43, 46, 48, 50, 57, 60, 67, 69, 70, 71, 72, 74, 89, 100, 104, 177, 179, 188, 189, 191, 221, 228, 229, 231, 234, 236, 237, 241, 243, 251, 267, 268, 270, 274, 276, 277, 278, 279, 281, 283, 286, 294, 296, 308, 310, 314, 315, 316, 321, 325, 326, 327, 332, 337, 338, 342], "build_app": [10, 71], "build_data_load": [6, 39, 40, 239, 256, 257], "build_data_loader_ray_train": [4, 32, 238, 249, 250, 253, 273, 283, 287], "build_data_loader_ray_train_ray_data": [273, 294, 295, 300], "build_data_loader_torch": [4, 31, 238, 247], "build_dataload": [274, 276, 309, 310, 321, 323], "build_inference_dataset": [274, 314], "build_openai_app": [212, 213, 219, 221, 224, 227, 229, 232, 233, 234], "build_resnet18": [4, 31, 32, 238, 247, 250, 253, 273, 282, 286, 292], "builder": [10, 71], "built": [0, 1, 3, 4, 5, 7, 8, 13, 27, 30, 32, 34, 36, 43, 50, 75, 90, 107, 110, 111, 117, 118, 122, 163, 164, 166, 173, 176, 178, 179, 181, 190, 191, 193, 212, 213, 217, 237, 241, 243, 269, 271, 273, 274, 275, 276, 278, 279, 280, 282, 294, 306, 310, 314, 316, 317, 318, 319, 325, 332, 335, 341], "bulk": [182, 186, 194, 200, 205, 207], "bundl": [179, 191], "bunni": [74, 85], "burden": 188, "burrito": [274, 306], "bursti": [177, 179, 189, 191, 212, 213, 217], "busi": [7, 10, 43, 69, 75, 92, 229, 234], "button": [178, 179, 183, 184, 190, 191, 195, 196], "bx1": [277, 328], "bx3xhxw": [277, 328], "bypass": [2, 19], "bystand": [74, 89], "byte": [9, 61, 169, 171, 274, 277, 307, 327, 331], "bytesio": [274, 277, 307, 308, 314, 327], "byth": [180, 192], "c": [2, 10, 11, 22, 70, 72, 73, 74, 82, 88, 179, 180, 191, 192, 273, 274, 275, 276, 279, 292, 312, 314, 316, 317, 323, 340], "cab": [3, 8, 28, 51, 54, 169, 171, 237, 244], "cabin": [74, 89], "cabl": [74, 85], "cach": [2, 18, 100, 104, 179, 191, 215, 217, 220, 221, 227, 240, 264, 273, 274, 275, 276, 277, 278, 279, 305, 307, 308, 314, 316, 321, 327, 333, 338, 342], "cactu": [73, 80, 82], "caesar": [274, 306], "cahse": [73, 82], "calcul": [4, 8, 32, 52, 76, 96, 97, 181, 193, 238, 249, 273, 283], "call": [2, 3, 4, 5, 6, 8, 9, 12, 15, 19, 24, 25, 28, 32, 36, 41, 54, 55, 60, 61, 64, 74, 76, 83, 89, 99, 169, 172, 173, 183, 195, 213, 220, 230, 231, 236, 238, 239, 240, 251, 252, 258, 260, 265, 273, 274, 275, 277, 279, 281, 283, 286, 289, 290, 294, 298, 300, 301, 302, 303, 307, 311, 316, 319, 327, 329, 330, 340], "call_id": [229, 234], "callabl": [9, 10, 62, 71, 73, 78, 81, 83, 229, 234, 240, 264], "callback": [5, 36, 275, 277, 278, 317, 319, 328, 329, 335, 336], "caller": [73, 82], "came": [73, 82], "camera": [74, 88, 89], "campu": [73, 82], "can": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 28, 30, 31, 32, 34, 35, 36, 38, 40, 41, 44, 46, 47, 48, 49, 51, 52, 53, 54, 56, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 92, 94, 97, 98, 99, 100, 103, 105, 107, 109, 110, 111, 112, 113, 115, 116, 117, 118, 122, 124, 125, 126, 127, 128, 133, 134, 135, 136, 138, 139, 140, 145, 146, 147, 148, 149, 151, 160, 161, 163, 164, 166, 168, 169, 170, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 202, 204, 206, 207, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 225, 226, 227, 229, 230, 231, 232, 234, 237, 238, 239, 240, 241, 242, 247, 251, 253, 257, 259, 260, 263, 264, 265, 266, 268, 269, 270, 273, 280, 281, 284, 285, 288, 290, 291, 292, 293, 296, 297, 299, 300, 301, 303, 305, 306, 307, 308, 315, 316, 320, 321, 324, 326, 327, 328, 329, 332, 334, 337, 338], "canadian": [74, 88], "cancel": [74, 88], "candid": [229, 232, 277, 278, 329, 335, 336], "cannon": [74, 89], "cannot": [2, 11, 18, 72, 74, 85, 111, 114, 116, 118, 123, 126, 130, 136, 142, 149, 158, 179, 191], "canopi": [275, 319], "cant": [74, 89], "canva": [4, 5, 32, 37, 238, 254], "capabl": [7, 10, 43, 47, 69, 75, 76, 90, 98, 107, 110, 164, 165, 167, 173, 178, 180, 190, 192, 221, 223, 228, 229, 230, 231, 234, 235, 236], "capac": [111, 113, 126, 128, 136, 139, 179, 191, 212, 213, 215, 216, 279, 337], "capit": [212, 213, 219, 229, 232], "captain": [74, 88], "caption_lat": [5, 35], "captur": [169, 172, 184, 196, 276, 277, 279, 320, 325, 329, 340], "car_typ": [229, 233], "card": [8, 51, 221, 224], "cardescript": [229, 233], "cardiffnlp": [73, 80], "care": [74, 88, 89, 273, 276, 289, 321], "carli": [73, 82], "carriag": [74, 89], "carrow": [73, 82], "cartograph": [275, 315], "cartpol": [278, 336], "cartyp": [229, 233], "case": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 19, 25, 27, 28, 32, 36, 37, 41, 42, 46, 54, 55, 62, 64, 71, 73, 74, 76, 82, 83, 89, 97, 107, 109, 111, 113, 126, 128, 136, 139, 164, 167, 179, 181, 191, 193, 212, 213, 216, 221, 223, 228, 231, 232, 236, 237, 238, 239, 240, 243, 244, 248, 254, 258, 264, 265, 273, 275, 276, 279, 301, 315, 323, 337], "cash": [8, 51], "castl": [74, 88], "casual": [73, 82], "cat": [276, 277, 278, 323, 328, 334], "catalog": [279, 337], "catch": [73, 82], "categor": [7, 44, 46, 181, 193], "categori": [9, 60, 76, 94, 107, 109, 274, 306], "cattl": [74, 88], "cattleman": [74, 88], "caus": [4, 5, 17, 30, 34, 111, 113, 126, 128, 136, 139, 164, 168, 273, 280], "cd": [0, 10, 71, 111, 115, 118, 124, 126, 133, 136, 145, 149, 160, 164, 168, 169, 172, 182, 183, 186, 187, 188, 194, 195, 200, 204, 205, 206, 207, 211, 229, 236, 274, 314], "cdot": [277, 278, 326, 332], "ceil_mod": [238, 247, 253], "cell": [3, 4, 5, 6, 8, 9, 10, 28, 29, 32, 33, 37, 42, 55, 66, 71, 178, 179, 180, 182, 186, 190, 191, 192, 194, 202, 205, 209, 238, 254, 273, 276, 279, 281, 292, 321, 338], "celsiu": [2, 25, 229, 234], "cena": [73, 80, 82], "center": [73, 74, 82, 85, 273, 274, 277, 287, 306, 307, 327], "center_input_sampl": [5, 35], "centercrop": [274, 277, 307, 327], "central": [164, 167, 229, 232, 273, 289], "centric": [7, 46], "ceph": [7, 43], "cerebr": [74, 88], "cert": [107, 110], "certain": [4, 5, 6, 8, 29, 33, 41, 52, 74, 75, 85, 92, 239, 259], "certif": [107, 110], "chain": [17, 240, 265], "chair": [73, 82], "chalk": [74, 89], "challeng": [4, 5, 9, 10, 30, 34, 57, 68, 188, 214, 220, 273, 280, 293], "chanc": [2, 20, 73, 82], "chang": [2, 4, 5, 6, 7, 9, 10, 18, 30, 34, 36, 41, 43, 65, 68, 71, 73, 74, 76, 82, 88, 98, 111, 113, 118, 122, 126, 128, 136, 139, 146, 164, 166, 169, 172, 180, 181, 183, 187, 192, 193, 195, 204, 206, 211, 221, 226, 228, 229, 232, 239, 260, 273, 274, 275, 276, 277, 278, 279, 280, 306, 313, 314, 315, 325, 326, 329, 331, 332, 337], "channel": [9, 61, 74, 89, 164, 168, 169, 172, 238, 240, 241, 247, 264, 270, 273, 274, 276, 277, 281, 282, 292, 306, 325, 326, 327, 328], "chao": [74, 89], "chap": [111, 112, 126, 127, 136, 138], "charact": [74, 75, 89, 93, 212, 213, 215], "characterist": [7, 8, 10, 43, 51, 69, 212, 213, 215, 240, 263], "charg": [8, 51], "charli": [73, 82], "charm": [73, 74, 82, 88], "chart": [126, 129, 131, 136, 141, 143, 149, 159, 275, 316], "chase": [73, 80, 82], "chat": [212, 213, 216, 219, 221, 225, 226, 229, 232, 233, 234, 235], "chatbot": [229, 234, 235], "cheap": [73, 82], "cheaper": [7, 45], "cheapli": [74, 85], "cheat": [74, 88], "check": [2, 4, 5, 8, 9, 10, 22, 31, 32, 36, 51, 53, 61, 62, 70, 73, 76, 82, 97, 111, 115, 118, 124, 126, 132, 133, 136, 144, 145, 146, 149, 154, 160, 169, 171, 173, 180, 181, 182, 183, 184, 192, 193, 194, 195, 196, 203, 210, 212, 213, 217, 229, 234, 238, 253, 273, 275, 278, 279, 281, 289, 290, 300, 316, 319, 329, 335, 338], "check_cal": [274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338], "check_val_every_n_epoch": [277, 278, 329, 335], "checkout": [181, 183, 193, 195], "checkpoint": [8, 9, 30, 34, 35, 55, 57, 76, 99, 180, 192, 237, 239, 244, 245, 246, 249, 252, 253, 258, 274, 280, 281, 282, 283, 290, 291, 294, 298, 299, 302, 304, 305, 306, 307, 310, 311, 312, 313, 314, 315, 317, 320, 321, 323, 325, 326, 327, 329, 332, 333, 336, 337, 338, 342], "checkpoint_": [274, 314], "checkpoint_000000": [238, 253], "checkpoint_000001": [238, 253], "checkpoint_at_end": [276, 323], "checkpoint_config": [274, 275, 276, 277, 278, 279, 311, 317, 323, 329, 335, 340], "checkpoint_dir": [274, 314], "checkpoint_dir_nam": [238, 253], "checkpoint_frequ": [274, 275, 277, 278, 311, 317, 329, 335], "checkpoint_nam": [275, 317], "checkpoint_path": [4, 5, 31, 36, 238, 247, 274, 276, 314, 325], "checkpoint_root": [274, 314], "checkpoint_score_": [276, 323], "checkpoint_score_attribut": [274, 275, 276, 277, 278, 311, 317, 323, 329, 335], "checkpoint_score_ord": [274, 275, 276, 277, 278, 311, 317, 323, 329, 335], "checkpointconfig": [274, 275, 276, 277, 278, 279, 307, 311, 315, 316, 317, 321, 323, 326, 327, 329, 332, 333, 335, 337, 338, 340], "cheekbon": [73, 82], "chelsea": [73, 82], "cheri": [73, 82], "chill": [73, 82], "chip": [11, 72], "chloe": [74, 85], "chmod": [163, 176], "choic": [7, 8, 48, 55, 75, 90, 107, 109, 212, 213, 219, 221, 225, 226, 229, 232, 233, 234, 276, 320], "choos": [9, 11, 63, 72, 74, 89, 100, 101, 103, 136, 146, 149, 156, 177, 178, 179, 189, 190, 191, 212, 213, 217, 230, 236, 240, 264, 279, 342], "choreo": [73, 82], "chose": [4, 32, 238, 251], "chown": [163, 176], "chri": [73, 82], "chrisbrown": [73, 82], "christian": [73, 82], "chromadb": [7, 43], "chronolog": [164, 166], "chuck": [73, 82], "chunk": [7, 9, 46, 64, 181, 193, 212, 213, 219, 221, 225, 226, 229, 232, 240, 265], "church": [73, 82], "churn": [276, 325], "chw": [277, 327], "ci": [182, 186, 194, 200, 205, 207, 229, 236, 273, 274, 305, 314], "ciara": [73, 82], "cidr": [100, 105], "cidr_block": [100, 105], "cif": [73, 82], "cifar": [273, 298, 301, 303], "cifar10": [273, 296, 304], "cinema": [74, 85, 88, 89], "cinemat": [74, 89], "cinematograph": [74, 89], "cinematographi": [74, 88, 89], "cineworld": [73, 82], "citi": [3, 8, 28, 51, 73, 74, 82, 89, 229, 234, 237, 244, 276, 320], "citizenship": [73, 82], "ckpt": [4, 5, 32, 36, 238, 253, 274, 275, 276, 277, 278, 279, 310, 317, 318, 323, 325, 326, 329, 331, 332, 335, 336, 340], "ckpt_dir": [4, 5, 32, 36, 238, 253, 273, 274, 276, 277, 278, 279, 292, 300, 310, 314, 323, 325, 331, 336, 340, 342], "ckpt_file": [277, 278, 331, 336], "ckpt_out": [274, 276, 279, 310, 323, 340], "ckpt_path": [5, 36, 277, 278, 329, 335], "ckpt_root": [277, 278, 329, 335], "claim": [7, 47, 74, 85, 88], "clamp": [277, 331], "clarifi": [185, 197], "class": [2, 3, 4, 5, 6, 9, 10, 25, 28, 32, 35, 36, 39, 41, 59, 62, 70, 75, 78, 82, 83, 92, 169, 172, 229, 233, 237, 239, 240, 241, 244, 245, 256, 258, 259, 263, 264, 269, 270, 273, 274, 276, 277, 278, 279, 282, 290, 292, 296, 306, 307, 308, 314, 315, 318, 321, 322, 325, 326, 327, 328, 331, 334, 339], "class_nam": [2, 25], "classic": [275, 278, 279, 315, 332, 336, 337], "classif": [3, 28, 67, 76, 94, 95, 97, 99, 237, 240, 244, 264, 273, 281, 282, 283, 310], "classifi": [6, 10, 40, 70, 76, 94, 239, 257, 267, 274, 306], "classmat": [74, 85], "claud": [229, 235], "cld": [180, 192, 221, 226], "cld_g54aiirwj1s8t9ktgzikqur41k": [180, 192], "cldrsrc_12345abcdefgh67890ijklmnop": [126, 130, 131, 136, 142, 143, 149, 158, 159], "clean": [0, 10, 69, 74, 89, 111, 116, 118, 125, 149, 161, 276, 289, 306, 307, 310, 313, 323, 325, 326, 328, 329, 338], "cleaner": [273, 281], "cleanli": [273, 279, 289, 338], "cleanup": [3, 4, 5, 6, 8, 9, 10, 28, 32, 37, 42, 55, 66, 71, 107, 109, 162, 182, 194, 237, 238, 240, 244, 245, 254, 266, 273, 274, 275, 277, 278, 279, 281, 304, 314, 319, 331, 336, 342], "clear": [0, 74, 88, 185, 199, 274, 275, 276, 278, 306, 314, 319, 325, 336], "clearli": [74, 89, 169, 170], "cli": [100, 101, 111, 112, 118, 120, 126, 127, 136, 138, 149, 151, 157, 169, 172, 173, 178, 180, 181, 182, 190, 192, 193, 194], "click": [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 16, 17, 25, 26, 29, 32, 33, 36, 38, 41, 49, 56, 61, 67, 73, 74, 75, 76, 78, 84, 90, 94, 100, 105, 111, 112, 126, 127, 136, 138, 146, 149, 151, 163, 169, 171, 172, 176, 177, 178, 179, 180, 181, 183, 184, 189, 190, 191, 192, 193, 195, 196, 212, 213, 214, 221, 222, 227, 229, 230, 238, 239, 253, 259], "client": [90, 180, 192, 212, 213, 218, 219, 221, 225, 226, 229, 232, 233, 234], "cliff": [73, 82], "clipboard": [183, 195], "clitori": [74, 85], "clock": [74, 89], "clog": [74, 89], "clone": [0, 183, 195, 278, 336], "close": [73, 74, 82, 89, 273, 289], "cloud": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 17, 26, 29, 32, 33, 38, 49, 56, 67, 72, 73, 74, 75, 76, 78, 84, 90, 94, 98, 101, 104, 105, 107, 109, 110, 112, 113, 115, 116, 117, 119, 120, 122, 124, 125, 127, 128, 131, 132, 133, 134, 135, 137, 139, 143, 144, 145, 147, 148, 150, 151, 153, 159, 160, 161, 162, 164, 168, 177, 179, 184, 187, 188, 189, 191, 196, 199, 203, 206, 210, 212, 213, 214, 218, 219, 221, 222, 226, 229, 230, 232, 273, 274, 275, 277, 279, 280, 289, 305, 306, 315, 331, 342], "cloud_deployment_id": [126, 131, 136, 143, 149, 159], "cloud_nam": [126, 133, 136, 145], "clouddeploymentid": [126, 128, 131, 136, 139, 143, 149, 159], "cloudflar": [7, 43], "cloudform": [100, 105], "cloudfound": [100, 105, 118, 122], "cloudprovid": [126, 128, 131, 136, 139, 143, 149, 159], "cloudresourcemanag": [118, 121, 149, 152], "cloudwatch": [100, 105], "club": [73, 74, 80, 82, 88], "cluster": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 19, 21, 23, 25, 26, 28, 29, 30, 33, 34, 38, 46, 49, 53, 55, 56, 57, 59, 60, 61, 62, 63, 66, 67, 69, 72, 74, 78, 80, 82, 84, 90, 94, 98, 100, 102, 103, 104, 105, 107, 108, 109, 110, 111, 113, 115, 116, 117, 118, 124, 125, 128, 132, 133, 134, 135, 138, 139, 140, 144, 145, 146, 147, 148, 155, 160, 161, 162, 164, 166, 167, 169, 170, 175, 177, 178, 179, 181, 182, 183, 184, 186, 187, 189, 190, 191, 193, 194, 195, 196, 200, 202, 203, 205, 206, 207, 209, 210, 212, 213, 214, 218, 219, 221, 222, 226, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 252, 258, 260, 262, 263, 264, 268, 269, 274, 275, 276, 277, 278, 279, 280, 281, 282, 285, 287, 289, 290, 291, 292, 293, 296, 297, 298, 301, 305, 306, 311, 314, 315, 316, 319, 320, 326, 327, 331, 332, 333, 336, 337, 338, 340, 342], "cluster_id": [181, 193], "cluster_storag": [3, 4, 5, 8, 9, 10, 28, 31, 36, 37, 53, 62, 70, 169, 171, 180, 192, 237, 238, 240, 241, 244, 245, 252, 253, 254, 264, 266, 269, 270, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 287, 289, 291, 296, 304, 306, 307, 308, 310, 311, 314, 316, 317, 319, 320, 321, 327, 329, 331, 335, 336, 338, 340, 342], "clusternam": [126, 129, 136, 141], "clusteronc": [111, 115], "cm": [275, 318], "cm_norm": [275, 318], "cmap": [4, 6, 9, 31, 32, 39, 60, 238, 239, 241, 247, 253, 256, 270, 273, 275, 281, 292, 318], "cnn": [273, 274, 277, 281, 306, 328, 331], "co": [229, 232, 276, 278, 322, 332, 333, 336], "coach": [73, 82], "coars": [10, 69], "cocki": [74, 88], "code": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 22, 25, 26, 30, 32, 34, 36, 41, 51, 70, 71, 72, 73, 75, 82, 90, 100, 101, 162, 164, 168, 169, 172, 178, 179, 180, 181, 182, 183, 184, 188, 190, 191, 192, 193, 194, 195, 196, 200, 202, 207, 209, 221, 226, 228, 231, 235, 238, 239, 241, 253, 259, 260, 269, 273, 274, 275, 276, 277, 278, 279, 280, 283, 284, 285, 286, 287, 288, 293, 305, 306, 314, 315, 316, 320, 321, 325, 326, 332, 337, 342], "coder": [229, 235], "cogent": [74, 85], "coher": [74, 88, 89], "coi": [74, 88], "col": [4, 5, 31, 32, 35, 273, 274, 276, 279, 281, 292, 312, 323, 340], "colbert": [73, 82], "cold": [212, 213, 219], "collabor": [185, 198, 199, 279, 337, 339, 342, 343], "collat": [5, 35, 76, 97], "collate_fn": [76, 97], "colleagu": [73, 82], "collect": [1, 2, 8, 9, 13, 16, 18, 51, 59, 64, 73, 82, 100, 102, 163, 175, 176, 241, 268, 273, 274, 275, 278, 281, 288, 290, 292, 310, 317, 318, 335, 336, 343], "collector": [273, 292], "collison": [73, 83], "colour": [74, 89], "column": [3, 5, 8, 9, 28, 35, 51, 54, 60, 73, 74, 81, 86, 89, 178, 190, 273, 274, 275, 276, 277, 278, 279, 291, 296, 307, 308, 312, 314, 316, 317, 321, 323, 325, 327, 329, 331, 335, 338, 340, 342], "column_nam": [275, 317], "column_stack": [275, 317], "columnar": [7, 43, 273, 274, 275, 276, 277, 296, 307, 316, 321, 327], "com": [0, 11, 72, 100, 105, 107, 110, 111, 112, 118, 120, 121, 122, 126, 127, 136, 138, 146, 149, 151, 152, 153, 173, 181, 183, 186, 187, 188, 193, 195, 200, 203, 204, 205, 206, 207, 210, 211, 221, 226, 238, 239, 252, 258, 260, 276, 321], "combin": [2, 7, 22, 43, 74, 88, 89, 273, 277, 289, 326], "combur": [73, 82], "comcast": [73, 82], "come": [2, 21, 73, 74, 82, 85, 88, 89, 107, 109, 179, 181, 188, 191, 193, 274, 275, 276, 314, 319, 325], "comedi": [74, 89], "comfort": [274, 314], "command": [100, 103, 106, 111, 113, 114, 123, 125, 126, 128, 129, 130, 134, 136, 139, 141, 142, 147, 149, 152, 158, 161, 163, 164, 168, 169, 172, 176, 178, 181, 182, 183, 190, 193, 194, 195, 202, 209, 278, 336], "commend": [74, 85], "comment": [75, 92, 183, 187, 195, 204, 206, 211], "commerci": [7, 43, 45], "commiss": [3, 8, 28, 51, 237, 244], "commit": [274, 277, 307, 327], "common": [1, 2, 3, 8, 9, 13, 25, 27, 51, 53, 59, 62, 73, 74, 82, 86, 88, 89, 136, 146, 149, 154, 163, 169, 172, 175, 229, 236], "common_prefix": [118, 122], "commonli": [7, 9, 43, 46, 58, 279, 339], "commun": [2, 7, 8, 9, 25, 47, 54, 64, 100, 105, 212, 213, 216, 221, 227, 228, 229, 236, 240, 265, 276, 321], "compact": [10, 68, 274, 277, 306, 307, 327, 331], "compani": [107, 110, 185, 197], "compar": [6, 7, 41, 46, 164, 165, 179, 191, 212, 213, 215, 221, 223, 239, 240, 259, 262, 274, 276, 277, 278, 312, 314, 323, 331, 336], "comparison": [74, 89], "compat": [169, 170, 212, 213, 218, 221, 224, 229, 232, 273, 274, 295, 306], "compet": [7, 47], "competit": [73, 82], "compil": [11, 72, 77], "complet": [2, 3, 4, 5, 11, 23, 28, 30, 32, 34, 72, 74, 76, 89, 99, 107, 109, 111, 113, 126, 133, 136, 145, 164, 168, 173, 178, 181, 182, 186, 188, 190, 193, 194, 200, 205, 207, 212, 213, 215, 216, 219, 221, 222, 225, 226, 229, 232, 233, 234, 235, 236, 238, 252, 253, 273, 274, 277, 278, 280, 290, 300, 301, 303, 305, 312, 329, 330, 331, 335, 336], "complex": [3, 7, 10, 28, 43, 46, 47, 48, 68, 107, 109, 188, 212, 213, 216, 218, 221, 223, 228, 229, 231, 235, 241, 270, 277, 326], "compli": [5, 36], "complianc": [100, 103, 221, 226, 229, 236], "compliant": [100, 105], "compon": [1, 5, 6, 7, 10, 13, 36, 41, 43, 68, 100, 103, 105, 108, 135, 148, 149, 155, 164, 166, 167, 173, 179, 181, 191, 193, 212, 213, 217, 218, 220, 239, 259, 273, 274, 276, 281, 307, 321], "compos": [4, 5, 6, 9, 10, 29, 31, 32, 35, 38, 39, 41, 56, 61, 68, 69, 238, 239, 240, 247, 250, 256, 260, 264, 273, 274, 277, 281, 287, 297, 307, 308, 314, 327], "composit": [7, 48, 74, 89], "comprehens": [8, 9, 11, 51, 53, 57, 65, 72, 100, 104, 149, 150, 162, 173, 212, 213, 214, 221, 227, 228, 229, 231, 232, 233, 234, 236, 240, 263, 266], "compress": [275, 276, 316, 325], "comput": [1, 2, 3, 4, 5, 8, 9, 10, 13, 15, 16, 17, 18, 22, 25, 28, 30, 31, 32, 34, 36, 43, 45, 47, 54, 55, 57, 61, 63, 64, 66, 68, 73, 74, 75, 76, 78, 86, 90, 94, 96, 97, 98, 100, 103, 105, 107, 109, 110, 111, 117, 121, 122, 125, 126, 128, 133, 136, 139, 145, 149, 152, 153, 154, 161, 162, 177, 178, 181, 182, 183, 185, 186, 187, 188, 189, 190, 193, 194, 195, 198, 199, 200, 204, 205, 206, 207, 211, 212, 213, 215, 216, 218, 219, 237, 240, 241, 242, 243, 244, 245, 247, 249, 263, 264, 265, 268, 270, 273, 275, 279, 280, 282, 283, 294, 310, 314, 317, 318, 319, 327, 337, 340, 342, 343], "computation": [6, 40, 239, 257], "compute_accuraci": [9, 64, 240, 265], "compute_config": [126, 133, 136, 145, 164, 168, 212, 213, 219, 221, 226], "compute_metr": [76, 96], "compute_nodes_service_account_email": [118, 122, 149, 153], "compute_tip_percentag": [8, 52], "computeconfig": [126, 133, 136, 145], "con": [73, 82], "conc": [118, 124], "concat_t": [275, 317], "concept": [7, 9, 45, 56, 107, 109, 162, 169, 170, 214, 255, 273, 282], "conceptu": [4, 31], "concern": [1, 10, 13, 69, 74, 88], "concert": [73, 82], "concis": [229, 232], "conclus": [74, 84, 230], "concomit": [73, 82], "concret": [100, 101], "concurr": [2, 3, 7, 8, 10, 22, 28, 48, 55, 62, 69, 73, 74, 78, 82, 87, 212, 213, 216, 228, 237, 240, 241, 244, 245, 264, 269, 273, 274, 276, 283, 314, 325], "concurrency_limit": [9, 61], "concuss": [73, 80, 82, 83], "conda": [163, 176], "condit": [2, 5, 18, 35, 74, 84, 100, 105, 229, 232, 277, 331], "conductor": [74, 89], "confid": [273, 275, 277, 278, 279, 299, 319, 331, 336, 342], "config": [3, 4, 5, 6, 10, 28, 32, 36, 41, 71, 76, 97, 118, 121, 149, 152, 163, 169, 172, 176, 237, 238, 239, 241, 244, 245, 249, 253, 258, 259, 260, 270, 273, 274, 275, 276, 277, 278, 279, 282, 283, 284, 285, 290, 294, 300, 303, 307, 310, 317, 323, 329, 332, 335, 340], "configur": [6, 9, 10, 17, 35, 41, 57, 61, 69, 71, 75, 76, 92, 94, 95, 97, 98, 99, 100, 102, 103, 104, 105, 107, 108, 110, 111, 112, 113, 117, 120, 122, 126, 127, 128, 133, 136, 138, 139, 145, 151, 153, 154, 163, 164, 168, 176, 177, 181, 182, 183, 186, 189, 193, 194, 195, 200, 202, 203, 205, 207, 209, 210, 218, 220, 222, 228, 231, 236, 239, 241, 260, 268, 274, 276, 277, 278, 279, 280, 281, 282, 284, 299, 303, 305, 306, 310, 314, 315, 320, 323, 329, 335, 337, 340, 341], "configure_optim": [5, 35, 277, 278, 328, 334], "confirm": [183, 195, 229, 232, 273, 274, 275, 276, 277, 279, 281, 290, 303, 307, 316, 319, 321, 327, 329, 338, 340], "conflict": [273, 292], "confluent": [7, 46], "confus": [9, 61, 274, 314], "confusion_matrix": [275, 316, 318], "congratul": [221, 228, 229, 236], "congress": [73, 82], "conjur": [74, 89], "connect": [2, 4, 5, 9, 19, 30, 34, 58, 73, 74, 80, 89, 100, 105, 107, 110, 111, 117, 126, 129, 136, 141, 149, 155, 177, 178, 180, 185, 189, 190, 192, 198, 273, 275, 280, 298, 318], "connector": [8, 9, 55, 58], "consecut": [73, 80, 82], "consid": [1, 2, 4, 5, 16, 18, 20, 21, 22, 24, 32, 36, 67, 74, 85, 87, 88, 169, 170, 221, 228, 229, 235, 238, 248, 277, 327], "consider": [107, 109, 188], "consist": [6, 7, 39, 43, 77, 162, 177, 179, 189, 191, 229, 231, 233, 236, 239, 240, 256, 263, 273, 275, 276, 278, 280, 281, 282, 293, 317, 319, 321, 322, 336], "consol": [111, 114, 115, 118, 123, 124, 126, 130, 133, 136, 142, 145, 146, 149, 158, 160, 163, 169, 170, 173, 175, 178, 182, 183, 190, 194, 195, 202, 209, 229, 236, 279, 340], "conspicu": [73, 82], "constant": [5, 35, 74, 89, 212, 213, 216, 276, 325], "constraint": [5, 35, 73, 74, 83, 89, 212, 213, 217, 229, 235], "construct": [2, 5, 25, 35, 273, 274, 276, 278, 286, 287, 308, 309, 323, 332], "constructor": [2, 9, 10, 25, 62, 70, 73, 78, 240, 241, 264, 269], "consum": [6, 8, 9, 41, 51, 53, 58, 60, 111, 113, 126, 128, 136, 139, 239, 240, 259, 264, 273, 276, 293, 298, 305, 321], "consumptionapi": [9, 60, 64], "contain": [0, 2, 3, 4, 5, 9, 19, 28, 32, 36, 59, 74, 89, 107, 109, 110, 126, 134, 136, 147, 149, 152, 155, 177, 178, 180, 183, 189, 190, 192, 195, 229, 234, 237, 238, 240, 244, 253, 263, 273, 274, 275, 276, 279, 282, 291, 303, 306, 312, 316, 317, 321, 325, 338, 343], "container": [10, 68, 162], "containerfil": [212, 213, 219, 221, 226], "content": [9, 61, 173, 180, 188, 192, 212, 213, 219, 221, 225, 226, 229, 232, 233, 234, 274, 279, 314, 342], "context": [164, 166, 168, 215, 217, 221, 224, 227, 274, 276, 277, 278, 279, 306, 320, 329, 335, 342], "contextu": [163, 164, 167, 168, 175], "contigu": [9, 59, 279, 337, 338], "continu": [4, 7, 32, 48, 73, 74, 82, 89, 164, 168, 177, 180, 189, 192, 218, 220, 229, 235, 238, 251, 273, 274, 277, 278, 279, 300, 301, 302, 303, 307, 310, 312, 313, 327, 336, 341], "contrast": [7, 46, 47, 74, 88, 89], "control": [3, 7, 8, 28, 43, 52, 73, 78, 82, 102, 103, 104, 108, 110, 134, 135, 147, 148, 164, 168, 169, 172, 179, 184, 185, 188, 191, 196, 198, 212, 213, 217, 219, 229, 232, 241, 270, 273, 274, 275, 278, 279, 283, 295, 306, 317, 332, 336, 337], "controversi": [74, 85], "conv1": [4, 6, 31, 40, 41, 238, 239, 247, 253, 257, 260, 273, 282], "conv2": [238, 247, 253], "conv2d": [4, 6, 31, 40, 41, 238, 239, 247, 253, 257, 260, 273, 277, 282, 328], "convei": [74, 89], "conveni": [3, 4, 11, 27, 31, 72, 273, 281], "convent": [74, 89, 273, 280], "converg": [274, 276, 277, 278, 279, 312, 323, 329, 331, 335, 340], "convers": [229, 232, 234, 235, 273, 297], "convert": [2, 5, 8, 25, 35, 52, 73, 75, 76, 80, 83, 84, 92, 97, 212, 213, 215, 238, 251, 273, 274, 275, 276, 277, 279, 287, 292, 295, 296, 297, 307, 308, 314, 315, 317, 321, 327, 329, 337, 338], "convnext": [274, 314], "convolut": [273, 277, 282, 326], "cool": [74, 89], "coordin": [100, 105, 274, 275, 277, 278, 306, 314, 315, 317, 326, 332], "cop": [186, 187, 200, 203, 205, 206, 207, 210], "copi": [2, 4, 7, 18, 32, 43, 73, 82, 169, 171, 178, 179, 182, 183, 186, 190, 191, 194, 195, 202, 205, 209, 273, 274, 275, 276, 279, 280, 287, 312, 314, 317, 323, 325, 338, 340], "cor": [100, 105, 136, 146], "core": [3, 5, 6, 9, 10, 13, 22, 27, 35, 40, 41, 57, 68, 73, 80, 181, 193, 237, 239, 243, 257, 259, 273, 274, 275, 277, 278, 279, 281, 306, 307, 317, 326, 327, 329, 331, 336, 337, 340, 343], "corner": [73, 82, 178, 190], "correct": [4, 7, 32, 48, 73, 82, 164, 168, 238, 249, 250, 273, 274, 275, 276, 279, 282, 283, 286, 287, 289, 294, 300, 301, 306, 309, 310, 315, 318, 319, 321, 341], "correct_squar": [2, 20], "correct_square_mod": [2, 20], "correctli": [11, 72, 136, 146, 185, 197, 273, 274, 275, 276, 277, 279, 281, 290, 292, 303, 307, 308, 316, 321, 327, 338, 340], "correl": [6, 41, 239, 258], "correspond": [1, 7, 15, 48, 238, 247, 273, 279, 291, 339], "corrupt": [277, 326, 327], "cost": [1, 4, 5, 7, 13, 32, 37, 43, 45, 47, 164, 168, 179, 181, 182, 183, 191, 193, 194, 195, 216, 218, 219, 220, 221, 223, 228, 232, 238, 254, 278, 336], "costum": [73, 80, 82], "could": [2, 8, 20, 24, 54, 73, 74, 75, 81, 82, 88, 89, 90, 212, 213, 217], "couldn": [73, 82], "count": [1, 8, 9, 13, 54, 55, 64, 74, 89, 163, 175, 240, 265, 275, 277, 278, 279, 316, 318, 320, 327, 333, 338, 340], "countri": [73, 74, 82, 85, 229, 234, 279, 342], "countrymen": [74, 85], "coup": [229, 233], "coupl": [74, 89], "cours": [74, 88, 89, 100, 103, 162, 163, 169, 170, 175, 176, 229, 236, 343], "cover": [73, 74, 76, 77, 83, 89, 99, 100, 103, 163, 174, 175, 177, 178, 179, 181, 188, 189, 190, 191, 193, 212, 213, 220, 221, 222, 228, 236, 318, 319], "cover_typ": [275, 316], "covtyp": [275, 316, 317, 319], "covtype_xgb_cpu": [275, 317], "coward": [74, 89], "cp": [9, 10, 62, 70, 240, 241, 264, 269], "cpu": [2, 3, 4, 5, 6, 8, 9, 10, 22, 23, 25, 28, 31, 32, 35, 36, 40, 41, 53, 55, 57, 59, 61, 62, 70, 71, 73, 74, 76, 78, 81, 82, 83, 84, 94, 97, 98, 99, 163, 164, 166, 175, 176, 177, 179, 181, 189, 191, 193, 237, 238, 239, 240, 241, 242, 244, 245, 249, 252, 253, 258, 259, 260, 262, 263, 264, 269, 270, 273, 274, 276, 277, 278, 279, 282, 285, 286, 287, 289, 290, 292, 293, 298, 305, 310, 314, 315, 316, 317, 319, 323, 325, 331, 332, 336, 340, 342], "cpus_per_work": [275, 317], "craft": [74, 88, 89], "crappi": [74, 85], "crash": [7, 46, 73, 80, 82, 83, 273, 274, 275, 302, 313, 315], "crazi": [73, 74, 80, 82, 89], "cream": [73, 82], "creat": [2, 3, 6, 7, 8, 9, 12, 16, 18, 25, 27, 28, 32, 41, 44, 45, 50, 52, 58, 75, 77, 78, 80, 81, 84, 90, 91, 92, 102, 103, 104, 106, 107, 109, 110, 114, 115, 117, 119, 120, 123, 124, 125, 129, 130, 131, 133, 135, 140, 141, 142, 143, 145, 148, 151, 155, 156, 158, 159, 160, 161, 163, 164, 168, 176, 177, 178, 180, 181, 183, 184, 185, 187, 189, 190, 192, 193, 195, 196, 198, 203, 206, 210, 212, 213, 216, 219, 221, 224, 225, 226, 228, 229, 232, 233, 234, 236, 237, 238, 239, 240, 241, 244, 245, 249, 251, 252, 258, 259, 263, 269, 272, 274, 275, 276, 277, 281, 289, 292, 295, 296, 303, 305, 306, 308, 309, 314, 319, 320, 327, 337], "create_us": 173, "creation": [2, 18, 162], "cred": [73, 80, 82], "credenti": [111, 112, 118, 121, 126, 127, 136, 138, 149, 152, 155], "credit": [8, 51, 74, 89], "creepi": [74, 89], "creepybr": [74, 89], "creighton": [73, 82], "crime": [74, 89], "criteria": [9, 10, 57, 68, 212, 213, 215], "criterion": [4, 6, 31, 32, 40, 41, 239, 257, 260, 273, 274, 283, 294, 300, 310], "critic": [7, 48, 73, 82, 180, 192], "crop": [274, 277, 306, 307, 327], "cross": [100, 104, 105, 107, 109, 136, 146, 273, 274, 276, 281, 306, 321], "cross_attention_dim": [5, 35], "crossattndownblock2d": [5, 35], "crossattnupblock2d": [5, 35], "crossentropyloss": [4, 6, 29, 31, 32, 38, 40, 41, 238, 239, 247, 249, 253, 257, 260, 273, 274, 281, 283, 294, 300, 310], "crouch": [74, 89], "crow": [74, 88], "crucial": [74, 76, 84, 97, 229, 235], "crush": [73, 82], "cry": [74, 85], "css": 0, "csv": [4, 7, 9, 29, 31, 43, 59, 180, 192, 238, 247, 273, 274, 275, 276, 279, 281, 293, 312, 314, 319, 320, 321, 338, 342], "csv_path": [276, 321], "ctor": [274, 314], "ctrl": [11, 72], "cu128": [164, 168, 212, 213, 219, 221, 226], "cub": [73, 82], "cuda": [4, 6, 9, 31, 32, 40, 62, 73, 76, 81, 82, 97, 212, 213, 218, 238, 239, 240, 241, 247, 250, 257, 260, 264, 269, 273, 274, 276, 277, 278, 283, 285, 286, 289, 290, 292, 294, 314, 325, 331, 336], "cultur": [74, 85], "cumprod": [5, 35], "cumul": [279, 342], "cup": [74, 89], "cure": [74, 89], "curiou": [74, 85, 273, 305], "curl": [11, 72, 173], "current": [2, 4, 7, 8, 22, 31, 47, 54, 76, 99, 126, 129, 136, 141, 149, 152, 169, 171, 183, 187, 195, 204, 206, 211, 229, 234, 237, 238, 239, 241, 244, 245, 252, 258, 260, 270, 273, 277, 278, 279, 288, 289, 295, 301, 329, 332, 335, 338], "current_training_step": [5, 35], "curti": [73, 80, 82, 83], "curv": [273, 276, 281, 291, 306, 307, 323, 328], "custom": [2, 3, 5, 10, 21, 22, 27, 35, 61, 68, 71, 74, 84, 85, 89, 103, 104, 107, 109, 110, 111, 112, 118, 119, 122, 126, 127, 136, 137, 149, 150, 164, 166, 169, 172, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195, 198, 204, 206, 211, 212, 213, 219, 221, 226, 228, 229, 232, 236, 237, 243, 273, 275, 277, 279, 282, 306, 314, 315, 319, 326, 337, 340], "custom_hid": 0, "custom_light": 0, "custom_nam": [229, 232], "customer_ingress_cidr_rang": [100, 105], "cut": [4, 5, 32, 37, 238, 254, 276, 321], "cv": [229, 232, 343], "cv_job_match": [229, 232], "cybersecur": [229, 232], "d": [8, 9, 51, 52, 54, 59, 60, 61, 73, 74, 80, 82, 83, 88, 89, 164, 168, 169, 171, 173, 240, 241, 263, 264, 269, 270, 274, 275, 276, 277, 278, 279, 307, 314, 317, 318, 321, 325, 327, 329, 333, 335, 336, 337, 342], "d3a9a7d0": [118, 122, 149, 153], "d89d0_00000": [238, 253], "d_": [4, 31, 238, 247], "d_model": [276, 322, 323, 325], "da": [73, 82], "dag": [7, 45, 48], "dai": [73, 74, 82, 89, 229, 234], "daili": [275, 319], "dalla": [73, 82], "damn": [73, 82], "dancer": [74, 89], "daniel": [73, 74, 82, 88], "dark": [74, 88, 89, 173], "darwin": [73, 82], "dash": [74, 89], "dashboard": [4, 5, 7, 11, 30, 34, 44, 46, 72, 107, 110, 163, 164, 166, 167, 168, 170, 172, 175, 176, 178, 183, 187, 190, 195, 203, 206, 210, 221, 227, 228, 241, 270, 273, 278, 280, 305, 336], "dask": [7, 46, 47], "data": [1, 10, 13, 17, 18, 31, 33, 35, 37, 38, 41, 42, 45, 46, 60, 62, 68, 69, 76, 80, 81, 83, 85, 86, 89, 94, 95, 97, 99, 100, 104, 105, 126, 134, 136, 147, 163, 164, 168, 170, 173, 175, 178, 179, 180, 188, 190, 191, 192, 213, 219, 229, 233, 234, 235, 236, 237, 241, 244, 245, 249, 250, 252, 254, 255, 258, 260, 269, 270, 278, 281, 283, 285, 287, 300, 304, 306, 307, 308, 315, 316, 317, 319, 320, 321, 323, 326, 329, 331, 332, 333, 335, 336, 337, 340, 342, 343], "data_dir": [276, 321, 323, 325], "data_load": [4, 5, 6, 31, 32, 35, 39, 40, 41, 238, 239, 247, 249, 253, 256, 257, 260, 273, 283, 294, 295, 300], "data_path": [8, 51, 54], "data_url": [279, 338], "databas": [9, 58, 173, 229, 231, 234], "databaseservic": [169, 172, 173], "databrick": [7, 9, 43, 59], "datadog": [163, 175], "datafram": [3, 7, 8, 28, 46, 52, 84, 237, 244, 245, 274, 275, 276, 277, 279, 280, 281, 296, 314, 318, 319, 321, 327, 329, 338, 340, 342], "dataload": [6, 32, 33, 36, 38, 39, 41, 95, 238, 239, 247, 250, 256, 260, 277, 278, 281, 283, 289, 293, 294, 297, 306, 307, 310, 323, 327, 333], "dataset": [3, 5, 6, 7, 9, 28, 29, 33, 35, 36, 38, 39, 40, 41, 43, 44, 45, 50, 52, 54, 55, 57, 58, 59, 60, 61, 63, 64, 65, 76, 78, 79, 81, 82, 84, 89, 94, 95, 97, 98, 99, 169, 171, 180, 192, 239, 241, 245, 246, 247, 248, 252, 253, 256, 257, 258, 260, 262, 264, 265, 266, 269, 274, 277, 280, 282, 284, 287, 289, 292, 293, 294, 295, 297, 298, 301, 303, 304, 305, 306, 307, 308, 309, 314, 315, 317, 319, 320, 323, 325, 326, 327, 329, 331, 335, 336, 337, 340, 342], "dataset_": [169, 171], "dataset_iter": [273, 295], "dataset_uri": [279, 338], "datasourc": [8, 9, 53, 59, 240, 263], "date": [73, 82, 229, 234, 238, 253], "datetim": [4, 29, 31, 238, 247, 273, 276, 281, 321], "david": [73, 80, 82, 173], "dawson": [74, 88], "day_of_week": [237, 244], "db": 173, "ddim": [277, 331], "ddp": [4, 5, 32, 36, 274, 276, 277, 279, 281, 282, 285, 286, 289, 290, 301, 305, 310, 314, 320, 323, 325, 329, 338, 339, 342], "ddpmschedul": [5, 33, 35], "ddpstrategi": [5, 35], "de": [2, 18, 73, 82, 276, 278, 325, 328, 331, 332, 336], "deactiv": [11, 72], "dead": [74, 89], "deadlock": [2, 23], "deal": [7, 48], "dear": [73, 82], "debat": [73, 82], "debug": [7, 10, 46, 71, 163, 164, 166, 168, 169, 170, 172, 175, 176, 178, 184, 188, 190, 196, 229, 236, 273, 277, 281, 288, 292, 326], "debut": [73, 82], "decemb": [73, 82], "decid": [1, 6, 13, 41, 73, 74, 82, 89, 229, 234, 239, 240, 259, 264, 273, 276, 278, 282, 290, 323, 332], "decis": [107, 109, 275, 315, 316], "declar": [273, 274, 275, 277, 278, 279, 285, 306, 315, 326, 332, 337], "decod": [220, 276, 320, 321, 322, 323, 325, 326, 331], "decode_and_norm": [277, 327], "decoder_input": [276, 322, 323], "decor": [1, 2, 9, 10, 14, 22, 25, 60, 64, 70, 75, 92, 241, 269, 270], "decoupl": [10, 71, 273, 305], "decreas": [273, 274, 276, 291, 312, 323], "dedic": [6, 41, 74, 89, 163, 176, 182, 183, 186, 187, 194, 195, 200, 203, 205, 206, 207, 210, 221, 226, 229, 235, 239, 260, 273, 292], "dedupl": [76, 99, 239, 260], "deep": [8, 51, 73, 74, 76, 78, 89, 94, 95, 98, 99, 221, 227, 273, 274, 279, 281, 307, 338, 342], "deeper": [7, 44, 212, 213, 220, 229, 231, 236, 255], "deepli": [164, 168], "deepseek": [229, 235], "deepspe": [4, 5, 32, 37, 238, 254, 273, 305], "def": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 16, 18, 19, 20, 21, 22, 23, 24, 25, 28, 31, 32, 35, 36, 39, 40, 41, 52, 61, 62, 64, 70, 72, 73, 74, 75, 76, 81, 89, 92, 96, 97, 98, 164, 168, 169, 171, 172, 178, 181, 182, 186, 190, 193, 194, 201, 205, 208, 229, 234, 237, 238, 239, 240, 241, 244, 245, 247, 249, 250, 251, 253, 256, 257, 258, 259, 260, 264, 265, 269, 270, 273, 274, 275, 276, 277, 278, 279, 282, 283, 286, 287, 288, 289, 292, 294, 295, 297, 300, 301, 308, 309, 310, 314, 317, 318, 321, 322, 323, 325, 327, 328, 329, 331, 333, 334, 335, 336, 338, 339, 340], "default": [0, 2, 4, 5, 6, 8, 9, 10, 20, 22, 31, 35, 41, 51, 52, 59, 61, 71, 74, 76, 87, 99, 111, 113, 118, 121, 122, 126, 128, 136, 139, 149, 152, 164, 168, 169, 172, 177, 179, 180, 181, 183, 184, 189, 191, 192, 193, 195, 196, 229, 234, 237, 244, 245, 260, 273, 274, 275, 276, 277, 282, 300, 314, 316, 320, 327, 329], "default_cluster_storag": [169, 171], "default_data_col": [76, 97], "default_root_dir": [5, 35, 277, 278, 329, 335], "default_tracing_servic": 173, "defens": [73, 82, 276, 323], "defin": [1, 2, 3, 4, 6, 9, 10, 16, 20, 22, 25, 28, 31, 32, 41, 61, 70, 71, 73, 74, 75, 76, 78, 82, 83, 89, 92, 94, 97, 98, 102, 126, 133, 136, 145, 149, 152, 164, 166, 177, 179, 182, 183, 186, 187, 189, 191, 194, 195, 202, 204, 205, 206, 209, 211, 212, 213, 216, 221, 224, 229, 233, 235, 237, 239, 244, 245, 258, 260, 274, 276, 277, 280, 285, 286, 287, 289, 292, 293, 298, 308, 309, 310, 314, 315, 318, 319, 320, 321, 325, 327, 329, 338], "definit": [179, 191, 229, 233, 234, 278, 333], "degre": [8, 9, 54, 64, 240, 265], "del": [273, 292], "delai": [74, 88, 241, 270], "deleg": [3, 28], "delet": [4, 31, 100, 105, 111, 113, 116, 118, 125, 126, 128, 134, 136, 139, 147, 149, 161, 180, 192, 273, 274, 275, 276, 277, 278, 279, 281, 304, 314, 319, 325, 331, 336, 342], "delete_object": [180, 192], "deleteobject": [100, 105], "delhi": [73, 82], "deliv": [74, 88, 164, 168], "deloy": [183, 195], "delta": [7, 43, 212, 213, 219, 221, 225, 226, 229, 232, 275, 319], "demand": [11, 72, 107, 108, 110, 177, 179, 183, 187, 189, 191, 195, 203, 206, 210, 241, 268, 277, 321, 331], "demo": [100, 103, 273, 277, 278, 279, 283, 331, 336, 342], "demonstr": [5, 33, 73, 74, 76, 83, 86, 88, 89, 94, 99, 103, 164, 168, 169, 170, 171, 173, 180, 192, 221, 222, 229, 231, 273, 276, 277, 278, 279, 292, 306, 307, 324, 327, 330, 335, 338, 341, 342], "denizen": [74, 85], "denni": [73, 82], "deped": [179, 191], "depend": [0, 7, 9, 17, 23, 45, 46, 47, 61, 76, 77, 94, 100, 105, 111, 113, 126, 128, 136, 139, 156, 173, 177, 178, 179, 188, 189, 190, 191, 212, 213, 215, 217, 273, 274, 275, 276, 277, 278, 279, 293, 307, 316, 320, 321, 327, 333, 338], "depict": [74, 89], "deploi": [3, 10, 28, 67, 71, 78, 90, 93, 100, 102, 103, 104, 105, 113, 122, 128, 139, 162, 169, 172, 173, 179, 183, 191, 195, 212, 213, 214, 215, 217, 218, 219, 220, 223, 225, 228, 230, 231, 233, 234, 236, 237, 241, 244, 267, 268, 275, 276, 278, 319, 325, 336, 343], "deploy": [0, 3, 7, 28, 48, 70, 71, 90, 101, 102, 105, 106, 109, 111, 113, 118, 119, 121, 122, 126, 128, 130, 131, 136, 139, 142, 143, 146, 149, 152, 153, 158, 159, 164, 167, 169, 172, 183, 184, 187, 195, 196, 203, 204, 206, 210, 211, 217, 218, 220, 222, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 244, 245, 269, 271, 274, 276, 279, 314, 325, 342], "deployment_config": [212, 213, 219, 221, 224, 227, 229, 234], "deploymenthandl": [75, 92], "deploymentrespons": [241, 270], "depress": [74, 85], "depth": [238, 251], "deriv": [279, 340], "derp": [111, 113, 126, 128, 136, 139], "desc": [4, 31, 274, 307], "descent": [74, 89], "describ": [74, 89, 111, 113, 126, 128, 136, 139, 140, 278, 332], "descript": [8, 11, 51, 72, 100, 104, 105, 212, 213, 217, 232, 234, 278, 333], "deseri": [1, 7, 13, 46], "design": [1, 7, 8, 9, 11, 13, 43, 46, 47, 50, 53, 60, 72, 74, 75, 76, 89, 90, 94, 98, 99, 162, 212, 213, 216, 229, 232, 236, 241, 268, 270, 274, 275, 276, 310, 317, 320], "desir": [238, 248, 274, 314], "desktop": [178, 190], "despit": [74, 89], "destin": [229, 232], "destroi": [73, 74, 82, 89, 111, 113, 116, 118, 125, 126, 134, 136, 147, 149, 161], "destruct": [74, 89], "detach": [273, 276, 292, 325], "detail": [2, 4, 5, 6, 8, 9, 10, 22, 30, 32, 34, 36, 39, 54, 57, 68, 71, 74, 75, 77, 89, 92, 100, 103, 105, 107, 108, 118, 122, 126, 128, 136, 139, 149, 153, 171, 172, 173, 180, 181, 185, 192, 193, 198, 221, 227, 229, 232, 238, 239, 241, 249, 252, 258, 269, 273, 280, 285, 289, 293, 294, 343], "detailsbr": [74, 89], "detect": [7, 46, 73, 81, 164, 168, 229, 232, 273, 274, 277, 303, 312, 330], "determin": [9, 60, 74, 87, 212, 213, 215], "determinist": [275, 316], "determint": [9, 64], "dev": [111, 113, 126, 128, 136, 139, 278, 332], "deval": [275, 317], "develop": [1, 3, 7, 8, 13, 27, 46, 47, 48, 54, 67, 68, 73, 76, 82, 98, 111, 112, 113, 118, 120, 126, 127, 128, 136, 138, 139, 149, 151, 163, 169, 172, 175, 177, 180, 181, 182, 183, 185, 186, 189, 192, 193, 194, 195, 198, 200, 205, 207, 221, 222, 237, 241, 243, 244, 245, 268, 271, 274, 306], "deviat": [274, 308], "devic": [1, 4, 5, 6, 9, 10, 13, 31, 32, 35, 36, 40, 41, 62, 70, 71, 73, 76, 78, 82, 94, 97, 107, 110, 134, 135, 147, 148, 212, 213, 219, 238, 247, 249, 250, 253, 273, 274, 276, 277, 278, 282, 283, 285, 286, 287, 289, 290, 292, 294, 295, 306, 309, 310, 314, 323, 325, 328, 329, 331, 332, 335, 336], "devop": [162, 188], "df": [3, 5, 8, 28, 35, 51, 52, 53, 54, 273, 274, 275, 276, 277, 278, 279, 296, 308, 312, 316, 318, 319, 321, 323, 329, 335, 338, 340, 342], "di": [2, 20, 74, 88], "diagnos": [163, 175], "diagnost": [276, 323], "diagon": [275, 318], "diagram": [3, 5, 6, 7, 28, 35, 36, 41, 46, 48, 76, 94, 100, 104, 107, 109, 188, 212, 213, 218, 238, 239, 247, 248, 259, 273, 280, 289], "diari": [73, 82], "dict": [2, 3, 4, 5, 6, 9, 10, 19, 28, 31, 32, 35, 36, 41, 61, 62, 64, 70, 73, 76, 79, 81, 95, 97, 164, 168, 212, 213, 219, 221, 224, 227, 229, 232, 234, 237, 238, 239, 240, 241, 244, 245, 247, 249, 251, 258, 264, 265, 269, 270, 273, 276, 277, 278, 283, 289, 294, 295, 297, 298, 300, 301, 305, 325, 327, 333], "dictat": [74, 89], "dictionari": [2, 3, 6, 22, 28, 41, 76, 97, 239, 258, 260, 273, 279, 284, 288, 294, 295, 340], "did": [2, 20, 73, 74, 82, 88, 89, 277, 278, 329, 331, 335, 336], "didn": [73, 82], "diego": [73, 82], "differ": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 16, 28, 30, 34, 41, 45, 54, 61, 64, 69, 72, 73, 74, 76, 78, 82, 85, 97, 98, 100, 101, 103, 105, 107, 109, 149, 154, 162, 163, 164, 167, 169, 170, 172, 176, 178, 180, 190, 192, 212, 213, 215, 216, 217, 223, 229, 231, 232, 237, 239, 240, 244, 245, 258, 259, 265, 273, 274, 275, 276, 277, 278, 280, 283, 285, 293, 294, 295, 306, 314, 317, 323, 331, 336], "differenti": [275, 315], "difficult": [7, 46, 164, 168], "diffus": [4, 32, 33, 36, 37, 238, 254, 329, 333, 335], "diffusionpolici": [335, 336], "digit": [6, 10, 39, 40, 70, 239, 256, 257, 282, 296], "dii": [212, 213, 217], "dilat": [238, 247, 253], "dim": [273, 274, 277, 278, 279, 292, 310, 314, 328, 332, 334, 339], "dimens": [273, 276, 279, 292, 321, 337, 340], "dimension": [275, 315, 316], "dinger": [73, 82], "dir": [179, 180, 181, 191, 192, 193, 273, 274, 275, 276, 277, 278, 279, 281, 307, 310, 314, 316, 321, 327, 333, 338], "direct": [6, 41, 74, 89, 100, 105, 107, 109, 229, 235, 239, 258, 274, 275, 276, 277, 278, 279, 314, 319, 325, 331, 336, 342], "directli": [2, 7, 18, 19, 43, 100, 105, 107, 109, 110, 178, 181, 185, 190, 193, 198, 229, 233, 273, 274, 275, 276, 277, 278, 279, 293, 306, 312, 316, 317, 320, 326, 329, 332, 336, 337, 338, 340], "director": [74, 88, 89], "directori": [0, 4, 8, 31, 53, 126, 129, 136, 141, 173, 178, 180, 183, 187, 190, 192, 195, 204, 206, 211, 273, 274, 275, 276, 277, 279, 281, 289, 292, 304, 310, 314, 315, 319, 321, 325, 331, 338, 342], "dirpath": [277, 278, 329, 335], "disabl": [5, 36, 76, 99, 238, 239, 241, 252, 258, 260, 269, 273, 292], "disaggreg": [9, 66], "disappoint": [73, 80, 82], "discern": [74, 85], "disconnect": [279, 340], "discontinu": [278, 332], "discount": [279, 342], "discov": [74, 89], "discret": [7, 46], "discuss": [6, 40, 41, 237, 239, 242, 259], "disengag": [8, 51], "disjoint": [74, 88, 89, 273, 283], "disk": [4, 9, 31, 57, 63, 163, 164, 166, 169, 171, 172, 175, 180, 181, 192, 193, 240, 264, 273, 274, 275, 276, 277, 279, 281, 296, 301, 314, 319, 325, 331, 342], "dismiss": [73, 82], "displai": [4, 11, 31, 72, 74, 85, 89, 100, 105, 169, 172, 182, 194, 221, 224, 238, 247, 273, 274, 281, 291, 292, 303, 312, 329], "disrupt": [74, 88], "dist": [273, 298], "dist_val_acc": [274, 310], "distanc": [3, 6, 8, 28, 41, 51, 54, 237, 239, 244, 258, 275, 277, 319, 331], "distil": [212, 213, 216, 276, 325], "distilbert": [75, 92], "distinct": [107, 109, 164, 167, 212, 213, 215, 273, 279, 282, 338], "distract": [74, 89], "distribut": [1, 2, 6, 8, 9, 11, 13, 17, 18, 23, 26, 27, 29, 30, 33, 34, 40, 43, 47, 48, 49, 50, 51, 53, 55, 56, 57, 59, 60, 63, 72, 73, 74, 75, 83, 84, 86, 89, 90, 95, 97, 98, 99, 100, 105, 173, 179, 181, 188, 191, 193, 212, 213, 217, 218, 221, 227, 228, 237, 239, 240, 242, 243, 246, 249, 253, 257, 261, 262, 264, 281, 282, 283, 285, 287, 290, 291, 293, 296, 297, 298, 305, 307, 310, 314, 316, 319, 321, 323, 327, 331, 332, 333, 336, 338, 342, 343], "distributeddataparallel": [4, 5, 32, 36, 238, 250, 251, 273, 279, 280, 283, 286, 289, 290, 339], "distributedsampl": [4, 32, 238, 250, 273, 274, 283, 287, 289, 309, 310], "div_term": [276, 322], "dive": [188, 212, 213, 220, 229, 230, 231, 236, 255, 276, 321], "divers": [7, 43, 46], "divid": [7, 46, 100, 105], "dl_dw": [6, 41, 239, 259], "dmatrix": [3, 28, 237, 244, 245, 275, 316, 317, 318], "dn": [107, 109], "do": [2, 3, 6, 8, 19, 22, 26, 40, 52, 53, 54, 73, 74, 78, 82, 85, 88, 89, 118, 120, 149, 151, 178, 180, 190, 192, 229, 232, 234, 237, 238, 239, 242, 251, 257], "doc": [4, 5, 8, 9, 10, 11, 32, 36, 51, 53, 54, 64, 65, 71, 72, 73, 75, 76, 80, 90, 92, 99, 100, 105, 111, 112, 118, 120, 126, 127, 136, 138, 149, 151, 169, 172, 181, 185, 186, 187, 193, 198, 200, 203, 205, 206, 207, 210, 212, 213, 219, 229, 232, 238, 239, 240, 241, 249, 251, 253, 260, 263, 265, 266, 270, 273, 285, 291], "docker": [107, 109], "dockerfil": [212, 213, 219, 221, 226], "document": [6, 7, 41, 43, 47, 48, 100, 105, 107, 108, 169, 172, 178, 182, 190, 194, 212, 213, 216, 220, 221, 228, 229, 233, 235, 236, 239, 241, 259, 270], "documentari": [74, 85], "doe": [2, 3, 5, 6, 7, 8, 9, 22, 23, 28, 35, 41, 47, 51, 61, 73, 74, 82, 83, 85, 89, 107, 110, 163, 164, 168, 176, 181, 182, 186, 193, 194, 200, 205, 207, 212, 213, 216, 217, 239, 259, 273, 277, 278, 279, 297, 331, 336, 342], "doesn": [2, 4, 8, 9, 19, 31, 54, 64, 73, 74, 82, 85, 88, 89, 107, 109, 180, 192, 212, 213, 217, 240, 265, 273, 276, 281, 321], "doesnt": [74, 89], "dog": [274, 306], "dogma": [74, 89], "dolocationid": [8, 51], "domain": [275, 318, 319], "domin": [7, 46, 275, 318], "don": [1, 2, 4, 8, 9, 11, 16, 21, 32, 52, 61, 72, 73, 74, 82, 85, 89, 188, 240, 264, 273, 274, 277, 279, 280, 281, 285, 306, 309, 327, 331, 342], "donald": [73, 82], "done": [4, 8, 32, 54, 74, 75, 88, 89, 93, 111, 116, 118, 125, 149, 161, 169, 171, 178, 181, 182, 183, 190, 193, 194, 195, 212, 213, 219, 221, 225, 226, 238, 253, 273, 275, 279, 283, 292, 315, 338], "dont": [74, 89], "dool": [73, 82], "dorset": [73, 82], "dot": [2, 22, 73, 80, 82, 274, 275, 277, 278, 279, 306, 315, 326, 332, 337, 339, 342], "dot_product": [279, 339], "doubl": [73, 74, 80, 82, 85], "down": [4, 5, 32, 37, 73, 74, 76, 78, 82, 84, 89, 94, 107, 109, 110, 136, 146, 182, 186, 194, 200, 205, 207, 212, 213, 217, 227, 238, 241, 254, 268, 270, 273, 278, 289, 332], "down_block_typ": [5, 35], "downblock2d": [5, 35], "download": [4, 6, 11, 31, 32, 39, 41, 72, 75, 91, 149, 152, 178, 180, 181, 183, 188, 190, 192, 193, 195, 229, 232, 238, 239, 240, 241, 247, 250, 252, 256, 258, 260, 264, 269, 276, 279, 287, 304, 320, 321, 338], "downsampl": [238, 247, 253], "downsample_pad": [5, 35], "downscal": [75, 93, 241, 270], "downscale_delay_": [241, 270], "downstream": [9, 63, 74, 84, 229, 231, 233, 240, 264, 279, 342], "downtim": [169, 172, 183, 187, 195, 203, 206, 210], "draft": [73, 80, 82, 83], "drag": [73, 82], "dragon": [74, 89], "drama": [74, 85], "draw": [74, 89], "dread": [73, 82], "dream": [74, 88, 89], "dreambr": [74, 89], "dreamnightmar": [74, 89], "dress": [73, 82], "drill": [7, 44], "drive": [74, 88], "driver": [2, 9, 19, 23, 57, 62, 73, 82, 107, 110, 164, 166, 169, 172, 274, 275, 276, 279, 314, 318, 325, 338], "driver_artifact": [76, 99, 237, 238, 244, 245, 252], "drop": [73, 82, 212, 213, 218, 273, 274, 275, 276, 277, 287, 292, 314, 317, 321, 327], "drop_column": [237, 244, 245, 277, 327], "drop_last": [4, 6, 31, 32, 39, 41, 238, 239, 247, 250, 256, 260, 273, 276, 287, 321], "dropdown": [169, 171, 178, 190, 221, 227], "dropna": [274, 276, 279, 312, 323, 340], "dropout": [276, 322], "ds_adjust": [8, 52, 53], "ds_block_based_shuffl": [8, 54], "ds_file_shuffl": [8, 54], "ds_iter": [275, 317], "ds_label": [9, 61], "ds_limit": [8, 53], "ds_meta": [74, 86, 88], "ds_normal": [9, 61, 62, 240, 264], "ds_pred": [9, 62, 63, 64, 65, 240, 264, 265, 266], "ds_randomized_block": [9, 64, 240, 265], "ds_randomized_row": [9, 64, 240, 265], "ds_review": [74, 86, 87], "ds_row_based_shuffl": [8, 54], "ds_tip": [8, 52], "ds_tmp": [274, 314], "dsl": [7, 45], "dtest": [3, 28], "dtrain": [3, 28, 275, 317], "dtype": [5, 35, 73, 82, 83, 164, 168, 237, 241, 244, 245, 270, 273, 276, 277, 278, 279, 297, 321, 322, 327, 333, 336, 340], "due": [2, 4, 5, 7, 9, 10, 20, 30, 34, 46, 57, 61, 68, 73, 74, 75, 80, 88, 89, 93, 164, 168, 212, 213, 216, 221, 227, 273, 280], "dummi": [212, 213, 219, 229, 234], "dummy_data_1000_500": [180, 192], "dummy_data_1000_720": [180, 192], "dummy_data_xxl": [180, 192], "dummy_kei": [213, 219], "dump": [10, 70, 169, 172, 229, 234, 241, 269, 270], "duplic": [273, 274, 276, 280, 289, 301, 306, 321], "durabl": [7, 43], "durat": [169, 172, 173, 181, 193], "dure": [0, 2, 4, 5, 22, 32, 36, 73, 74, 76, 83, 85, 89, 94, 97, 111, 113, 126, 128, 136, 139, 169, 172, 180, 192, 212, 213, 216, 217, 238, 253, 273, 274, 275, 276, 277, 279, 280, 288, 291, 298, 307, 312, 316, 317, 320, 322, 325, 326, 327, 329, 340, 342], "dustin": [73, 82], "dvd": [74, 88], "dynam": [7, 8, 10, 48, 54, 68, 100, 105, 107, 108, 229, 232, 241, 268, 276, 320], "dynamic_lora_loading_path": [229, 232], "e": [0, 1, 2, 4, 6, 7, 8, 9, 10, 13, 19, 20, 22, 32, 41, 43, 46, 52, 53, 57, 59, 61, 68, 69, 71, 73, 78, 100, 105, 107, 109, 110, 111, 112, 113, 118, 119, 126, 127, 128, 132, 133, 134, 136, 137, 139, 144, 145, 147, 149, 150, 163, 164, 166, 169, 171, 175, 178, 180, 181, 183, 184, 187, 190, 192, 193, 195, 196, 203, 204, 206, 210, 211, 212, 213, 216, 221, 227, 237, 239, 240, 241, 244, 245, 259, 263, 264, 270, 273, 274, 275, 276, 277, 278, 279, 283, 284, 289, 290, 291, 293, 295, 299, 302, 305, 307, 317, 325, 326, 332, 337, 340], "each": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 16, 18, 28, 31, 32, 35, 36, 39, 41, 48, 51, 52, 53, 59, 61, 62, 69, 72, 73, 74, 75, 76, 78, 80, 81, 82, 86, 89, 90, 92, 94, 97, 98, 100, 103, 105, 126, 129, 136, 141, 162, 164, 166, 167, 168, 169, 171, 172, 173, 179, 180, 181, 183, 185, 187, 191, 192, 193, 195, 198, 203, 206, 210, 212, 213, 215, 216, 221, 224, 227, 229, 231, 232, 236, 237, 238, 239, 240, 241, 242, 249, 256, 258, 259, 260, 263, 264, 268, 270, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 301, 306, 307, 308, 310, 312, 314, 315, 316, 317, 318, 320, 321, 323, 325, 326, 327, 332, 335, 337, 338, 339, 340, 342], "earli": [6, 41, 73, 82, 239, 259, 274, 275, 276, 277, 314, 319, 321, 325, 331], "earlier": [74, 88, 178, 179, 181, 190, 191, 193, 274, 277, 313, 327], "early_stopping_round": [275, 319], "earn": [73, 82], "earth": [74, 88], "eas": [7, 45], "easi": [0, 7, 8, 48, 50, 75, 76, 90, 97, 98, 164, 168, 169, 172, 188, 212, 213, 217, 221, 227, 229, 233, 234, 236, 241, 268, 273, 277, 291, 293, 326], "easier": [5, 7, 35, 48, 74, 76, 89, 94, 107, 109, 279, 342], "easili": [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 17, 26, 29, 33, 38, 49, 56, 67, 73, 74, 75, 76, 78, 84, 90, 94, 98, 164, 168, 183, 187, 195, 203, 206, 210, 212, 213, 214, 218, 221, 222, 229, 230, 278, 332], "eastern": [74, 88, 89], "eat": [74, 88], "ec2": [101, 103, 105, 106, 107, 109, 110, 113, 114, 117, 126, 128, 136, 139, 181, 193, 343], "echo": [73, 82, 149, 152], "eclips": [73, 82], "ecolog": [275, 319], "ecosystem": [3, 4, 5, 7, 27, 30, 34, 46, 47, 237, 243, 244, 245, 273, 280], "ed": [73, 82], "eddi": [73, 74, 82, 88, 89], "edg": [73, 82, 276, 325], "edgecolor": [279, 338], "edit": [73, 82, 178, 179, 190, 191], "editor": [118, 120, 149, 151, 178, 179, 182, 190, 191, 194], "educ": [274, 306], "ef": [104, 106, 107, 109, 111, 113, 117, 136, 140, 240, 241, 264, 269], "effect": [2, 3, 19, 28, 74, 88, 89, 100, 105, 180, 192, 221, 223, 229, 232, 273, 276, 279, 284, 325, 339, 340], "effici": [7, 9, 10, 11, 43, 46, 47, 57, 68, 69, 72, 73, 74, 76, 78, 83, 84, 89, 94, 97, 98, 99, 179, 185, 191, 199, 212, 213, 215, 221, 223, 229, 232, 273, 274, 276, 277, 279, 280, 292, 293, 296, 305, 306, 308, 314, 320, 321, 325, 326, 327, 337, 338, 340, 342], "efs_id": [100, 106, 111, 113], "egress": [107, 109], "eid": [73, 82], "eight": [274, 278, 311, 335], "eip": [111, 113, 126, 128, 136, 139], "eipalloc": [111, 113, 126, 128, 136, 139], "either": [8, 9, 52, 58, 73, 74, 82, 89, 169, 172, 182, 186, 194, 202, 205, 209, 212, 213, 217, 240, 264, 273, 279, 292, 338], "eject": [73, 82], "ek": [100, 101, 103, 105, 107, 110, 128, 129, 130, 135, 138, 139, 141, 142, 148, 343], "eks_cluster_nam": [126, 128, 129, 136, 139, 140, 141], "elam": [74, 88], "elaps": [237, 244], "elast": [100, 105, 111, 113, 126, 128, 136, 139], "element": [74, 89, 241, 268], "elev": [275, 315, 318], "elif": [76, 97, 276, 277, 321, 331], "elimin": [7, 43, 212, 213, 216, 275, 316], "ellipsi": [184, 196], "els": [4, 6, 9, 31, 32, 40, 62, 73, 75, 76, 82, 92, 97, 183, 195, 229, 234, 273, 274, 275, 276, 277, 278, 279, 292, 304, 310, 314, 317, 321, 323, 325, 331, 336, 338, 340, 342], "elt": [7, 43], "email": [118, 122, 149, 153, 169, 172, 173, 185, 186, 198, 200, 205, 207], "emb": [73, 81, 82, 279, 337, 342], "embed": [0, 73, 77, 78, 81, 82, 83, 182, 186, 194, 200, 205, 207, 277, 278, 331, 336, 338, 339, 342], "embedd": 0, "embedding_dim": [273, 279, 284, 339, 340, 342], "emit": [212, 213, 216, 277, 329], "emmanuel": [73, 82], "emotion": [74, 88, 89], "emploi": [7, 43], "empti": [111, 113, 116, 118, 125, 126, 128, 134, 136, 147, 149, 161, 183, 187, 195, 204, 206, 211], "emption": [276, 320], "en": [11, 72, 73, 74, 75, 76, 80, 89, 90, 92, 99, 212, 213, 219, 239, 260], "enabl": [1, 2, 3, 7, 8, 9, 10, 13, 17, 22, 27, 43, 46, 48, 53, 55, 57, 69, 73, 74, 76, 78, 83, 84, 86, 89, 98, 100, 104, 105, 107, 108, 110, 120, 151, 161, 163, 164, 168, 169, 172, 173, 176, 179, 180, 181, 183, 187, 188, 191, 192, 193, 195, 203, 206, 210, 228, 229, 231, 232, 234, 236, 237, 243, 244, 245, 274, 275, 276, 277, 278, 279, 293, 299, 301, 302, 305, 306, 311, 314, 315, 320, 321, 325, 326, 327, 331, 332, 336, 337, 340], "enable_access_log": [169, 172], "enable_auto_tool_choic": [229, 234], "enable_checkpoint": [5, 36], "enable_filestor": [118, 122], "enable_lora": [229, 232], "enable_progress_bar": [277, 278, 329, 335], "encapsul": [273, 274, 290, 306], "encod": [73, 74, 81, 82, 84, 169, 172, 212, 213, 215, 276, 278, 321, 322, 331, 332, 337, 342], "encode_batch": [279, 338], "encount": [2, 4, 5, 20, 29, 33, 75, 93, 149, 154], "encourag": [277, 279, 326, 337], "end": [2, 7, 22, 26, 27, 46, 47, 73, 74, 75, 82, 88, 89, 93, 107, 109, 162, 169, 172, 181, 193, 212, 213, 215, 216, 217, 219, 221, 225, 226, 228, 229, 232, 235, 241, 242, 243, 245, 268, 274, 275, 276, 277, 278, 279, 298, 303, 305, 306, 314, 315, 319, 320, 321, 325, 326, 331, 332, 336, 342], "endpoint": [10, 70, 75, 92, 107, 109, 183, 195, 212, 213, 217, 219, 221, 225, 226, 228, 241, 269, 270, 275, 277, 279, 319, 331, 342], "enforc": [0, 2, 7, 9, 22, 43, 61, 179, 191, 229, 233], "engag": [8, 51, 229, 232], "engin": [1, 4, 5, 8, 9, 10, 13, 30, 34, 43, 45, 46, 47, 55, 57, 68, 76, 94, 99, 100, 103, 107, 110, 122, 152, 153, 161, 162, 169, 170, 184, 196, 219, 221, 227, 229, 232, 241, 271, 273, 275, 276, 280, 318, 319, 320, 321], "engine_arg": [212, 213, 219], "engine_kwarg": [212, 213, 219, 221, 224, 227, 229, 232, 233, 234], "english": [75, 92], "enhanc": [7, 43, 47, 163, 164, 167, 168, 175, 229, 231, 234], "enjoi": [73, 74, 82, 89], "enough": [2, 9, 25, 64, 179, 191, 240, 265, 273, 274, 277, 279, 289, 307, 327, 338], "ensembl": [26, 275, 315], "ensu": [74, 89], "ensur": [2, 4, 5, 7, 8, 11, 22, 25, 30, 32, 34, 43, 53, 72, 73, 76, 77, 82, 97, 111, 112, 114, 118, 120, 123, 126, 127, 130, 136, 138, 142, 146, 149, 151, 158, 163, 169, 170, 176, 177, 179, 189, 191, 229, 233, 238, 249, 251, 273, 274, 275, 276, 277, 278, 279, 280, 283, 286, 289, 292, 293, 295, 297, 301, 310, 316, 317, 319, 321, 329, 336, 338, 340], "enter": [74, 85, 173, 178, 182, 183, 186, 190, 194, 195, 202, 205, 209], "enterpris": [183, 187, 195, 203, 206, 210, 212, 213, 218, 221, 226, 228, 229, 236], "entir": [2, 4, 5, 7, 8, 9, 22, 32, 35, 46, 51, 60, 63, 64, 74, 76, 78, 89, 97, 107, 108, 149, 150, 184, 196, 212, 213, 216, 240, 264, 265, 273, 274, 275, 276, 277, 279, 280, 283, 306, 317, 321, 331, 338, 342], "entiti": [184, 196], "entri": [173, 273, 276, 297, 325], "entropi": [274, 306], "entrypoint": [182, 186, 194, 202, 205, 209, 241, 270], "enum": [229, 233, 234], "enumer": [6, 39, 74, 86, 239, 256, 279, 338, 342], "env": [2, 21, 179, 191, 221, 226, 273, 274, 275, 276, 277, 278, 279, 281, 307, 316, 321, 327, 333, 338], "env_var": [2, 21, 22, 221, 224, 227, 229, 232, 233, 234], "environ": [0, 3, 4, 5, 7, 8, 9, 10, 17, 22, 28, 31, 35, 46, 53, 62, 69, 70, 73, 75, 76, 78, 90, 94, 95, 98, 100, 102, 105, 111, 112, 117, 118, 119, 126, 127, 135, 136, 137, 148, 149, 150, 152, 162, 163, 164, 168, 169, 170, 171, 172, 174, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 192, 193, 194, 195, 196, 198, 200, 205, 207, 212, 213, 215, 221, 224, 227, 229, 232, 234, 274, 275, 276, 277, 279, 306, 307, 315, 316, 321, 326, 327, 329, 333, 335, 336, 338], "environment": [179, 191], "eot": [111, 113, 118, 122, 126, 128, 136, 139, 149, 153], "ep": [238, 247, 253], "ephemer": [273, 281], "epic": [73, 82], "episod": [278, 336], "epoch": [4, 6, 31, 32, 40, 41, 76, 97, 98, 99, 238, 239, 247, 249, 251, 252, 253, 257, 259, 260, 273, 274, 276, 277, 278, 279, 283, 288, 289, 291, 293, 294, 299, 300, 301, 303, 305, 306, 310, 311, 312, 313, 314, 320, 323, 325, 328, 329, 330, 331, 332, 334, 335, 336, 337, 340, 341, 342], "epoch_loss": [4, 31], "equival": [8, 51, 53], "ergonom": [278, 333], "ernst": [74, 89], "ernsthugo": [74, 89], "erotica": [74, 85], "errno": [8, 53], "error": [2, 4, 5, 7, 8, 9, 20, 29, 33, 46, 53, 57, 61, 75, 78, 93, 126, 134, 136, 147, 149, 154, 163, 164, 168, 169, 172, 175, 179, 191, 229, 233, 238, 239, 252, 258, 260, 275, 277, 279, 315, 326, 337, 342], "erupt": [74, 89], "escap": [74, 89], "especi": [2, 4, 8, 19, 22, 32, 55, 73, 74, 76, 78, 88, 97, 179, 191, 240, 241, 262, 270, 273, 274, 276, 277, 287, 293, 307, 323, 327], "essenti": [7, 46, 76, 97, 100, 104, 212, 213, 220, 221, 228, 273, 299], "establish": [6, 40, 100, 102, 229, 235, 239, 257], "estim": [4, 32, 181, 193, 273, 283], "estimate_pi": [181, 193], "eta": [3, 28, 275, 277, 278, 317, 319, 326, 332], "etc": [1, 2, 3, 4, 5, 8, 9, 13, 25, 28, 30, 34, 51, 53, 59, 100, 101, 102, 105, 106, 107, 110, 169, 171, 172, 221, 224, 237, 244, 273, 275, 280, 284, 303, 317], "ether": [74, 89], "etl": [7, 43, 46, 240, 262], "euler": [277, 331], "europ": [74, 88, 89], "europa": [73, 74, 82, 89], "ev": [73, 82], "eval": [3, 4, 5, 9, 10, 28, 31, 32, 36, 62, 70, 76, 97, 238, 240, 241, 247, 253, 264, 269, 273, 274, 275, 276, 277, 278, 279, 292, 310, 314, 317, 323, 325, 331, 336, 340, 342], "eval_arrow": [275, 317], "eval_dataset": [76, 97], "eval_metr": [3, 28, 275, 317], "eval_pr": [76, 96], "evals_result": [3, 28, 275, 317], "evalu": [2, 6, 22, 40, 76, 94, 95, 96, 97, 100, 103, 162, 229, 235, 239, 241, 257, 270, 273, 274, 277, 278, 279, 292, 306, 310, 314, 315, 316, 319, 326, 331, 332, 336, 337, 342], "evan": [73, 82], "even": [2, 22, 73, 74, 75, 82, 85, 89, 92, 93, 164, 168, 181, 193, 212, 213, 216, 229, 236, 241, 270, 273, 274, 276, 289, 310, 325], "evenli": [273, 283, 284], "event": [4, 5, 7, 30, 34, 46, 163, 164, 166, 167, 175, 181, 193, 273, 280], "eventu": [273, 292], "ever": [3, 27, 74, 85, 88, 237, 243], "everi": [0, 1, 16, 73, 74, 82, 86, 180, 182, 184, 186, 192, 194, 196, 200, 205, 207, 273, 274, 275, 277, 278, 279, 283, 288, 289, 297, 298, 306, 307, 308, 312, 314, 315, 316, 317, 328, 332, 335, 340], "every_n_epoch": [277, 278, 329, 335], "everyon": [74, 89], "everyth": [74, 85, 89, 107, 109, 183, 195, 273, 274, 276, 277, 278, 279, 290, 307, 323, 331, 336, 338, 342], "evil": [74, 88, 89], "evolut": [7, 43], "evolv": [275, 279, 315, 340], "ex": [277, 327], "exact": [2, 9, 11, 22, 61, 72, 177, 189, 221, 226, 229, 233], "exactli": [100, 105, 274, 277, 279, 306, 307, 308, 314, 327, 341], "examin": [126, 133, 136, 145], "exampl": [2, 4, 6, 7, 8, 9, 10, 18, 21, 22, 23, 25, 31, 41, 43, 52, 53, 57, 59, 60, 62, 70, 71, 73, 74, 75, 76, 78, 81, 82, 84, 87, 92, 97, 101, 103, 105, 109, 111, 113, 114, 118, 121, 126, 127, 128, 136, 137, 139, 149, 150, 152, 154, 163, 165, 169, 170, 171, 172, 175, 176, 178, 182, 183, 190, 194, 195, 212, 213, 216, 217, 225, 227, 230, 231, 235, 236, 240, 241, 242, 246, 264, 270, 273, 274, 275, 276, 277, 278, 279, 281, 284, 289, 291, 293, 305, 306, 307, 312, 314, 315, 317, 318, 319, 325, 326, 331, 332, 337, 342], "exce": [75, 92], "excel": [221, 223, 229, 235], "except": [2, 5, 20, 36, 73, 82, 241, 269, 273, 274, 277, 300, 307, 327], "excess": [274, 306], "excit": [73, 82], "exclus": [4, 32, 212, 213, 217], "exdb": [238, 239, 252, 258], "execut": [2, 3, 4, 5, 6, 7, 12, 14, 16, 18, 19, 21, 23, 24, 28, 32, 35, 36, 41, 43, 47, 48, 51, 55, 56, 57, 61, 62, 63, 73, 76, 82, 83, 94, 97, 100, 101, 105, 126, 133, 136, 145, 149, 152, 163, 169, 171, 175, 178, 181, 182, 185, 190, 193, 194, 198, 200, 202, 207, 209, 212, 213, 218, 229, 231, 234, 239, 258, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 297, 306, 307, 310, 314, 315, 316, 317, 319, 320, 321, 323, 327, 333, 338, 340], "execute_notebook": 0, "exercis": 188, "exhaust": [212, 213, 217, 273, 275, 299, 317], "exhibit": [74, 89, 276, 321], "exisitng": [107, 110], "exist": [3, 8, 28, 53, 73, 74, 75, 76, 80, 85, 90, 98, 107, 108, 109, 110, 111, 112, 113, 126, 127, 128, 135, 138, 139, 148, 169, 171, 172, 178, 179, 180, 181, 183, 187, 190, 191, 192, 193, 195, 204, 206, 211, 229, 234, 273, 274, 275, 276, 277, 278, 279, 304, 306, 310, 312, 314, 319, 321, 323, 325, 329, 331, 335, 336, 338, 340, 342, 343], "exist_ok": [4, 31, 238, 247, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 329, 335, 338], "existing_vpc_id": [100, 105], "exit": [277, 330], "exogen": [276, 325], "exp": [276, 322], "expand": [111, 112, 126, 127, 136, 138, 146, 149, 151, 169, 171, 181, 193, 273, 277, 292, 328], "expect": [4, 6, 32, 41, 178, 190, 212, 213, 217, 239, 241, 258, 270, 273, 274, 275, 276, 277, 279, 282, 289, 307, 316, 325, 329, 338], "expens": [3, 6, 9, 10, 28, 40, 62, 68, 73, 74, 81, 88, 221, 223, 229, 232, 237, 239, 240, 244, 245, 257, 264, 277, 327], "expensive_comput": [2, 24], "expensive_squar": [1, 2, 16, 19, 23, 24], "experi": [1, 4, 6, 13, 32, 41, 74, 75, 85, 93, 162, 163, 164, 168, 169, 170, 175, 177, 178, 179, 183, 188, 189, 190, 191, 195, 221, 228, 229, 232, 236, 258, 273, 274, 275, 278, 281, 283, 305, 306, 313, 314, 319, 336], "experiment": [10, 71, 279, 341], "experiment_nam": [5, 36, 273, 301], "experinc": [187, 203, 206, 210], "expert": [7, 47, 74, 89, 274, 277, 278, 307, 326, 332], "expertli": [74, 89], "explain": [73, 74, 82, 88, 100, 104, 107, 109, 169, 172, 185, 197], "explan": [75, 92, 169, 171, 229, 232], "explicit": [74, 85, 229, 235, 275, 279, 317, 337], "explicitli": [5, 6, 8, 9, 35, 41, 52, 60, 73, 78, 80, 183, 187, 195, 204, 206, 211], "explor": [7, 43, 74, 85, 169, 172, 178, 180, 183, 188, 190, 192, 195, 212, 213, 214, 217, 221, 227, 228, 229, 230, 231, 236, 273, 274, 275, 276, 277, 278, 279, 305, 314, 319, 325, 331, 336, 342], "explos": [164, 168], "export": [149, 152, 163, 173, 176, 221, 224, 225, 229, 232, 278, 336], "expos": [73, 82, 107, 110], "exposehead": [100, 105, 136, 146], "exposur": [100, 105], "expr": [74, 87], "express": [4, 31, 74, 87, 181, 193, 273, 277, 281, 331], "expresswai": [73, 82], "extend": [2, 25, 100, 105, 273, 274, 275, 276, 277, 278, 279, 293, 300, 301, 305, 308, 314, 317, 319, 325, 326, 331, 336, 342], "extens": [9, 59, 178, 180, 190, 192, 229, 234], "extern": [7, 8, 46, 50, 100, 105, 110, 136, 146, 149, 156, 164, 166, 229, 231, 234, 236, 273, 305], "extra": [73, 82, 212, 213, 217, 274, 275, 276, 279, 312, 316, 323, 325, 337, 338, 340], "extra_st": [273, 300, 301], "extract": [5, 7, 9, 35, 43, 61, 136, 140, 238, 239, 252, 260, 274, 279, 312, 338, 342], "extract_dir": [279, 338], "extractal": [279, 338], "extrem": [75, 93, 164, 168, 180, 192, 212, 213, 216, 241, 270, 275, 315], "f": [2, 3, 4, 5, 6, 8, 9, 10, 18, 21, 22, 24, 28, 31, 32, 33, 35, 40, 51, 53, 62, 65, 70, 76, 97, 98, 111, 113, 164, 168, 169, 171, 173, 179, 180, 181, 183, 191, 192, 193, 195, 212, 213, 219, 221, 226, 229, 232, 233, 234, 238, 239, 247, 253, 257, 273, 274, 275, 276, 277, 278, 279, 283, 288, 292, 304, 307, 310, 314, 316, 317, 318, 319, 320, 321, 322, 323, 325, 327, 329, 331, 335, 336, 338, 340, 342], "f1": [73, 82, 274, 314], "f_": [274, 275, 276, 277, 278, 306, 315, 320, 326, 332], "face": [4, 5, 30, 34, 73, 74, 75, 78, 80, 82, 83, 85, 89, 91, 93, 95, 96, 99, 100, 105, 188, 212, 213, 217, 221, 224, 225, 229, 232, 234, 241, 268, 273, 274, 280, 293, 306, 307, 314], "facial": [74, 89], "facilit": [7, 43, 47, 180, 192], "fact": [73, 74, 82, 85, 88, 89], "factor": [340, 342], "fahrenheit": [2, 25, 229, 234], "fail": [2, 4, 5, 9, 20, 30, 34, 57, 61, 164, 168, 238, 239, 252, 258, 260, 273, 275, 276, 279, 280, 299, 301, 302, 317, 324, 340], "failur": [4, 5, 7, 9, 17, 20, 30, 34, 46, 57, 61, 163, 164, 166, 168, 175, 182, 183, 186, 187, 194, 195, 200, 203, 205, 206, 207, 210, 221, 226, 273, 274, 275, 276, 278, 280, 299, 301, 302, 303, 306, 310, 314, 315, 319, 320, 323, 325, 335], "failure_config": [273, 274, 275, 276, 277, 278, 279, 301, 303, 311, 317, 323, 329, 335, 340], "failureconfig": [274, 275, 276, 277, 278, 279, 299, 302, 305, 307, 310, 311, 314, 315, 316, 317, 320, 321, 323, 326, 327, 329, 332, 333, 335, 337, 338, 340], "fair": [73, 82], "fake": [74, 86], "fake_kei": [212, 221, 225, 229, 232, 233, 234], "fall": [279, 338], "fallback": [277, 278, 331, 336], "fals": [3, 4, 5, 6, 10, 28, 31, 32, 35, 36, 40, 41, 70, 169, 172, 237, 238, 239, 241, 244, 245, 247, 253, 257, 260, 269, 270, 273, 274, 275, 276, 277, 278, 279, 282, 285, 310, 312, 314, 316, 317, 321, 323, 325, 328, 329, 331, 334, 335, 336, 338, 340], "famili": [73, 82], "familiar": [162, 169, 170, 273, 295], "fan": [73, 74, 82, 85], "fanatic": [74, 89], "fanchant": [73, 82], "fantasi": [74, 88], "fantast": [74, 89], "far": [74, 85, 88, 275, 317], "fare": [74, 89], "fare_amount": [3, 28, 237, 244], "fashion": [8, 55, 240, 262], "fast": [4, 8, 9, 31, 50, 61, 107, 109, 188, 212, 213, 217, 218, 229, 235, 273, 274, 275, 278, 280, 281, 307, 310, 315, 319, 332], "fastapi": [3, 7, 26, 28, 48, 91, 93, 173, 183, 187, 195, 204, 206, 211], "fastapideploy": [183, 187, 195, 204, 206, 211], "faster": [6, 41, 74, 76, 87, 98, 221, 227, 228, 273, 277, 292, 331], "fastest": [178, 190], "fate": [74, 89], "father": [74, 89], "fault": [4, 5, 7, 8, 9, 30, 34, 46, 50, 55, 57, 100, 104, 105, 187, 203, 206, 210, 221, 226, 275, 276, 277, 278, 279, 280, 300, 301, 303, 306, 310, 311, 314, 315, 316, 317, 319, 320, 324, 325, 326, 327, 329, 331, 332, 335, 336, 337, 340, 341, 342], "fc": [238, 247, 253], "feasibl": [278, 336], "featur": [3, 7, 28, 43, 46, 47, 49, 51, 52, 74, 75, 85, 90, 100, 103, 107, 109, 163, 164, 165, 169, 172, 176, 178, 190, 212, 213, 217, 218, 221, 226, 227, 228, 236, 245, 267, 274, 276, 277, 279, 307, 314, 315, 316, 317, 319, 325, 327, 342], "feature_col": [275, 317, 318], "feature_column": [275, 316, 317, 318, 319], "feature_nam": [275, 317], "feb": [73, 82], "fed": [73, 82], "feder": [100, 105], "fee": [3, 28, 237, 244], "feed": [8, 50, 52, 73, 82, 273, 274, 276, 293, 308, 320, 323], "feedback": [178, 190], "feel": [73, 74, 82, 85, 88, 89, 183, 187, 195, 204, 206, 211, 273, 274, 275, 277, 278, 279, 295, 314, 319, 331, 336, 342], "femal": [74, 85], "fenc": [73, 82], "fend": [74, 88], "ferrari": [73, 82], "ferri": [73, 82], "fetch": [4, 5, 17, 18, 19, 30, 34, 76, 97, 273, 275, 276, 280, 281, 292, 295, 316, 321], "fetch_covtyp": [275, 316], "few": [73, 74, 83, 84, 85, 88, 89, 149, 156, 169, 170, 177, 178, 179, 189, 190, 191, 229, 232, 274, 275, 276, 277, 278, 279, 314, 319, 325, 326, 329, 331, 336, 337, 338, 342], "fewer": [7, 46, 118, 122], "ff": [73, 82], "fiat": [73, 82], "fid": [277, 331], "field": [8, 51, 183, 187, 195, 204, 206, 211, 229, 233, 273, 278, 297, 333], "fifo": [76, 99, 237, 238, 239, 244, 245, 252, 258, 260], "fifoschedul": [6, 41, 239, 259, 260], "fig": [6, 39, 238, 239, 247, 253, 256, 274, 277, 307, 327, 331], "figsiz": [4, 6, 31, 32, 39, 238, 239, 247, 253, 256, 273, 274, 275, 276, 277, 278, 279, 281, 292, 307, 312, 316, 321, 323, 325, 327, 329, 331, 335, 338, 340], "figur": [4, 8, 31, 32, 51, 240, 263, 273, 274, 276, 277, 278, 279, 281, 292, 312, 321, 323, 325, 329, 335, 338, 340], "file": [0, 3, 4, 5, 6, 7, 10, 11, 28, 31, 32, 35, 37, 42, 43, 51, 53, 55, 58, 59, 62, 66, 71, 72, 73, 77, 82, 100, 105, 107, 109, 111, 113, 117, 118, 121, 122, 126, 128, 129, 136, 139, 141, 149, 152, 153, 164, 168, 169, 172, 178, 179, 181, 182, 183, 188, 190, 191, 193, 194, 195, 229, 232, 237, 238, 244, 245, 254, 263, 264, 273, 274, 276, 277, 278, 279, 281, 289, 296, 301, 304, 305, 306, 307, 308, 310, 314, 321, 327, 331, 332, 336, 337, 338, 342], "file_nam": [229, 232], "filenam": [178, 190, 273, 277, 278, 281, 329, 335], "filenotfounderror": [8, 53, 274, 277, 314, 331], "filestor": [107, 109, 118, 119, 122, 149, 153], "filestore_capacity_gb": [118, 122], "filestore_instance_nam": [118, 122, 149, 153], "filestore_loc": [118, 122, 149, 153], "filestore_ti": [118, 122], "filesystem": [5, 35, 76, 99, 169, 171, 238, 239, 253, 258, 273, 276, 281, 320], "fill": [73, 74, 82, 89], "film": [73, 74, 82, 85, 88, 89], "filmbr": [74, 89], "filmmak": [74, 85], "filter": [8, 55, 84, 88, 89, 111, 113, 118, 121, 126, 128, 136, 139, 149, 152, 154, 169, 172, 179, 181, 191, 193, 273, 279, 282, 337, 339, 342], "filterwarn": [277, 278, 329, 335], "final": [1, 2, 6, 9, 16, 23, 41, 65, 74, 76, 88, 89, 99, 149, 152, 181, 188, 193, 229, 234, 239, 240, 260, 266, 273, 274, 275, 276, 277, 278, 279, 282, 287, 291, 292, 298, 302, 303, 304, 305, 310, 311, 313, 314, 317, 319, 323, 325, 328, 336, 338, 342], "find": [2, 3, 4, 5, 9, 21, 28, 30, 34, 61, 74, 85, 88, 89, 100, 105, 111, 113, 118, 125, 126, 128, 136, 139, 140, 146, 149, 152, 161, 169, 171, 180, 181, 182, 183, 184, 188, 192, 193, 194, 195, 196, 273, 280, 305], "fine": [6, 40, 74, 88, 107, 109, 182, 186, 194, 200, 205, 207, 229, 231, 232, 273, 274, 275, 277, 305, 306, 310, 319, 329, 331], "finer": [2, 22], "finest": [73, 80, 82], "finetun": [4, 5, 6, 32, 37, 41, 75, 92, 238, 239, 254, 259], "finish": [2, 24, 212, 213, 216, 273, 274, 275, 278, 291, 313, 317, 336], "fiorentina": [73, 82], "fiorina": [73, 82], "fir": [73, 82, 275, 315], "fire": [73, 82], "firewal": [100, 105, 118, 122, 125, 149, 153], "firewall_policy_nam": [118, 122, 149, 153], "first": [1, 2, 3, 5, 6, 10, 11, 13, 14, 16, 19, 21, 27, 36, 41, 68, 70, 72, 73, 74, 80, 82, 85, 88, 89, 111, 114, 118, 121, 123, 126, 130, 136, 140, 142, 149, 152, 158, 164, 168, 180, 181, 192, 193, 212, 213, 216, 217, 221, 224, 229, 232, 237, 238, 239, 241, 243, 247, 251, 260, 269, 270, 273, 274, 276, 277, 279, 282, 292, 310, 321, 327, 329, 331, 338, 342], "fit": [3, 4, 6, 9, 28, 31, 32, 35, 40, 41, 64, 74, 76, 84, 98, 212, 213, 216, 237, 238, 239, 240, 244, 245, 247, 252, 253, 257, 258, 259, 260, 263, 265, 274, 275, 276, 277, 278, 279, 291, 296, 298, 302, 303, 306, 311, 313, 315, 317, 319, 323, 324, 325, 329, 330, 335, 340, 341], "fit_model": [6, 40], "five": [74, 88, 89, 274, 277, 278, 311, 329, 335], "fix": [76, 97, 273, 276, 277, 279, 283, 321, 326, 338], "flag": [181, 193], "flap": [74, 85], "flashi": [74, 89], "flatten": [274, 277, 307, 327], "flavor": [179, 191], "flawless": [73, 82], "fleet": [74, 85, 89, 276, 320], "flew": [73, 82], "flexibl": [2, 7, 8, 10, 23, 43, 45, 46, 51, 69, 73, 74, 76, 78, 84, 98, 185, 199, 212, 213, 218, 229, 230, 231, 240, 241, 262, 263, 268], "flexibli": [241, 268], "flink": [7, 46], "flip": [274, 306], "flip_sin_to_co": [5, 35], "flippen": [74, 88], "float": [2, 3, 4, 5, 6, 8, 9, 10, 20, 28, 31, 32, 35, 40, 41, 51, 62, 70, 73, 83, 237, 238, 239, 240, 241, 244, 247, 251, 257, 258, 259, 264, 269, 273, 275, 276, 277, 278, 279, 289, 301, 318, 323, 328, 334, 340], "float16": [5, 35], "float32": [10, 70, 73, 82, 83, 237, 244, 245, 276, 277, 278, 279, 321, 322, 325, 327, 333, 336, 340], "floral": [111, 113, 126, 128, 136, 139], "flore": [73, 82], "flow": [46, 100, 105, 163, 175, 185, 199, 229, 234, 278, 332], "flush": [2, 20, 212, 213, 219, 221, 225, 226, 229, 232], "fly": [273, 293], "fmt": [275, 318], "fn_arg": [229, 234], "fn_call": [229, 234], "fn_callabl": [229, 234], "fn_constructor_arg": [274, 275, 276, 314, 318, 319, 325], "fn_constructor_kwarg": [9, 62, 240, 241, 264, 269], "fn_kwarg": [240, 264], "fname": [276, 321], "foam": [73, 82], "focu": [74, 85, 164, 167, 188, 229, 231, 273, 276, 293, 320], "focus": [0, 7, 43, 45, 73, 82, 107, 109, 163, 176, 212, 213, 215, 276, 325], "folder": [3, 4, 8, 9, 10, 28, 31, 53, 62, 70, 77, 173, 178, 180, 182, 190, 192, 194, 229, 232, 274, 275, 276, 314, 319, 325], "follow": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 16, 18, 30, 31, 32, 34, 35, 36, 41, 45, 48, 51, 57, 62, 68, 72, 74, 76, 89, 98, 100, 103, 105, 107, 109, 111, 112, 118, 120, 125, 126, 127, 129, 134, 136, 138, 141, 146, 147, 149, 151, 154, 161, 162, 163, 164, 168, 169, 172, 173, 175, 176, 177, 178, 179, 180, 181, 184, 188, 189, 190, 191, 192, 193, 196, 202, 209, 212, 213, 219, 221, 223, 229, 232, 235, 238, 239, 240, 241, 253, 258, 259, 263, 265, 268, 273, 274, 275, 276, 278, 279, 280, 282, 293, 294, 306, 308, 315, 320, 325, 336, 337, 342], "followup": [74, 89], "fontsiz": [274, 277, 307, 327], "food": [314, 326, 331], "food101": [274, 277, 306, 307, 314, 327], "food101_diffusion_ft": [277, 329], "food101_diffusion_result": [277, 329], "food101_ft_resum": [274, 311, 314], "food101_ft_run": [274, 314], "food101_lit": [274, 277, 307, 308, 310, 311, 314, 327], "food101_single_run": [274, 314], "food101dataset": [306, 309, 314], "footag": [73, 82], "footbal": [73, 80, 82], "footer": 0, "forbidden": [238, 239, 252, 258, 260], "forc": [273, 274, 292, 314, 320, 322, 325], "forcibli": [74, 88], "ford": [74, 85], "forecast": 325, "foreground": [74, 89], "foregroundbr": [74, 89], "forest": [316, 318], "forg": [11, 72], "forget": [73, 74, 82, 89], "forgotten": [74, 89], "forgottenbr": [74, 89], "fork": 0, "form": [2, 18, 74, 89, 237, 243, 244, 245, 279, 338], "format": [6, 9, 39, 59, 61, 76, 97, 173, 188, 229, 231, 232, 233, 234, 236, 239, 256, 273, 274, 276, 277, 279, 293, 296, 306, 307, 321, 327, 338], "fort": [74, 88], "forum": [221, 228, 229, 236], "forward": [5, 35, 73, 76, 82, 97, 238, 247, 273, 275, 276, 278, 279, 280, 283, 294, 300, 316, 322, 325, 328, 329, 332, 334, 335, 337, 339], "found": [6, 41, 74, 89, 100, 105, 239, 258, 273, 274, 275, 277, 278, 279, 285, 300, 303, 304, 314, 317, 331, 336, 337], "foundat": [6, 7, 40, 43, 220, 229, 236, 239, 257, 279, 342], "four": [4, 5, 32, 36, 212, 213, 216, 273, 279, 282, 338], "fourth": [73, 74, 82, 88, 89], "fox": [73, 82], "foxx": [73, 82], "fp16": [212, 213, 217, 221, 223, 227], "fp8": [221, 227], "frac": [274, 308], "fraction": [10, 17, 25, 68, 75, 92, 268], "fragrant": [73, 82], "frame": [275, 316, 317], "framework": [3, 4, 5, 8, 10, 27, 28, 30, 34, 43, 48, 50, 67, 69, 74, 75, 84, 90, 188, 236, 237, 240, 241, 243, 244, 245, 263, 267, 268, 273, 280], "franc": [212, 213, 219, 229, 232], "francisco": [229, 234], "frank": [73, 82], "fraud": [7, 46], "freak": [74, 88], "free": [2, 21, 73, 76, 78, 82, 94, 181, 182, 183, 187, 188, 193, 194, 195, 204, 206, 211, 221, 227, 273, 274, 275, 276, 292, 314, 319, 325], "freed": [273, 292], "freeli": [8, 51, 240, 263], "freq_shift": [5, 35], "frequenc": [5, 35, 275, 279, 316, 338], "frequent": [7, 43, 46], "fresh": [273, 292], "fri": [274, 306], "friction": [1, 13], "fridai": [73, 82], "friend": [73, 82], "friendli": [164, 168, 182, 186, 194, 200, 205, 207, 221, 223, 274, 277, 279, 306, 307, 327, 342], "frighten": [74, 88, 89], "frill": [74, 89], "from": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 18, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 38, 41, 46, 50, 52, 53, 55, 56, 57, 59, 61, 62, 67, 68, 71, 72, 73, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 88, 89, 91, 93, 95, 96, 97, 100, 104, 105, 107, 110, 111, 112, 113, 115, 118, 119, 122, 124, 125, 126, 127, 128, 130, 131, 133, 136, 137, 140, 142, 143, 145, 146, 149, 150, 158, 159, 160, 161, 163, 164, 166, 168, 169, 171, 172, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 189, 190, 191, 192, 193, 194, 195, 196, 199, 202, 204, 206, 209, 211, 212, 213, 215, 216, 217, 219, 221, 222, 224, 225, 226, 227, 229, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 244, 245, 247, 249, 251, 252, 256, 257, 258, 259, 262, 263, 264, 269, 270, 271, 274, 280, 281, 282, 283, 287, 288, 289, 290, 291, 292, 293, 294, 296, 297, 298, 299, 300, 301, 304, 305, 306, 307, 308, 310, 312, 313, 314, 315, 316, 317, 320, 321, 323, 325, 326, 327, 329, 332, 333, 335, 337, 340, 342], "from_directori": [4, 32, 238, 251, 273, 274, 276, 279, 289, 301, 310, 314, 323, 325, 340], "from_huggingfac": [73, 80], "from_item": [74, 86, 164, 168, 241, 269, 276, 277, 278, 325, 327, 333], "from_numpi": [274, 276, 314, 325], "from_pretrain": [5, 35, 76, 97], "from_pydict": [274, 307], "from_pylist": [276, 321], "from_torch": [9, 59], "fromarrai": [273, 297], "front": [274, 275, 307, 316], "frontal": [74, 85], "fr\u00e9chet": [277, 331], "fsdp": [4, 32, 273, 281, 286, 305], "ft": [73, 80, 82], "fuck": [73, 82], "full": [3, 5, 8, 9, 28, 35, 51, 57, 63, 74, 88, 89, 100, 103, 105, 107, 109, 136, 146, 164, 168, 169, 172, 182, 194, 200, 207, 229, 232, 240, 264, 274, 275, 276, 277, 278, 279, 284, 291, 299, 305, 307, 308, 312, 314, 317, 323, 326, 332, 338, 340, 342], "full_path": [274, 308], "fulli": [2, 4, 5, 18, 30, 34, 100, 101, 177, 185, 188, 189, 198, 241, 270, 273, 274, 275, 277, 278, 279, 280, 282, 293, 298, 306, 310, 315, 317, 326, 329, 332, 337, 338, 340], "fullyshardeddataparallel": [4, 32, 273, 286], "function": [2, 3, 4, 5, 6, 8, 9, 10, 12, 16, 20, 25, 28, 32, 33, 35, 36, 41, 43, 47, 51, 52, 53, 54, 59, 61, 64, 65, 69, 73, 74, 75, 82, 87, 89, 92, 94, 96, 99, 136, 146, 180, 184, 192, 196, 221, 224, 229, 231, 234, 236, 238, 239, 240, 249, 250, 251, 253, 258, 259, 260, 263, 265, 266, 273, 274, 275, 276, 277, 278, 279, 282, 283, 286, 288, 289, 297, 306, 310, 315, 319, 320, 327, 332, 335, 336, 337, 338, 340], "fundament": [2, 10, 17, 69, 107, 109, 162, 169, 170, 212, 213, 214, 215, 219, 241, 268], "further": [9, 59, 62, 74, 75, 89, 92, 179, 191, 273, 274, 291, 308], "fuse": [9, 61], "futur": [1, 8, 15, 54, 74, 85, 88, 89, 169, 172, 276, 278, 320, 321, 322, 323, 332], "future_tru": [276, 325], "g": [0, 1, 2, 6, 7, 8, 9, 10, 13, 19, 20, 22, 41, 43, 46, 53, 57, 59, 61, 68, 69, 73, 78, 80, 82, 100, 105, 107, 109, 110, 111, 112, 118, 119, 125, 126, 127, 136, 137, 149, 150, 161, 163, 164, 166, 169, 171, 175, 178, 180, 181, 183, 184, 187, 190, 192, 193, 195, 196, 203, 204, 206, 210, 211, 221, 227, 237, 239, 240, 244, 245, 259, 263, 264, 273, 275, 276, 279, 283, 284, 289, 290, 291, 293, 295, 299, 302, 305, 317, 325, 340], "g54aiirwj1": [180, 192], "g54aiirwj1s8t9ktgzikqur41k": [180, 192], "gain": [275, 279, 318, 342], "galaxi": [73, 82], "galleri": [73, 82], "gallo": [74, 85], "gambl": [74, 88], "game": [73, 82], "gan": [277, 326], "gandhi": [73, 82], "gang": [74, 88], "gannon": [74, 88], "gap": [7, 43, 229, 232, 241, 271, 276, 321], "gape": [74, 85], "garbag": [1, 13, 273, 281, 292], "garden": [73, 82], "gate": [221, 224, 225, 229, 233, 234], "gatewai": [100, 105, 111, 113, 126, 128, 136, 139], "gather": [3, 28, 181, 193, 274, 307], "gaussian": [276, 277, 278, 325, 326, 331, 333], "gb": [2, 18, 180, 192, 221, 223, 227], "gc": [8, 53, 107, 109, 118, 119, 125, 149, 161, 229, 232, 273, 274, 279, 281, 289, 292, 305, 314, 342], "gca": [275, 318], "gce": [100, 101, 107, 110, 123, 149, 152, 343], "gcloud": [118, 120, 121, 149, 151, 152, 154, 155, 161], "gcp": [100, 101, 105, 107, 109, 110, 120, 121, 122, 124, 125, 149, 151, 152, 153, 157, 162, 184, 185, 196, 198], "gcp_if_": [118, 122], "gcp_project_id": [118, 121, 122, 149, 152, 153, 155, 161], "gcp_region": [118, 121, 122, 149, 152, 153, 154, 155, 159, 161], "gcs_bucket_nam": [118, 122, 125, 149, 153, 161], "gear": [73, 82], "gee": [73, 82], "gener": [1, 8, 9, 10, 11, 13, 35, 52, 54, 60, 62, 64, 70, 72, 73, 74, 77, 78, 81, 85, 118, 122, 164, 168, 169, 171, 172, 173, 180, 181, 182, 186, 192, 193, 194, 200, 205, 207, 216, 220, 229, 231, 232, 233, 234, 235, 236, 238, 240, 241, 247, 253, 263, 264, 265, 270, 273, 276, 279, 281, 292, 305, 320, 325, 329, 332, 336, 337, 342, 343], "generate_synthetic_imag": [164, 168], "generated_bi": [164, 168], "generative_cv": [277, 329, 331], "genit": [74, 85], "geniu": [74, 89], "genr": [279, 342], "geo": [275, 315], "german": [74, 89], "germani": [73, 74, 82, 89], "get": [3, 4, 5, 7, 10, 11, 17, 18, 19, 20, 21, 22, 23, 25, 26, 28, 32, 36, 40, 46, 70, 72, 73, 74, 75, 76, 82, 88, 89, 92, 93, 97, 100, 105, 111, 112, 113, 114, 118, 122, 123, 126, 127, 128, 130, 132, 133, 134, 136, 138, 140, 142, 144, 145, 146, 147, 149, 155, 156, 158, 164, 167, 169, 171, 172, 178, 180, 181, 182, 186, 190, 192, 193, 194, 201, 205, 208, 214, 220, 221, 224, 226, 227, 230, 232, 234, 237, 241, 242, 244, 245, 257, 260, 269, 270, 273, 274, 275, 276, 277, 278, 279, 282, 283, 292, 307, 310, 314, 317, 321, 323, 325, 326, 329, 331, 335, 336, 338, 340], "get_best_result": [3, 6, 28, 41, 237, 239, 244, 245, 258, 260], "get_checkpoint": [273, 274, 275, 276, 277, 278, 279, 299, 300, 305, 306, 307, 310, 316, 317, 319, 321, 323, 327, 329, 333, 335, 338, 340], "get_config_dict": [5, 35], "get_context": [3, 4, 28, 32, 238, 249, 251, 253, 273, 274, 275, 276, 277, 278, 279, 283, 288, 289, 294, 300, 301, 307, 310, 316, 317, 321, 323, 327, 329, 333, 335, 338, 340], "get_current_temperatur": [229, 234], "get_dataset_shard": [5, 36, 273, 275, 277, 278, 279, 295, 316, 317, 329, 335, 338, 340], "get_devic": [5, 33, 35, 76, 97], "get_linear_schedule_with_warmup": [5, 33, 35], "get_model": [275, 317, 318], "get_scor": [275, 318, 319], "get_temperature_d": [229, 234], "get_us": 173, "get_user_profil": 173, "get_world_rank": [3, 4, 28, 32, 238, 251, 253, 273, 274, 275, 276, 277, 278, 279, 288, 289, 301, 310, 317, 323, 329, 335, 340], "get_world_s": [4, 32, 238, 249, 253, 273, 283, 294, 300], "getbucketloc": [100, 105], "getenv": [179, 180, 191, 192], "getlogg": [169, 172], "getobject": [100, 105], "getsizeof": [2, 18], "gettempdir": [277, 278, 329, 335], "gettingstart": 188, "getvalu": [274, 277, 307, 327], "gh": 0, "ghetto": [73, 82], "giant": [74, 89], "gib": [237, 239, 244, 245, 258, 260], "gift": [73, 82], "girl": [73, 82], "git": [0, 183, 184, 187, 188, 195, 196, 204, 206, 211], "github": [0, 11, 72, 100, 105, 126, 129, 131, 136, 141, 143, 149, 156, 159, 183, 188, 195, 276, 321], "githubusercont": [276, 321], "give": [6, 40, 74, 88, 136, 140, 177, 179, 183, 184, 189, 191, 195, 196, 212, 213, 218, 239, 257, 273, 275, 279, 282, 301, 316, 338, 342], "given": [1, 3, 4, 6, 7, 9, 14, 28, 32, 41, 46, 47, 48, 64, 75, 92, 169, 172, 184, 196, 229, 234, 238, 239, 240, 251, 259, 265, 275, 276, 277, 278, 315, 320, 328, 332, 334], "gke": [100, 101, 103, 107, 110, 152, 153, 155, 158, 343], "glanc": [177, 189], "glass": [73, 82], "glob": [277, 278, 331, 333, 336], "global": [111, 113, 118, 122, 126, 128, 136, 139, 149, 152, 153, 273, 274, 277, 279, 283, 308, 310, 328, 338], "global_batch_s": [4, 32, 76, 98, 238, 249, 252, 253, 273, 283, 284, 290, 294, 298, 300, 301, 303], "gloriou": [74, 88], "gloss": [74, 89], "gm": [73, 80, 82], "go": [2, 19, 73, 74, 78, 82, 88, 89, 100, 105, 177, 189, 212, 213, 218, 221, 227, 229, 232, 274, 307], "goal": [3, 28, 73, 76, 82, 94, 187, 188, 203, 206, 210, 274, 278, 306, 332], "goaldotcom": [73, 82], "god": [73, 82], "goe": [73, 74, 80, 82, 85, 181, 184, 193, 196], "gold": [74, 88], "golions2012": [73, 82], "gone": [74, 89], "gonna": [73, 82], "good": [6, 7, 40, 48, 73, 74, 82, 85, 88, 89, 111, 113, 126, 128, 136, 139, 164, 168, 221, 223, 239, 257, 273, 275, 276, 281, 292, 296, 318, 321], "googl": [7, 43, 73, 82, 100, 102, 103, 107, 110, 120, 122, 151], "google_cloud": [149, 152], "google_project_id": [118, 122, 149, 153, 161], "google_region": [118, 122, 149, 153, 161], "googleapi": [118, 121, 149, 152], "gore": [74, 89], "got": [2, 24, 73, 74, 82, 89], "gotrib": [73, 82], "gotten": [74, 89], "gpu": [2, 3, 6, 7, 8, 9, 10, 11, 22, 25, 26, 28, 29, 30, 33, 34, 38, 40, 41, 47, 55, 57, 61, 62, 66, 68, 69, 72, 73, 75, 76, 78, 82, 83, 92, 94, 97, 98, 99, 107, 110, 111, 117, 126, 129, 136, 141, 163, 164, 166, 175, 177, 179, 189, 191, 212, 213, 214, 215, 216, 217, 218, 221, 222, 223, 224, 225, 227, 228, 229, 230, 235, 236, 237, 239, 240, 242, 244, 245, 252, 257, 258, 260, 262, 264, 268, 273, 274, 275, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 292, 293, 298, 305, 306, 309, 311, 314, 317, 320, 326, 329, 331, 332, 335, 336, 337, 340, 342], "grab": [274, 277, 314, 327], "gracefulli": [75, 93, 183, 187, 195, 203, 206, 210], "grad": [273, 283], "grade": [212, 213, 214, 218, 221, 228, 229, 236, 277, 331], "grader": [73, 82], "gradient": [3, 4, 5, 28, 32, 36, 76, 97, 237, 238, 244, 249, 273, 275, 276, 280, 283, 286, 289, 315, 316, 321], "gradual": [164, 168, 187, 203, 206, 210, 275, 277, 315, 326], "grafana": [169, 171, 172, 175, 181, 183, 187, 193, 195, 203, 206, 210, 221, 227], "grai": [4, 6, 9, 31, 32, 39, 60, 238, 239, 241, 247, 253, 256, 270, 273, 281, 292], "grain": [2, 10, 22, 69, 107, 109], "grand": [73, 82], "grant": [74, 85, 100, 105, 107, 109], "granular": [169, 172, 184, 196], "granularli": [241, 270], "graph": [1, 2, 13, 19, 163, 164, 168, 176], "grass": [74, 89], "grayscal": [4, 6, 31, 39, 238, 239, 247, 256, 273, 281, 282, 292], "great": [73, 74, 75, 82, 89, 93, 184, 196, 221, 226], "greater": [180, 181, 192, 193, 237, 244], "greec": [73, 82], "green": [274, 277, 306, 326], "grei": [73, 82], "grep": [111, 113, 126, 128, 132, 133, 134, 136, 139, 144, 145, 147], "grid": [6, 41, 239, 259, 273, 274, 276, 277, 278, 279, 281, 292, 312, 321, 323, 325, 329, 335, 336, 340], "grim": [74, 88, 89], "grimnoir": [74, 89], "ground": [9, 61, 64, 240, 265, 273, 274, 276, 278, 281, 314, 320, 323, 325, 333], "ground_truth_label": [9, 64, 240, 265], "group": [49, 56, 73, 82, 104, 106, 107, 109, 111, 113, 117, 126, 128, 136, 139, 140, 146, 162, 177, 181, 184, 185, 189, 193, 196, 198, 239, 258, 260, 261, 273, 274, 276, 277, 279, 280, 290, 306, 307, 308, 312, 320, 326, 342], "groupbi": [7, 47, 55, 274, 276, 279, 312, 323, 338, 340], "grouplen": [279, 338], "grow": [1, 3, 13, 27, 74, 85, 188, 237, 243], "grpc": [7, 10, 48, 68, 70, 241, 269], "gserviceaccount": [118, 122, 149, 153], "gsutil": [118, 125, 149, 161], "gt": [74, 89], "guarante": [7, 47, 229, 233, 273, 289], "guard": [274, 279, 312, 340], "gucci": [73, 82], "gui": [73, 82], "guid": [4, 5, 9, 11, 32, 36, 59, 72, 73, 75, 76, 82, 92, 99, 107, 109, 111, 112, 126, 127, 136, 137, 149, 150, 162, 163, 169, 170, 172, 174, 176, 183, 188, 195, 204, 211, 221, 228, 229, 231, 232, 233, 234, 236, 238, 239, 251, 253, 260, 273, 289], "gunman": [74, 88], "gym": [278, 332, 333, 336], "gymnasium": [278, 332, 333], "gz": [238, 239, 252, 258, 260], "h": [4, 10, 31, 70, 173, 238, 247, 273, 274, 276, 277, 292, 314, 321, 326, 327, 328], "ha": [2, 4, 5, 7, 11, 18, 30, 31, 34, 46, 47, 48, 72, 73, 74, 75, 76, 80, 82, 83, 85, 88, 89, 93, 97, 100, 103, 118, 125, 136, 146, 149, 161, 164, 168, 169, 172, 177, 179, 180, 181, 184, 185, 187, 189, 191, 192, 193, 196, 198, 203, 206, 210, 221, 223, 229, 232, 236, 241, 270, 273, 279, 280, 282, 296, 297, 338], "had": [8, 54, 73, 74, 82, 85], "hadoop": [7, 46], "hahha": [73, 82], "hail": [74, 89, 276, 320], "half": [73, 74, 82, 88, 89, 276, 320, 321], "halfstarv": [74, 89], "halloween": [73, 80, 82], "halv": [6, 41, 239, 259], "ham": [73, 82], "hamburg": [274, 306], "hand": [11, 72, 73, 74, 82, 88, 162, 188, 212, 213, 220, 229, 231, 274, 279, 306, 338], "handheld": [74, 89], "handl": [2, 3, 7, 9, 10, 20, 22, 25, 28, 43, 46, 47, 48, 57, 68, 69, 73, 74, 75, 76, 83, 84, 90, 94, 97, 100, 105, 107, 108, 109, 149, 156, 163, 164, 168, 173, 175, 177, 182, 187, 188, 189, 194, 200, 203, 206, 207, 210, 212, 213, 215, 218, 221, 224, 226, 229, 233, 237, 241, 244, 245, 270, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 287, 292, 293, 294, 298, 299, 301, 304, 306, 307, 309, 310, 312, 316, 317, 320, 326, 329, 331, 332, 333, 337, 338, 340, 342], "handwritten": [6, 10, 39, 40, 70, 239, 256, 257, 273, 281], "hang": [74, 88, 89, 163, 175, 278, 332], "happen": [1, 6, 7, 15, 41, 48, 74, 88, 89, 164, 168, 239, 258, 273, 301], "happi": [73, 82], "happybirthdayremuslupin": [73, 80, 82, 83], "hard": [74, 88, 89], "hardli": [74, 85], "hardwar": [4, 5, 9, 10, 30, 34, 57, 61, 62, 68, 69, 76, 94, 97, 98, 99, 179, 181, 188, 191, 193, 212, 213, 217, 218, 223, 224, 225, 236, 273, 275, 280, 293, 301, 315], "hark": [74, 88], "harm": 12, "harri": [73, 82], "hasattr": [274, 310], "hasek": [73, 82], "hash": [8, 54], "hashicorp": [111, 112, 118, 120, 126, 127, 136, 138, 149, 151], "hashtag": [73, 82], "hat": [274, 279, 306, 337], "hater": [73, 82], "have": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 20, 23, 24, 26, 32, 36, 41, 45, 46, 53, 57, 61, 62, 64, 68, 70, 72, 73, 74, 75, 76, 82, 85, 86, 87, 88, 89, 90, 97, 111, 112, 113, 117, 118, 120, 126, 127, 128, 129, 135, 136, 138, 139, 141, 148, 149, 151, 164, 168, 169, 170, 178, 179, 180, 184, 188, 190, 191, 192, 196, 212, 213, 216, 221, 228, 238, 239, 240, 241, 248, 259, 263, 265, 268, 273, 274, 275, 279, 289, 298, 306, 314, 316, 317, 337], "hdf": [7, 8, 46, 53], "he": [73, 74, 82, 88, 89], "head": [2, 8, 18, 22, 51, 53, 74, 89, 100, 104, 105, 107, 109, 126, 133, 136, 145, 164, 167, 168, 177, 179, 180, 181, 182, 183, 184, 189, 191, 192, 193, 194, 195, 196, 203, 210, 239, 258, 260, 275, 276, 277, 278, 316, 321, 325, 329, 335], "head_nod": [126, 133, 136, 145], "header": [0, 4, 31, 238, 247, 279, 342], "headlei": [73, 80, 82], "headless": [107, 109], "headnodeconfig": [126, 133, 136, 145], "health": [107, 108, 173, 187, 203, 206, 210, 212, 213, 217, 277, 329], "healthi": [179, 183, 191, 195, 276, 323], "heap": [2, 24], "hear": [73, 82], "heard": [74, 85], "heart": [74, 88, 276, 323], "heat": [75, 93], "heatmap": [275, 318], "heaven": [74, 88], "heavi": [4, 8, 9, 32, 54, 64, 177, 189, 240, 265, 273, 293], "heavili": [74, 89], "heavli": [273, 287], "hebdo": [73, 82], "hei": [73, 82, 100, 105], "height": [9, 61, 164, 168, 240, 241, 264, 270, 275, 277, 319, 327], "held": [73, 82], "hell": [73, 82], "hello": [73, 82, 169, 172, 178, 180, 182, 186, 190, 192, 194, 201, 202, 205, 208, 209, 212], "hello_world": [11, 72, 178, 182, 190, 194], "helm": [107, 108, 126, 127, 128, 129, 131, 134, 136, 138, 139, 141, 143, 147, 149, 151, 156, 159, 161], "helm_upgrade_command": [126, 128, 136, 139], "help": [6, 7, 9, 11, 40, 46, 61, 72, 74, 89, 100, 103, 107, 109, 163, 169, 171, 175, 178, 190, 212, 213, 216, 239, 241, 257, 270, 273, 275, 276, 281, 316, 318, 320, 321], "helper": [229, 234, 273, 275, 279, 281, 286, 287, 288, 289, 306, 314, 316, 321, 338], "helper_tool_map": [229, 234], "her": [73, 74, 82, 85, 88, 89], "here": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 16, 17, 25, 26, 28, 29, 31, 32, 33, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 52, 53, 54, 55, 56, 59, 61, 64, 67, 69, 70, 71, 73, 74, 75, 76, 78, 82, 84, 86, 88, 89, 90, 92, 94, 111, 113, 126, 128, 136, 146, 164, 168, 169, 170, 171, 172, 188, 212, 213, 214, 215, 218, 221, 222, 228, 229, 230, 232, 233, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 251, 253, 254, 255, 257, 258, 259, 261, 263, 264, 265, 267, 268, 269, 270, 273, 277, 278, 279, 281, 283, 284, 291, 292, 294, 301, 305, 326, 327, 332, 337], "herm": [229, 234], "hero": [73, 74, 82, 89], "herself": [74, 89], "heteregen": [9, 57], "heterogen": [3, 7, 8, 9, 28, 47, 55, 57, 66, 240, 262, 264], "hf": [221, 224], "hf_d": [277, 327], "hf_dataset": [73, 80], "hf_token": [221, 224, 225, 226, 227, 229, 232, 233, 234], "hi": [73, 74, 80, 82, 85, 88, 89], "hidden": [0, 74, 89, 278, 336], "hide": [74, 88], "hierarch": [169, 172], "hierarchi": [185, 199], "high": [2, 4, 5, 6, 7, 9, 10, 22, 31, 32, 36, 40, 43, 46, 47, 61, 68, 69, 73, 74, 75, 82, 83, 89, 90, 93, 100, 105, 183, 187, 195, 203, 206, 210, 212, 213, 215, 216, 217, 218, 229, 235, 238, 239, 241, 247, 249, 257, 268, 273, 274, 278, 279, 282, 290, 306, 336, 337, 338, 340], "higher": [7, 47, 180, 192, 212, 213, 216, 221, 227, 228, 229, 235, 240, 262, 273, 279, 293, 337, 338], "highest": [74, 88, 277, 279, 329, 342], "highli": [100, 105, 107, 109, 164, 168, 275, 316], "highlight": [2, 5, 7, 18, 36, 46, 74, 89, 273, 275, 279, 289, 318, 338], "hike": [73, 82], "him": [73, 74, 82, 88, 89], "himself": [73, 74, 82, 88], "hindu": [73, 82], "hint": [1, 2, 4, 5, 6, 9, 16, 25, 32, 36, 41, 61, 76, 95, 238, 239, 253, 259], "hire": [74, 88], "hist": [3, 28, 275, 279, 317, 338], "histor": [7, 43, 46, 276, 279, 320, 337], "histori": [229, 234, 273, 274, 276, 279, 291, 312, 314, 321, 323, 325, 340], "hit": [73, 82, 111, 113, 126, 128, 136, 139, 164, 168, 279, 342], "hitchhik": [73, 82], "hiya": [73, 82], "hmu": [73, 82], "hmw": [73, 82], "hoc": [275, 278, 318, 332], "hogan": [73, 82], "hogwart": [73, 80, 82, 83], "hold": [8, 9, 51, 59, 76, 97, 240, 263, 273, 283], "hole": [73, 82], "holidai": [276, 325], "hollywood": [73, 74, 82, 88, 89], "home": [73, 82, 180, 192, 239, 258], "homebrew": [111, 112, 126, 127, 136, 138, 149, 151, 163, 176], "homecom": [73, 82], "homepath": [163, 176], "hong": [74, 88], "hood": [2, 9, 19, 59, 273, 285], "hook": [74, 88, 89, 278, 336], "hop": [73, 82], "hope": [73, 74, 82, 88], "horizon": [276, 321, 322, 323, 325], "horizont": [221, 224, 227], "horribl": [74, 89], "horror": [74, 89], "hospit": [74, 89], "host": [4, 32, 100, 103, 105, 107, 110, 177, 178, 185, 189, 190, 197, 198, 273, 277, 287, 326], "hostnam": [238, 253], "hot": [10, 71, 274, 275, 306, 318], "hotwif": [73, 82], "hour": [73, 82, 164, 168, 212, 213, 217, 237, 244, 274, 276, 277, 307, 320, 321, 325, 327], "hourli": [320, 325], "hous": [74, 88], "housekeep": [273, 281], "hoverboard": [73, 82], "how": [2, 3, 4, 5, 6, 7, 20, 28, 31, 32, 33, 35, 36, 41, 42, 44, 49, 51, 52, 53, 54, 56, 62, 73, 74, 75, 76, 77, 78, 80, 82, 83, 84, 87, 88, 89, 93, 94, 99, 101, 111, 113, 118, 119, 122, 126, 128, 136, 139, 146, 164, 168, 169, 170, 172, 173, 177, 180, 183, 185, 187, 188, 189, 192, 195, 197, 204, 206, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 222, 223, 226, 230, 231, 234, 236, 238, 239, 240, 241, 245, 247, 249, 258, 259, 261, 269, 270, 282, 283, 284, 285, 289, 293, 294, 299, 301, 314, 317, 321, 338, 340, 342], "howev": [2, 7, 8, 9, 22, 46, 53, 54, 64, 74, 88, 107, 109, 240, 265, 273, 289], "html": [0, 11, 72, 73, 75, 76, 80, 90, 92, 99, 111, 112, 126, 127, 136, 138, 186, 187, 200, 203, 205, 206, 207, 210, 212, 213, 219, 239, 260], "http": [0, 3, 7, 10, 11, 28, 48, 70, 72, 73, 75, 76, 80, 90, 92, 93, 99, 100, 105, 107, 110, 111, 112, 118, 120, 126, 127, 129, 131, 136, 138, 141, 143, 146, 149, 151, 156, 159, 163, 169, 172, 173, 176, 181, 183, 186, 187, 188, 193, 195, 200, 203, 204, 205, 206, 207, 210, 211, 212, 213, 219, 221, 225, 226, 229, 232, 233, 234, 237, 238, 239, 241, 244, 245, 252, 258, 260, 269, 270, 276, 279, 321, 338], "huddleston": [73, 82], "hudi": [7, 43], "hug": [73, 74, 75, 78, 80, 83, 85, 91, 93, 95, 96, 99, 221, 224, 225, 229, 232, 234, 274, 306, 307, 314], "huge": [74, 89], "huggingfac": [4, 5, 30, 34, 73, 79, 212, 213, 219, 221, 224, 225, 226, 229, 232, 233], "huggingface_hub": [229, 232], "hugo": [74, 89], "hulk": [73, 82], "human": [8, 51, 73, 82, 273, 289], "humbl": [74, 88], "humor": [74, 88, 89], "hundr": [275, 315], "hunt": [73, 82], "hurt": [73, 82], "hustl": [73, 82], "hvar": [73, 82], "hxwxc": [164, 168], "hybrid": [229, 235, 279, 342], "hydrologi": [275, 315], "hyperband": [6, 41, 239, 259], "hyperparam": [276, 325], "hyperparamet": [4, 5, 7, 26, 32, 36, 38, 40, 42, 46, 237, 238, 244, 245, 249, 255, 257, 258, 259, 273, 274, 275, 276, 277, 278, 279, 283, 284, 290, 298, 301, 303, 314, 317, 319, 325, 331, 336, 342], "hypnot": [74, 89], "i": [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 28, 29, 31, 32, 33, 35, 36, 38, 39, 40, 41, 43, 44, 46, 49, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 97, 98, 99, 105, 107, 108, 110, 111, 115, 116, 118, 122, 124, 126, 127, 128, 132, 133, 134, 136, 137, 139, 144, 145, 146, 147, 149, 150, 160, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173, 176, 177, 178, 179, 180, 181, 182, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 200, 203, 206, 207, 210, 214, 216, 217, 218, 222, 225, 229, 230, 231, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 247, 248, 249, 251, 253, 255, 256, 257, 258, 259, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287, 288, 289, 290, 292, 293, 295, 296, 297, 299, 300, 301, 303, 306, 307, 308, 310, 311, 312, 314, 315, 316, 317, 320, 321, 322, 323, 324, 326, 327, 329, 331, 332, 336, 337, 338, 339, 340, 342], "iam": [103, 104, 106, 107, 109, 110, 111, 112, 113, 117, 118, 119, 121, 122, 126, 127, 128, 138, 139, 146, 149, 152, 153], "ic": [73, 82], "iceberg": [7, 9, 43, 59], "ichiro": [73, 82], "icon": [74, 89, 178, 190, 229, 233], "id": [12, 74, 76, 86, 87, 88, 89, 97, 106, 111, 113, 118, 121, 122, 126, 128, 130, 131, 136, 139, 142, 143, 149, 152, 153, 158, 159, 164, 168, 169, 172, 173, 177, 181, 189, 193, 229, 232, 234, 273, 274, 290, 307, 337], "idea": [74, 89, 111, 113, 126, 128, 136, 139], "ideal": [3, 7, 28, 43, 46, 74, 89, 178, 190, 221, 223, 228, 275, 276, 316, 325], "ident": [100, 105, 107, 110, 118, 122, 126, 128, 136, 139, 149, 153, 184, 196, 273, 280, 289, 294], "identifi": [111, 113, 126, 128, 136, 139, 164, 168, 221, 224, 229, 232, 273, 288, 289], "idiot": [73, 74, 82, 89], "idl": [111, 113, 126, 128, 136, 139, 212, 213, 216, 217], "idx": [5, 35, 273, 274, 276, 279, 292, 308, 314, 321, 342], "idx1": [238, 239, 252, 260], "idx2item": [279, 342], "idx3": [238, 239, 252, 258, 260], "ig": [73, 82], "ignor": [277, 278, 329, 335], "iid": [279, 338, 342], "ill": [73, 82], "illustr": [9, 59, 73, 82, 107, 109, 188, 277, 331], "iloc": [5, 35, 274, 276, 279, 308, 321, 325, 342], "im": [73, 74, 82, 89], "imag": [2, 4, 5, 6, 7, 9, 21, 29, 31, 32, 35, 36, 39, 40, 41, 43, 59, 60, 61, 62, 67, 74, 89, 107, 109, 164, 168, 169, 172, 177, 180, 183, 187, 189, 192, 195, 204, 206, 211, 212, 213, 219, 221, 226, 238, 239, 240, 241, 247, 248, 249, 252, 253, 256, 257, 258, 260, 263, 264, 269, 270, 281, 282, 283, 287, 292, 294, 295, 296, 300, 314, 328, 331], "image_arr": [273, 297], "image_arrai": [164, 168], "image_batch": [241, 270], "image_byt": [274, 277, 307, 308, 314, 327], "image_bytes_raw": [277, 327], "image_classifi": [241, 270], "image_classifier_ingress": [241, 270], "image_height": [164, 168], "image_id": [9, 61, 164, 168], "image_latents_256": [5, 35], "image_uri": [164, 168, 212, 213, 219, 221, 226], "image_width": [164, 168], "imagebatchpredictor": [274, 314], "imagenet": [273, 274, 282, 307, 308], "imagenet_mean": [274, 308, 314], "imagenet_std": [274, 308, 314], "imageri": [74, 89], "imageserviceingress": [241, 270], "imagin": [74, 88, 89], "imbalanc": [275, 316], "imdb": [74, 84, 85, 86, 88, 89], "img": [4, 9, 31, 32, 60, 273, 274, 277, 281, 292, 307, 308, 314, 327, 331], "immatur": [7, 47], "immedi": [1, 2, 7, 9, 15, 22, 46, 60, 183, 187, 195, 204, 206, 211, 212, 213, 216, 273, 274, 277, 303, 308, 330], "immut": [2, 18], "impact": [7, 47, 74, 89, 180, 192, 212, 213, 216, 217, 229, 235], "implement": [1, 4, 5, 6, 7, 8, 9, 16, 32, 35, 36, 40, 43, 47, 48, 54, 61, 62, 67, 68, 73, 78, 81, 100, 104, 169, 172, 173, 183, 187, 195, 204, 206, 211, 239, 240, 257, 264, 267, 268, 273, 274, 282, 283, 293, 306], "implementaiton": [238, 250], "impli": [74, 85], "implicit": [277, 331], "import": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 16, 17, 22, 26, 28, 29, 31, 33, 38, 49, 56, 67, 71, 72, 78, 84, 89, 90, 93, 94, 164, 167, 168, 169, 171, 172, 173, 178, 179, 180, 181, 182, 183, 186, 187, 190, 191, 192, 193, 194, 195, 201, 202, 204, 205, 206, 208, 209, 211, 212, 213, 219, 221, 224, 225, 226, 227, 229, 232, 233, 234, 237, 238, 239, 240, 241, 244, 245, 247, 249, 250, 251, 252, 256, 257, 258, 263, 264, 269, 270, 288, 310, 314, 315, 319, 323, 325, 329, 331, 335, 340], "import_path": [183, 187, 195, 204, 206, 211, 212, 213, 219, 221, 226, 229, 233], "importance_typ": [275, 318], "imposs": [74, 89], "impress": [74, 89], "improv": [1, 8, 9, 10, 16, 54, 55, 57, 66, 68, 163, 164, 167, 175, 229, 232, 233, 273, 275, 276, 277, 293, 315, 319, 323, 325, 331], "imshow": [4, 6, 9, 31, 32, 39, 60, 238, 239, 241, 247, 253, 256, 270, 273, 274, 277, 281, 292, 307, 314, 327, 331], "in_channel": [4, 5, 31, 35, 238, 247, 273, 282], "in_count": [181, 193], "in_featur": [238, 247, 253], "in_proj": [276, 322], "inaccess": [164, 168], "incept": [277, 331], "includ": [0, 2, 3, 4, 7, 8, 9, 10, 21, 22, 28, 31, 32, 43, 46, 47, 51, 64, 69, 75, 76, 90, 94, 97, 98, 100, 103, 107, 109, 111, 117, 126, 135, 136, 148, 163, 169, 171, 175, 176, 178, 179, 180, 181, 182, 183, 186, 190, 191, 192, 193, 194, 195, 200, 205, 207, 212, 213, 217, 229, 232, 237, 238, 242, 244, 252, 253, 273, 274, 276, 279, 291, 301, 304, 312, 323, 338, 340], "include_path": [9, 59, 61, 240, 241, 263, 270], "incom": [241, 268, 274, 314], "incomplet": [273, 287], "incorpor": [7, 43, 274, 306], "incorrect_squar": [2, 20], "increas": [75, 93, 164, 168, 212, 213, 216, 221, 227, 273, 277, 279, 305, 331, 340], "increasingli": [11, 72], "increment": [7, 43, 183, 187, 195, 203, 206, 210, 274, 306], "incur": [7, 47, 169, 172], "independ": [8, 9, 10, 55, 59, 69, 180, 192, 221, 227, 273, 274, 276, 285, 293, 297, 308, 320, 323], "index": [0, 3, 7, 28, 43, 75, 90, 169, 171, 186, 187, 200, 203, 205, 206, 207, 210, 273, 274, 275, 276, 279, 281, 292, 308, 314, 316, 317, 321, 338], "index_col": [275, 317], "indi": [74, 85], "indian": [73, 82], "indic": [2, 11, 22, 72, 273, 274, 275, 279, 292, 308, 318, 337, 338, 342], "individu": [1, 3, 16, 27, 164, 167, 237, 243], "indonesiasayshbdforjustinbieb": [73, 82], "industri": 343, "ineffect": [74, 88], "ineffici": [73, 78, 212, 213, 216], "infer": [2, 7, 8, 9, 10, 22, 25, 26, 46, 47, 48, 51, 55, 57, 62, 66, 70, 75, 77, 82, 92, 179, 182, 186, 187, 191, 194, 200, 203, 205, 206, 207, 210, 217, 222, 228, 229, 232, 235, 237, 240, 241, 244, 245, 264, 266, 269, 277, 278, 280, 281, 289, 291, 305, 306, 315, 317, 320, 321, 326, 331, 332, 336, 338, 343], "inference_mod": [273, 292], "inference_row": [274, 314], "inferf": [279, 337], "infinit": [9, 61], "influenc": [74, 89, 278, 332], "info": [11, 72, 73, 82, 169, 172, 221, 227, 237, 239, 244, 245, 258, 260], "inform": [4, 32, 73, 75, 82, 90, 126, 127, 136, 137, 149, 150, 163, 164, 168, 169, 171, 173, 176, 180, 192, 229, 234, 238, 253, 273, 274, 283, 312], "infrastructur": [4, 5, 11, 30, 34, 72, 101, 102, 108, 109, 118, 119, 162, 164, 167, 168, 178, 183, 185, 187, 188, 190, 195, 197, 198, 203, 206, 210, 217, 221, 226, 228, 229, 232, 236, 273, 274, 278, 279, 280, 302, 306, 332, 342], "ingest": [3, 28, 237, 240, 244, 263, 273, 274, 275, 293, 298, 305, 306, 315, 316, 319], "ingmar": [74, 85], "ingress": [3, 28, 75, 92, 107, 109, 110, 134, 135, 147, 148, 161, 241, 270], "ingress_from_cidr_map": [100, 105], "ingress_with_self": [100, 105], "inher": 188, "inherit": [3, 27, 183, 187, 195, 204, 206, 211, 237, 243], "ini": [163, 176], "init": [11, 72, 73, 74, 76, 80, 85, 98, 111, 113, 118, 122, 126, 128, 136, 139, 149, 153, 181, 193, 273, 281], "initi": [2, 5, 7, 9, 11, 19, 35, 36, 47, 62, 72, 73, 76, 77, 80, 81, 84, 94, 97, 98, 107, 109, 111, 113, 118, 122, 126, 128, 136, 139, 149, 153, 183, 195, 229, 232, 238, 240, 247, 249, 264, 273, 276, 279, 280, 290, 292, 320, 337], "initial_replica": [241, 270], "inject": [169, 172, 273, 274, 277, 278, 284, 287, 309, 326, 332, 333, 334], "inlin": [75, 92], "inner": [74, 88], "inning": [73, 80, 82], "innings": [73, 80, 82], "innoc": [74, 88, 89], "inplac": [238, 247, 253, 275, 316], "input": [5, 8, 9, 10, 35, 51, 53, 54, 57, 61, 62, 63, 64, 65, 70, 73, 74, 75, 76, 81, 87, 92, 97, 169, 171, 180, 192, 212, 213, 215, 237, 240, 241, 244, 245, 263, 265, 266, 270, 273, 275, 276, 281, 282, 283, 293, 305, 315, 320, 321, 322, 323, 325, 328], "input_window": [276, 321, 322, 323, 325], "inputdatabuff": [9, 63], "inscrut": [7, 46], "insert": [212, 213, 216], "insid": [2, 8, 23, 25, 55, 74, 85, 169, 171, 172, 180, 181, 185, 192, 193, 198, 273, 274, 275, 277, 278, 281, 284, 298, 306, 308, 314, 317, 319, 329, 332], "insight": [7, 46, 164, 166, 168], "inspect": [2, 3, 4, 5, 22, 28, 31, 32, 35, 73, 78, 180, 183, 192, 195, 238, 247, 253, 277, 280, 281, 305, 307, 327], "inspir": [73, 82], "instal": [2, 3, 21, 26, 107, 108, 110, 111, 112, 113, 120, 127, 128, 134, 135, 137, 138, 139, 147, 148, 150, 151, 155, 157, 162, 178, 179, 180, 190, 191, 192, 221, 225, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338], "instanc": [2, 3, 4, 8, 9, 11, 22, 25, 28, 31, 32, 54, 57, 64, 72, 73, 74, 78, 89, 92, 104, 106, 107, 109, 113, 117, 122, 126, 133, 136, 145, 149, 153, 164, 168, 169, 172, 173, 177, 179, 181, 189, 191, 193, 221, 227, 237, 240, 244, 245, 265, 273, 281, 287, 292], "instance_iam_role_arn": [100, 106], "instance_profile_arn": [111, 113], "instance_typ": [126, 133, 136, 145, 164, 168], "instanceid": [111, 113, 126, 128, 136, 139], "instant": [178, 190], "instanti": [2, 5, 25, 35, 241, 269, 274, 306, 311], "instead": [2, 3, 4, 6, 8, 19, 20, 24, 28, 32, 41, 51, 53, 54, 73, 74, 78, 81, 88, 89, 180, 181, 183, 187, 192, 193, 195, 204, 206, 211, 238, 239, 250, 251, 260, 273, 274, 275, 276, 277, 279, 280, 285, 286, 293, 294, 295, 300, 307, 310, 315, 317, 318, 320, 326, 337, 338], "instruct": [107, 108, 179, 188, 191, 212, 213, 219, 221, 223, 224, 227, 229, 232, 233, 235], "instrument": [73, 82, 173], "insubordin": [74, 88], "int": [2, 3, 4, 5, 6, 9, 18, 20, 22, 28, 31, 32, 35, 39, 40, 41, 61, 64, 164, 168, 238, 239, 240, 247, 250, 251, 256, 257, 259, 265, 273, 274, 275, 276, 277, 278, 279, 287, 288, 292, 295, 301, 307, 309, 310, 314, 318, 319, 321, 323, 327, 328, 333, 334, 339], "int32": [275, 276, 318, 321], "int4": [221, 227], "int64": [73, 74, 80, 83, 86, 87, 278, 279, 333, 338], "intact": [273, 283], "integ": [237, 244, 273, 274, 279, 297, 306, 337, 338], "integr": [3, 4, 5, 7, 8, 27, 30, 32, 34, 43, 46, 47, 48, 55, 73, 74, 75, 76, 78, 84, 90, 98, 100, 104, 107, 110, 162, 163, 164, 168, 169, 172, 175, 176, 177, 183, 187, 189, 195, 203, 206, 210, 212, 213, 218, 221, 227, 229, 231, 233, 234, 236, 237, 238, 243, 244, 245, 250, 274, 275, 276, 278, 279, 280, 287, 294, 295, 298, 306, 314, 319, 321, 326, 332, 335, 336, 337], "intellig": [7, 43, 229, 234], "intend": [74, 76, 85, 98, 180, 183, 192, 195], "intens": [2, 3, 6, 7, 11, 22, 28, 39, 47, 72, 74, 89, 212, 213, 217, 221, 223, 239, 256, 273, 293], "intent": [74, 88, 89, 275, 277, 315, 326], "interact": [7, 46, 74, 89, 107, 109, 178, 185, 188, 190, 198, 212, 213, 217, 218, 229, 234, 236, 241, 270, 275, 279, 319, 337, 338], "interconnect": [221, 227], "interest": [73, 74, 82, 85], "interfac": [3, 4, 7, 27, 31, 45, 75, 90, 163, 176, 273, 279, 281, 293, 340], "intermedi": [2, 4, 9, 19, 32, 57, 212, 213, 215, 238, 251, 273, 274, 275, 276, 289, 314, 319, 325], "intern": [6, 42, 100, 105, 164, 166, 273, 274, 279, 294, 309, 340, 342], "internet": [100, 105], "interoper": [7, 43], "interpret": [275, 276, 279, 319, 325, 342], "interrupt": [179, 191, 273, 274, 276, 301, 303, 306, 324], "interv": [5, 7, 35, 48, 241, 270, 276, 320, 321], "intervent": [273, 274, 276, 279, 301, 306, 324, 341], "interview": [8, 55, 73, 82, 240, 266], "intrigu": [74, 88, 89], "intro": [10, 71, 126, 127, 136, 138, 149, 151, 237, 244, 245], "introduc": [1, 7, 10, 13, 46, 67, 163, 164, 167, 174, 178, 190, 241, 267], "introduct": [76, 94, 240, 241, 255, 261, 267, 293], "introductori": [188, 246], "intuit": [7, 48], "invari": [74, 85], "invent": [74, 88, 89, 276, 320], "invert_yaxi": [275, 318], "invest": [73, 80, 82], "invit": [185, 198], "invoc": [1, 14, 16], "invok": [1, 2, 15, 16, 23, 275, 319], "involv": [8, 9, 54, 64, 73, 74, 75, 82, 88, 89, 90, 212, 213, 219, 240, 265], "io": [0, 9, 11, 17, 58, 72, 73, 75, 76, 80, 90, 92, 99, 126, 127, 129, 131, 136, 138, 141, 143, 149, 151, 156, 159, 239, 260, 273, 274, 276, 277, 281, 307, 308, 314, 321, 327], "iot": [7, 46], "ip": [100, 103, 105, 111, 113, 126, 128, 136, 139, 149, 156, 238, 239, 252, 253, 260, 273, 274, 290, 306], "ipykernel": [11, 72], "ipynb": [178, 180, 188, 190, 192], "iran": [73, 82], "iron": [73, 82], "irrupt": [73, 82], "is_avail": [4, 6, 31, 32, 40, 76, 97, 274, 276, 277, 278, 314, 325, 331, 336], "is_big_tip": [237, 244, 245], "isdir": [273, 274, 304, 314], "isfil": [274, 314], "isinst": [2, 19], "islam": [73, 82], "isn": [4, 32, 73, 74, 82, 85, 136, 146, 212, 213, 217, 238, 250, 273, 274, 277, 287, 306, 331], "isol": [7, 10, 43, 69, 100, 102, 105], "issu": [1, 4, 5, 7, 13, 30, 34, 46, 74, 85, 136, 146, 149, 154, 163, 164, 168, 175, 181, 193, 229, 236, 273, 274, 280, 292, 301, 306], "itali": [73, 82], "itbr": [74, 89], "item": [1, 2, 4, 5, 6, 9, 16, 24, 31, 32, 35, 39, 41, 61, 74, 76, 89, 97, 163, 164, 168, 176, 229, 232, 238, 239, 247, 251, 253, 256, 260, 273, 274, 275, 276, 277, 278, 288, 310, 314, 318, 323, 325, 327, 333, 339, 340], "item2idx": [279, 338, 342], "item_col": [279, 338], "item_embed": [279, 339, 342], "item_id": [279, 338, 342], "item_idx": [279, 337, 338, 339, 340], "item_metadata": [279, 342], "item_vec": [279, 339], "item_vector": [279, 342], "iter": [8, 9, 10, 52, 58, 68, 76, 97, 177, 178, 189, 190, 212, 213, 216, 229, 235, 237, 239, 244, 245, 258, 260, 273, 275, 276, 277, 278, 279, 283, 295, 319, 321, 326, 329, 331, 335, 336, 338, 340, 341], "iter_torch_batch": [273, 277, 278, 279, 293, 295, 305, 329, 335, 337, 340, 342], "iterations_since_restor": [238, 253], "iterrow": [279, 342], "its": [1, 2, 4, 6, 7, 8, 9, 10, 13, 18, 22, 31, 41, 43, 53, 55, 59, 69, 73, 74, 75, 78, 82, 89, 90, 93, 136, 146, 164, 167, 168, 169, 171, 172, 180, 183, 185, 187, 192, 195, 198, 203, 206, 210, 212, 213, 216, 218, 221, 227, 239, 240, 260, 263, 266, 273, 274, 275, 276, 277, 279, 280, 281, 283, 290, 291, 292, 308, 310, 315, 316, 317, 318, 320, 326, 327, 337, 340], "itself": [8, 51, 164, 168], "iv": [74, 89], "j": [0, 1, 16, 73, 82, 279, 338, 342], "jack": [74, 88], "jackson": [73, 82], "jadwal": [73, 82], "jai": [73, 74, 80, 82, 88], "jail": [73, 82], "jame": [73, 74, 82, 88], "jami": [73, 82], "jan": [73, 82], "janet": [73, 82], "januari": [73, 80, 82], "java": [7, 46], "jean": [74, 89], "jeanmarc": [74, 89], "jeff": [74, 88], "jgz99": [221, 226], "jit": [9, 10, 62, 70, 240, 241, 264, 269], "job": [3, 5, 6, 28, 30, 34, 36, 41, 73, 74, 80, 82, 83, 88, 89, 100, 105, 107, 109, 110, 111, 115, 117, 118, 124, 126, 133, 136, 145, 146, 149, 160, 164, 166, 168, 169, 171, 179, 180, 181, 184, 185, 191, 192, 193, 196, 198, 229, 232, 239, 246, 248, 253, 258, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 285, 289, 290, 292, 298, 299, 301, 302, 305, 306, 311, 314, 315, 319, 325, 331, 336, 342, 343], "job_config": [182, 186, 194, 202, 205, 209], "job_descript": [229, 232], "job_id": [182, 186, 194, 202, 205, 209], "jobconfig": [182, 186, 194, 202, 205, 209], "joe": [73, 82], "john": [73, 74, 80, 82, 85, 88], "johnson": [73, 74, 82, 85], "joi": [73, 82], "join": [4, 5, 7, 8, 31, 32, 36, 47, 55, 73, 82, 84, 86, 89, 100, 105, 164, 168, 173, 180, 192, 229, 232, 236, 238, 247, 251, 253, 273, 274, 275, 276, 277, 278, 289, 292, 300, 301, 307, 310, 314, 316, 317, 321, 323, 325, 329, 331, 335, 336, 338, 340], "join_typ": [74, 88], "joint": [274, 276, 277, 307, 320, 326], "joke": [221, 225], "journei": [74, 89, 173], "jpeg": [7, 43, 274, 277, 307, 326, 327, 331], "jq": [111, 113, 126, 128, 136, 139], "json": [3, 7, 9, 10, 28, 43, 59, 67, 70, 75, 93, 169, 172, 173, 213, 219, 230, 231, 232, 234, 236, 237, 241, 244, 245, 269, 270, 274, 275, 277, 278, 279, 307, 310, 316, 327, 333, 338], "json_method1": [229, 233], "json_request": [10, 70, 169, 172, 241, 269, 270], "json_schema": [229, 233], "juda": [73, 82], "judg": [74, 88, 275, 316], "juggl": [10, 68], "jule": [73, 82], "jump": [10, 70, 74, 89, 241, 269, 278, 332], "jun": [73, 82], "june": [3, 28, 149, 152, 188, 237, 244], "jupyt": [179, 191], "jupyter_execute_notebook": 0, "jupyterlab": [178, 190], "just": [6, 8, 40, 41, 51, 53, 73, 74, 82, 85, 86, 88, 89, 111, 115, 118, 124, 126, 133, 136, 145, 149, 160, 178, 184, 190, 196, 212, 213, 217, 229, 234, 239, 240, 259, 263, 273, 274, 277, 279, 293, 301, 307, 314, 326, 327, 337], "justifi": [73, 74, 82, 88], "justin": [73, 82], "j\u00e4reg\u00e5rd": [74, 89], "k": [73, 76, 80, 82, 97, 212, 213, 216, 274, 275, 276, 278, 279, 314, 315, 325, 332, 336, 342], "k8": [100, 101, 103, 110, 126, 128, 129, 133, 136, 139, 141, 145, 149, 152, 160, 162, 163, 176, 343], "kafka": [7, 46], "katharina": [74, 89], "ke": [73, 82], "keep": [5, 35, 73, 74, 82, 83, 87, 179, 184, 191, 196, 212, 213, 219, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 289, 292, 293, 294, 301, 304, 307, 310, 311, 312, 314, 316, 317, 323, 325, 327, 329, 332, 335, 336, 337, 338, 340], "keeper": [74, 88], "kei": [6, 7, 8, 9, 10, 31, 41, 43, 54, 56, 64, 69, 76, 94, 103, 105, 107, 109, 111, 113, 126, 128, 136, 139, 169, 171, 180, 188, 192, 214, 215, 219, 227, 239, 240, 259, 265, 273, 274, 275, 279, 281, 282, 283, 289, 294, 300, 310, 317, 318, 342], "kenni": [73, 82], "kept": [73, 82, 279, 338], "kernel": [11, 72, 107, 110, 212, 213, 218, 273, 282], "kernel_s": [4, 6, 31, 40, 41, 238, 239, 247, 253, 257, 260, 273, 282], "kerri": [73, 80, 82], "kessler": [74, 89], "keylogg": [229, 232], "kick": [274, 279, 311, 340], "kiddo": [73, 82], "kill": [6, 41, 74, 85, 88, 89, 239, 259, 273, 277, 292, 327], "kim": [73, 82], "kind": [11, 72, 275, 316], "kingdom": [74, 89], "kitchen": [73, 82], "klaviyo": [241, 271], "klondik": [74, 88], "km": [100, 105], "know": [2, 7, 24, 47, 74, 88, 89, 164, 168, 229, 232, 273, 276, 299, 301, 320], "knowledg": [162, 169, 170, 221, 228, 275, 318, 319], "known": [277, 326], "kong": [74, 88], "kpop": [73, 82], "kri": [73, 82], "kube": [126, 129, 134, 136, 141, 146, 147], "kubeconfig": [126, 129, 136, 141], "kubectl": [126, 127, 129, 132, 133, 134, 136, 138, 141, 144, 145, 146, 147, 151, 156, 161], "kuberai": [163, 176], "kubernet": [10, 68, 100, 102, 103, 127, 128, 130, 135, 138, 139, 142, 148, 151, 152, 156, 158, 161, 162], "kucinich": [73, 82], "kueue": [107, 109], "kurt": [73, 82], "kv": [215, 217, 220, 221, 227, 275, 318], "kv_cache_util": [221, 227], "kvedzwag2qa8i5bj": [221, 226], "l": [3, 4, 8, 9, 28, 31, 32, 51, 53, 59, 136, 146, 238, 240, 247, 263, 274, 277, 278, 279, 306, 326, 332, 337], "l4": [212, 213, 219, 229, 232, 233], "l40": [221, 224, 225, 227, 229, 234], "l6": [73, 81], "lab": [6, 41, 239, 259], "label": [3, 4, 6, 9, 10, 28, 31, 32, 39, 40, 41, 61, 62, 64, 70, 73, 74, 75, 76, 80, 82, 83, 84, 85, 86, 87, 88, 89, 93, 96, 97, 169, 170, 237, 238, 239, 240, 244, 245, 247, 249, 252, 253, 256, 257, 260, 264, 265, 273, 275, 276, 277, 278, 279, 281, 283, 292, 294, 295, 296, 297, 300, 307, 308, 312, 314, 316, 317, 318, 319, 323, 325, 327, 329, 331, 335, 340], "label_col": [275, 317], "label_column": [3, 28, 237, 244, 245, 275, 317], "label_nam": [274, 277, 307, 314, 327], "labeled_batch": [9, 61], "labia": [74, 85], "labour": [73, 82], "lack": [7, 43, 47], "lag": [276, 325], "lai": [73, 82], "lakehous": [9, 59, 273, 293, 305], "lambda": [5, 35, 74, 87, 164, 168, 180, 192, 275, 318, 319], "land": [0, 275, 315], "landscap": [46, 75, 93, 343], "languag": [4, 5, 7, 32, 37, 43, 45, 46, 215, 238, 254, 279, 342], "laptop": [11, 72, 74, 84], "lar": [74, 89], "larami": [74, 88], "larg": [0, 3, 4, 5, 7, 8, 9, 17, 18, 19, 25, 28, 32, 35, 36, 43, 46, 55, 57, 73, 74, 75, 76, 78, 81, 83, 84, 87, 89, 90, 94, 97, 98, 99, 164, 168, 180, 192, 215, 216, 217, 221, 223, 228, 229, 235, 238, 240, 248, 262, 273, 274, 275, 276, 277, 279, 299, 307, 315, 316, 319, 320, 325, 326, 327, 331, 338, 342], "large_mat_from_object_stor": [2, 18], "large_matrix": [2, 18], "larger": [7, 47, 74, 87, 180, 192, 241, 270, 273, 274, 277, 278, 279, 305, 314, 331, 336, 337], "largest": [229, 232], "last": [2, 4, 5, 7, 24, 30, 34, 46, 47, 73, 76, 80, 82, 83, 99, 136, 140, 183, 187, 195, 204, 206, 211, 212, 213, 216, 229, 234, 240, 262, 274, 275, 276, 279, 280, 287, 291, 299, 300, 301, 308, 312, 317, 319, 321, 323, 340, 341], "last_login": 173, "lastmodifi": [180, 192], "latenc": [7, 10, 46, 48, 68, 169, 172, 180, 192, 216, 220, 221, 227, 229, 235, 274, 278, 314, 336], "latent": [5, 35, 279, 337], "later": [6, 40, 74, 89, 173, 178, 190, 212, 213, 218, 238, 239, 241, 247, 257, 270, 273, 274, 275, 277, 278, 279, 281, 285, 289, 291, 296, 297, 308, 316, 327, 328, 334, 337, 338, 340], "latest": [4, 11, 32, 72, 73, 74, 75, 80, 88, 90, 92, 111, 112, 126, 127, 136, 138, 149, 157, 178, 190, 238, 253, 273, 274, 276, 278, 279, 291, 299, 300, 301, 302, 303, 306, 313, 320, 323, 324, 325, 329, 332, 335, 336, 337, 340, 341, 342], "latin": [279, 342], "laugh": [73, 82], "laughter": [73, 82], "launch": [1, 2, 3, 5, 6, 8, 9, 10, 12, 17, 24, 26, 29, 33, 38, 49, 56, 67, 73, 74, 75, 76, 78, 84, 87, 90, 94, 100, 102, 170, 179, 183, 184, 185, 187, 191, 195, 196, 198, 203, 206, 210, 212, 213, 214, 222, 229, 230, 246, 249, 275, 283, 285, 292, 306, 315, 317, 319, 320, 325, 326, 332], "layer": [3, 27, 100, 102, 221, 224, 227, 273, 276, 277, 279, 282, 320, 331, 338], "layer1": [238, 247, 253], "layer2": [238, 247, 253], "layer3": [238, 247, 253], "layer4": [238, 247, 253], "layers_per_block": [5, 35], "layout": [279, 338], "lazi": [8, 51, 52, 56, 73, 82, 83, 240, 264, 275, 279, 316, 338], "lbc": [126, 129, 136, 141], "lbl": [277, 327], "le": [73, 82], "lead": [2, 7, 22, 24, 47, 73, 74, 82, 89], "leader": [73, 82], "leagu": [73, 82], "leakag": [276, 321], "lean": [274, 306], "learn": [2, 3, 4, 5, 6, 8, 9, 20, 22, 27, 32, 35, 41, 43, 46, 55, 66, 73, 74, 75, 76, 77, 78, 81, 83, 84, 85, 89, 90, 94, 95, 97, 98, 99, 111, 117, 164, 165, 166, 212, 213, 214, 221, 228, 236, 238, 239, 240, 250, 253, 259, 266, 281, 283, 291, 307, 314, 316, 321, 323, 325, 331, 336, 338, 339, 340, 342, 343], "learning_r": [273, 284], "least": [2, 20, 22, 100, 105, 107, 109, 110, 212, 213, 219], "leav": [4, 31, 74, 88, 111, 113, 126, 128, 177, 179, 183, 189, 191, 195, 274, 275, 276, 277, 314, 316, 319, 325, 329], "lecun": [238, 239, 252, 258], "left": [1, 7, 16, 48, 74, 89, 178, 179, 190, 191, 212, 213, 216, 275, 276, 279, 319, 324, 341, 342], "leftarrow": [277, 278, 326, 332], "leftov": [273, 304], "legend": [274, 276, 277, 278, 279, 312, 323, 325, 329, 335, 340], "leigh": [73, 82], "lemieux": [73, 82], "len": [2, 4, 5, 24, 31, 32, 35, 74, 75, 86, 92, 181, 193, 273, 274, 275, 276, 279, 281, 292, 307, 308, 316, 317, 318, 319, 321, 323, 338, 340, 342], "lena": [74, 85], "length": [9, 57, 75, 92, 111, 113, 126, 128, 136, 139, 212, 213, 215, 216, 217, 221, 224, 229, 235, 276, 321], "leo": [74, 89], "leopold": [74, 89], "lesnar": [73, 82], "less": [74, 85, 229, 232, 277, 331], "lesson": [237, 242], "let": [1, 2, 3, 4, 5, 6, 8, 9, 10, 16, 18, 19, 20, 22, 24, 25, 26, 28, 31, 32, 35, 36, 39, 40, 41, 51, 52, 53, 54, 59, 60, 62, 64, 65, 70, 118, 122, 126, 128, 129, 136, 141, 169, 171, 178, 179, 180, 183, 184, 190, 191, 192, 195, 196, 212, 213, 217, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 247, 248, 249, 252, 256, 257, 258, 263, 265, 266, 269, 270, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 287, 289, 307, 309, 314, 315, 316, 323, 326, 327, 328, 329, 332, 337], "level": [0, 4, 5, 6, 7, 10, 17, 20, 31, 32, 36, 40, 46, 69, 74, 85, 100, 105, 107, 110, 163, 164, 167, 169, 172, 175, 181, 184, 193, 196, 212, 213, 216, 218, 238, 239, 241, 247, 249, 257, 268, 273, 274, 276, 279, 282, 290, 306, 320, 325, 338, 340], "leverag": [7, 9, 10, 43, 57, 66, 68, 73, 74, 76, 78, 83, 84, 94, 98, 107, 108, 169, 172, 275, 277, 278, 319, 331, 336], "lexu": [73, 82, 229, 233], "lgbm": [4, 5, 30, 34, 273, 280], "lh": [8, 53], "li": [74, 89], "liar": [73, 82], "lib": [73, 82, 180, 192, 343], "libomp": [3, 26], "librari": [1, 4, 6, 7, 8, 9, 10, 13, 32, 40, 41, 46, 47, 50, 51, 59, 61, 62, 64, 69, 70, 77, 78, 84, 90, 93, 94, 96, 98, 164, 168, 177, 180, 181, 189, 192, 193, 239, 240, 241, 257, 258, 262, 263, 264, 265, 269, 270, 273, 274, 276, 277, 278, 279, 281, 307, 321, 327, 333, 338], "licens": [11, 72], "lie": [73, 82, 278, 333], "life": [73, 74, 82, 85, 89], "lifecycl": [7, 10, 48, 69, 107, 108, 109, 180, 182, 186, 188, 192, 194, 200, 202, 205, 207, 209, 238, 251, 280], "lift": [177, 189], "light": [0, 9, 61, 73, 82], "lightli": 188, "lightn": [4, 30, 34, 278, 326, 327, 328, 331, 332, 333, 335, 336, 343], "lightning_training_loop": [5, 35], "lightningmodul": [5, 35, 326, 332], "lightweight": [10, 68, 73, 81, 181, 193, 274, 275, 276, 279, 306, 316, 321, 325, 337, 338, 342], "lik": [74, 89], "like": [2, 3, 4, 5, 6, 7, 8, 9, 19, 20, 21, 22, 28, 31, 32, 36, 41, 43, 46, 47, 48, 51, 53, 55, 62, 73, 74, 75, 76, 82, 84, 85, 88, 89, 90, 93, 98, 100, 106, 107, 109, 110, 111, 113, 114, 118, 121, 122, 123, 124, 126, 128, 130, 136, 139, 142, 145, 149, 158, 178, 179, 190, 191, 212, 213, 218, 221, 224, 225, 229, 233, 238, 239, 240, 241, 247, 248, 249, 259, 263, 264, 270, 273, 275, 277, 278, 279, 280, 283, 284, 288, 290, 295, 315, 317, 331, 336, 337, 338, 342], "likeeeeeeeee": [73, 82], "likelihood": [274, 306], "limit": [6, 7, 8, 41, 43, 53, 62, 107, 110, 111, 113, 126, 128, 136, 139, 179, 180, 191, 192, 212, 213, 215, 229, 232], "limousin": [3, 8, 28, 51, 237, 244], "line": [74, 89, 163, 176, 178, 183, 187, 190, 195, 204, 206, 211, 274, 276, 277, 306, 320, 326], "linear": [2, 6, 25, 41, 238, 239, 247, 253, 259, 276, 278, 322, 334], "linearmodel": [2, 25], "lineup": [73, 80, 82, 83], "link": [0, 107, 109, 183, 195, 229, 231, 236, 273, 285], "linux": [163, 176], "list": [2, 3, 8, 9, 19, 28, 51, 53, 59, 65, 74, 88, 111, 113, 115, 116, 118, 121, 124, 126, 128, 132, 133, 134, 136, 139, 140, 144, 145, 147, 149, 152, 154, 160, 161, 164, 168, 178, 180, 182, 185, 190, 192, 194, 198, 212, 213, 217, 229, 232, 240, 263, 266, 273, 275, 276, 277, 278, 279, 291, 292, 296, 316, 317, 325, 327, 333, 338, 342], "list_": [276, 321], "list_objects_v2": [180, 192, 229, 232], "listbucket": [100, 105], "listbucketmultipartupload": [100, 105], "listdir": [274, 314], "listfil": [9, 63], "listmultipartuploadpart": [100, 105], "lit": [73, 80, 82], "lite": [277, 307, 326, 327], "liter": [73, 82], "littl": [73, 74, 82, 88, 89], "live": [73, 74, 75, 82, 88, 93, 276, 279, 320, 342], "ll": [4, 5, 6, 11, 32, 35, 40, 72, 73, 74, 82, 88, 100, 101, 118, 120, 126, 130, 133, 136, 142, 145, 149, 151, 153, 158, 164, 165, 177, 178, 181, 188, 189, 190, 193, 212, 213, 214, 220, 221, 222, 223, 224, 226, 230, 232, 239, 257, 276, 281, 282, 284, 285, 290, 305, 320], "llama": [212, 213, 219, 224, 225, 226, 227, 229, 232, 235], "llamafactoryai": [229, 232], "llm": [9, 57, 216, 220, 223, 225, 226, 228, 231, 233, 234, 236], "llm_config": [212, 213, 219, 221, 224, 227, 229, 232, 233, 234], "llmconfig": [212, 213, 219, 221, 224, 227, 229, 232, 234], "lo": [11, 72], "load": [2, 3, 7, 10, 22, 25, 28, 32, 35, 38, 43, 48, 49, 55, 56, 57, 62, 68, 70, 75, 76, 78, 81, 83, 84, 89, 90, 92, 94, 95, 96, 97, 98, 99, 100, 105, 107, 110, 111, 117, 134, 135, 147, 148, 178, 179, 183, 187, 190, 191, 195, 203, 204, 206, 210, 211, 212, 213, 218, 219, 221, 224, 226, 229, 232, 234, 237, 241, 244, 245, 253, 255, 261, 262, 264, 269, 270, 278, 280, 281, 287, 289, 293, 299, 301, 303, 305, 306, 308, 310, 314, 315, 318, 319, 320, 323, 325, 326, 330, 331, 336, 337, 340, 342], "load_data": [3, 28], "load_dataset": [73, 74, 76, 79, 80, 85, 95, 97, 274, 276, 277, 307, 314, 321, 327], "load_from_checkpoint": [5, 36], "load_model": [3, 28, 237, 244, 245, 275, 317], "load_model_ray_train": [4, 32, 238, 249, 250, 253, 273, 283, 286, 294, 300], "load_model_torch": [4, 31, 238, 247], "load_state_dict": [4, 31, 32, 238, 247, 253, 273, 274, 276, 277, 278, 279, 292, 300, 310, 314, 323, 325, 331, 336, 340, 342], "loadbalanc": [107, 110], "loaded_df": [5, 35], "loaded_model": [4, 31, 238, 247], "loaded_model_ray_train": [4, 5, 32, 36, 238, 253], "loader": [4, 32, 76, 94, 249, 250, 273, 274, 276, 277, 279, 294, 295, 297, 300, 306, 308, 309, 314, 321, 329, 337], "loan": [73, 82], "loc": [237, 239, 244, 245, 258, 260], "local": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 17, 23, 26, 28, 29, 31, 32, 33, 38, 46, 49, 53, 56, 62, 65, 67, 68, 70, 71, 73, 74, 75, 76, 78, 80, 82, 84, 90, 94, 98, 99, 100, 105, 111, 114, 118, 123, 126, 130, 134, 136, 142, 147, 149, 158, 169, 170, 171, 172, 174, 175, 183, 187, 188, 195, 204, 206, 211, 212, 213, 214, 219, 222, 228, 229, 230, 239, 240, 241, 251, 253, 258, 264, 268, 273, 274, 275, 276, 277, 278, 279, 280, 281, 289, 292, 293, 310, 314, 315, 317, 320, 326, 327, 329, 332, 335, 337], "local_fil": [180, 192], "local_file_path": [229, 232], "local_idx": [274, 308], "local_path": [4, 9, 10, 31, 32, 62, 70, 169, 172, 229, 232, 238, 240, 241, 247, 264, 269, 270], "local_pred_fold": [9, 65], "local_storag": [5, 35, 238, 247], "local_zip": [279, 338], "localhost": [0, 3, 10, 28, 70, 75, 93, 169, 172, 212, 213, 219, 221, 225, 229, 232, 233, 234, 237, 241, 244, 245, 269, 270], "locat": [4, 11, 31, 32, 72, 118, 122, 149, 153, 169, 172, 180, 184, 192, 196, 229, 234, 273, 275, 281, 289, 291, 315], "lock": [11, 72, 74, 77, 88, 181, 193], "lodg": [275, 315], "lofton": [73, 82], "log": [4, 5, 7, 9, 30, 32, 34, 35, 43, 46, 63, 76, 99, 100, 104, 105, 111, 114, 115, 118, 119, 123, 124, 126, 130, 133, 136, 142, 145, 146, 149, 158, 160, 163, 164, 166, 167, 168, 170, 173, 175, 178, 182, 183, 188, 190, 194, 195, 200, 207, 212, 213, 218, 221, 227, 237, 238, 239, 244, 245, 251, 260, 273, 274, 275, 276, 277, 278, 279, 280, 281, 283, 288, 289, 290, 291, 294, 298, 303, 305, 306, 310, 315, 317, 319, 322, 323, 325, 328, 329, 331, 332, 333, 334, 335, 336, 337, 340, 343], "log_engine_metr": [221, 224, 227], "log_every_n_step": [5, 35], "log_level": [169, 172], "log_result": [181, 193], "logdir": [76, 99, 237, 238, 244, 245, 252], "logger": [169, 172], "logging_config": [169, 172], "logic": [2, 4, 5, 7, 9, 10, 22, 32, 36, 47, 48, 61, 69, 75, 76, 92, 93, 99, 100, 102, 105, 229, 235, 237, 238, 239, 241, 244, 245, 252, 258, 260, 270, 273, 274, 276, 277, 278, 279, 282, 294, 300, 306, 310, 321, 326, 329, 331, 332, 337, 340], "login": [111, 114, 118, 121, 123, 126, 130, 136, 142, 149, 152, 158, 178, 190], "logist": [237, 244, 245], "logit": [9, 10, 62, 70, 76, 96, 97, 240, 241, 264, 269, 273, 274, 282, 292, 310, 314], "logloss": [237, 244, 245], "loguniform": [6, 41, 239, 259, 260], "loki": [73, 82], "lol": [73, 82], "london": [73, 82], "long": [2, 3, 4, 5, 6, 22, 28, 30, 32, 34, 36, 40, 73, 74, 82, 89, 100, 104, 185, 198, 212, 213, 216, 217, 229, 235, 237, 238, 239, 244, 245, 248, 257, 273, 276, 279, 280, 299, 320, 338, 340], "longer": [4, 7, 32, 48, 273, 277, 283, 292, 294, 304, 305, 331], "longrightarrow": [276, 278, 320, 332], "look": [2, 3, 4, 6, 7, 8, 9, 25, 28, 31, 32, 41, 48, 54, 60, 64, 73, 74, 80, 82, 88, 89, 111, 113, 114, 118, 123, 126, 128, 130, 136, 139, 140, 142, 149, 158, 169, 171, 221, 228, 229, 232, 236, 238, 240, 241, 247, 249, 265, 269, 270, 273, 275, 277, 281, 283, 295, 316, 331], "look_back_period_": [241, 270], "lookup": [279, 337], "loop": [12, 30, 34, 36, 76, 94, 97, 229, 234, 246, 251, 274, 277, 280, 281, 282, 284, 285, 289, 290, 291, 293, 297, 298, 299, 301, 302, 303, 305, 306, 310, 314, 315, 319, 320, 326, 329, 331, 336, 337], "lora": [212, 213, 216, 220, 230, 231, 236], "lora_checkpoint": [229, 232], "lora_config": [229, 232], "lose": [273, 299, 302], "loss": [4, 5, 6, 31, 32, 35, 40, 41, 74, 76, 89, 97, 98, 99, 237, 238, 239, 244, 245, 247, 249, 251, 252, 253, 257, 259, 260, 273, 275, 281, 282, 283, 288, 289, 291, 294, 300, 306, 307, 310, 311, 315, 317, 319, 325, 326, 328, 332, 334, 336], "loss_fn": [5, 35, 276, 277, 278, 323, 328, 334], "loss_funct": [238, 247, 249, 253], "loss_ms": [5, 35], "lot": [4, 5, 30, 34, 273, 280], "louboutin": [73, 80, 82], "loung": [73, 82], "love": [73, 74, 75, 82, 89, 93], "low": [7, 46, 48, 212, 213, 216, 218, 229, 232, 235, 274, 276, 278, 306, 314, 325, 334, 336], "lower": [73, 75, 82, 92, 180, 181, 192, 193, 221, 223, 228, 229, 235], "lowercas": [75, 92], "lowest": [274, 311], "lr": [4, 5, 6, 31, 32, 35, 36, 40, 41, 76, 97, 98, 238, 239, 247, 249, 253, 257, 259, 260, 273, 274, 276, 277, 278, 279, 283, 294, 300, 310, 311, 323, 325, 328, 334, 340], "lr_schedul": [5, 35], "lssf": [11, 72], "lstm": [276, 325], "lstrip": [180, 192], "lt": [74, 89], "luck": [73, 74, 82, 88], "lucki": [73, 82], "lupin": [73, 80, 82, 83], "lustr": [7, 43], "ly": [74, 89], "m": [0, 2, 4, 11, 24, 31, 72, 73, 74, 82, 88, 89, 238, 247, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338], "m1": [11, 72], "m2": [11, 72], "m3": [11, 72], "m5": [164, 168, 181, 193], "mac": [73, 76, 82, 97], "machin": [2, 3, 8, 11, 20, 27, 43, 46, 55, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 89, 90, 94, 98, 99, 100, 103, 110, 163, 164, 166, 174, 177, 178, 179, 182, 186, 189, 190, 191, 194, 202, 205, 209, 241, 268, 275, 315], "maco": [3, 11, 26, 72, 163, 176], "macosx": [11, 72], "maddon": [73, 82], "made": [5, 10, 36, 69, 74, 85, 88, 89, 279, 340], "madison": [73, 82], "magnitud": [276, 321], "mai": [2, 4, 5, 22, 24, 25, 29, 33, 73, 74, 75, 82, 88, 89, 90, 111, 113, 118, 122, 125, 126, 128, 136, 139, 146, 149, 153, 154, 156, 161, 163, 176, 179, 180, 181, 184, 186, 187, 191, 192, 193, 196, 200, 203, 205, 206, 207, 210, 273, 274, 275, 293, 314, 318], "maiden": [73, 82], "main": [0, 2, 4, 5, 8, 10, 23, 32, 35, 54, 71, 73, 74, 82, 89, 94, 111, 113, 118, 122, 126, 128, 136, 139, 164, 168, 169, 172, 173, 182, 183, 194, 195, 202, 203, 209, 210, 212, 213, 219, 221, 224, 240, 262, 273, 276, 279, 283, 321, 338], "maintain": [2, 7, 10, 25, 43, 48, 69, 74, 89, 100, 102, 180, 192, 212, 213, 216, 278, 332], "mainten": [107, 108], "major": [74, 85], "make": [0, 2, 3, 4, 5, 6, 7, 8, 10, 11, 19, 23, 28, 32, 35, 36, 41, 42, 48, 51, 54, 55, 71, 72, 73, 74, 75, 76, 78, 82, 84, 85, 88, 89, 90, 92, 94, 98, 149, 161, 164, 168, 178, 180, 183, 184, 187, 188, 190, 192, 195, 196, 204, 206, 211, 212, 213, 215, 229, 231, 232, 237, 239, 244, 245, 259, 273, 274, 275, 276, 277, 278, 279, 281, 287, 291, 293, 296, 297, 298, 299, 301, 302, 306, 308, 315, 316, 320, 321, 324, 325, 326, 327, 333, 342], "make_pendulum_dataset": [278, 333], "makedir": [274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 329, 335, 338], "malaga": [73, 82], "male": [74, 85, 89], "mall": [73, 82], "malloc": [163, 176], "mamba": 77, "man": [73, 74, 82, 85, 88, 89], "manag": [1, 4, 5, 7, 13, 17, 30, 34, 43, 73, 75, 76, 82, 90, 98, 100, 102, 103, 104, 105, 108, 110, 118, 120, 122, 126, 130, 134, 136, 142, 147, 149, 151, 158, 162, 163, 164, 167, 169, 172, 176, 177, 178, 179, 180, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 202, 204, 205, 207, 209, 211, 218, 220, 221, 226, 237, 242, 273, 274, 275, 276, 277, 278, 280, 294, 306, 309, 310, 314, 315, 320, 326, 329, 331, 332, 335, 337, 342], "mani": [1, 3, 8, 9, 10, 13, 17, 21, 22, 28, 51, 61, 68, 73, 74, 75, 80, 81, 82, 85, 89, 93, 111, 113, 126, 128, 136, 139, 181, 183, 187, 193, 195, 204, 206, 211, 212, 213, 216, 221, 228, 229, 233, 235, 273, 274, 275, 276, 279, 282, 284, 285, 287, 306, 317, 325, 337, 338], "manipul": [74, 88], "mann": [74, 88, 89], "manner": [3, 8, 9, 28, 49, 51, 53, 56, 74, 76, 84, 98, 240, 261, 274, 306], "mansbridg": [73, 82], "manual": [4, 32, 73, 76, 82, 97, 118, 125, 149, 161, 274, 275, 276, 277, 278, 279, 280, 281, 283, 285, 286, 289, 299, 301, 305, 306, 309, 310, 312, 315, 317, 319, 320, 324, 326, 331, 332, 337, 341], "manual_se": [276, 323], "map": [5, 35, 76, 97, 164, 168, 229, 232, 234, 237, 241, 244, 245, 269, 273, 274, 275, 276, 278, 279, 297, 306, 307, 308, 310, 314, 315, 316, 325, 332, 338, 342], "map_batch": [3, 8, 9, 28, 52, 61, 62, 64, 73, 78, 81, 82, 83, 169, 171, 237, 240, 241, 244, 245, 264, 265, 269, 274, 275, 276, 277, 278, 279, 314, 318, 319, 325, 327, 333, 338], "map_group": [8, 9, 54, 55, 64, 240, 265], "map_loc": [4, 5, 32, 36, 238, 253, 273, 274, 276, 277, 278, 279, 292, 310, 314, 323, 325, 331, 336, 340, 342], "mapbatch": [9, 63, 73, 83], "mapreduc": [7, 46], "mar": [75, 93], "marathon": [73, 82], "marc": [74, 89], "march": [73, 82], "mario": [73, 82], "mark": [74, 89, 163, 176], "markdown": 0, "marker": [274, 276, 277, 278, 279, 312, 323, 325, 329, 335, 340], "market_typ": [164, 168], "marlei": [73, 82], "marri": [74, 85], "martial": [74, 88], "martin": [73, 82], "mask": [76, 97], "mass": [73, 82], "massiv": [9, 57], "master": [11, 72, 74, 76, 88, 99, 229, 231, 239, 260, 276, 321], "mat1_ref": [2, 18], "mat2_ref": [2, 18], "match": [2, 9, 22, 62, 73, 82, 126, 134, 136, 147, 181, 183, 187, 193, 195, 204, 206, 211, 221, 224, 229, 232, 233, 235, 273, 275, 303, 316], "matching_analysi": [229, 232], "materi": [3, 8, 10, 28, 51, 52, 54, 56, 57, 59, 60, 61, 62, 64, 70, 73, 78, 83, 164, 168, 169, 171, 237, 241, 244, 245, 263, 265, 269, 270, 274, 275, 279, 314, 316, 317, 319, 338], "materialized_d": [73, 83], "materializeddataset": [73, 74, 83, 86], "math": [1, 16, 276, 278, 321, 322, 333], "mathbb": [274, 275, 276, 277, 278, 279, 306, 315, 320, 326, 332, 337], "mathcal": [274, 277, 278, 279, 306, 326, 332, 337], "mathemat": [229, 235], "matmul": [2, 18, 279, 342], "matplotlib": [4, 6, 9, 29, 38, 56, 238, 239, 241, 247, 256, 270, 273, 274, 275, 276, 277, 278, 279, 281, 307, 316, 321, 327, 333, 338], "matric": [274, 314], "matrix": [340, 342], "matrixfactorizationmodel": [279, 339, 340, 342], "matt": [73, 82], "matter": [73, 74, 82, 85, 212, 213, 217, 274, 308], "matur": [7, 47, 183, 187, 195, 203, 206, 210], "max": [8, 9, 54, 61, 64, 74, 89, 180, 192, 221, 224, 240, 241, 264, 265, 270, 274, 276, 277, 278, 279, 310, 321, 323, 329, 335, 340], "max_": [240, 264], "max_depth": [3, 28, 237, 244, 245, 275, 317, 319], "max_epoch": [5, 35, 36, 277, 278, 329, 331, 335], "max_failur": [273, 274, 275, 276, 277, 278, 279, 301, 305, 306, 311, 317, 323, 329, 335, 340], "max_len": [276, 322], "max_length": [76, 97], "max_lora": [229, 232], "max_lora_rank": [229, 232], "max_model_len": [212, 213, 219, 224, 229, 232, 233, 234], "max_nod": [126, 133, 136, 145, 164, 168], "max_num_adapters_per_replica": [229, 232], "max_ongoing_request": [7, 48], "max_replica": [212, 213, 219, 221, 224, 227, 229, 234, 241, 270], "max_retri": [2, 20], "max_siz": [9, 62], "max_step": [5, 36], "max_t": [277, 278, 328, 334], "maxim": [7, 8, 43, 48, 54, 73, 78, 212, 213, 215, 217, 218], "maximum": [126, 133, 136, 145, 177, 179, 189, 191, 212, 213, 215, 216, 221, 223, 227, 228, 229, 232, 235, 241, 270, 273, 303], "maxpool": [238, 247, 253], "maxpool2d": [238, 247, 253], "maxpumperla": [0, 76, 99], "mayb": [74, 88], "mb": [276, 321], "mcintir": [74, 88], "md": [0, 77, 169, 172], "mdmad": [73, 82], "me": [73, 74, 82, 85, 88, 89, 100, 105, 221, 225, 226], "mean": [2, 6, 8, 9, 11, 18, 22, 41, 51, 52, 54, 60, 64, 72, 149, 152, 212, 213, 216, 239, 240, 258, 259, 265, 273, 274, 276, 277, 278, 279, 286, 292, 308, 309, 314, 321, 325, 326, 332, 337, 342], "meant": [7, 43, 45, 279, 342], "meantim": [74, 88], "measur": [279, 342], "meat": [74, 85], "mechan": [7, 9, 43, 57, 163, 175, 274, 306, 310], "medium": [212, 213, 217, 220, 225, 228, 229, 235], "meet": [9, 10, 11, 57, 68, 72, 73, 82, 212, 213, 216, 241, 268], "melodrama": [74, 89], "member": [184, 196], "memori": [1, 2, 4, 5, 8, 9, 13, 18, 22, 24, 29, 31, 32, 33, 35, 46, 47, 50, 51, 57, 59, 60, 61, 63, 64, 74, 78, 82, 84, 89, 163, 164, 166, 168, 175, 181, 193, 215, 216, 218, 220, 221, 223, 225, 227, 229, 231, 232, 235, 237, 239, 240, 244, 245, 258, 260, 264, 265, 273, 274, 276, 277, 279, 281, 287, 292, 297, 307, 308, 314, 316, 325, 327, 331, 338], "memory_usag": [8, 51], "memorydb": 104, "men": [74, 85, 88], "mental": [74, 85], "mention": [74, 89, 276, 325], "merg": [8, 55, 279, 342], "merlin": [73, 82], "messag": [2, 7, 11, 25, 46, 72, 164, 166, 173, 212, 213, 219, 221, 225, 226, 229, 232, 233, 234, 238, 239, 241, 252, 258, 269, 277, 278, 329, 335], "messages_cv": [229, 232], "messages_nemoguard": [229, 232], "messages_yara": [229, 232], "messi": [73, 80, 82], "meta": [212, 213, 219, 221, 223, 224, 227, 229, 232, 274, 276, 279, 310, 323, 340], "meta_path": [274, 310], "metadata": [7, 8, 48, 55, 73, 74, 80, 83, 84, 86, 88, 164, 168, 181, 193, 273, 274, 275, 278, 279, 301, 308, 319, 335, 340, 342], "method": [2, 4, 5, 6, 8, 9, 10, 18, 25, 30, 34, 35, 41, 52, 60, 62, 63, 64, 70, 73, 74, 76, 78, 81, 89, 97, 111, 112, 118, 119, 126, 127, 136, 137, 149, 150, 163, 176, 239, 240, 241, 260, 264, 269, 273, 277, 279, 280, 331, 337, 342], "method_nam": [2, 25], "metric": [3, 5, 6, 28, 30, 34, 36, 40, 41, 94, 98, 99, 100, 105, 163, 164, 166, 167, 168, 170, 175, 176, 178, 183, 186, 190, 195, 200, 205, 207, 221, 224, 227, 229, 236, 237, 239, 241, 244, 245, 246, 249, 253, 257, 258, 259, 260, 270, 274, 275, 276, 277, 278, 280, 281, 282, 283, 290, 294, 298, 300, 301, 303, 305, 306, 310, 311, 312, 313, 314, 316, 317, 319, 320, 323, 324, 325, 329, 330, 331, 332, 335, 336, 337, 338, 342, 343], "metrics_datafram": [4, 5, 32, 36, 238, 253, 273, 274, 276, 277, 278, 279, 291, 312, 323, 329, 335, 340], "metrics_interval_": [241, 270], "mf_ray_train": [279, 340], "miami": [73, 82], "mic": [73, 80, 82], "michael": [73, 74, 82, 89], "micro": [274, 310], "microservic": [173, 185, 198, 212, 213, 217], "mid": [273, 274, 275, 303, 313, 315], "mid_block_scale_factor": [5, 35], "middl": [73, 82], "midwai": [73, 82], "might": [3, 4, 5, 6, 9, 26, 28, 30, 34, 41, 61, 74, 85, 107, 109, 111, 113, 126, 128, 136, 139, 149, 152, 212, 213, 216, 237, 244, 245, 273, 274, 275, 277, 280, 314, 317, 319, 331], "migrat": [10, 35, 70, 241, 246, 269, 273, 278, 282, 332], "milan": [73, 82], "mile": [3, 7, 8, 28, 46, 47, 51, 237, 240, 244, 262], "million": [3, 28, 73, 74, 80, 82, 89, 237, 244], "min": [3, 6, 8, 9, 28, 41, 54, 60, 61, 64, 221, 224, 237, 239, 240, 241, 244, 245, 258, 259, 260, 264, 265, 270, 274, 275, 311, 317, 323, 325], "min_": [240, 264], "min_nod": [126, 133, 136, 145], "min_replica": [212, 213, 219, 221, 224, 227, 229, 234, 241, 270], "min_siz": [9, 62], "mind": [73, 74, 82, 85, 89], "mine": [73, 82], "minecraft": [73, 80, 82, 83], "miner": [74, 88], "mini": [4, 5, 31, 32, 36, 273, 277, 278, 280, 326, 332], "miniconda": [11, 72], "miniforge3": [11, 72], "minilm": [73, 81], "minim": [4, 5, 6, 7, 30, 34, 35, 41, 43, 46, 76, 98, 100, 103, 105, 107, 109, 237, 239, 244, 245, 258, 259, 273, 274, 275, 277, 278, 279, 280, 306, 314, 315, 328, 332, 336, 337], "minimalist": 0, "minimum": [100, 105, 126, 133, 136, 145, 177, 179, 189, 191, 241, 270], "minu": [237, 244, 245], "minut": [4, 8, 31, 51, 73, 82, 118, 122, 126, 128, 136, 139, 149, 153, 156, 177, 179, 189, 191, 276, 320, 321], "mirror": [274, 276, 306, 320], "mise": [74, 89], "miseenscen": [74, 89], "misfortun": [74, 88], "mismatch": [276, 323], "miss": [73, 74, 82, 88, 89, 277, 278, 331, 335], "missouri": [73, 82], "mistak": [74, 88], "mistral": [229, 235], "mitch": [73, 82], "mix": [5, 35, 36, 274, 277, 278, 314, 331, 336], "mkdir": [4, 31, 238, 247], "ml": [3, 7, 8, 10, 11, 27, 28, 45, 47, 48, 55, 67, 68, 69, 70, 72, 73, 75, 78, 83, 90, 91, 92, 179, 180, 182, 184, 186, 187, 188, 191, 192, 194, 196, 200, 203, 205, 206, 207, 210, 237, 240, 241, 243, 244, 245, 262, 266, 267, 268, 269, 274, 279, 306, 338, 342], "mlbcentral": [73, 82], "mlflow": [273, 274, 275, 276, 277, 278, 279, 305, 314, 319, 325, 331, 336, 342], "mlogloss": [275, 317], "mlop": [3, 27, 229, 232, 237, 243, 244, 245, 274, 275, 276, 277, 278, 279, 314, 319, 325, 331, 336, 342], "mlp": [278, 279, 334, 336, 342], "mm": [2, 22], "mmlu": [229, 235], "mnist": [4, 6, 9, 10, 29, 31, 32, 38, 39, 40, 41, 59, 61, 62, 64, 70, 169, 172, 239, 240, 241, 250, 252, 253, 256, 257, 258, 260, 263, 264, 265, 269, 270, 280, 283, 286, 287, 289, 291, 292, 296, 304, 305], "mnist_app": [10, 70, 71, 169, 172, 241, 269, 270], "mnist_app_handl": [10, 70], "mnist_classifi": [10, 70, 241, 269, 270], "mnist_classifier_arg": [9, 62], "mnist_deploy": [241, 269], "mnist_deployment_handl": [241, 269, 270], "mnist_pr": [9, 65, 66, 240, 266], "mnist_preprocessor": [241, 270], "mnistclassifi": [9, 10, 62, 63, 70, 240, 241, 264, 269, 270], "mnt": [3, 4, 5, 8, 9, 10, 28, 31, 35, 36, 37, 53, 62, 70, 169, 171, 180, 192, 237, 238, 240, 241, 244, 245, 247, 252, 253, 254, 264, 266, 269, 270, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 287, 289, 291, 296, 304, 306, 307, 308, 310, 311, 314, 316, 317, 319, 320, 321, 327, 329, 331, 335, 336, 338, 340, 342], "mock": [74, 88], "modal": [8, 9, 55, 66, 240, 266], "modano": [73, 82], "mode": [0, 3, 6, 10, 28, 41, 56, 71, 76, 97, 229, 233, 237, 239, 244, 245, 258, 259, 260, 273, 283, 292], "model": [2, 7, 8, 9, 10, 22, 25, 29, 33, 37, 38, 40, 43, 44, 47, 48, 52, 55, 57, 62, 64, 66, 68, 69, 70, 71, 74, 76, 77, 78, 81, 83, 89, 91, 93, 94, 95, 96, 97, 98, 99, 179, 180, 182, 186, 191, 192, 194, 200, 205, 207, 215, 217, 218, 219, 222, 225, 226, 228, 230, 231, 232, 233, 234, 236, 237, 240, 241, 244, 245, 246, 249, 251, 253, 254, 255, 259, 264, 265, 268, 269, 270, 271, 272, 277, 278, 280, 281, 283, 289, 290, 291, 292, 294, 299, 300, 301, 303, 305, 307, 310, 314, 315, 316, 318, 319, 320, 321, 323, 325, 326, 328, 329, 331, 332, 333, 335, 336, 338, 340, 341, 342], "model1": [3, 28], "model1_predict": [3, 28], "model2": [3, 28], "model2_predict": [3, 28], "model_config": [5, 35], "model_dump": [3, 28, 229, 234], "model_id": [212, 213, 219, 221, 224, 227, 229, 232, 233, 234], "model_json_schema": [229, 233], "model_kwarg": [276, 325], "model_loading_config": [212, 213, 219, 221, 224, 227, 229, 232, 233, 234], "model_nam": [5, 35, 36, 73, 81, 179, 191], "model_path": [3, 4, 10, 28, 32, 70, 238, 253, 273, 275, 292, 317], "model_predict": [3, 28], "model_select": [3, 26, 275, 316], "model_sourc": [212, 213, 219, 221, 224, 227, 229, 232, 233, 234], "model_state_dict": [273, 300], "modelcheckpoint": [277, 278, 329, 331, 335], "modelwork": [273, 292], "moder": [212, 213, 216, 229, 232, 235], "modern": [7, 43, 46, 73, 74, 76, 78, 83, 84, 94, 99], "modif": [183, 187, 195, 204, 206, 211], "modifi": [2, 3, 4, 8, 9, 10, 24, 28, 31, 53, 62, 70, 111, 113, 118, 122, 126, 128, 136, 139, 149, 154, 169, 171, 172, 180, 181, 183, 187, 192, 193, 195, 204, 206, 211, 274, 277, 299, 306, 326], "modul": [4, 31, 32, 100, 104, 105, 118, 122, 136, 140, 169, 172, 212, 213, 214, 219, 220, 221, 226, 228, 229, 231, 236, 238, 247, 250, 251, 274, 276, 279, 286, 289, 293, 299, 300, 301, 306, 314, 322, 325, 339, 342], "modular": [100, 104, 105, 279, 337], "mofo": [73, 82], "mom": [73, 82], "momentum": [76, 97, 238, 247, 253], "mondai": [73, 82], "monei": [74, 85, 88], "mongodb": [7, 43], "monitor": [4, 5, 7, 30, 32, 34, 46, 107, 108, 163, 164, 166, 167, 168, 169, 170, 171, 175, 176, 178, 182, 183, 188, 190, 194, 195, 212, 222, 224, 226, 228, 229, 235, 236, 238, 251, 273, 275, 276, 280, 288, 289, 319, 325], "monro": [73, 82], "month": [3, 8, 28, 51, 73, 82, 229, 234], "moon": [73, 82], "more": [1, 2, 6, 7, 8, 9, 10, 16, 19, 20, 22, 23, 24, 30, 31, 34, 37, 40, 41, 45, 47, 48, 54, 55, 57, 61, 64, 66, 68, 69, 71, 74, 75, 76, 77, 88, 89, 90, 92, 93, 97, 99, 100, 105, 109, 111, 113, 126, 127, 128, 136, 137, 139, 149, 150, 163, 164, 166, 168, 169, 171, 176, 181, 185, 193, 198, 212, 213, 216, 218, 219, 223, 228, 231, 238, 239, 241, 249, 250, 253, 254, 257, 259, 260, 262, 265, 266, 268, 270, 273, 274, 275, 276, 277, 279, 280, 281, 285, 289, 291, 305, 314, 318, 319, 323, 331, 342], "morn": [73, 82], "moron": [74, 88], "morri": [74, 88], "morti": [73, 82], "mosh": [73, 82], "most": [2, 7, 8, 9, 19, 25, 45, 52, 58, 60, 74, 75, 88, 89, 93, 107, 109, 177, 179, 180, 185, 189, 191, 192, 198, 229, 232, 233, 240, 264, 273, 274, 275, 277, 278, 279, 291, 303, 307, 310, 312, 318, 329, 335, 338, 341], "most_rec": [180, 192], "mostli": [212, 213, 217], "motion": [278, 332], "motiv": [74, 85], "mount": [100, 105, 180, 192], "mountaincar": [278, 336], "mouth": [74, 88], "move": [4, 5, 6, 7, 31, 32, 35, 40, 41, 43, 73, 74, 76, 78, 89, 97, 188, 212, 213, 220, 238, 239, 247, 250, 257, 260, 273, 274, 275, 276, 277, 278, 282, 283, 286, 287, 289, 290, 292, 294, 295, 306, 316, 321, 331, 332, 336], "movement": [5, 35], "movi": [73, 74, 82, 85, 88, 89, 337, 338], "movielen": [337, 342], "mp": [73, 76, 81, 82, 83, 94, 97, 98], "mse": [277, 278, 279, 326, 329, 335, 337, 340], "mse_loss": [5, 35, 279, 340], "mseloss": [277, 278, 328, 334], "mta": [73, 82], "mtv": [74, 89], "mtvstar": [73, 82], "mtvstarsof2015": [73, 82], "much": [7, 8, 47, 51, 53, 73, 74, 82, 85, 88, 89, 164, 168, 212, 213, 217, 229, 232, 275, 277, 279, 316, 331, 337], "muck": [74, 89], "muddi": [74, 88], "multi": [8, 9, 10, 53, 55, 66, 68, 100, 102, 179, 191, 212, 213, 216, 217, 218, 221, 223, 227, 228, 229, 232, 235, 236, 240, 241, 266, 268, 273, 274, 275, 277, 278, 279, 280, 281, 290, 293, 298, 305, 306, 314, 315, 317, 326, 329, 331, 332, 337, 338, 342], "multi_actor_tracing_ray_serve_exampl": 173, "multiclass": [238, 247, 249, 253], "multiclassaccuraci": [274, 310], "multimod": [277, 326], "multipl": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 16, 18, 22, 23, 27, 30, 32, 34, 36, 41, 43, 46, 53, 55, 61, 62, 68, 69, 73, 74, 75, 76, 78, 81, 84, 87, 90, 94, 97, 98, 99, 100, 105, 107, 109, 162, 164, 166, 169, 172, 173, 179, 185, 191, 198, 212, 213, 216, 217, 218, 221, 222, 223, 224, 227, 228, 229, 231, 232, 235, 236, 237, 239, 240, 241, 243, 244, 245, 258, 263, 264, 268, 273, 274, 275, 276, 277, 278, 279, 280, 287, 289, 293, 296, 307, 312, 314, 315, 319, 323, 325, 326, 327, 331, 332, 336, 338, 340, 342, 343], "multiplex": [9, 10, 57, 68, 241, 268], "multipli": [2, 25, 276, 325], "multiprocess": [2, 22, 274, 308], "multithread": [2, 9, 22, 61], "multivari": [276, 325], "mum": [73, 80, 82], "muslim": [73, 82], "must": [229, 232, 273, 277, 278, 303, 326, 332], "mutat": [2, 25, 73, 83], "mutual": [212, 213, 217], "my": [0, 2, 21, 73, 74, 80, 82, 85, 89, 100, 105, 118, 121, 122, 173, 178, 190, 212, 213, 219, 221, 224, 225, 226, 227, 229, 232, 233, 234], "my_custom_env": [2, 21], "my_simple_model": [6, 41, 239, 258], "my_xgboost_func": [3, 28], "myself": [73, 74, 82, 85, 89], "mysentimentmodel": [75, 92], "mysql": [7, 43], "n": [2, 4, 11, 22, 24, 32, 72, 73, 74, 77, 80, 82, 83, 89, 107, 109, 136, 146, 180, 192, 221, 226, 229, 232, 234, 273, 274, 275, 277, 278, 283, 314, 318, 319, 326, 328, 329, 332, 336, 337], "n_step": [278, 333, 336], "nab": [276, 321], "naiv": [9, 10, 57, 68, 277, 331], "naiveti": [74, 88, 89], "nake": [74, 88], "nam": [136, 140], "name": [4, 5, 6, 7, 10, 11, 31, 32, 36, 41, 48, 70, 71, 72, 74, 85, 88, 100, 104, 105, 106, 111, 113, 114, 115, 118, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 133, 136, 139, 140, 141, 142, 145, 149, 152, 153, 158, 160, 161, 164, 168, 169, 171, 172, 173, 177, 178, 179, 180, 181, 182, 183, 184, 189, 190, 191, 192, 193, 194, 195, 196, 202, 209, 212, 213, 219, 221, 226, 229, 232, 233, 234, 237, 238, 239, 241, 244, 245, 247, 252, 258, 260, 269, 270, 273, 274, 275, 276, 277, 278, 279, 283, 289, 290, 292, 298, 301, 303, 307, 311, 314, 317, 323, 327, 329, 331, 335, 336, 338, 340, 342], "namespac": [126, 128, 129, 131, 132, 133, 134, 136, 139, 141, 143, 144, 145, 147, 149, 156, 159, 161, 273, 289], "nandito": [73, 82], "narrat": [74, 89], "naruto": [73, 82], "nash": [73, 82], "nashnewvideo": [73, 82], "nat": [100, 105, 107, 109, 111, 113, 126, 128, 136, 139], "natgatewai": [111, 113, 126, 128, 136, 139], "nation": [73, 74, 75, 82, 88, 93], "nativ": [1, 3, 7, 15, 27, 43, 45, 46, 47, 74, 88, 89, 107, 108, 110, 164, 167, 168, 188, 212, 213, 217, 218, 274, 276, 277, 278, 306, 320, 329, 331, 332, 335, 336], "nativesbr": [74, 89], "natur": [75, 93, 229, 234], "navig": [163, 169, 171, 173, 176, 178, 182, 183, 184, 187, 190, 194, 195, 196, 204, 206, 211], "nbsp": [177, 180, 189, 192], "nc": [74, 85], "nccl": [76, 98, 274, 306], "ndarrai": [6, 9, 10, 41, 61, 62, 64, 70, 73, 81, 83, 239, 240, 241, 258, 264, 265, 269, 276, 325], "ndcg": [279, 342], "ndim": [273, 292], "necessari": [4, 8, 32, 52, 73, 75, 76, 80, 92, 94, 95, 111, 112, 113, 117, 118, 121, 122, 126, 127, 128, 135, 136, 137, 139, 146, 148, 149, 150, 152, 238, 241, 250, 270, 273, 274, 275, 287, 306, 316], "need": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 16, 18, 19, 21, 30, 32, 34, 36, 41, 43, 46, 51, 53, 55, 57, 62, 65, 68, 70, 72, 73, 74, 75, 76, 82, 88, 89, 90, 98, 100, 101, 102, 110, 111, 112, 113, 114, 118, 119, 120, 122, 123, 125, 126, 127, 128, 130, 136, 138, 139, 140, 142, 146, 149, 151, 153, 154, 158, 161, 162, 163, 164, 168, 176, 177, 178, 179, 183, 189, 190, 191, 195, 212, 213, 217, 221, 222, 223, 224, 226, 228, 229, 230, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 244, 245, 251, 258, 262, 269, 273, 274, 275, 277, 278, 279, 280, 281, 282, 285, 287, 289, 293, 294, 297, 304, 305, 306, 307, 308, 309, 310, 312, 316, 317, 318, 327, 331, 332, 338, 340, 342], "neg": [7, 47, 74, 75, 85, 93], "nemoguard": [229, 232], "nephew": [73, 82], "nest": [17, 19], "net": [5, 35, 274, 277, 278, 310, 328, 334], "netflix": [8, 9, 55, 66, 240, 266], "network": [4, 5, 6, 8, 9, 30, 34, 40, 54, 57, 64, 100, 103, 105, 107, 109, 110, 136, 146, 149, 161, 162, 163, 164, 166, 168, 175, 180, 192, 212, 213, 217, 239, 240, 257, 265, 273, 274, 277, 280, 306, 326, 328], "networkinterfaceid": [111, 113, 126, 128, 136, 139], "neural": [6, 40, 239, 257, 274, 276, 279, 306, 322, 342], "never": [0, 73, 74, 82, 88, 89], "new": [2, 3, 7, 8, 24, 25, 28, 44, 51, 73, 74, 77, 82, 85, 88, 89, 107, 110, 111, 113, 129, 133, 136, 141, 145, 177, 178, 179, 180, 181, 182, 184, 187, 189, 190, 191, 192, 193, 194, 196, 202, 203, 206, 209, 210, 212, 213, 216, 221, 226, 229, 234, 238, 241, 249, 270, 273, 274, 275, 276, 278, 303, 313, 315, 319, 320, 329, 336, 343], "newaxi": [275, 318], "newli": [149, 155], "newsha": [73, 82], "next": [2, 6, 8, 24, 41, 53, 73, 82, 100, 103, 118, 120, 126, 130, 136, 142, 149, 151, 158, 169, 171, 175, 177, 179, 182, 183, 187, 189, 191, 194, 195, 204, 206, 211, 215, 230, 232, 235, 238, 239, 241, 252, 258, 259, 260, 270, 281, 286, 307, 310, 317, 320, 321, 327, 338], "nf": [180, 192], "nfl": [73, 80, 82], "nginx": [107, 110, 134, 135, 147, 148, 161], "nhead": [276, 322, 323, 325], "nhl": [73, 80, 82, 83], "nia": [73, 82], "niall": [73, 82], "nice": [273, 305], "nicer": [7, 47], "nick": [73, 82], "nicki": [73, 82], "nigga": [73, 82], "night": [73, 74, 80, 82, 83, 88, 89], "nightli": [276, 278, 325, 336], "nightmar": [74, 88, 89], "nightmarish": [74, 89], "nine": [274, 277, 307, 327], "nirvana": [73, 82], "nlb": [107, 110], "nlp": [273, 305], "nn": [4, 5, 6, 29, 31, 32, 33, 38, 40, 41, 76, 95, 238, 239, 247, 250, 251, 257, 260, 273, 274, 276, 277, 278, 279, 281, 282, 286, 289, 301, 307, 310, 321, 322, 323, 327, 328, 333, 334, 338, 339], "no_grad": [4, 9, 10, 31, 32, 62, 70, 238, 240, 241, 247, 253, 264, 269, 274, 276, 277, 278, 279, 310, 323, 331, 336, 340, 342], "no_restart": [273, 292], "node": [1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 18, 20, 22, 30, 31, 32, 34, 40, 51, 53, 61, 68, 72, 73, 74, 82, 84, 86, 100, 103, 104, 105, 107, 109, 110, 111, 113, 118, 119, 122, 126, 128, 129, 133, 139, 141, 145, 146, 149, 153, 155, 164, 166, 167, 168, 175, 177, 180, 181, 182, 183, 186, 189, 192, 193, 194, 195, 200, 203, 205, 207, 210, 212, 213, 216, 217, 221, 223, 224, 226, 227, 228, 229, 235, 237, 239, 241, 242, 257, 258, 260, 268, 273, 274, 275, 277, 278, 279, 280, 281, 283, 289, 290, 293, 297, 301, 302, 305, 306, 310, 314, 315, 316, 317, 321, 324, 325, 326, 331, 332, 337, 342], "node_ip": [238, 253], "nodegroup": [136, 140], "noderol": [136, 140], "nofril": [74, 89], "noir": [74, 88, 89], "noirlik": [74, 89], "nois": [5, 35, 276, 278, 321, 328, 331, 332, 333, 334, 336], "noise_schedul": [5, 35], "noised_lat": [5, 35], "noiser": [277, 331], "noisi": [277, 278, 328, 332, 334], "noisy_act": [278, 333, 334], "noisy_img": [277, 328], "non": [4, 10, 31, 71, 169, 172, 212, 213, 217, 218, 219, 221, 225, 227, 229, 232, 233, 234, 273, 274, 275, 276, 277, 278, 279, 280, 281, 307, 310, 316, 321, 327, 333, 338], "non_block": [273, 292], "none": [4, 5, 6, 9, 31, 32, 35, 41, 62, 76, 99, 238, 239, 247, 251, 258, 259, 273, 274, 275, 276, 277, 278, 279, 288, 289, 292, 301, 308, 314, 317, 322, 323, 329, 331, 335, 336, 340, 342], "norm": [273, 276, 292, 321, 325], "norm_ep": [5, 35], "norm_num_group": [5, 35], "normal": [4, 6, 9, 29, 31, 32, 38, 39, 41, 56, 61, 63, 238, 239, 240, 241, 247, 250, 253, 256, 260, 264, 270, 273, 274, 275, 277, 279, 281, 287, 292, 293, 297, 306, 308, 314, 316, 318, 320, 325, 326, 331, 332, 334, 336, 337, 342], "normalci": [74, 89], "normalis": [274, 276, 308, 321], "normalize_cpu": [273, 292], "normalized_batch": [9, 61, 240, 241, 264, 270], "normalized_img": [4, 31, 32], "north": [73, 74, 82, 88], "not_ready_ref": [2, 24], "note": [6, 8, 10, 11, 12, 17, 19, 20, 31, 35, 41, 52, 53, 54, 60, 61, 64, 70, 71, 72, 73, 74, 75, 82, 83, 87, 88, 89, 93, 100, 103, 107, 110, 111, 113, 118, 122, 126, 128, 130, 136, 139, 142, 149, 153, 158, 163, 169, 172, 175, 176, 212, 213, 218, 238, 239, 240, 241, 250, 251, 258, 264, 265, 269, 274, 275, 279, 281, 293, 295, 297, 306, 310, 316, 342], "notebook": [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 17, 19, 26, 29, 31, 33, 38, 49, 56, 67, 73, 75, 76, 77, 78, 83, 89, 90, 93, 94, 99, 111, 112, 118, 119, 126, 127, 136, 137, 163, 164, 165, 169, 170, 174, 177, 179, 180, 185, 188, 189, 191, 192, 197, 212, 213, 214, 221, 222, 229, 230, 238, 239, 240, 241, 246, 255, 261, 267, 273, 274, 275, 276, 277, 278, 279, 280, 281, 306, 307, 314, 319, 320, 326, 332, 336, 337, 338, 343], "noth": [73, 74, 82, 89, 273, 292], "notic": [4, 8, 32, 53, 73, 74, 82, 89, 273, 283], "notif": [169, 172, 173], "notificationservic": 173, "nov": [73, 82, 238, 247], "novelti": [279, 342], "now": [2, 3, 4, 5, 9, 10, 11, 19, 28, 31, 32, 36, 62, 70, 72, 73, 74, 82, 88, 89, 100, 105, 111, 113, 117, 118, 122, 126, 128, 134, 135, 136, 146, 147, 148, 149, 153, 169, 171, 172, 178, 181, 182, 184, 190, 193, 194, 196, 212, 213, 219, 221, 225, 226, 227, 228, 229, 231, 232, 233, 234, 236, 238, 240, 241, 247, 248, 249, 264, 269, 270, 273, 274, 275, 277, 278, 279, 282, 287, 289, 290, 292, 294, 295, 296, 297, 298, 300, 301, 314, 316, 319, 326, 327, 329, 331, 336, 338, 340, 342], "nowher": [74, 85], "np": [1, 2, 4, 5, 6, 9, 10, 12, 17, 18, 22, 24, 29, 31, 32, 33, 35, 38, 41, 56, 61, 62, 64, 67, 70, 73, 76, 79, 81, 82, 95, 96, 164, 168, 169, 172, 239, 240, 241, 258, 259, 264, 265, 269, 270, 273, 274, 275, 276, 277, 278, 279, 281, 292, 297, 307, 314, 316, 317, 318, 321, 325, 327, 333, 336, 338], "nthread": [275, 317], "ntop": [279, 342], "nude": [74, 85], "nuditi": [74, 85], "nuge": [73, 82], "nugent": [73, 82], "null": [111, 113, 126, 128, 136, 139], "num": [163, 176, 238, 249], "num_actor": [274, 314], "num_block": [73, 74, 83, 86], "num_boost_round": [3, 28, 275, 317], "num_class": [4, 31, 238, 247, 249, 253, 273, 274, 275, 282, 310, 314, 317], "num_cpu": [2, 8, 9, 22, 54, 61, 275, 277, 318, 319, 327], "num_decoder_lay": [276, 322], "num_encoder_lay": [276, 322], "num_epoch": [4, 6, 31, 32, 40, 41, 238, 239, 247, 249, 252, 253, 257, 260, 273, 283, 284, 290, 294, 298, 300, 301, 303], "num_gpu": [2, 6, 9, 22, 40, 61, 62, 73, 82, 240, 241, 264, 269, 270, 273, 274, 276, 292, 314, 325], "num_imag": [164, 168], "num_item": [279, 338, 339, 340, 342], "num_label": [76, 97], "num_lay": [276, 322, 323, 325], "num_parquet_shard": [279, 338], "num_partit": [74, 88], "num_replica": [75, 92, 93, 241, 269, 270], "num_return": [2, 24], "num_row": [73, 74, 80, 83, 85, 86, 87, 274, 308], "num_row_group": [274, 308], "num_sampl": [3, 6, 28, 41, 237, 239, 244, 245, 258, 259, 260], "num_to_keep": [274, 275, 276, 277, 278, 279, 311, 317, 323, 329, 335, 340], "num_training_step": [5, 35], "num_us": [279, 338, 339, 340, 342], "num_warmup_step": [5, 35, 36], "num_work": [3, 4, 5, 28, 32, 35, 36, 76, 98, 99, 237, 238, 244, 245, 249, 273, 274, 275, 276, 277, 278, 279, 280, 285, 305, 306, 308, 309, 311, 314, 317, 321, 323, 329, 335, 340], "number": [2, 3, 4, 5, 6, 8, 9, 22, 24, 28, 32, 36, 39, 41, 51, 53, 59, 61, 62, 73, 74, 75, 76, 78, 81, 85, 87, 92, 93, 94, 97, 98, 99, 107, 110, 164, 166, 169, 171, 172, 177, 179, 181, 189, 191, 193, 221, 224, 227, 229, 233, 237, 238, 239, 240, 241, 244, 245, 249, 252, 256, 258, 259, 263, 264, 270, 273, 274, 278, 279, 282, 283, 284, 290, 298, 303, 314, 333, 338, 340], "numenta": [276, 321], "numer": [8, 51, 229, 232, 273, 274, 275, 277, 281, 307, 315, 331], "numpi": [1, 2, 4, 5, 6, 8, 9, 10, 12, 17, 29, 33, 38, 52, 56, 59, 62, 67, 70, 73, 76, 79, 83, 95, 164, 168, 169, 172, 239, 240, 241, 258, 264, 269, 270, 273, 274, 275, 276, 277, 278, 279, 281, 292, 297, 307, 314, 316, 317, 321, 325, 327, 331, 333, 338], "nuremburg": [73, 82], "nutshel": [237, 243], "nvdp": [126, 129, 134, 136, 141, 147], "nvidia": [73, 82, 107, 110, 134, 135, 147, 148, 229, 232], "nvlink": [221, 227], "nvme": [4, 31, 273, 280, 281], "nyc": [3, 8, 28, 51, 54, 169, 171, 325], "nyc_taxi": [276, 321], "nyc_taxi_2021": [237, 244, 245], "nyc_taxi_t": [276, 321], "nyc_taxi_transform": [276, 323], "o": [1, 2, 4, 5, 10, 12, 17, 21, 22, 29, 31, 32, 33, 35, 36, 71, 73, 76, 77, 80, 82, 95, 163, 164, 168, 175, 179, 180, 191, 192, 212, 221, 224, 227, 229, 232, 234, 238, 247, 251, 253, 273, 274, 275, 276, 277, 278, 279, 281, 289, 292, 300, 301, 304, 307, 310, 312, 314, 316, 317, 319, 321, 323, 325, 327, 329, 331, 333, 335, 336, 338, 340, 342], "ob": [278, 333, 334, 336], "obj": [180, 192, 229, 232], "obj_ref": [2, 18], "object": [1, 3, 4, 6, 7, 8, 9, 10, 15, 16, 19, 25, 28, 32, 40, 41, 47, 48, 51, 63, 64, 68, 70, 73, 78, 82, 83, 100, 105, 118, 125, 149, 161, 181, 183, 187, 193, 195, 204, 206, 211, 212, 213, 216, 221, 224, 229, 232, 234, 237, 238, 239, 240, 241, 244, 245, 249, 252, 253, 257, 258, 259, 263, 264, 265, 269, 273, 274, 275, 289, 290, 291, 297, 303, 305, 307, 312, 317, 319, 338, 342], "object_ref": [2, 24], "objectref": [1, 2, 15, 18, 19, 24], "oblig": [74, 89], "oblivi": [74, 89], "obs_dim": [278, 334, 336], "obs_sampl": [278, 336], "observ": [4, 5, 30, 32, 34, 76, 99, 168, 183, 187, 195, 203, 206, 210, 212, 229, 236, 239, 260, 273, 274, 275, 276, 278, 280, 308, 316, 322, 332, 333, 334], "observed_data": [169, 171], "obtain": [76, 97, 180, 192], "obtus": [74, 85], "obviou": [74, 85], "occupi": [273, 304], "occur": [7, 46, 164, 168, 273, 279, 297, 302, 338], "ocean": [73, 82], "oct": [73, 80, 82], "octob": [73, 82], "off": [0, 4, 6, 9, 31, 32, 39, 60, 73, 74, 80, 82, 89, 221, 227, 229, 232, 235, 238, 239, 247, 253, 256, 273, 274, 275, 276, 277, 279, 281, 292, 307, 308, 311, 314, 318, 319, 324, 327, 331, 340, 341], "offenc": [73, 82], "offend": [73, 82], "offer": [3, 7, 10, 27, 43, 45, 46, 47, 68, 69, 74, 85, 89, 107, 108, 109, 182, 185, 186, 194, 199, 200, 205, 207, 221, 223, 227, 277, 327], "offici": [100, 105, 118, 122, 163, 169, 172, 176, 178, 179, 190, 191], "offlin": [181, 193, 237, 244, 245, 275, 278, 279, 315, 332, 333, 338], "offlinemnistclassifi": [241, 269], "offlinepredictor": [3, 28, 237, 244, 245], "offload": [180, 192, 274, 306], "often": [7, 43, 46, 47, 188, 273, 275, 279, 292, 318, 338], "oh": [73, 82], "olap": [7, 43], "old": [73, 74, 82, 85, 89, 187, 203, 206, 210, 274, 275, 276, 314, 319, 325], "older": [169, 172, 277, 331], "oltp": [7, 43], "olympics2012": [73, 82], "omp_num_thread": [2, 22], "on_demand": [164, 168], "on_epoch": [277, 278, 328, 334], "on_fit_start": [5, 35], "on_step": [5, 35], "onc": [1, 9, 16, 17, 19, 23, 25, 61, 62, 73, 76, 78, 81, 97, 111, 115, 118, 124, 126, 133, 136, 145, 149, 160, 178, 179, 180, 182, 190, 191, 192, 194, 212, 213, 216, 219, 221, 225, 226, 229, 232, 237, 240, 244, 245, 264, 273, 274, 275, 276, 277, 279, 280, 281, 289, 292, 306, 308, 314, 318, 320, 325, 329, 337], "one": [1, 2, 3, 4, 5, 7, 8, 9, 10, 16, 20, 22, 23, 24, 28, 30, 34, 44, 48, 53, 55, 57, 68, 69, 73, 74, 78, 82, 85, 88, 89, 100, 105, 107, 109, 110, 111, 113, 126, 128, 136, 139, 163, 169, 171, 172, 176, 179, 180, 181, 185, 187, 191, 192, 193, 198, 203, 206, 210, 212, 213, 215, 216, 217, 218, 229, 231, 232, 235, 238, 241, 247, 268, 273, 274, 275, 277, 280, 282, 285, 287, 291, 293, 305, 306, 308, 310, 311, 316, 317, 320, 323, 324, 329], "onehellofanighttour": [73, 82], "ones": [2, 7, 24, 45, 73, 75, 82, 90, 111, 113, 126, 128, 136, 139, 178, 190, 212, 213, 216, 221, 223], "ongo": [107, 108, 162, 241, 270], "onli": [1, 2, 4, 8, 9, 16, 25, 32, 51, 53, 60, 61, 63, 64, 73, 74, 76, 82, 84, 85, 87, 88, 89, 97, 98, 100, 103, 107, 109, 111, 114, 118, 123, 126, 130, 136, 142, 149, 158, 163, 164, 167, 168, 171, 176, 179, 180, 181, 184, 185, 188, 191, 192, 193, 196, 198, 212, 213, 216, 218, 229, 232, 238, 240, 241, 251, 264, 265, 270, 274, 275, 276, 277, 278, 279, 280, 282, 283, 288, 301, 304, 305, 306, 308, 310, 312, 315, 317, 321, 323, 325, 331, 332, 336, 338, 340, 342], "onlin": [7, 10, 43, 70, 73, 77, 82, 92, 181, 187, 193, 203, 206, 210, 237, 241, 244, 245, 269, 273, 274, 276, 293, 314, 325, 343], "onlinemnistclassifi": [10, 70, 241, 269, 270], "onlinemnistpreprocessor": [241, 270], "onlinepredictor": [237, 244, 245], "onto": [2, 6, 9, 22, 40, 60, 239, 257, 273, 292], "onu": [7, 46], "oom": [4, 5, 9, 29, 33, 61, 164, 168], "op": [274, 307], "open": [0, 1, 4, 7, 11, 13, 31, 43, 72, 73, 74, 82, 88, 118, 121, 163, 164, 167, 169, 171, 173, 176, 178, 179, 180, 181, 182, 188, 190, 191, 192, 193, 194, 229, 234, 235, 238, 247, 274, 277, 307, 308, 314, 327], "openai": [212, 213, 218, 219, 221, 224, 225, 226, 229, 232, 233, 234], "openapi": [241, 270], "opentelemetri": [163, 173, 175], "oper": [7, 43, 44, 47, 49, 55, 56, 57, 60, 61, 73, 74, 76, 83, 84, 86, 87, 88, 95, 100, 102, 109, 118, 122, 127, 128, 129, 132, 134, 135, 137, 139, 141, 144, 146, 147, 148, 150, 153, 161, 162, 163, 169, 170, 171, 173, 175, 176, 185, 198, 212, 213, 215, 217, 237, 244, 245, 261, 274, 277, 306, 326, 327], "opinion": [74, 85, 100, 104], "oppos": [74, 89], "opt": [163, 176], "opt_path": [274, 310], "opt_state_path": [276, 323], "optim": [4, 5, 6, 7, 8, 9, 10, 29, 31, 32, 35, 38, 40, 41, 43, 47, 52, 54, 55, 61, 68, 73, 76, 78, 81, 97, 100, 105, 107, 109, 214, 218, 220, 222, 228, 229, 235, 236, 237, 238, 239, 241, 244, 245, 247, 249, 253, 257, 258, 259, 260, 268, 273, 274, 276, 277, 278, 279, 281, 283, 294, 299, 300, 301, 303, 305, 306, 307, 310, 314, 321, 323, 328, 334, 340, 342], "optimizerlrschedul": [5, 33, 35], "option": [2, 3, 4, 8, 9, 10, 20, 22, 26, 27, 31, 54, 59, 62, 64, 69, 71, 73, 75, 76, 78, 82, 92, 93, 99, 103, 104, 109, 111, 113, 117, 118, 119, 122, 125, 128, 134, 135, 139, 147, 148, 161, 164, 168, 169, 172, 179, 191, 212, 213, 216, 219, 229, 232, 233, 237, 239, 240, 241, 243, 244, 245, 260, 265, 269, 270, 273, 276, 277, 278, 279, 281, 292, 295, 317, 319, 322, 323, 325, 327, 329, 335, 340, 343], "optuna": [6, 38, 41, 239, 258, 259], "optunasearch": [6, 41, 239, 259], "orang": [2, 18], "orc": [7, 43], "orchestr": [43, 107, 108, 109, 173, 217, 221, 224, 273, 274, 275, 276, 277, 278, 279, 280, 281, 285, 290, 306, 307, 310, 319, 320, 321, 323, 325, 326, 331, 332, 337, 340, 342], "order": [1, 6, 13, 41, 73, 74, 82, 88, 164, 166, 177, 189, 239, 259, 275, 316, 317], "ordinari": [74, 85], "oregon": [73, 82], "org": [185, 198, 199, 279, 338, 343], "org_967t9ah1lbk1yqf1zau6a1v247": [180, 192], "org_xxxxxxx": [118, 122], "organ": [7, 8, 43, 51, 100, 102, 105, 118, 122, 162, 181, 184, 188, 193, 196, 199, 273, 289], "organiz": [107, 109, 188], "orient": [237, 244, 245], "origin": [4, 32, 73, 74, 80, 82, 83, 89, 136, 146, 238, 249, 273, 274, 276, 277, 279, 282, 306, 321, 327, 341, 342], "original_user_id": [279, 342], "oscar": [73, 82], "oss": [164, 167, 168], "ossci": [238, 239, 252, 258, 260], "other": [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 23, 25, 27, 28, 30, 31, 32, 34, 36, 41, 43, 45, 47, 51, 69, 72, 73, 74, 75, 76, 82, 83, 88, 89, 90, 94, 100, 105, 107, 108, 149, 161, 163, 175, 178, 180, 181, 190, 192, 193, 229, 231, 237, 239, 240, 243, 244, 245, 259, 262, 263, 273, 274, 276, 277, 278, 279, 280, 281, 284, 292, 293, 306, 323, 329, 331, 336, 338, 340], "otherwis": [4, 7, 31, 48, 229, 232, 279, 338], "otlp": 173, "our": [2, 4, 5, 6, 8, 9, 10, 23, 32, 35, 36, 39, 40, 41, 51, 59, 65, 70, 73, 74, 76, 82, 89, 96, 169, 171, 173, 181, 182, 193, 194, 224, 225, 226, 229, 232, 233, 234, 239, 240, 241, 249, 257, 258, 263, 266, 270, 273, 289], "out": [2, 3, 4, 5, 7, 8, 9, 20, 24, 28, 29, 32, 33, 38, 41, 47, 55, 58, 74, 78, 80, 82, 85, 88, 89, 163, 164, 168, 175, 179, 180, 181, 182, 183, 187, 191, 192, 193, 194, 195, 203, 204, 206, 210, 211, 241, 270, 273, 274, 275, 276, 277, 278, 279, 280, 308, 314, 319, 325, 326, 327, 333, 338, 342], "out_channel": [4, 5, 31, 35, 238, 247, 273, 282], "out_featur": [238, 247, 253], "out_img_byt": [277, 327], "out_label": [277, 327], "out_proj": [276, 322], "out_ref": [2, 19], "outbound": [100, 105], "outbr": [74, 89], "outdoor": [73, 82], "outhous": [73, 82], "outlier": [10, 68], "outlook": 222, "output": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 16, 20, 28, 31, 32, 35, 40, 41, 50, 51, 53, 59, 61, 63, 65, 70, 72, 73, 76, 78, 97, 111, 113, 114, 118, 122, 123, 125, 126, 128, 130, 136, 139, 140, 142, 149, 152, 153, 156, 158, 161, 169, 171, 172, 181, 183, 186, 193, 195, 200, 205, 207, 212, 213, 215, 217, 220, 221, 224, 230, 231, 232, 234, 236, 238, 239, 240, 247, 249, 253, 257, 260, 262, 263, 264, 266, 273, 276, 277, 279, 280, 281, 282, 283, 289, 291, 294, 300, 304, 320, 322, 325, 328, 331, 338, 342], "output_column": [74, 89], "output_csv": [279, 338], "output_dir": [274, 277, 307, 327], "output_path": [164, 168], "output_s": [238, 247, 253], "outsid": [4, 5, 32, 36, 107, 110, 238, 249], "outstand": [7, 48], "over": [2, 3, 6, 8, 9, 25, 28, 41, 52, 55, 57, 58, 73, 74, 76, 82, 88, 89, 97, 107, 109, 177, 179, 180, 181, 182, 183, 189, 191, 192, 193, 194, 195, 237, 239, 244, 245, 259, 273, 274, 275, 277, 279, 283, 294, 299, 306, 314, 315, 319, 323, 325, 331, 340, 342], "overal": [76, 94, 240, 262, 275, 317], "overcom": [73, 82], "overfit": [274, 312], "overhead": [1, 4, 5, 7, 13, 30, 34, 46, 178, 188, 190, 212, 213, 216, 221, 227, 273, 278, 280, 336], "overlap": [164, 167, 273, 276, 280, 320], "overload": [164, 168], "overr": [74, 89], "overrid": [111, 113, 118, 122, 126, 128, 136, 139, 183, 187, 195, 204, 206, 211, 273, 282], "overriden": [182, 186, 194, 200, 205, 207], "overse": [238, 249], "oversubscrib": [2, 22], "overview": [6, 7, 8, 9, 12, 26, 36, 40, 43, 49, 56, 67, 164, 165, 169, 171, 174, 176, 180, 192, 222, 228, 230, 239, 240, 242, 246, 257, 261, 267, 273, 280, 343], "overwhelm": [164, 168], "overwrit": [178, 190], "own": [2, 4, 8, 10, 18, 31, 53, 69, 73, 74, 75, 80, 82, 89, 90, 93, 107, 109, 118, 122, 177, 180, 183, 185, 187, 189, 192, 195, 198, 203, 206, 210, 221, 226, 229, 236, 237, 242, 273, 274, 275, 276, 277, 278, 279, 281, 308, 310, 314, 315, 316, 317, 320, 326, 327, 329, 332, 335, 337, 340], "owner": [118, 120, 149, 151, 185, 198], "ownership": [74, 88], "ox": [73, 82], "p": [74, 89, 276, 277, 325, 326], "p50": [169, 172], "p90": [169, 172], "p99": [169, 172], "pa": [274, 275, 276, 277, 307, 316, 317, 321, 327], "pack": [73, 82, 277, 328], "packag": [11, 72, 77, 179, 180, 191, 192, 273, 275, 277, 278, 279, 301, 319, 331, 333, 342], "pad": [4, 6, 31, 40, 41, 76, 97, 238, 239, 247, 253, 257, 260, 273, 277, 282, 328], "page": [0, 8, 9, 54, 64, 169, 171, 212, 213, 220, 221, 227, 240, 265], "pagedattent": [212, 213, 218], "pagerduti": [169, 172], "pai": [74, 89, 212, 213, 218], "paid": [3, 8, 28, 51], "pain": [7, 46], "pair": [4, 31, 274, 279, 308, 337], "pal": [73, 82], "pale": [74, 89], "pan": [74, 88], "pancak": [274, 306], "panda": [3, 4, 5, 8, 26, 29, 33, 49, 51, 52, 53, 54, 84, 85, 237, 238, 244, 245, 247, 273, 274, 275, 276, 277, 278, 279, 281, 291, 296, 307, 314, 316, 317, 318, 319, 321, 325, 327, 333, 338], "panel": [1, 2, 16, 24], "pant": [73, 82], "paper": [73, 82], "par": [74, 89], "parallel": [6, 7, 8, 9, 11, 12, 35, 41, 46, 50, 52, 54, 57, 59, 72, 73, 74, 75, 76, 78, 80, 82, 83, 84, 86, 87, 90, 97, 215, 217, 224, 228, 239, 240, 249, 258, 263, 264, 274, 275, 276, 277, 278, 279, 283, 285, 293, 296, 297, 298, 306, 307, 314, 315, 316, 318, 320, 325, 326, 327, 331, 332, 333, 335, 337, 338, 342], "parallel_strategi": [4, 32, 273, 286, 305], "parallel_strategy_kwarg": [4, 32, 273, 286], "param": [3, 28, 75, 93, 237, 244, 245, 275, 279, 317, 340], "param_nam": [273, 284], "param_spac": [3, 6, 28, 41, 237, 239, 244, 245, 258, 259, 260], "paramet": [1, 3, 4, 5, 6, 8, 9, 16, 28, 31, 32, 35, 40, 41, 52, 54, 61, 62, 64, 73, 74, 75, 76, 82, 87, 92, 94, 97, 111, 114, 118, 123, 126, 130, 136, 142, 149, 158, 179, 183, 187, 191, 195, 204, 206, 211, 212, 213, 217, 221, 223, 228, 229, 232, 234, 236, 238, 239, 240, 247, 249, 253, 257, 259, 260, 265, 273, 274, 275, 276, 277, 278, 279, 283, 284, 286, 293, 294, 298, 300, 305, 306, 310, 317, 323, 328, 334, 340], "parameter": [10, 71, 273, 300], "paramor": [73, 82], "parcel": [74, 89], "parent": [4, 31, 238, 247], "parish": [73, 82], "pariti": [107, 109], "park": [73, 75, 82, 93], "parquet": [3, 5, 7, 8, 9, 28, 35, 43, 51, 53, 54, 55, 59, 65, 164, 168, 169, 171, 180, 192, 237, 240, 244, 245, 266, 273, 278, 293, 296, 304, 305, 306, 310, 314, 315, 317, 319, 320, 323, 326, 331, 336, 337], "parquet_256": [274, 277, 307, 308, 327], "parquet_dir": [275, 276, 279, 316, 321, 323, 338], "parquet_fil": [274, 308], "parquet_path": [274, 276, 277, 308, 309, 314, 321, 327], "parquetdataset": [5, 35], "parquetfil": [274, 308], "pars": [7, 10, 48, 70, 181, 193, 213, 219, 221, 225, 229, 233, 241, 269, 276, 321], "parseabl": [229, 231, 233], "part": [0, 6, 8, 38, 49, 51, 74, 76, 89, 94, 98, 136, 140, 178, 190, 221, 224, 238, 239, 240, 241, 246, 255, 261, 267, 279, 338], "parti": [73, 82, 163, 175], "particular": [9, 61, 74, 85], "particularli": [2, 23, 73, 82], "partit": [9, 57, 73, 78, 80, 274, 275, 277, 278, 308, 316, 326, 332], "partner": [73, 74, 82, 88], "pass": [3, 4, 5, 6, 7, 8, 9, 10, 17, 22, 25, 28, 32, 35, 36, 41, 47, 51, 53, 57, 62, 70, 73, 76, 82, 97, 183, 195, 237, 238, 239, 240, 241, 244, 245, 247, 249, 258, 260, 263, 264, 269, 270, 273, 274, 275, 279, 280, 283, 284, 285, 286, 290, 293, 294, 296, 297, 298, 300, 301, 303, 305, 314, 317, 337], "passeng": [3, 8, 28, 51, 54, 74, 89, 237, 244, 325], "passenger_count": [3, 8, 28, 51, 237, 244], "passrol": [100, 105], "past": [73, 82, 178, 179, 181, 182, 190, 191, 193, 194, 202, 209, 276, 320, 321, 322, 323, 325], "past_list": [276, 325], "past_norm": [276, 325], "patch": [7, 47, 107, 109], "path": [3, 4, 5, 7, 8, 9, 10, 28, 29, 31, 32, 35, 36, 48, 53, 60, 61, 62, 64, 65, 70, 71, 76, 99, 107, 110, 149, 152, 164, 168, 169, 171, 172, 180, 192, 221, 224, 229, 232, 237, 238, 239, 240, 244, 245, 247, 251, 252, 253, 258, 265, 273, 274, 275, 276, 277, 278, 279, 281, 289, 290, 291, 292, 298, 300, 301, 303, 304, 307, 310, 314, 316, 317, 319, 321, 323, 325, 327, 329, 330, 331, 335, 336, 338, 340, 342], "pathlib": [3, 4, 28, 29, 31, 238, 247, 273, 276, 281, 321], "paths_to_delet": [273, 274, 304, 314], "patient": [73, 82], "pattern": [7, 10, 12, 17, 19, 22, 43, 71, 273, 289, 294, 305, 314, 319, 331, 338, 342, 343], "payload": [3, 28, 237, 244, 245], "payment_typ": [8, 51, 54], "pb": [164, 168], "pc": [73, 80, 82, 83], "pd": [3, 4, 5, 8, 26, 28, 29, 31, 33, 35, 49, 51, 52, 74, 85, 237, 238, 244, 245, 247, 273, 274, 275, 276, 277, 278, 279, 281, 296, 307, 316, 318, 319, 321, 325, 327, 333, 338, 342], "pdf": [279, 338], "pe": [276, 322], "peac": [73, 82], "peak": [212, 213, 217], "peer": [107, 109], "penalti": [169, 172], "pend": [76, 99, 237, 238, 244, 245, 252], "pendulum": [335, 336], "pendulum_diffus": [278, 335, 336], "pendulum_diffusion_ft": [278, 335], "pendulum_diffusion_result": [278, 335], "peopl": [73, 74, 82, 85, 89], "per": [8, 9, 51, 54, 59, 60, 94, 98, 163, 169, 171, 172, 175, 180, 187, 192, 203, 206, 210, 212, 213, 217, 218, 221, 224, 227, 229, 232, 240, 241, 263, 270, 274, 275, 276, 277, 278, 282, 284, 285, 289, 290, 298, 306, 310, 312, 314, 315, 317, 318, 319, 320, 323, 325, 328, 329, 334, 335, 338, 340], "per_worker_batch": [273, 283], "percentag": [8, 52], "percentil": [276, 325], "perceptu": [277, 331], "perfect": [73, 74, 82, 89, 221, 223, 277, 327], "perform": [1, 2, 3, 5, 6, 7, 8, 9, 10, 16, 22, 26, 27, 28, 35, 36, 38, 41, 43, 46, 47, 48, 49, 52, 54, 55, 56, 57, 61, 62, 64, 67, 68, 70, 73, 74, 76, 81, 82, 83, 87, 88, 89, 94, 99, 163, 164, 167, 169, 172, 175, 179, 180, 181, 183, 187, 191, 192, 193, 195, 203, 206, 210, 212, 213, 214, 215, 221, 223, 227, 228, 229, 233, 234, 235, 236, 237, 239, 240, 241, 243, 244, 259, 262, 264, 265, 269, 270, 273, 274, 275, 276, 277, 278, 279, 289, 291, 293, 295, 299, 305, 306, 310, 314, 316, 318, 319, 320, 323, 325, 327, 331, 332, 336, 342], "performantli": [11, 72], "perhap": [6, 7, 41, 47, 74, 88, 89, 239, 259], "period": [177, 179, 189, 191, 212, 213, 217, 241, 270, 276, 325], "permiss": [100, 105, 107, 109, 111, 112, 118, 119, 126, 127, 136, 138, 146, 180, 184, 192, 196], "permut": [277, 331], "persi": [73, 82], "persis": [273, 281], "persist": [4, 5, 31, 32, 36, 56, 57, 100, 104, 105, 107, 109, 163, 164, 167, 175, 180, 181, 186, 192, 193, 200, 205, 207, 251, 261, 275, 278, 279, 280, 281, 282, 283, 287, 296, 299, 301, 304, 306, 310, 315, 316, 335, 338, 341], "person": [74, 89, 279, 337, 342], "perspect": [74, 85, 89], "pertain": [181, 193], "phase": [76, 97, 220], "philip": [73, 82], "philosop": [74, 89], "philosophi": [74, 88], "photo": [73, 82, 274, 306], "photograph": [74, 89, 274, 277, 307, 326], "physic": [2, 9, 22, 61, 164, 166], "pi": [181, 193, 278, 332, 333, 336], "pi4_sampl": [181, 193], "pi_": [278, 332], "pic": [73, 82], "pick": [73, 74, 82, 89, 136, 140, 212, 213, 219, 273, 274, 276, 279, 303, 305, 306, 313, 314, 324, 341], "pickup": [276, 320], "pid": [76, 99, 238, 239, 252, 253, 258, 260], "piec": [73, 82, 169, 171], "pil": [4, 29, 273, 274, 277, 281, 287, 292, 297, 307, 327], "pile": [74, 85], "pin": [273, 274, 277, 292, 306, 309, 327], "pine": [275, 315], "pinecon": [7, 43], "pink": [74, 85], "pinterest": [9, 66], "pioneer": [7, 46], "pip": [0, 11, 17, 72, 77, 149, 157, 163, 173, 176, 178, 179, 180, 190, 191, 192, 221, 225, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338], "pipelin": [3, 5, 7, 8, 9, 17, 28, 35, 43, 46, 53, 55, 57, 61, 62, 66, 74, 75, 84, 89, 91, 92, 164, 168, 170, 179, 191, 212, 213, 216, 217, 228, 229, 236, 240, 262, 264, 266, 273, 274, 275, 276, 277, 278, 279, 281, 293, 295, 296, 298, 305, 306, 308, 314, 315, 319, 320, 325, 326, 327, 332, 336, 337], "pipeline_parallel_s": [221, 227], "pitch": [73, 82], "pivot": [278, 332], "pixel": [6, 9, 39, 61, 239, 256, 273, 274, 278, 281, 287, 296, 297, 307, 326, 327, 329, 333], "pixeldiffus": [277, 328, 329, 331], "pizza": [274, 306], "pl": [5, 33, 35, 36, 277, 278, 327, 328, 329, 333, 334, 335], "pl_ckpt": [277, 331], "place": [0, 7, 48, 73, 74, 75, 82, 89, 93, 212, 213, 216, 273, 281, 300, 302], "placehold": [111, 112, 118, 119, 126, 127, 134, 136, 137, 147, 149, 150, 178, 190, 212, 213, 219, 221, 225], "placement": [273, 274, 276, 278, 283, 286, 287, 290, 306, 309, 310, 314, 325, 332], "plai": [73, 74, 82, 88, 178, 190, 274, 314], "plain": [274, 277, 306, 327], "plan": [9, 60, 61, 63, 73, 80, 82, 100, 105, 107, 109, 110, 111, 113, 118, 122, 126, 128, 134, 136, 139, 147, 149, 153, 162, 169, 172, 229, 232], "plane": [102, 104], "planner": [276, 320], "plate": [73, 82], "plateau": [276, 325], "platform": [7, 43, 45, 46, 73, 76, 80, 94, 98, 100, 103, 107, 109, 162, 163, 164, 167, 169, 170, 171, 172, 175, 176, 181, 186, 187, 188, 193, 200, 203, 205, 206, 207, 210, 212, 213, 217, 241, 271, 273, 281], "plausibl": [278, 336], "pleas": [77, 100, 105, 111, 114, 118, 123, 126, 127, 130, 134, 136, 137, 142, 147, 149, 150, 158, 161, 169, 171, 172, 188, 229, 232, 274, 279, 306, 342], "plot": [74, 85, 88, 89, 241, 270, 273, 275, 281, 291, 292, 307, 314, 316, 319, 321, 325, 327, 328, 333, 334, 338], "plotlin": [74, 89], "plt": [4, 6, 9, 29, 31, 32, 38, 39, 56, 60, 238, 239, 241, 247, 253, 256, 270, 273, 274, 275, 276, 277, 278, 279, 281, 292, 307, 312, 314, 316, 318, 321, 323, 325, 327, 329, 331, 333, 335, 338, 340], "plu": [100, 105, 273, 274, 278, 281, 307, 333], "plugin": [5, 36, 107, 110, 134, 135, 147, 148, 149, 155, 277, 278, 329, 335], "pm": [278, 332], "pndm": [277, 331], "png": [9, 61], "poc": [107, 109], "pod": [107, 109, 110, 126, 132, 133, 134, 136, 144, 145, 147, 149, 156, 212, 213, 217], "point": [4, 9, 10, 31, 61, 71, 73, 74, 78, 82, 88, 89, 100, 103, 107, 109, 111, 112, 118, 119, 126, 127, 136, 137, 149, 150, 173, 180, 192, 212, 213, 219, 221, 226, 273, 275, 283, 289, 303, 316], "pointless": [74, 85], "pole": [275, 315], "polici": [100, 104, 105, 107, 109, 110, 118, 122, 125, 146, 149, 153, 328, 331, 333, 343], "polish": [74, 89], "polit": [74, 85, 89, 229, 232], "politician": [74, 85], "poll": [73, 82], "pont": [73, 82], "pool": [9, 62], "poor": [10, 68], "poorli": [6, 41, 239, 259], "popul": [8, 51, 111, 113, 126, 128], "popular": [3, 7, 27, 28, 45, 46, 73, 81, 229, 232, 279, 338], "porn": [74, 85], "porno": [74, 85], "pornograph": [74, 85], "port": [163, 176, 275, 315], "portion": [76, 97], "pos_enc": [276, 322], "posit": [7, 47, 73, 74, 75, 80, 82, 84, 85, 87, 89, 93, 276, 322], "posix": [100, 105], "possibl": [74, 84, 221, 228, 229, 234, 236], "possibli": [7, 48], "post": [3, 4, 5, 6, 10, 28, 32, 37, 42, 70, 75, 92, 100, 105, 169, 172, 173, 237, 238, 241, 244, 245, 254, 269, 270, 277, 279, 326, 337, 342], "poster": [74, 89], "postgresql": [7, 43], "postwar": [74, 88, 89], "potato": [74, 85], "potemkin": [74, 89], "potenti": [2, 20, 164, 166], "potter": [73, 82], "power": [2, 23, 73, 74, 76, 78, 84, 94, 98, 229, 230, 231, 234, 275, 319], "powershel": [229, 232], "pq": [274, 276, 277, 307, 308, 321, 327], "practic": [4, 11, 32, 72, 73, 74, 76, 83, 86, 94, 100, 103, 118, 122, 162, 163, 175, 185, 197, 212, 230, 231, 232, 236, 273, 274, 276, 280, 289, 292, 305, 306, 321], "practition": [3, 27, 237, 243], "prayer": [73, 82], "pre": [9, 62, 75, 76, 91, 92, 97, 118, 122, 178, 179, 180, 190, 191, 192, 240, 262, 263, 264, 274, 276, 277, 279, 306, 320, 331, 342], "preced": [74, 89], "precis": [5, 35, 36, 212, 213, 217, 221, 223, 274, 277, 278, 314, 331, 336], "precomput": [212, 213, 215, 274, 308], "preconfigur": [273, 282], "pred": [4, 31, 32, 273, 274, 275, 276, 278, 279, 292, 310, 314, 318, 319, 322, 323, 325, 334, 340], "pred_d": [274, 275, 276, 314, 318, 319, 325], "pred_label": [275, 317, 318], "pred_nois": [277, 278, 328, 331, 336], "pred_norm": [276, 325], "pred_prob": [275, 317], "pred_row": [274, 276, 314, 325], "predefin": [179, 191], "predic": [8, 55], "predict": [3, 6, 7, 9, 10, 28, 41, 44, 62, 65, 70, 73, 75, 76, 78, 90, 92, 93, 96, 97, 229, 235, 238, 239, 240, 241, 245, 247, 253, 258, 264, 266, 269, 270, 274, 275, 276, 277, 278, 279, 307, 314, 315, 317, 318, 319, 320, 321, 322, 325, 326, 328, 331, 332, 334, 337, 339, 342], "predicted_label": [9, 10, 62, 64, 70, 169, 172, 240, 241, 264, 265, 269, 270, 274, 314], "predicted_prob": [237, 244, 245], "prediction_pipelin": [3, 28], "predictor": [3, 28, 212, 213, 215, 237, 244, 245, 274, 314], "preemption": [9, 57, 181, 193, 273, 302], "prefer": [74, 89, 107, 109, 111, 114, 118, 123, 126, 130, 136, 142, 149, 156, 158, 173, 178, 190, 229, 235, 277, 331], "prefer_spot": [164, 168], "prefetch": [274, 314], "prefetch_batch": [5, 36, 273, 295], "prefil": 220, "prefix": [4, 8, 32, 53, 118, 122, 180, 192, 274, 276, 279, 314, 325, 342], "prefix_for_the_resources_ad": [118, 122], "preinstal": [177, 189], "prem": [177, 189], "premier": [73, 82], "premis": [100, 103, 184, 196], "prepar": [2, 5, 21, 36, 74, 76, 84, 88, 89, 94, 97, 162, 183, 187, 195, 204, 206, 211, 229, 232, 278, 283, 286, 290, 293, 305, 306, 314, 323, 325, 335], "prepare_data_load": [4, 32, 238, 250, 274, 276, 280, 282, 283, 289, 305, 306, 307, 309, 314, 321], "prepare_model": [4, 32, 238, 250, 274, 276, 279, 280, 282, 283, 289, 305, 306, 307, 310, 314, 321, 323, 337, 338, 340], "prepare_train": [277, 278, 329, 335], "preprocess": [4, 5, 7, 8, 9, 32, 35, 43, 51, 55, 61, 76, 77, 84, 94, 179, 191, 237, 240, 241, 244, 245, 262, 270, 273, 274, 276, 277, 278, 279, 281, 287, 292, 293, 297, 298, 305, 306, 307, 308, 320, 326, 327, 332, 336, 338, 342], "preprocess_imag": [277, 327], "preprocessed_df": [74, 89], "preprocessor": [8, 55, 74, 85, 241, 270], "preprocessor_app": [241, 270], "preprocessor_handl": [241, 270], "prerequisit": [119, 137, 150], "presenc": [275, 319], "present": [4, 7, 31, 46, 74, 88, 89, 169, 170, 212, 213, 217, 229, 233, 273, 274, 275, 276, 279, 281, 314, 315, 317, 321, 325, 338], "preserv": [273, 275, 276, 280, 282, 316, 320], "press": [11, 72], "pressur": [75, 93, 164, 168, 212, 213, 216], "pretend": [74, 86], "pretenti": [74, 85], "pretrain": [5, 35, 36, 37, 73, 81], "pretrainedconfig": [5, 35], "pretti": [74, 88, 274, 314], "prevent": [9, 61, 276, 321], "preview": [0, 111, 113, 118, 122, 126, 128, 136, 139, 230], "previou": [75, 93, 126, 131, 136, 143, 149, 159, 169, 170, 178, 190, 212, 213, 215, 216, 229, 234, 273, 274, 275, 279, 293, 294, 300, 303, 306, 315, 342], "previous": [4, 31, 273, 300], "price": [3, 28, 73, 80, 82, 237, 244, 276, 320], "priest": [73, 82], "primari": [7, 46, 163, 176], "primarili": [5, 9, 35, 63, 107, 109, 240, 241, 264, 270, 276, 323], "prime": [73, 82], "primit": [274, 307], "princip": [100, 105], "print": [2, 3, 4, 5, 6, 9, 11, 18, 20, 22, 24, 28, 31, 32, 35, 40, 41, 61, 72, 73, 74, 75, 76, 80, 83, 85, 86, 87, 88, 89, 93, 97, 98, 169, 171, 178, 179, 180, 181, 182, 186, 190, 191, 192, 193, 194, 201, 205, 208, 212, 213, 219, 221, 225, 226, 229, 232, 233, 234, 238, 239, 247, 251, 253, 257, 259, 260, 273, 274, 275, 276, 277, 278, 279, 283, 288, 304, 307, 308, 310, 311, 313, 314, 316, 317, 318, 319, 321, 323, 324, 325, 327, 329, 330, 331, 333, 335, 336, 338, 340, 342], "print_metrics_ray_train": [4, 32, 238, 249, 251, 253, 273, 283, 288, 294, 300], "printout": [274, 314], "prior": [7, 43, 178, 190, 275, 317], "priorit": [6, 41, 239, 259], "privat": [100, 103, 105, 107, 109, 111, 113, 114, 118, 123, 126, 128, 130, 136, 139, 142, 149, 152, 158, 180, 184, 192, 196], "private_subnet": [100, 105], "privileg": [100, 105, 107, 109], "pro": [229, 235], "prob": [2, 20, 275, 318], "probabilist": [276, 325], "probabl": [73, 82, 274, 306], "problem": [8, 51, 163, 175, 181, 193, 212, 213, 218, 229, 235], "proce": [5, 35, 238, 241, 247, 252, 253, 270], "process": [1, 3, 4, 5, 6, 8, 9, 10, 14, 15, 16, 17, 19, 21, 23, 28, 31, 32, 35, 36, 40, 41, 43, 48, 50, 53, 55, 57, 58, 59, 61, 62, 69, 73, 75, 76, 78, 80, 81, 82, 83, 89, 90, 94, 97, 98, 99, 100, 103, 105, 149, 150, 164, 166, 169, 171, 179, 181, 183, 187, 191, 193, 195, 203, 206, 210, 216, 218, 219, 220, 221, 222, 233, 234, 238, 239, 240, 241, 247, 248, 249, 257, 259, 262, 264, 268, 273, 274, 275, 276, 278, 279, 280, 282, 283, 285, 287, 289, 290, 292, 293, 296, 308, 315, 318, 320, 321, 325, 327, 332, 336, 337, 338, 343], "processed_d": [277, 327], "prod": [2, 21], "produc": [3, 4, 5, 7, 8, 9, 28, 31, 32, 36, 44, 53, 59, 74, 88, 238, 240, 247, 253, 263, 273, 276, 277, 279, 289, 325, 326, 342], "product": [2, 7, 11, 22, 29, 33, 38, 47, 49, 56, 72, 73, 74, 75, 78, 89, 90, 100, 103, 162, 169, 172, 179, 180, 191, 192, 212, 213, 214, 215, 217, 218, 221, 222, 223, 226, 227, 228, 229, 230, 231, 233, 235, 236, 267, 273, 274, 275, 277, 279, 289, 305, 306, 315, 317, 331, 337, 338, 339, 342], "production": [182, 186, 187, 188, 194, 200, 203, 205, 206, 207, 210, 273, 305], "profession": 162, "profil": [107, 109, 111, 113, 117, 163, 175, 274, 278, 314, 336], "profile_data": 173, "prog_bar": [5, 35, 277, 278, 328, 334], "program": [2, 24, 169, 172, 229, 235], "programm": [241, 268], "programmat": [169, 172, 182, 186, 194, 200, 202, 205, 207, 209, 241, 270], "progress": [4, 32, 181, 193, 238, 251, 273, 274, 279, 280, 288, 291, 299, 301, 302, 306, 307, 338, 340], "project": [0, 9, 10, 57, 68, 73, 74, 82, 89, 100, 102, 105, 118, 120, 121, 122, 149, 151, 152, 153, 155, 161, 173, 180, 192, 197, 199], "project_numb": [118, 122], "prometheu": 175, "promot": [276, 325], "promote_opt": [275, 317], "prompt": [11, 72, 177, 178, 189, 190, 212, 213, 215, 216, 229, 232], "promptli": [74, 88], "proof": [73, 82, 107, 109], "proper": [111, 112, 118, 119, 126, 127, 136, 138, 273, 283], "properli": [7, 11, 46, 72, 75, 76, 77, 93, 94, 163, 176, 229, 234, 274, 307], "properti": [7, 43, 229, 234], "proport": [8, 9, 53, 59, 62, 240, 263, 279, 338], "proprietari": [7, 43], "prosper": [73, 82], "protect": [275, 315], "protocol": [7, 10, 43, 48, 68, 173], "prototyp": [178, 190, 221, 223, 228], "prove": [241, 270, 274, 277, 313, 330], "provid": [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 17, 27, 30, 31, 34, 43, 46, 48, 49, 50, 56, 64, 68, 69, 71, 72, 74, 75, 76, 84, 90, 94, 98, 102, 103, 104, 105, 107, 110, 111, 113, 118, 122, 124, 128, 133, 136, 139, 149, 153, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 192, 193, 194, 195, 196, 199, 202, 203, 205, 206, 209, 210, 212, 213, 214, 216, 218, 221, 223, 224, 226, 227, 228, 229, 231, 232, 233, 234, 237, 238, 239, 240, 241, 243, 244, 245, 252, 258, 260, 261, 262, 265, 268, 270, 273, 274, 275, 276, 277, 279, 280, 281, 284, 293, 298, 303, 306, 318, 321, 323, 329, 338, 340], "provis": [4, 5, 9, 30, 34, 57, 107, 108, 109, 110, 162, 185, 198, 237, 244, 245, 274, 275, 277, 306, 315, 326], "proxi": [7, 48, 169, 172, 173], "proxim": [275, 315], "proxy_http_request": [169, 172, 173], "proxy_route_to_replica": [169, 172, 173], "prune": [275, 277, 278, 319, 331, 336], "pseudo": [277, 331], "pt": [4, 9, 10, 31, 32, 62, 66, 70, 71, 238, 240, 241, 247, 251, 253, 264, 269, 270, 272, 273, 274, 276, 277, 279, 289, 292, 300, 301, 310, 314, 323, 325, 331, 340, 342], "public": [3, 8, 9, 10, 28, 51, 54, 59, 61, 62, 64, 70, 73, 74, 78, 82, 83, 84, 85, 88, 89, 100, 103, 105, 169, 171, 221, 224, 240, 241, 263, 264, 265, 269, 270], "public_subnet": [100, 105], "publicli": [184, 196, 229, 232], "publish": [183, 195], "pull": [73, 82, 178, 190, 274, 275, 276, 277, 279, 306, 307, 308, 312, 317, 323, 327, 340], "pulocationid": [8, 51], "pumpkin": [73, 82], "pun": [74, 85], "punchestown": [73, 82], "punctuat": [74, 89], "pure": [277, 279, 326, 337], "purpl": [73, 82], "purpos": [1, 11, 13, 43, 72, 74, 85, 86, 105, 240, 264, 274, 306], "push": [73, 74, 82, 89, 178, 190, 274, 306], "pushdown": [8, 55], "put": [2, 12, 18, 73, 74, 82, 88, 89, 100, 105], "putobject": [100, 105], "pwd": [4, 31], "py": [0, 9, 10, 11, 60, 64, 71, 72, 111, 115, 118, 124, 126, 133, 136, 145, 149, 160, 163, 164, 168, 169, 171, 176, 178, 180, 181, 182, 183, 190, 192, 193, 194, 195, 202, 203, 209, 210, 212, 213, 219, 221, 224, 227, 229, 232, 233, 234], "py311": [212, 213, 219, 221, 226], "py312": [164, 168], "pyarrow": [7, 9, 43, 59, 169, 171, 274, 275, 276, 277, 278, 279, 306, 307, 316, 317, 321, 327, 333, 338], "pydant": [3, 26, 229, 233, 241, 270], "pydata": [276, 321], "pyflink": [7, 46], "pypi": [180, 192], "pyplot": [4, 6, 9, 29, 38, 56, 238, 239, 241, 247, 256, 270, 273, 274, 275, 276, 277, 278, 279, 281, 307, 316, 321, 327, 333, 338], "pyproj": [179, 191], "pyspark": [7, 46], "python": [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 19, 20, 25, 27, 32, 36, 41, 43, 45, 46, 47, 50, 68, 72, 75, 76, 77, 90, 94, 99, 111, 115, 118, 124, 126, 133, 136, 145, 149, 160, 163, 169, 171, 172, 173, 176, 178, 179, 180, 181, 182, 183, 188, 190, 191, 192, 193, 194, 195, 200, 207, 212, 213, 217, 219, 221, 224, 226, 229, 232, 234, 237, 239, 241, 243, 258, 268, 273, 274, 275, 276, 277, 278, 279, 282, 292, 307, 316, 321, 327, 333, 337, 338], "python3": [0, 180, 192], "pythonmalloc": [163, 176], "pytorch": [9, 30, 34, 38, 59, 75, 90, 95, 98, 99, 246, 249, 250, 251, 255, 273, 274, 277, 278, 279, 280, 281, 282, 283, 287, 289, 293, 294, 295, 297, 305, 306, 307, 308, 314, 320, 322, 326, 327, 329, 331, 332, 333, 335, 337, 338, 339, 340, 342, 343], "pyyaml": 0, "q": [212, 213, 216, 229, 235, 279, 338], "q2": [73, 82], "q_q": [73, 82], "qp": [169, 172], "qt": [73, 80, 82, 83], "qtr": [73, 82], "quad": [277, 278, 279, 326, 332, 337], "qualif": [229, 232], "qualit": [274, 314], "qualiti": [7, 43, 221, 223, 277, 279, 331, 342], "quantiz": [212, 213, 216, 228], "queri": [7, 43, 75, 93, 111, 113, 126, 128, 136, 139, 140, 164, 167, 168, 169, 172, 173, 181, 183, 193, 195, 221, 225, 229, 232], "question": [73, 82], "queu": [107, 109], "queue": [1, 7, 10, 13, 48, 69, 107, 109, 169, 172, 241, 268], "quick": [1, 4, 12, 26, 31, 32, 100, 103, 107, 109, 181, 193, 229, 232, 237, 242, 273, 274, 275, 277, 279, 281, 289, 290, 307, 314, 316, 317, 327, 329, 338], "quickli": [8, 11, 51, 72, 74, 89, 178, 184, 190, 196, 212, 213, 217, 274, 275, 276, 277, 307, 312, 316, 323, 327], "quickstart": [111, 112, 118, 120, 126, 127, 136, 138, 149, 151], "quit": [73, 74, 82, 89], "quot": [73, 82], "quota": [100, 102, 212, 213, 217], "qwen": [229, 233, 234, 235], "qwen2": [229, 233], "qwen3": [229, 234], "r": [0, 7, 11, 43, 72, 73, 74, 77, 82, 85, 118, 125, 149, 161, 173, 238, 247, 274, 275, 276, 278, 279, 306, 307, 315, 317, 320, 332, 337, 338], "r1": [229, 235], "r2": [7, 43], "race": [73, 74, 82, 85, 88], "radio": [73, 82], "rafe": [73, 82], "ragnarok": [73, 82], "rahul": [73, 82], "rai": [13, 14, 15, 18, 19, 20, 21, 22, 23, 35, 40, 46, 51, 52, 53, 54, 59, 60, 61, 63, 64, 65, 70, 71, 79, 80, 81, 82, 91, 92, 95, 97, 98, 100, 102, 103, 105, 108, 110, 117, 125, 132, 133, 134, 135, 144, 145, 147, 148, 162, 167, 168, 174, 175, 177, 178, 179, 180, 182, 183, 185, 187, 189, 190, 191, 192, 194, 195, 198, 199, 200, 203, 204, 206, 207, 210, 211, 217, 220, 226, 227, 228, 231, 233, 234, 236, 247, 251, 252, 253, 257, 263, 264, 265, 269, 281, 282, 284, 285, 286, 287, 288, 290, 291, 300, 301, 302, 303, 304, 307, 310, 311, 312, 313, 319, 324, 328, 331, 333, 336, 339, 341, 342], "railwai": [74, 89], "rais": [2, 20, 274, 277, 278, 314, 329, 331, 335], "ram": [9, 61], "ramen": [274, 306], "rammstein": [73, 82], "rand": [2, 10, 18, 22, 70, 169, 172, 241, 269, 270], "randint": [2, 4, 5, 6, 24, 31, 32, 35, 41, 164, 168, 229, 234, 237, 239, 244, 245, 258, 259, 273, 277, 278, 281, 292, 328, 333], "randn": [277, 278, 331, 333, 336], "randn_lik": [5, 35, 277, 328], "random": [1, 2, 4, 6, 8, 9, 10, 12, 17, 18, 20, 22, 24, 31, 32, 41, 54, 64, 70, 118, 124, 136, 145, 164, 168, 169, 172, 181, 193, 229, 234, 237, 239, 240, 241, 244, 245, 259, 265, 269, 270, 273, 274, 275, 277, 278, 279, 281, 292, 306, 307, 316, 327, 331, 332, 333, 336, 338, 342], "random_shuffl": [8, 9, 54, 64, 240, 265, 275, 277, 278, 316, 327, 333], "random_st": [3, 28, 274, 275, 308, 316], "randomize_block_ord": [8, 9, 54, 64, 240, 265, 279, 338], "randomli": [4, 8, 9, 31, 32, 54, 64, 240, 265, 273, 279, 281, 342], "rang": [1, 2, 4, 6, 9, 16, 18, 20, 24, 31, 32, 40, 41, 61, 74, 75, 76, 86, 90, 97, 100, 105, 164, 168, 181, 193, 237, 238, 239, 241, 244, 247, 249, 253, 257, 259, 260, 269, 270, 273, 274, 275, 276, 277, 278, 279, 281, 283, 292, 294, 297, 300, 308, 310, 318, 320, 321, 323, 331, 333, 336, 340], "rank": [3, 4, 5, 28, 32, 36, 229, 232, 238, 251, 274, 275, 276, 277, 280, 282, 283, 288, 290, 301, 305, 309, 310, 317, 318, 319, 323, 329, 340, 342], "rap": [73, 82], "rapid": [178, 190], "rapidli": [177, 189], "rate": [5, 6, 35, 41, 73, 74, 76, 82, 85, 97, 98, 107, 110, 169, 172, 212, 213, 216, 239, 259, 273, 274, 276, 277, 278, 283, 314, 325, 331, 336, 339, 340, 342], "rather": [9, 60, 74, 85, 89, 212, 213, 215, 275, 276, 279, 316, 320, 342], "ratings_d": [279, 338], "ratings_parquet": [279, 338], "ratings_parquet_uri": [279, 338], "ratio": [173, 275, 316, 318], "rattl": [74, 89], "rattler": [73, 82], "ravenstein": [74, 89], "raw": [7, 43, 73, 76, 82, 97, 136, 140, 164, 167, 168, 237, 238, 239, 244, 252, 258, 260, 273, 274, 275, 276, 277, 279, 296, 297, 306, 307, 314, 315, 316, 318, 321, 327, 331, 338, 342], "raw_path": [279, 338], "ray_actor_opt": [241, 269, 270], "ray_data_synthet": [164, 168], "ray_dedup_log": [76, 99, 239, 260], "ray_enable_windows_or_osx_clust": [163, 176], "ray_pl_ckpt": [277, 278, 329, 335], "ray_result": [76, 99, 239, 258], "ray_scheduler_ev": [238, 239, 241, 252, 258, 269], "ray_train_v2_en": [274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338], "rayddp": [277, 329], "rayddpstrategi": [5, 33, 36, 277, 278, 329, 335], "raylightningenviron": [5, 33, 36, 277, 278, 327, 329, 333, 335], "rayproject": [179, 191], "rayserv": [75, 92], "raytaskerror": [2, 20], "raytrainreportcallback": [5, 33, 36, 275, 277, 278, 315, 316, 317, 318, 319, 326, 329, 332, 335, 336], "raytrainwork": [76, 99, 238, 252], "raytrainxgboosttrain": [3, 26, 28], "rayturbo": [9, 10, 57, 68], "rbac": [185, 198, 199], "rbi": [73, 80, 82], "rd": [73, 82, 100, 105, 275, 316], "rdata": [274, 276, 314, 321, 325], "re": [2, 6, 9, 10, 11, 20, 40, 64, 71, 72, 73, 74, 82, 85, 111, 114, 116, 118, 123, 125, 126, 129, 130, 136, 141, 142, 149, 158, 161, 162, 178, 181, 182, 183, 190, 193, 194, 195, 221, 225, 226, 229, 236, 240, 265, 273, 274, 275, 276, 277, 278, 279, 286, 287, 292, 304, 305, 306, 307, 315, 316, 319, 325, 326, 327, 331, 332, 336, 337, 342], "reach": [74, 88, 107, 109, 212, 213, 215, 273, 303], "read": [1, 2, 4, 5, 7, 10, 16, 19, 23, 24, 31, 32, 37, 43, 51, 53, 56, 59, 71, 74, 88, 89, 100, 105, 185, 198, 237, 238, 244, 245, 254, 263, 273, 274, 275, 276, 277, 279, 281, 292, 293, 296, 306, 307, 308, 314, 316, 321, 327, 338], "read_csv": [4, 8, 31, 51, 238, 247, 276, 279, 321, 338, 342], "read_databricks_t": [9, 59], "read_imag": [9, 59, 61, 64, 240, 241, 263, 265, 270], "read_json": [8, 51], "read_parquet": [3, 5, 8, 9, 28, 35, 51, 54, 59, 169, 171, 237, 244, 245, 273, 274, 275, 277, 279, 296, 314, 316, 327, 338], "read_row_group": [274, 308], "read_tabl": [274, 276, 308, 321], "readabl": [8, 51, 273, 289], "readfil": [9, 63], "readi": [2, 11, 24, 72, 73, 76, 82, 97, 111, 114, 118, 123, 126, 129, 130, 136, 141, 142, 149, 158, 162, 177, 178, 183, 187, 189, 190, 195, 203, 206, 210, 212, 213, 220, 221, 223, 228, 229, 232, 233, 236, 273, 274, 276, 278, 279, 287, 297, 305, 306, 308, 321, 333, 336, 337, 338], "readm": [77, 169, 172, 343], "ready_ref": [2, 24], "real": [7, 46, 73, 74, 76, 82, 86, 97, 212, 213, 215, 229, 234, 236, 274, 275, 276, 279, 306, 319, 325, 332, 336, 337, 342], "realist": [74, 89, 274, 277, 279, 314, 326, 338], "realiti": [74, 85], "realknowncaus": [276, 321], "realli": [73, 74, 82, 85, 88, 89], "reason": [164, 166, 180, 192, 212, 213, 216, 218, 221, 223, 229, 235, 278, 336], "reasoning_pars": [229, 234], "reassur": [274, 308], "rebuild": [0, 273, 279, 292, 342], "rec": [274, 276, 277, 307, 321, 327, 343], "rec_sys_tutori": [279, 338, 340, 342], "recal": [241, 269], "recalcul": [212, 213, 216], "recap": [6, 41, 239, 258], "receiv": [4, 5, 7, 32, 36, 48, 75, 90, 107, 109, 229, 232, 234, 273, 276, 279, 280, 321, 337, 338, 340], "recent": [74, 89, 180, 192, 273, 274, 276, 277, 278, 279, 291, 303, 310, 312, 325, 329, 335, 341], "recent_kei": [180, 192], "recent_nam": [180, 192], "recip": [4, 31, 73, 82], "recipi": 173, "reclaim": [277, 279, 331, 342], "recommend": [0, 3, 4, 5, 6, 7, 8, 9, 10, 26, 29, 32, 33, 35, 38, 46, 49, 52, 56, 66, 67, 71, 100, 105, 107, 109, 169, 170, 179, 191, 212, 213, 214, 221, 222, 230, 232, 233, 273, 281, 289, 305, 339], "recomput": [279, 342], "record": [3, 8, 28, 51, 163, 175, 237, 244, 245, 274, 276, 277, 307, 312, 321, 327], "recov": [9, 57, 273, 275, 276, 277, 299, 319, 320, 325, 326], "recoveri": [273, 274, 276, 277, 278, 279, 289, 299, 301, 305, 306, 314, 323, 326, 336, 337, 340], "recreat": [241, 270, 276, 325], "recurr": [276, 320], "recurs": [111, 116, 126, 134, 136, 147, 273, 304], "red": [73, 74, 82, 89, 212, 213, 216, 274, 277, 306, 326], "redefin": [3, 28], "redeploi": [278, 332], "redi": [100, 104], "redshift": [7, 43], "reduc": [3, 6, 7, 8, 9, 10, 26, 38, 43, 49, 56, 60, 61, 67, 73, 76, 83, 98, 179, 191, 212, 213, 214, 229, 231, 233, 276, 325], "reduct": [221, 227], "redund": [212, 213, 216, 273, 274, 289, 314], "ref": [1, 2, 15, 16, 18, 19, 22, 24, 25], "refer": [1, 2, 5, 8, 9, 13, 15, 18, 19, 20, 22, 35, 51, 61, 65, 76, 96, 100, 105, 111, 112, 118, 120, 126, 127, 136, 137, 138, 149, 150, 151, 163, 169, 172, 175, 176, 178, 180, 185, 190, 192, 198, 212, 213, 215, 238, 240, 250, 263, 266, 273, 291, 292], "reflect": [7, 43, 274, 312], "refresh": [74, 88], "reg": [3, 28], "regard": [73, 82], "regardless": [4, 5, 32, 35, 276, 323], "region": [100, 106, 111, 113, 118, 121, 122, 126, 128, 129, 131, 136, 139, 141, 143, 149, 152, 153, 155, 159, 180, 192, 229, 232, 237, 238, 239, 244, 245, 252, 258, 260], "regist": [75, 92, 103, 105, 107, 109, 112, 113, 116, 117, 119, 122, 124, 127, 128, 132, 135, 137, 139, 144, 148, 150, 153, 163, 169, 170, 175, 179, 185, 188, 191, 198, 274, 277, 279, 314, 331, 342], "register_buff": [276, 322], "register_us": 173, "registr": [118, 122, 125, 149, 161, 162], "registration_complet": 173, "registri": [274, 314], "regress": [6, 41, 239, 259, 275, 319], "regular": [1, 14, 181, 193, 273, 274, 279, 282, 308, 314, 342], "reimplement": [273, 294], "reinforc": [7, 46], "rel": [0, 4, 32, 276, 325], "rel_path": [229, 232], "relat": [126, 134, 136, 140, 147, 173, 181, 193], "relationship": [6, 41, 100, 102, 105, 239, 259], "releas": [11, 72, 73, 74, 82, 85, 111, 113, 126, 128, 134, 136, 139, 147, 273, 278, 292, 336], "relev": [4, 8, 9, 32, 54, 55, 64, 229, 232, 240, 265, 274, 276, 279, 308, 323, 342], "reli": [7, 10, 43, 46, 48, 68, 273, 274, 275, 293, 307, 316], "reliabl": [4, 5, 7, 9, 30, 34, 46, 57, 182, 183, 186, 187, 194, 195, 200, 203, 205, 206, 207, 210, 229, 231, 233, 236, 273, 274, 280, 299, 305, 310], "religi": [73, 82], "religion": [73, 82], "reload": [10, 71, 229, 232, 273, 279, 292, 301, 302, 342], "relpath": [229, 232], "relu": [238, 247, 253, 277, 278, 328, 334], "remain": [73, 80, 82, 83, 164, 168, 221, 223, 273, 276, 277, 279, 282, 289, 323, 327, 338], "remaind": [279, 338], "remark": [73, 82], "remast": [73, 82], "remateri": [275, 316], "rememb": [74, 89, 181, 182, 183, 193, 194, 195, 221, 227, 273, 277, 289, 331], "remind": [73, 74, 82, 89], "remot": [2, 3, 6, 9, 10, 11, 12, 16, 18, 19, 20, 21, 22, 23, 24, 25, 28, 40, 59, 70, 72, 173, 178, 181, 182, 186, 190, 193, 194, 201, 205, 208, 240, 241, 263, 269, 270, 273, 274, 275, 292, 314, 315, 318, 319], "remote_add": [1, 2, 14, 15, 19, 22, 23], "remote_funct": [1, 15], "remote_path": [9, 10, 62, 70], "remov": [2, 5, 8, 11, 25, 35, 53, 72, 126, 134, 136, 147, 212, 213, 218, 273, 274, 279, 281, 289, 304, 314, 342], "remove_code_output": 0, "remu": [73, 80, 82, 83], "renam": [178, 190, 275, 276, 316, 321], "renew": [107, 110], "rent": [74, 85], "repackag": [278, 332], "repartit": [8, 54, 73, 78, 80, 83, 164, 168], "repeat": [76, 99, 238, 239, 252, 260, 275, 276, 318, 322], "repeatedli": [73, 81], "replac": [73, 74, 82, 89, 111, 112, 113, 114, 118, 119, 121, 123, 124, 126, 127, 128, 130, 131, 136, 137, 139, 140, 142, 143, 145, 149, 150, 152, 155, 158, 159, 164, 168, 178, 180, 183, 190, 192, 195, 212, 213, 216, 218, 229, 232, 273, 274, 276, 277, 278, 279, 293, 294, 297, 314, 325, 331, 332, 336, 342], "replic": [2, 4, 5, 18, 32, 36, 273, 280, 289], "replica": [7, 48, 68, 75, 92, 93, 169, 172, 173, 183, 195, 212, 213, 218, 219, 224, 229, 232, 241, 268, 270, 273, 274, 280, 314], "replica_handle_request": [169, 172, 173], "repo": [0, 126, 129, 131, 136, 141, 143, 149, 156, 159, 212, 213, 219, 229, 232], "repo_id": [229, 232], "report": [5, 6, 7, 31, 36, 40, 41, 44, 73, 82, 237, 239, 241, 244, 245, 246, 249, 253, 257, 258, 259, 260, 270, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 291, 294, 298, 300, 301, 305, 306, 310, 312, 314, 317, 320, 323, 325, 326, 328, 329, 331, 332, 333, 337, 338, 340], "report_metrics_torch": [4, 31, 238, 247], "reportedli": [73, 82], "repositori": [0, 149, 156, 183, 195], "repres": [3, 6, 8, 28, 39, 51, 74, 89, 212, 213, 217, 237, 244, 279, 337, 339], "represent": [212, 213, 215, 217], "reproduc": [11, 72, 77, 274, 275, 277, 279, 308, 315, 316, 327, 338], "republican": [73, 82], "req": [178, 190], "request": [1, 3, 7, 9, 10, 16, 17, 23, 26, 28, 48, 62, 67, 68, 69, 70, 90, 92, 107, 109, 110, 169, 172, 212, 213, 215, 216, 218, 224, 227, 229, 232, 233, 234, 237, 241, 244, 245, 268, 269, 270, 273, 274, 276, 292, 314, 321, 325], "request_data": [237, 244, 245], "requir": [0, 2, 4, 5, 7, 8, 9, 10, 22, 23, 29, 30, 33, 34, 43, 46, 54, 63, 64, 68, 69, 73, 76, 77, 82, 97, 98, 100, 102, 103, 104, 105, 107, 109, 110, 120, 122, 129, 135, 141, 148, 151, 157, 161, 162, 173, 179, 180, 182, 185, 186, 188, 191, 192, 194, 198, 200, 205, 207, 216, 218, 221, 223, 224, 225, 228, 233, 234, 240, 264, 265, 273, 274, 276, 277, 278, 279, 280, 286, 293, 301, 306, 310, 312, 321, 326, 332, 338], "rerun": [275, 319], "res18": [273, 298], "resampl": 320, "rescal": [277, 331], "research": [73, 76, 78, 98, 221, 223, 228, 229, 233], "reserv": [1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 17, 22, 26, 29, 33, 38, 43, 49, 56, 73, 74, 75, 76, 78, 84, 90, 94, 111, 112, 118, 119, 126, 127, 136, 137, 149, 150, 163, 164, 165, 169, 170, 174, 212, 213, 214, 221, 222, 229, 230], "reset": [238, 247, 249, 253, 273, 278, 283, 333], "reshap": [5, 9, 35, 57], "resid": [276, 325], "residu": [276, 320], "resili": [183, 187, 195, 203, 206, 210, 221, 226, 273, 275, 276, 278, 299, 301, 305, 315, 320, 332], "resiz": 306, "resnet": [238, 247, 253, 274, 277, 280, 281, 286, 292, 306, 331], "resnet18": [4, 6, 29, 31, 32, 38, 40, 41, 238, 239, 247, 252, 253, 257, 260, 273, 274, 281, 282, 289, 291, 307, 310, 314], "resolut": [5, 35, 36], "resolv": [4, 32, 136, 146], "resourc": [4, 5, 6, 7, 10, 17, 23, 25, 32, 36, 41, 47, 48, 57, 68, 69, 73, 74, 76, 78, 87, 94, 98, 99, 101, 102, 105, 106, 107, 108, 109, 110, 116, 117, 119, 120, 125, 133, 134, 135, 140, 145, 146, 147, 148, 151, 161, 162, 163, 169, 171, 175, 177, 179, 181, 182, 183, 185, 187, 189, 191, 193, 194, 195, 198, 199, 204, 206, 211, 215, 216, 223, 235, 237, 238, 239, 241, 244, 245, 252, 258, 259, 260, 268, 270, 272, 273, 274, 276, 278, 282, 285, 290, 292, 298, 301, 303, 306, 320, 332, 336], "resources_per_work": [76, 98, 275, 317], "resp": [169, 172], "respect": [74, 88, 89], "respond": [229, 232], "respons": [10, 70, 75, 90, 93, 107, 110, 169, 172, 173, 179, 180, 191, 192, 212, 213, 215, 217, 219, 221, 225, 226, 229, 231, 232, 233, 234, 241, 268, 269, 270], "response_format": [229, 233], "rest": [74, 75, 89, 90, 126, 134, 136, 147, 273, 274, 279, 282, 294, 300, 307, 338], "restart": [11, 72, 75, 93, 179, 180, 191, 192, 229, 232, 273, 275, 278, 279, 281, 299, 301, 302, 315, 335, 336, 340], "restor": [74, 89, 277, 279, 291, 299, 300, 301, 305, 326, 331, 341, 342], "restored_train": [273, 303], "restrict": [184, 196], "result": [3, 6, 9, 11, 12, 16, 17, 18, 19, 25, 28, 41, 57, 72, 73, 74, 75, 76, 78, 88, 89, 92, 93, 97, 98, 99, 111, 115, 118, 124, 126, 133, 136, 145, 149, 160, 178, 181, 182, 186, 190, 193, 194, 201, 205, 208, 229, 234, 237, 239, 244, 245, 246, 252, 258, 259, 260, 274, 275, 277, 278, 279, 280, 281, 282, 289, 290, 292, 303, 305, 307, 311, 312, 313, 314, 317, 319, 323, 324, 329, 330, 332, 335, 338, 340, 341, 342], "resum": [4, 5, 30, 34, 274, 275, 278, 280, 289, 299, 300, 301, 302, 305, 306, 310, 313, 315, 317, 319, 323, 325, 326, 329, 331, 332, 335, 337, 340, 342], "resume_from_checkpoint": [279, 341], "retain": [164, 167, 181, 193, 275, 276, 317, 323], "retent": [74, 89, 274, 306], "rethink": [74, 89], "retrain": [274, 275, 276, 278, 279, 314, 319, 325, 336, 337], "retri": [4, 5, 9, 17, 30, 34, 61, 186, 200, 205, 207, 274, 275, 277, 278, 279, 280, 299, 302, 303, 305, 306, 310, 311, 317, 329, 332, 337, 340], "retriev": [1, 2, 15, 25, 273, 275, 279, 292, 295, 317, 340], "retry_except": [2, 20], "return": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 28, 31, 32, 35, 39, 41, 52, 61, 62, 64, 70, 71, 72, 73, 74, 75, 76, 81, 82, 89, 92, 96, 97, 164, 168, 169, 171, 178, 181, 182, 186, 190, 193, 194, 201, 205, 208, 229, 233, 234, 237, 238, 239, 240, 241, 244, 245, 247, 250, 251, 253, 256, 258, 264, 265, 269, 270, 273, 274, 275, 276, 277, 278, 279, 282, 283, 286, 287, 288, 291, 292, 295, 297, 303, 308, 309, 314, 316, 317, 318, 319, 321, 322, 325, 327, 328, 331, 333, 334, 336, 337, 338, 339], "reus": [2, 9, 18, 62, 73, 81, 100, 105, 212, 213, 216, 240, 264, 273, 275, 276, 281, 292, 316, 318, 319, 325], "reusabl": [276, 320], "reveal": [279, 338], "reveng": [74, 88], "revers": [274, 275, 276, 314, 318, 321], "review": [9, 59, 74, 76, 84, 87, 88, 89, 94, 99, 111, 113, 118, 122, 126, 128, 136, 139, 149, 153, 180, 192], "rewrit": [4, 5, 30, 34, 273, 274, 279, 280, 306, 342], "rf": [3, 4, 5, 6, 8, 9, 28, 32, 37, 42, 53, 55, 66, 237, 238, 240, 241, 244, 245, 254, 266, 272], "rg_idx": [274, 308], "rg_meta": [274, 308], "rgb": [273, 274, 277, 282, 292, 306, 308, 314, 326, 327], "rice": [274, 306], "rich": [7, 46, 178, 190], "richer": [278, 336], "rick": [73, 82], "ricki": [73, 82], "ride": [3, 28, 74, 88, 237, 244, 276, 320, 325], "ridicul": [74, 89], "ridlei": [74, 89], "rifl": [74, 88], "riget": [74, 89], "right": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 16, 17, 26, 29, 32, 33, 38, 43, 49, 56, 60, 70, 73, 74, 75, 76, 78, 82, 84, 88, 89, 90, 94, 111, 112, 118, 119, 126, 127, 136, 137, 149, 150, 163, 164, 165, 169, 170, 174, 178, 190, 212, 213, 214, 216, 221, 222, 229, 230, 235, 236, 238, 241, 250, 269, 273, 278, 282, 289, 295, 332], "rightarrow": [275, 315], "rigid": [7, 45], "rip": [74, 88], "rise": [274, 312], "risibl": [74, 85], "risk": [74, 89], "riskbr": [74, 89], "river": [73, 74, 82, 89], "riverboat": [74, 88], "rkn": [164, 168], "rllib": [7, 46], "rm": [3, 4, 5, 6, 8, 9, 10, 28, 32, 37, 42, 53, 55, 66, 71, 111, 116, 118, 125, 126, 134, 136, 147, 149, 161, 237, 238, 240, 241, 244, 245, 254, 266, 272], "rmse": [3, 6, 28, 41, 239, 258, 259, 279, 342], "rmtree": [273, 274, 275, 276, 277, 278, 279, 304, 314, 319, 325, 331, 336, 338, 342], "road": [275, 315], "roadmap": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 17, 26, 29, 32, 33, 38, 43, 49, 56, 67, 169, 170, 212, 213, 214, 221, 222, 229, 230, 237, 238, 239, 240, 241, 242, 246, 255, 261, 267, 273, 282], "roar": [74, 88], "robert": [74, 88], "robin": [73, 82, 107, 110], "robot": [278, 336], "robust": [0, 7, 43, 229, 233, 273, 274, 276, 278, 302, 305, 306, 325, 332], "rock": [73, 80, 82], "role": [74, 89, 104, 106, 107, 109, 110, 111, 113, 117, 118, 120, 126, 128, 139, 146, 149, 151, 164, 167, 180, 192, 212, 213, 219, 221, 225, 226, 229, 232, 233, 234], "roll": [73, 82, 169, 172, 183, 187, 195, 203, 206, 210, 221, 226, 278, 333], "rollin": [73, 82], "rollout": [10, 68, 169, 172, 278, 332], "roma": [73, 82], "roman": [74, 88], "ronda": [74, 88], "roof": [73, 82], "root": [1, 4, 5, 6, 16, 30, 31, 32, 34, 39, 41, 163, 169, 172, 173, 176, 180, 192, 229, 232, 238, 239, 247, 250, 256, 260, 273, 279, 280, 281, 287, 342], "roughli": [274, 277, 278, 307, 327, 333], "round": [73, 74, 82, 89, 107, 110, 275, 279, 315, 317, 319, 338], "rout": [2, 10, 25, 68, 75, 92, 100, 105, 107, 109, 110, 173, 212, 213, 217, 241, 270], "route_prefix": [3, 28, 183, 187, 195, 204, 206, 211, 229, 233, 241, 270], "row": [4, 5, 31, 32, 35, 51, 59, 61, 73, 74, 78, 80, 82, 83, 84, 85, 86, 87, 88, 89, 169, 171, 263, 273, 274, 275, 276, 277, 279, 281, 291, 292, 296, 297, 306, 308, 312, 314, 315, 316, 318, 321, 325, 327, 329, 337, 338, 340, 342], "row_group": [274, 308], "row_group_idx": [274, 308], "row_group_map": [274, 308], "royal": [73, 82], "rpc": [2, 7, 25, 43], "rstrip": [180, 192], "rubbish": [74, 89], "rubbl": [74, 88, 89], "rube": [74, 88], "ruin": [74, 89], "rule": [7, 48, 73, 82, 100, 105, 179, 191, 229, 232], "rumor": [73, 82], "run": [0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 17, 19, 21, 25, 26, 28, 29, 33, 35, 37, 38, 40, 41, 42, 47, 49, 50, 51, 53, 55, 56, 57, 60, 62, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 78, 81, 82, 84, 88, 89, 90, 92, 93, 94, 97, 98, 99, 100, 102, 104, 108, 110, 111, 113, 114, 115, 116, 117, 123, 124, 125, 126, 128, 129, 130, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 144, 145, 147, 148, 149, 152, 158, 160, 161, 163, 164, 166, 167, 168, 170, 172, 173, 175, 176, 177, 178, 179, 180, 181, 183, 185, 188, 189, 190, 191, 192, 193, 195, 198, 202, 203, 209, 210, 212, 213, 214, 217, 219, 222, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 238, 239, 240, 241, 248, 252, 253, 254, 257, 258, 259, 260, 262, 269, 270, 275, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287, 289, 290, 291, 293, 297, 298, 299, 302, 303, 304, 305, 306, 308, 310, 311, 312, 313, 315, 317, 319, 320, 323, 324, 326, 327, 329, 330, 331, 332, 335, 336, 337, 340, 341, 342], "run_command": [178, 190], "run_config": [3, 4, 5, 28, 32, 36, 237, 238, 244, 245, 252, 253, 273, 274, 275, 276, 277, 278, 279, 289, 290, 298, 301, 303, 311, 317, 323, 329, 335, 340], "runawai": [73, 82], "runconfig": [3, 4, 5, 26, 28, 29, 32, 36, 237, 238, 244, 245, 252, 274, 275, 276, 277, 278, 279, 280, 281, 298, 301, 303, 305, 306, 307, 310, 311, 312, 314, 315, 316, 317, 320, 321, 323, 327, 329, 333, 335, 338, 340], "runnabl": [169, 170], "runnng": [181, 193], "runtim": [10, 17, 69, 75, 76, 92, 95, 169, 172, 177, 179, 189, 191, 229, 231, 232, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338, 343], "runtime_env": [2, 21, 22, 76, 95, 221, 224, 227, 229, 232, 233, 234], "runtimeenv": [76, 95], "runwai": [8, 55, 240, 266], "ruse": [74, 88], "rust": [7, 43], "ruth": [74, 88], "rw": [238, 247], "ryan": [73, 82], "s3": [3, 7, 8, 9, 10, 28, 43, 51, 53, 54, 59, 61, 62, 64, 70, 104, 106, 107, 109, 111, 113, 116, 117, 126, 128, 134, 136, 139, 140, 146, 147, 169, 171, 180, 192, 229, 232, 237, 238, 239, 240, 241, 244, 245, 252, 258, 260, 263, 264, 265, 269, 270, 273, 274, 277, 279, 289, 305, 314, 331, 342], "s3_bucket_id": [100, 106, 111, 113], "s3_f": [169, 171], "s3_kei": [180, 192, 229, 232], "s3_path": [5, 35], "s3f": [5, 33, 35], "s3filesystem": [5, 35, 169, 171], "s5": [212, 213, 216], "s6": [73, 82, 212, 213, 216], "s7": [212, 213, 216], "s_": [278, 332], "s_k": [278, 332], "saatchi": [73, 82], "sacrif": [278, 332], "safe": [73, 82, 111, 113, 126, 128, 136, 139, 187, 203, 206, 210, 273, 274, 276, 277, 281, 299, 310, 323, 327], "safetensor": [229, 232], "safeti": [229, 233], "sai": [2, 19, 73, 74, 82, 88, 89, 100, 105, 182, 186, 194, 202, 205, 209], "said": [73, 74, 82, 85], "sake": [8, 52], "salad": [274, 306], "sam": [73, 82], "same": [2, 3, 4, 5, 6, 8, 10, 11, 21, 28, 32, 35, 36, 41, 52, 54, 69, 70, 72, 73, 74, 82, 83, 85, 89, 107, 109, 111, 113, 126, 128, 136, 139, 180, 182, 192, 194, 200, 207, 212, 213, 219, 221, 226, 228, 229, 236, 237, 238, 239, 241, 244, 245, 251, 260, 269, 273, 274, 275, 276, 277, 278, 279, 282, 285, 286, 287, 289, 294, 297, 298, 300, 303, 305, 308, 312, 314, 317, 319, 325, 326, 327, 329, 336, 342], "sampl": [3, 4, 5, 6, 28, 32, 35, 41, 74, 75, 85, 89, 92, 100, 105, 111, 113, 117, 118, 122, 126, 128, 129, 136, 139, 141, 149, 153, 156, 163, 175, 180, 192, 237, 239, 240, 241, 244, 258, 259, 264, 270, 274, 275, 276, 279, 280, 283, 284, 292, 306, 307, 308, 314, 316, 321, 327, 333, 342], "sample_act": [278, 336], "sample_batch": [237, 244, 245, 275, 318], "sample_count": [181, 193], "sample_idx": [4, 31, 32, 273, 281], "sample_imag": [277, 331], "sample_s": [5, 35], "sampler": [4, 5, 32, 36, 238, 249, 253, 273, 274, 280, 283, 294, 310], "samsara": [241, 271], "samsung": [73, 82], "san": [73, 82, 229, 234], "sander": [73, 82], "saniti": [273, 275, 278, 279, 281, 290, 316, 329, 335, 338], "sat": [73, 82], "satisfi": [2, 18], "satur": [164, 168], "saturdai": [73, 82], "save": [2, 4, 5, 22, 31, 32, 36, 74, 88, 136, 146, 179, 180, 181, 191, 192, 193, 212, 213, 217, 229, 232, 249, 251, 252, 253, 274, 275, 276, 277, 278, 279, 280, 282, 283, 291, 294, 298, 299, 300, 303, 305, 306, 307, 310, 314, 315, 317, 319, 320, 323, 326, 327, 328, 331, 332, 335, 336, 340, 341, 342], "save_checkpoint_and_metrics_ray_train": [4, 32, 238, 249, 251, 253, 273, 283, 289, 294], "save_checkpoint_and_metrics_ray_train_with_extra_st": [273, 300, 301], "save_checkpoint_and_metrics_torch": [4, 31, 238, 247], "save_hyperparamet": [5, 35], "save_last": [277, 278, 329, 335], "save_model": [3, 28], "save_top_k": [277, 278, 329, 335], "saw": [73, 82], "sayhellodebuglog": [169, 172], "sayhellodefaultlog": [169, 172], "scaffold": [10, 71], "scala": [7, 46], "scalabl": [3, 7, 8, 10, 27, 43, 46, 47, 50, 67, 69, 73, 74, 75, 76, 78, 83, 84, 89, 90, 98, 99, 100, 105, 180, 182, 183, 185, 186, 187, 188, 192, 194, 195, 199, 200, 203, 205, 206, 207, 210, 215, 220, 221, 228, 229, 236, 237, 241, 243, 267, 273, 274, 275, 276, 277, 278, 279, 293, 305, 306, 308, 315, 318, 319, 320, 325, 326, 336, 337, 338], "scalar": [2, 25, 277, 278, 328, 332, 334], "scale": [1, 2, 6, 7, 8, 9, 10, 11, 13, 22, 30, 34, 35, 41, 43, 46, 48, 51, 55, 61, 62, 69, 72, 74, 76, 78, 83, 84, 90, 93, 94, 98, 100, 105, 107, 108, 109, 110, 126, 133, 136, 145, 146, 162, 164, 168, 179, 181, 183, 185, 187, 188, 191, 193, 195, 199, 203, 206, 210, 212, 213, 214, 217, 218, 223, 224, 226, 229, 235, 237, 239, 240, 241, 244, 248, 258, 266, 268, 270, 274, 275, 276, 277, 279, 280, 281, 282, 287, 290, 293, 297, 299, 301, 303, 305, 306, 307, 308, 314, 315, 316, 319, 320, 321, 325, 326, 327, 331, 333, 337, 338, 339, 342], "scaling_config": [3, 4, 5, 28, 32, 36, 76, 98, 237, 238, 244, 245, 249, 252, 253, 273, 274, 275, 276, 277, 278, 279, 285, 290, 298, 301, 303, 311, 317, 323, 329, 335, 340], "scalingconfig": [3, 4, 5, 28, 29, 32, 36, 76, 95, 98, 237, 238, 244, 245, 249, 274, 275, 276, 277, 278, 279, 280, 281, 305, 307, 311, 315, 316, 317, 320, 321, 323, 326, 327, 329, 332, 333, 335, 337, 338, 340], "scan": [74, 88], "scari": [74, 89], "scenario": [2, 7, 22, 24, 43, 76, 97, 100, 105, 111, 113, 126, 128, 136, 139, 221, 223, 228, 241, 268, 274, 306], "scene": [74, 85, 89, 279, 337], "schedul": [1, 2, 3, 4, 5, 6, 8, 9, 13, 15, 16, 22, 23, 25, 28, 31, 35, 41, 53, 61, 62, 73, 76, 82, 99, 107, 109, 181, 193, 212, 213, 218, 237, 239, 244, 245, 252, 258, 259, 260, 274, 275, 276, 277, 278, 279, 306, 314, 319, 325, 331, 336, 342], "schema": [7, 8, 43, 51, 73, 74, 80, 83, 86, 87, 229, 233, 234, 276, 277, 279, 321, 327, 338], "schemat": [4, 5, 32, 36, 273, 280], "schlong": [74, 85], "school": [73, 82], "schumer": [73, 82], "scienc": [7, 46], "scientif": [7, 46, 229, 235, 275, 278, 316, 333], "scikit": [75, 90, 275, 316], "scipt": [169, 172], "scope": [100, 102, 164, 167, 180, 185, 192, 198], "score": [73, 75, 82, 93, 229, 232, 275, 276, 277, 279, 317, 319, 321, 323, 329, 331, 337, 338, 342], "scoreless": [73, 80, 82], "scott": [74, 89], "scotu": [73, 82], "scratch": [6, 41, 111, 113, 126, 128, 136, 139, 239, 259, 274, 277, 278, 306, 314, 329, 335], "screen": [74, 89, 177, 178, 184, 189, 190, 196], "script": [0, 164, 168, 169, 172, 178, 181, 183, 185, 190, 193, 195, 198, 229, 232, 274, 275, 277, 278, 306, 315, 331, 336], "scroll": [136, 146], "scrumptiou": [111, 113, 126, 128, 136, 139], "sdk": [118, 120, 149, 151, 152, 182, 194], "sea": [73, 82], "seaborn": [275, 316], "seal": [74, 89], "seamless": [7, 43, 46, 47, 74, 84, 180, 192, 212, 213, 218, 229, 234, 276, 278, 324, 325, 332, 336], "seamlessli": [7, 8, 11, 47, 55, 72, 73, 78, 188, 273, 274, 275, 276, 277, 278, 279, 294, 301, 306, 315, 319, 320, 321, 326, 332, 342], "search": [3, 6, 28, 38, 41, 100, 105, 181, 193, 237, 239, 244, 245, 258, 259, 260, 274, 275, 276, 277, 278, 314, 319, 325, 331, 336], "search_alg": [6, 41, 239, 259, 260], "season": [276, 321], "seattl": [74, 88], "second": [0, 1, 2, 6, 16, 19, 41, 73, 82, 84, 136, 140, 149, 152, 169, 171, 172, 221, 224, 237, 239, 244, 260, 274, 275, 306, 313, 319], "secondarili": [9, 63, 240, 264], "secret": [73, 82, 100, 105, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338], "section": [0, 6, 40, 100, 103, 104, 107, 109, 163, 169, 171, 172, 175, 176, 178, 179, 181, 190, 191, 193, 239, 257, 273, 281], "secur": [102, 104, 106, 107, 109, 111, 113, 117, 118, 122, 136, 146, 162, 185, 199, 212, 213, 217, 218, 221, 226, 229, 236], "security_group_descript": [100, 105], "security_group_id": [100, 106, 111, 113], "security_group_nam": [100, 105], "securitygroup": [100, 104, 105], "sedan": [229, 233], "see": [1, 2, 4, 5, 6, 8, 9, 10, 11, 16, 19, 25, 32, 35, 36, 40, 42, 53, 54, 55, 64, 66, 71, 72, 73, 74, 75, 76, 77, 80, 82, 85, 88, 89, 92, 93, 99, 100, 105, 107, 108, 111, 113, 115, 118, 122, 124, 126, 128, 133, 136, 139, 145, 146, 149, 152, 160, 163, 164, 168, 169, 171, 172, 176, 180, 183, 184, 185, 192, 195, 196, 198, 212, 213, 217, 219, 221, 224, 226, 229, 232, 233, 234, 238, 239, 240, 241, 249, 250, 251, 253, 257, 260, 265, 266, 270, 271, 273, 274, 276, 277, 278, 283, 285, 290, 291, 299, 306, 323, 325, 326, 331, 332], "seed": [274, 277, 278, 279, 308, 331, 333, 338], "seek": [277, 327], "seem": [74, 89], "seen": [73, 74, 82, 85, 89, 221, 228], "segment": [100, 105, 240, 264, 276, 325], "seiz": [74, 85], "select": [4, 11, 31, 32, 72, 76, 94, 97, 111, 113, 126, 128, 136, 139, 169, 171, 177, 178, 183, 184, 189, 190, 195, 196, 221, 227, 234, 236, 273, 274, 275, 276, 279, 281, 312, 317, 325, 342], "select_column": [3, 28], "selector": [184, 196], "self": [2, 3, 5, 9, 10, 25, 28, 35, 62, 70, 73, 75, 81, 92, 169, 172, 237, 240, 241, 244, 245, 264, 269, 270, 273, 274, 275, 276, 277, 278, 279, 292, 306, 308, 314, 318, 320, 321, 322, 325, 328, 329, 334, 335, 339], "sell": [74, 88], "semant": [2, 23, 25], "semi": [7, 43], "send": [2, 25, 90, 92, 164, 168, 169, 172, 173, 229, 233, 234, 237, 241, 244, 245, 268, 270], "send_welcome_email": 173, "sens": [3, 6, 26, 41, 74, 88, 89, 237, 239, 242, 259], "sent": [7, 48, 74, 88, 169, 172, 173, 273, 292], "sentenc": [73, 78, 79, 81, 82, 229, 232, 235, 273, 285], "sentence_transform": [73, 79], "sentencetransform": [73, 78, 79, 81, 82, 83], "sentiment": [73, 75, 80, 90, 92, 93], "sep": [279, 338, 342], "separ": [1, 2, 4, 6, 7, 10, 15, 23, 32, 41, 43, 69, 163, 176, 180, 185, 192, 199, 239, 259, 273, 275, 279, 287, 316, 337], "sept": [73, 82], "sequenc": [8, 51, 74, 76, 89, 94, 95, 97, 99, 212, 213, 216, 275, 315, 321, 322], "sequenti": [6, 40, 212, 213, 215, 238, 239, 247, 253, 257, 276, 277, 278, 320, 328, 334], "sequoia": [163, 176], "seri": [162, 321, 325, 343], "serial": [1, 7, 13, 43, 46, 274, 275, 306, 317], "serializ": [277, 327], "series_id": [276, 321], "serious": [74, 85, 89], "serv": [46, 47, 70, 71, 77, 91, 92, 100, 102, 103, 111, 112, 126, 127, 136, 137, 163, 164, 167, 170, 176, 183, 184, 187, 195, 196, 203, 204, 206, 210, 211, 216, 220, 223, 226, 227, 228, 231, 233, 234, 236, 237, 244, 245, 269, 272, 273, 274, 275, 276, 277, 278, 279, 292, 305, 314, 319, 325, 331, 336, 338, 339, 342], "serve_llama": [212, 213, 219], "serve_llama_3_1_70b": [221, 224, 225, 226, 227], "serve_my_lora_app": [229, 232], "serve_my_qwen": [229, 233], "serve_my_qwen3": [229, 234], "server": [0, 163, 176, 212], "serverless": [100, 103], "servic": [6, 7, 42, 48, 67, 68, 75, 77, 90, 100, 104, 105, 107, 109, 110, 111, 113, 118, 119, 121, 122, 126, 128, 136, 139, 146, 149, 152, 153, 156, 161, 169, 172, 173, 179, 180, 181, 184, 185, 191, 192, 193, 196, 198, 212, 213, 216, 219, 222, 225, 227, 228, 267, 270, 343], "session": [173, 180, 185, 192, 198], "session_2024": [237, 238, 244, 245, 252], "session_2025": [76, 99], "session_latest": [163, 169, 172, 173, 176], "set": [0, 1, 2, 4, 5, 6, 9, 10, 16, 21, 22, 30, 32, 34, 35, 36, 39, 41, 61, 70, 71, 73, 74, 75, 76, 82, 87, 88, 89, 92, 94, 97, 98, 99, 100, 101, 104, 107, 110, 111, 113, 117, 118, 121, 122, 126, 128, 129, 131, 135, 136, 139, 141, 143, 148, 149, 152, 153, 159, 169, 172, 174, 177, 179, 183, 185, 187, 189, 191, 195, 197, 204, 206, 211, 222, 227, 228, 230, 232, 233, 237, 238, 241, 244, 245, 252, 255, 256, 258, 260, 268, 269, 270, 273, 274, 275, 276, 277, 279, 280, 281, 282, 285, 287, 290, 292, 299, 306, 308, 317, 320, 323, 327, 338, 340], "set_epoch": [4, 32, 238, 249, 253, 273, 274, 283, 294, 310], "set_float32_matmul_precis": [278, 336], "set_grad_en": [76, 97, 274, 276, 314, 325], "set_index": [276, 321], "set_titl": [6, 39, 238, 239, 247, 253, 256, 274, 277, 307, 327], "seth": [73, 82], "setup": [5, 7, 35, 36, 46, 77, 94, 99, 100, 101, 103, 107, 108, 109, 126, 134, 136, 147, 162, 169, 170, 172, 175, 179, 184, 188, 191, 196, 198, 199, 221, 223, 227, 228, 229, 234, 237, 244, 245, 273, 282, 290, 298, 300, 301, 303, 307, 316, 319, 321, 329, 332, 338, 340, 343], "seven": [212, 213, 216], "sever": [7, 46, 74, 75, 88, 90, 100, 104, 181, 193, 212, 213, 216, 217, 218], "sevigni": [74, 85], "sex": [73, 74, 82, 85], "sexist": [73, 82], "sg": [100, 105, 111, 113], "sgd": [76, 97], "sh": [11, 72, 126, 127, 136, 138, 149, 151, 277, 327], "shallow": [275, 315], "shame": [74, 89], "shape": [2, 5, 8, 9, 22, 35, 53, 61, 73, 82, 83, 107, 109, 164, 168, 179, 180, 191, 192, 240, 241, 264, 270, 273, 274, 275, 276, 277, 278, 292, 308, 316, 321, 322, 325, 328, 333, 336], "shard": [3, 4, 5, 28, 32, 36, 212, 213, 217, 238, 249, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 285, 287, 289, 293, 294, 295, 296, 298, 301, 314, 315, 317, 319, 320, 321, 326, 327, 329, 331, 332, 333, 335, 336, 337, 338, 340, 342], "shard_0": [274, 307, 308], "share": [0, 2, 4, 7, 8, 10, 18, 31, 43, 53, 68, 73, 82, 100, 104, 105, 107, 109, 111, 117, 118, 119, 136, 146, 163, 176, 184, 188, 196, 229, 232, 236, 273, 274, 275, 276, 278, 281, 289, 304, 306, 315, 316, 320, 321, 325, 328, 334], "shared_path": [180, 192], "shared_storag": [180, 192], "sharetea": [73, 82], "she": [73, 74, 82, 85, 88, 89], "sheeran": [73, 82], "shell": [9, 10, 62, 70, 149, 152], "sheriff": [74, 88], "shift": [178, 182, 183, 186, 187, 190, 194, 195, 202, 203, 205, 206, 209, 210, 275, 276, 316, 320, 323], "shine": [73, 74, 82, 88, 89], "shippuden": [73, 82], "shit": [73, 82], "shock": [74, 85], "shoe": [74, 89], "shoot": [73, 74, 82, 88], "shootout": [74, 88], "short": [74, 85, 275, 276, 317, 320], "shorter": [221, 227], "shot": [74, 85, 89], "should": [2, 4, 5, 8, 9, 10, 11, 24, 25, 32, 36, 50, 54, 63, 70, 72, 73, 74, 75, 82, 85, 88, 93, 107, 109, 136, 146, 169, 171, 183, 187, 195, 204, 206, 211, 229, 232, 238, 249, 273, 274, 275, 277, 278, 279, 282, 285, 314, 319, 331, 336, 342], "should_checkpoint": [238, 253], "show": [2, 3, 5, 6, 9, 22, 28, 35, 36, 41, 60, 62, 63, 73, 74, 77, 78, 80, 82, 83, 84, 85, 86, 87, 88, 89, 111, 116, 118, 119, 126, 134, 136, 147, 169, 170, 173, 181, 193, 212, 213, 216, 218, 229, 231, 239, 241, 259, 270, 273, 274, 275, 276, 277, 278, 279, 280, 281, 289, 290, 291, 292, 294, 306, 307, 312, 314, 316, 318, 321, 323, 325, 327, 329, 331, 332, 335, 338, 340], "showcas": [73, 74, 75, 77, 82, 85, 89, 90, 229, 230], "shown": [74, 85, 169, 172, 273, 281], "shuffl": [4, 5, 6, 31, 32, 35, 36, 39, 41, 49, 56, 76, 95, 97, 238, 239, 247, 250, 256, 260, 261, 273, 274, 275, 276, 278, 279, 283, 287, 293, 294, 295, 305, 308, 309, 310, 319, 321, 323, 333, 338], "shut": [73, 74, 76, 78, 84, 94, 227, 273, 289], "shutdown": [90, 94, 221, 225, 227, 229, 232, 233, 234, 237, 241, 244, 245, 270, 272], "shutil": [273, 274, 275, 276, 277, 278, 279, 281, 304, 307, 314, 316, 319, 321, 325, 327, 331, 333, 336, 338, 342], "sick": [73, 82], "sid": [100, 105], "side": [7, 47, 74, 88, 89, 274, 277, 279, 314, 331, 342], "sidebar": 0, "sidecar": [212, 213, 217], "sidestep": [277, 326], "sidewalk": [74, 88], "sight": [74, 85], "sign": [9, 10, 62, 70, 73, 82, 163, 169, 170, 175], "signal": [164, 166, 276, 321], "signatur": [5, 6, 36, 41, 239, 258, 259], "signifi": [8, 51], "signific": [4, 5, 7, 30, 34, 47, 73, 78, 212, 213, 217, 273, 280], "significantli": [164, 167, 179, 191], "signup": 188, "silicon": [11, 72, 73, 76, 78, 82, 94, 97], "silu": [5, 35], "sim": [277, 278, 326, 332], "similar": [3, 28, 107, 110, 163, 169, 171, 176, 221, 224, 225, 237, 244, 245, 277, 331], "similarli": [2, 7, 25, 44, 75, 92, 279, 338], "simpl": [1, 3, 4, 5, 6, 7, 8, 10, 13, 14, 28, 31, 35, 40, 41, 48, 50, 52, 70, 74, 75, 77, 88, 89, 90, 92, 170, 178, 180, 183, 184, 190, 192, 195, 196, 212, 213, 216, 219, 220, 221, 223, 229, 235, 237, 239, 241, 244, 257, 259, 269, 270, 273, 274, 277, 279, 281, 284, 290, 292, 306, 314, 326, 331, 337, 338, 339, 342], "simple_pipelin": [169, 171], "simpler": [221, 228, 229, 232], "simpli": [4, 31, 75, 90, 274, 275, 313, 319], "simplifi": [118, 122, 177, 188, 189, 240, 264, 274, 306], "simul": [2, 20, 90, 173, 274, 278, 279, 306, 332, 336, 338], "simultan": [164, 168, 212, 213, 215], "sin": [276, 278, 322, 332, 333, 336], "sinc": [2, 8, 22, 51, 73, 74, 82, 89, 164, 168, 169, 172, 180, 192, 212, 213, 216, 273, 277, 279, 282, 289, 327, 341], "sing": [73, 82], "singl": [2, 7, 8, 9, 11, 22, 29, 32, 33, 36, 46, 51, 60, 72, 73, 74, 82, 84, 89, 169, 172, 212, 213, 215, 216, 217, 221, 223, 228, 229, 231, 232, 236, 237, 241, 244, 245, 248, 270, 273, 274, 275, 276, 277, 278, 279, 280, 281, 283, 292, 304, 306, 307, 308, 315, 316, 319, 320, 326, 327, 335, 337], "single_gpu_mnist": [238, 247, 254], "sink": [3, 9, 28, 58], "sinusoid": [276, 322], "sisterlif": [73, 82], "sit": [73, 74, 80, 82, 85], "site": [0, 74, 85, 100, 105, 180, 192], "situat": [74, 89], "six": [212, 213, 216], "size": [4, 6, 8, 9, 32, 39, 52, 57, 61, 73, 74, 76, 80, 82, 88, 97, 98, 164, 168, 169, 172, 177, 179, 181, 189, 191, 193, 212, 213, 216, 217, 220, 225, 227, 229, 232, 238, 239, 249, 256, 273, 274, 276, 277, 278, 279, 281, 282, 283, 284, 307, 317, 321, 322, 325, 328, 329, 331, 333, 336, 342], "size_in_byt": [2, 18], "sj": [73, 80, 82, 83], "skagwai": [74, 88], "skew": [9, 57, 275, 279, 316, 338], "skill": [229, 232], "skip": [274, 275, 277, 314, 315, 316, 327], "sklearn": [3, 26, 275, 316], "skylynn": [73, 82], "slack": [169, 172], "sleazi": [74, 88], "sleep": [1, 2, 16, 19, 24, 74, 89, 169, 171], "slice": [3, 28, 274, 275, 277, 279, 306, 308, 315, 317, 326, 338], "slick": [74, 89], "slide": [73, 82, 320], "slim": [179, 191], "slip": [74, 88], "slo": [212, 213, 216], "slope": [275, 315], "slot": [273, 276, 281, 320], "slow": [10, 68, 73, 78, 221, 223], "slow_adjust_total_amount": [169, 171], "slower": [74, 87], "slowest": [8, 9, 54, 64, 240, 265], "slowli": [74, 89], "slowyourrol": [73, 82], "sm": [73, 82], "small": [7, 8, 9, 43, 51, 52, 60, 64, 181, 193, 212, 213, 217, 221, 223, 228, 229, 232, 235, 240, 241, 264, 265, 270, 274, 276, 277, 278, 279, 306, 325, 326, 336, 338, 342], "small_siz": [76, 97], "small_unet_model_config": [5, 35], "smaller": [0, 2, 22, 100, 105, 164, 168, 229, 232, 235], "smallest": [177, 189], "smart": [74, 88, 212, 213, 218], "smith": [73, 80, 82, 83], "smoke": [273, 305], "smoothl1": [276, 323], "smoothl1loss": [276, 323], "smoothli": [74, 84], "sn": [275, 316, 318], "snake": [73, 82], "snap": [73, 80, 82], "snapshot": [180, 192, 275, 277, 319, 330], "snapshot_download": [229, 232], "snicker": [74, 89], "snippet": [4, 5, 32, 36, 178, 180, 190, 192, 238, 253], "snowflak": [7, 9, 43, 59], "so": [0, 5, 6, 8, 35, 41, 52, 53, 73, 74, 75, 80, 82, 85, 88, 89, 90, 163, 164, 168, 175, 177, 178, 179, 181, 183, 187, 189, 190, 191, 193, 195, 204, 206, 211, 229, 235, 238, 239, 251, 258, 260, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 287, 288, 289, 290, 292, 293, 294, 296, 297, 299, 300, 301, 303, 306, 308, 314, 316, 317, 320, 321, 323, 328, 334, 338], "socket": [7, 46], "softbal": [73, 82], "softmax": [275, 315], "softprob": [275, 317], "softwar": [163, 175, 221, 225, 229, 232], "soil": [275, 315, 318], "sole": [74, 89, 100, 105], "solid": [279, 342], "solut": [1, 2, 4, 5, 6, 7, 9, 10, 16, 25, 30, 32, 34, 36, 41, 47, 57, 61, 68, 76, 98, 100, 104, 212, 213, 214, 218, 220, 229, 236, 238, 239, 240, 253, 259, 262, 273, 276, 280, 293, 325], "solv": [212, 213, 217, 229, 235], "some": [3, 4, 5, 6, 7, 8, 9, 28, 32, 37, 42, 45, 46, 54, 55, 59, 60, 64, 73, 74, 82, 85, 86, 87, 88, 89, 136, 146, 164, 167, 168, 169, 170, 180, 181, 192, 193, 212, 213, 216, 221, 225, 229, 231, 233, 238, 240, 254, 263, 265], "someth": [2, 19, 73, 74, 82, 88, 126, 130, 136, 142, 149, 158, 274, 306], "sometim": [2, 22, 74, 89, 111, 113, 126, 128, 136, 139], "somewher": [2, 25], "song": [73, 82], "sonnet": [229, 235], "soon": [2, 24, 73, 82], "sophist": [6, 7, 41, 47, 229, 231, 234, 236, 239, 259], "sorri": [73, 80, 82, 83], "sort": [7, 8, 43, 54, 74, 85, 180, 192, 274, 275, 279, 314, 318, 337, 338, 342], "sort_index": [274, 276, 279, 312, 323, 340], "soul": [73, 80, 82], "sound": [73, 74, 82, 89], "soup": [73, 82], "sourc": [0, 1, 9, 11, 13, 59, 72, 74, 86, 88, 89, 100, 105, 149, 152, 163, 164, 167, 176, 188, 273, 293, 305], "south": [73, 82], "sox": [73, 82], "space": [2, 3, 6, 7, 24, 28, 41, 44, 74, 88, 100, 105, 239, 258, 259, 273, 274, 275, 276, 277, 278, 279, 304, 314, 319, 325, 331, 336, 337, 342], "span": [73, 82, 173, 276, 321], "spark": [8, 46, 55], "spatial": [275, 315], "spawn": [273, 290, 292], "speak": [74, 89], "speci": [275, 319], "special": [7, 8, 43, 53, 73, 74, 80, 82, 88, 221, 228, 229, 231, 232, 235, 236], "specif": [2, 5, 8, 12, 13, 22, 25, 35, 36, 54, 64, 107, 110, 118, 119, 122, 149, 153, 163, 164, 166, 169, 172, 175, 176, 179, 180, 181, 184, 191, 192, 193, 196, 212, 213, 218, 221, 224, 229, 235, 240, 265, 273, 274, 275, 277, 290, 292, 308, 318, 319, 326, 331], "specifi": [2, 4, 5, 6, 8, 9, 10, 11, 20, 22, 32, 36, 41, 51, 61, 62, 69, 71, 72, 75, 76, 92, 94, 97, 98, 99, 107, 109, 173, 177, 178, 179, 183, 184, 187, 189, 190, 191, 195, 196, 204, 206, 211, 229, 232, 233, 238, 239, 241, 249, 252, 258, 259, 270, 273, 279, 286, 298, 303, 340], "specific": [273, 282], "speed": [3, 4, 5, 6, 7, 10, 28, 30, 34, 40, 43, 68, 74, 84, 89, 180, 192, 229, 235, 239, 241, 257, 270, 273, 274, 277, 280, 307, 314, 327], "speedup": [73, 78], "speific": [181, 193], "spend": [74, 85], "spike": [212, 213, 217, 218, 221, 226], "spiki": [177, 179, 189, 191], "spill": [9, 63, 240, 264], "spillov": [181, 193], "spin": [9, 62, 73, 78, 81, 181, 183, 193, 195, 203, 210, 240, 264, 275, 315], "split": [3, 9, 28, 61, 64, 73, 74, 80, 82, 85, 89, 180, 192, 212, 213, 217, 221, 224, 237, 240, 244, 245, 265, 273, 276, 280, 281, 283, 284, 294, 307, 314, 317, 318, 321, 326], "split_at_indic": [277, 278, 327, 333], "split_idx": [278, 333], "split_notebook": 0, "split_proportion": [279, 338], "spoil": [74, 88, 89], "spoiler": [74, 89], "spoken": [74, 88], "spot": [9, 57, 73, 82, 179, 181, 191, 193, 275, 318], "spotifi": [6, 8, 42, 55], "sprai": [74, 89], "sprang": [74, 89], "spruce": [275, 315], "spur": [74, 88], "spy": [163, 176], "sql": [7, 43, 47], "sqrt": [1, 6, 16, 41, 239, 258, 259, 276, 322], "sqrt_add": [1, 16], "squad": [73, 82], "squar": [1, 2, 16, 19, 73, 82, 273, 277, 279, 281, 326, 337, 342], "square_ref": [2, 19], "square_ref_1": [2, 23], "square_ref_2": [2, 23], "square_valu": [2, 19], "squarederror": [3, 28], "squeez": [6, 39, 239, 256, 276, 277, 278, 279, 322, 323, 331, 336, 342], "src": [276, 322], "ssh": [100, 103, 105, 274, 306], "ssl": [107, 110], "sso": [212, 213, 218], "sst": [75, 92], "st": [73, 82, 100, 105], "stabil": [276, 277, 279, 321, 331, 340], "stabilityai": [5, 35, 36], "stabl": [4, 32, 33, 36, 37, 212, 213, 219, 238, 254], "stablediffus": [5, 35, 36], "stack": [5, 35, 111, 115, 118, 122, 124, 126, 128, 133, 136, 139, 145, 149, 153, 160, 164, 167, 168, 188, 274, 275, 276, 278, 314, 316, 320, 325, 333], "stadium": [73, 82], "stage": [2, 4, 5, 7, 9, 22, 31, 35, 45, 57, 61, 62, 73, 82, 169, 171, 273, 281, 289], "stagnant": [276, 323], "stai": [73, 82, 273, 277, 280, 294, 326], "stakehold": [276, 325], "standalon": [273, 282], "standard": [5, 7, 35, 36, 43, 74, 85, 118, 122, 169, 172, 179, 182, 186, 191, 194, 200, 205, 207, 273, 274, 275, 276, 277, 278, 279, 281, 283, 287, 289, 297, 306, 307, 308, 316, 321, 322, 327, 333, 338], "stander": [74, 89], "stapl": [74, 85], "star": [73, 74, 82, 89, 278, 279, 332, 338], "stare": [74, 85], "starlett": [10, 67, 237, 241, 244, 245, 269], "start": [2, 3, 4, 5, 8, 9, 10, 11, 17, 21, 22, 26, 28, 29, 31, 32, 33, 35, 36, 38, 49, 56, 59, 61, 62, 67, 72, 73, 74, 75, 78, 84, 85, 88, 90, 93, 94, 97, 98, 100, 103, 111, 112, 115, 118, 119, 124, 126, 127, 133, 136, 137, 138, 145, 149, 150, 152, 160, 164, 168, 169, 170, 171, 172, 175, 178, 179, 180, 181, 182, 186, 190, 191, 192, 193, 194, 200, 205, 207, 214, 216, 220, 221, 222, 225, 229, 230, 232, 235, 237, 238, 241, 242, 244, 247, 252, 257, 270, 273, 274, 276, 277, 278, 279, 280, 281, 282, 285, 286, 290, 298, 299, 300, 302, 306, 307, 313, 321, 323, 326, 331, 332, 336, 338], "start_epoch": [273, 274, 276, 279, 300, 310, 323, 340], "start_token": [276, 323], "starter": [183, 195], "startswith": [274, 279, 314, 342], "startup": [2, 21, 107, 109, 163, 176, 221, 228], "starv": [74, 89], "state": [2, 3, 7, 10, 25, 28, 46, 47, 56, 68, 69, 73, 74, 81, 82, 85, 89, 100, 105, 111, 113, 126, 128, 136, 139, 164, 166, 169, 171, 178, 190, 212, 213, 218, 229, 234, 237, 244, 245, 274, 275, 276, 277, 279, 299, 300, 303, 305, 314, 318, 320, 325, 331, 333, 334, 336, 341], "state_dict": [4, 31, 32, 238, 247, 251, 253, 273, 274, 276, 277, 278, 279, 289, 292, 301, 310, 314, 323, 325, 331, 336, 340, 342], "stateless": [1, 9, 14, 62, 277, 327], "statement": [4, 31, 100, 105], "static": [7, 9, 48, 57, 212, 213, 216], "station": [74, 89], "statist": [8, 55, 164, 166], "stats_d": [275, 318, 319], "statu": [73, 76, 82, 99, 111, 115, 118, 124, 126, 133, 136, 145, 149, 160, 164, 168, 169, 171, 173, 182, 183, 194, 195, 202, 209, 237, 238, 239, 241, 244, 245, 252, 258, 260, 269], "std": [8, 9, 54, 64, 240, 265, 273, 274, 276, 292, 308, 314, 321, 325], "stderr": [169, 172], "steadi": [277, 329], "steadili": [273, 276, 291, 323], "steak": [274, 306], "steal": [74, 88], "steam": [74, 85], "step": [1, 2, 4, 5, 6, 7, 9, 12, 14, 16, 17, 31, 32, 35, 40, 41, 48, 58, 61, 73, 74, 76, 82, 84, 97, 111, 112, 113, 118, 120, 122, 125, 126, 127, 128, 130, 131, 136, 137, 138, 139, 142, 143, 149, 150, 151, 154, 158, 159, 161, 164, 168, 169, 170, 183, 187, 188, 195, 204, 206, 211, 216, 230, 235, 238, 239, 247, 249, 250, 253, 257, 258, 259, 260, 280, 282, 283, 284, 287, 291, 292, 294, 296, 297, 300, 306, 310, 315, 316, 320, 321, 323, 326, 332, 333, 337, 338, 340], "step_size_hour": [276, 325], "sterl": [73, 82], "steven": [73, 82], "stewart": [74, 88], "still": [2, 4, 7, 19, 32, 47, 73, 74, 75, 82, 88, 93, 178, 190, 238, 251, 273, 274, 276, 277, 289, 294, 295, 301, 303, 307, 323, 327], "stillkidrauhl": [73, 82], "stockholm": [74, 85], "stop": [6, 11, 41, 72, 74, 75, 89, 93, 212, 213, 215, 239, 259, 273, 274, 275, 276, 277, 292, 314, 319, 325, 331], "storag": [5, 7, 8, 9, 31, 35, 36, 43, 46, 50, 52, 53, 59, 65, 100, 104, 105, 111, 117, 118, 119, 121, 122, 149, 152, 153, 164, 167, 168, 177, 188, 189, 229, 232, 240, 241, 251, 262, 263, 264, 266, 269, 274, 276, 278, 280, 281, 282, 287, 290, 291, 298, 299, 301, 303, 305, 306, 307, 314, 320, 321, 336, 338, 343], "storage_fold": [3, 4, 8, 9, 10, 28, 31, 32, 53, 55, 62, 65, 66, 70, 71], "storage_path": [3, 4, 5, 28, 32, 35, 36, 237, 238, 244, 245, 252, 273, 274, 275, 276, 277, 278, 279, 280, 289, 298, 301, 303, 310, 311, 317, 323, 329, 335, 340, 341], "store": [5, 7, 9, 10, 24, 25, 35, 36, 43, 47, 48, 63, 64, 68, 73, 74, 78, 82, 83, 85, 89, 100, 104, 163, 169, 171, 176, 181, 193, 237, 240, 244, 245, 264, 265, 273, 274, 275, 276, 277, 278, 279, 281, 282, 289, 290, 291, 296, 297, 298, 301, 305, 306, 307, 310, 312, 315, 316, 319, 321, 323, 326, 329, 331, 336, 338, 340, 342], "stori": [74, 85, 89], "storylin": [74, 88, 89], "str": [3, 4, 5, 6, 9, 10, 28, 31, 32, 35, 41, 61, 62, 64, 70, 73, 75, 81, 92, 164, 168, 229, 233, 234, 238, 239, 240, 241, 247, 251, 258, 264, 265, 269, 270, 273, 274, 275, 276, 289, 301, 308, 309, 314, 317, 325], "strang": [74, 89], "stranger": [74, 88], "strategi": [5, 36, 183, 187, 195, 203, 206, 210, 228, 277, 278, 327, 329, 335], "stratifi": [275, 316], "streak": [73, 80, 82], "stream": [3, 8, 9, 10, 28, 48, 55, 57, 68, 73, 80, 82, 83, 169, 172, 212, 213, 217, 219, 221, 225, 226, 229, 232, 240, 241, 262, 264, 268, 273, 275, 276, 277, 278, 279, 293, 294, 295, 297, 298, 305, 315, 316, 317, 321, 325, 326, 332, 336, 337, 338, 340, 342], "streaming_split": [9, 60], "streamlin": [100, 104, 107, 108, 182, 186, 194, 202, 205, 209], "street": [74, 88], "strength": [229, 232], "stretch": [73, 82], "strftime": [4, 31, 238, 247], "strict": [277, 278, 331, 336], "stride": [4, 6, 31, 40, 41, 238, 239, 247, 253, 257, 260, 273, 276, 282, 321], "strike": [74, 88, 89], "string": [73, 74, 80, 83, 86, 87, 89, 118, 122, 126, 128, 131, 136, 139, 143, 149, 159, 181, 193, 229, 233, 234, 274, 307], "strip": [274, 276, 279, 314, 325, 342], "strong": [164, 168, 212, 213, 217, 221, 223, 276, 279, 321, 339], "stronger": [221, 223], "structur": [9, 47, 55, 59, 74, 85, 169, 172, 197, 199, 213, 220, 230, 231, 232, 234, 236, 237, 244, 245, 276, 277, 278, 279, 322, 323, 326, 332, 336, 337, 343], "stuck": [10, 68, 164, 168], "student": [74, 85], "studi": [4, 5, 32, 37, 74, 85, 238, 254], "studio": [73, 74, 82, 88, 178, 190], "stuff": [73, 82], "stun": [74, 88, 89], "stupid": [74, 89], "style": [0, 5, 35, 74, 89, 273, 274, 276, 277, 279, 295, 305, 306, 320, 331, 338, 340], "sub": [2, 23], "subdirectori": 0, "subfold": [5, 35], "subject": [229, 235], "submiss": [6, 41, 182, 186, 194, 200, 205, 207, 239, 259], "submit": [6, 40, 73, 82, 111, 115, 118, 124, 126, 133, 136, 145, 149, 160, 164, 168, 178, 181, 190, 193, 239, 257], "subnet": [103, 106, 111, 113, 117, 118, 119, 122, 149, 153, 162], "subnet_id": [100, 106, 111, 113], "suboptim": [9, 57], "subplot": [6, 39, 238, 239, 247, 253, 256, 273, 274, 277, 279, 281, 292, 307, 327, 331, 338], "subprocess": [9, 10, 56, 62, 67, 70, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338], "subraman": [73, 82], "subsampl": [275, 319], "subsequ": [74, 89, 163, 169, 172, 175, 176, 212, 213, 217], "subset": [4, 6, 8, 9, 32, 41, 52, 59, 60, 76, 97, 184, 196, 240, 264, 274, 275, 276, 277, 279, 306, 307, 316, 321, 326, 327, 338], "substanc": [74, 89], "substanti": [276, 323], "subtract": [277, 331], "subword": [212, 213, 215], "success": [6, 41, 173, 239, 259], "successfulli": [111, 114, 117, 118, 123, 126, 130, 135, 136, 142, 148, 149, 158, 179, 182, 191, 194, 221, 228, 229, 232, 278, 336], "suck": [73, 82], "sudo": [163, 176], "suffer": [10, 68], "suffici": [221, 225, 277, 327], "suffix": [1, 15], "suggest": [9, 62, 229, 232, 275, 318], "suit": [7, 43, 273, 279, 280, 339], "suitabl": [0, 74, 76, 89, 94, 277, 279, 331, 338], "sum": [2, 8, 9, 18, 51, 54, 60, 64, 74, 85, 181, 193, 240, 265, 275, 279, 318, 319, 339, 340], "sum_ref": [2, 19], "sum_valu": [2, 19], "summar": [73, 83, 212, 213, 216, 221, 228, 229, 232, 235, 236], "summari": [78, 94, 222, 229, 232, 236], "summer": [73, 74, 80, 82, 89], "summerslam": [73, 80, 82], "summit": [8, 9, 55, 66, 238, 240, 252, 253, 266], "sun": [73, 74, 82, 88, 89], "sunbeam": [111, 113], "sunda": [73, 82], "sundai": [73, 80, 82], "super": [5, 35, 73, 77, 82, 276, 277, 278, 279, 322, 328, 334, 339], "superior": [74, 89], "supervis": [274, 276, 278, 306, 321, 333], "suppli": [74, 88, 278, 336], "support": [2, 4, 5, 7, 8, 9, 10, 11, 22, 25, 30, 31, 34, 43, 46, 47, 48, 55, 57, 59, 64, 65, 68, 71, 72, 74, 75, 76, 84, 90, 98, 105, 107, 109, 110, 163, 164, 167, 169, 170, 172, 175, 188, 212, 221, 227, 229, 232, 233, 236, 240, 265, 266, 273, 274, 275, 278, 279, 280, 281, 295, 300, 301, 303, 310, 316, 319, 332, 341, 342], "suppos": [73, 82], "suptitl": [274, 277, 307, 327, 331], "sur": [73, 82], "sure": [0, 4, 32, 73, 80, 82, 83, 149, 161, 178, 190, 229, 232, 273, 274, 277, 279, 304, 308, 331, 342], "surfac": [169, 172], "surg": [276, 325], "surpris": [74, 89], "surprisingli": [2, 25, 73, 82], "surround": [74, 85], "surviv": [73, 80, 82, 83, 278, 336], "sushi": [274, 306], "suspens": [74, 89], "suv": [229, 233], "swai": [74, 89], "swap": [179, 191, 274, 277, 278, 314, 331, 336], "swede": [74, 85], "swedish": [74, 85], "sweep": [274, 275, 276, 277, 278, 279, 314, 319, 325, 331, 336, 342], "swing": [73, 74, 82, 85, 278, 332], "switch": [229, 231, 232, 273, 279, 305, 342], "switcher": 0, "sy": [1, 2, 12, 17, 18, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 333, 338, 343], "sydow": [74, 89], "symbol": [11, 72, 278, 332], "sync": [73, 82, 178, 190, 273, 274, 280, 306, 310], "sync_dist": [5, 35, 277, 278, 328, 334], "sync_on_comput": [274, 310], "synchron": [2, 4, 18, 32, 238, 251, 273, 274, 277, 280, 285, 286, 289, 301, 310, 329], "synthet": [164, 168, 278, 336], "synthetic_image_output": [164, 168], "system": [1, 2, 5, 7, 8, 9, 13, 20, 35, 43, 46, 53, 61, 66, 100, 105, 107, 109, 126, 129, 134, 136, 141, 146, 147, 149, 152, 163, 164, 166, 167, 169, 172, 175, 176, 180, 183, 187, 188, 192, 195, 203, 206, 210, 229, 231, 232, 233, 234, 237, 239, 244, 245, 258, 260, 273, 274, 276, 305, 306, 324, 342], "t": [1, 2, 4, 7, 8, 9, 11, 16, 19, 20, 21, 31, 32, 47, 52, 54, 61, 64, 72, 73, 74, 80, 82, 85, 88, 89, 107, 109, 136, 146, 169, 171, 180, 188, 192, 212, 213, 217, 238, 240, 250, 264, 265, 273, 274, 276, 277, 278, 279, 280, 281, 285, 287, 292, 306, 307, 308, 309, 314, 320, 321, 322, 323, 325, 326, 327, 328, 331, 332, 334, 336, 338, 342], "t10k": [238, 239, 252, 260], "t4": [9, 62, 149, 154, 179, 191, 237, 238, 239, 244, 245, 252, 258, 260], "t_": [278, 332], "t_futur": [276, 325], "t_img": [277, 328], "t_past": [276, 325], "t_scale": [277, 328], "tab": [169, 171, 177, 178, 179, 180, 182, 183, 189, 190, 191, 192, 194, 195], "tabl": [6, 7, 9, 41, 43, 59, 111, 113, 126, 128, 136, 139, 274, 275, 276, 307, 308, 317, 321], "tabular": [8, 51, 240, 263, 273, 274, 279, 296, 307, 316, 319, 337, 342, 343], "tackl": [276, 320], "tag": [7, 47, 111, 113, 126, 128, 136, 139, 275, 279, 319, 342], "tail": [279, 338], "tailor": [273, 282], "take": [2, 3, 4, 5, 6, 8, 9, 10, 25, 28, 30, 31, 32, 34, 36, 40, 41, 51, 52, 60, 61, 70, 73, 74, 76, 82, 85, 88, 89, 96, 100, 106, 111, 113, 118, 122, 126, 128, 136, 139, 149, 153, 156, 169, 171, 229, 231, 237, 238, 239, 241, 244, 245, 248, 253, 257, 269, 281, 282, 289, 307, 316, 321, 327, 339], "take_al": [279, 338], "take_batch": [8, 9, 52, 60, 61, 62, 73, 82, 237, 240, 241, 244, 245, 264, 269, 270, 275, 316], "takeawai": 214, "taken": [74, 85, 274, 314], "talent": [73, 74, 82, 89], "talk": [8, 9, 55, 66, 73, 74, 82, 89, 229, 232, 240, 266], "taman": [73, 82], "tank": [73, 82], "target": [3, 4, 5, 9, 28, 31, 35, 61, 77, 100, 105, 229, 232, 241, 270, 273, 275, 276, 278, 279, 289, 296, 316, 319, 321, 323, 333, 337, 338], "target_num_rows_per_block": [164, 168], "target_ongoing_request": [241, 270], "target_path": [277, 278, 279, 331, 336, 342], "task": [1, 3, 6, 7, 8, 9, 10, 13, 14, 15, 16, 18, 24, 25, 28, 40, 45, 46, 47, 51, 53, 57, 59, 61, 62, 68, 74, 76, 84, 89, 95, 98, 126, 127, 136, 138, 149, 151, 169, 171, 178, 179, 181, 190, 191, 193, 212, 213, 217, 221, 223, 232, 238, 239, 240, 247, 249, 253, 257, 263, 264, 274, 275, 277, 278, 279, 306, 318, 319, 327, 331, 336, 339, 342], "task_id": [181, 193], "taskpoolmapoper": [9, 63], "tatum": [74, 88], "tavakolian": [73, 82], "tax": [3, 28, 73, 82, 237, 244], "taxi": [3, 8, 28, 51, 54, 169, 171, 325], "taximet": [8, 51], "taxiwindowdataset": [276, 321], "tb": [9, 57], "tbh": [73, 82], "tbl": [275, 317], "tc": [229, 234], "tcm": [74, 88], "tcp": [100, 105], "td3": [278, 336], "tea": [73, 82], "teach": [277, 278, 326, 332], "teacher": [74, 85, 320, 322], "team": [7, 46, 73, 82, 184, 185, 196, 197, 199, 274, 306], "tear": [182, 186, 194, 200, 205, 207], "teardown": [107, 109], "tech": [73, 82], "technic": [1, 13, 74, 89], "techniqu": [6, 40, 74, 89, 239, 257], "technologi": [7, 43], "ted": [73, 82], "teen": [73, 82], "telemetri": [163, 164, 168, 175], "tell": [3, 5, 28, 36, 73, 74, 82, 85, 88, 221, 225, 226, 273, 274, 285, 289, 311], "temp": [111, 113, 126, 128, 136, 139, 273, 274, 276, 281, 289, 307, 310, 323], "temp_checkpoint_dir": [4, 32, 238, 251, 273, 289, 301], "tempdir": [274, 310], "temperatur": [2, 25, 229, 234], "tempfil": [4, 29, 32, 238, 251, 273, 274, 275, 276, 277, 278, 279, 281, 289, 301, 307, 310, 316, 323, 327, 329, 335, 338, 340], "templat": [107, 110, 183, 195, 212, 213, 220, 223, 273, 305], "tempor": [278, 336], "temporari": [212, 213, 217, 273, 274, 275, 276, 278, 279, 289, 310, 314, 319, 325, 336, 340], "temporarydirectori": [4, 32, 238, 251, 273, 274, 276, 279, 289, 301, 310, 323, 340], "ten": [274, 306], "tenant": [229, 236, 279, 342], "tenni": [73, 82], "tensor": [4, 5, 9, 10, 31, 32, 35, 62, 70, 76, 95, 212, 213, 217, 221, 224, 228, 238, 240, 241, 247, 251, 264, 269, 273, 276, 278, 279, 287, 288, 292, 295, 297, 321, 327, 328, 331, 336, 342], "tensor_parallel_s": [221, 224, 227, 229, 234], "tensorflow": [75, 90], "term": [2, 7, 19, 48, 74, 85, 100, 104, 276, 320], "termin": [4, 11, 31, 72, 76, 99, 107, 110, 111, 114, 115, 116, 118, 123, 125, 126, 130, 133, 134, 136, 142, 145, 147, 149, 152, 158, 161, 169, 171, 173, 178, 180, 181, 182, 183, 190, 192, 193, 194, 195, 203, 210, 213, 219, 221, 224, 226, 237, 238, 244, 245, 252, 273, 278, 281, 303, 333], "terminologi": [185, 197], "terraform": [100, 104, 105, 106, 112, 114, 116, 117, 120, 123, 125, 127, 130, 134, 135, 138, 140, 142, 147, 148, 151, 154, 158, 161, 162], "terrain": [275, 319], "terribl": [74, 89], "test": [3, 6, 10, 28, 39, 68, 70, 71, 73, 76, 82, 90, 97, 98, 113, 122, 128, 139, 153, 162, 163, 176, 178, 183, 190, 195, 221, 225, 228, 229, 233, 234, 235, 236, 237, 239, 241, 244, 245, 256, 269, 270, 273, 274, 305, 308], "test_job": [111, 115, 118, 124, 126, 133, 136, 145, 149, 160], "test_siz": [3, 28, 237, 244, 245, 275, 316], "texan": [73, 82], "text": [5, 7, 35, 43, 73, 74, 75, 76, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 97, 136, 140, 164, 167, 180, 192, 216, 220, 229, 234, 276, 278, 279, 320, 332, 337], "text_token": [74, 89], "textembedd": [73, 81, 82, 83], "tf": [111, 113, 118, 122, 126, 128, 136, 139], "tfenv": [111, 112, 126, 127, 136, 138, 149, 151], "tfi": [73, 82], "tfvar": [111, 113, 126, 128, 136, 139], "tgt": [276, 322], "than": [4, 7, 9, 32, 45, 47, 48, 60, 61, 74, 85, 87, 88, 89, 164, 168, 178, 190, 212, 213, 215, 219, 221, 223, 229, 232, 237, 244, 275, 276, 279, 316, 318, 320, 323, 342], "thank": [73, 82, 173, 229, 236, 276, 323], "thats": [74, 89], "theater": [74, 85, 89], "thei": [2, 7, 8, 9, 10, 11, 21, 22, 24, 46, 47, 52, 60, 66, 69, 72, 73, 74, 75, 82, 85, 88, 89, 90, 111, 113, 126, 128, 136, 139, 164, 168, 177, 179, 189, 191, 212, 213, 215, 217, 240, 264, 273, 275, 284, 293, 299, 316], "them": [0, 1, 2, 4, 5, 7, 9, 13, 15, 21, 23, 24, 25, 31, 35, 48, 62, 73, 74, 82, 89, 100, 103, 106, 107, 110, 136, 146, 164, 168, 178, 180, 181, 184, 190, 192, 193, 196, 221, 226, 229, 232, 273, 274, 277, 278, 279, 281, 292, 306, 307, 308, 312, 327, 328, 331, 332, 335, 336, 337, 340, 342], "theme": [0, 173], "themselv": [74, 88], "theoret": [74, 89], "therefor": [74, 85, 182, 186, 194, 200, 205, 207], "theta": [274, 275, 276, 277, 278, 306, 315, 320, 326, 332, 336], "theta_": [278, 332], "theta_dot": [278, 336], "thfc": [73, 82], "thi": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 28, 29, 31, 32, 33, 35, 36, 37, 38, 40, 41, 42, 43, 49, 50, 51, 52, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 117, 118, 119, 120, 121, 122, 126, 127, 128, 130, 133, 135, 136, 137, 139, 140, 142, 145, 146, 148, 149, 150, 151, 152, 153, 158, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 184, 185, 186, 188, 189, 190, 191, 192, 193, 196, 197, 198, 200, 205, 207, 212, 213, 214, 215, 216, 218, 220, 221, 222, 223, 227, 229, 230, 231, 232, 234, 238, 239, 240, 241, 245, 246, 248, 249, 250, 251, 253, 254, 255, 257, 258, 259, 261, 263, 264, 265, 266, 267, 269, 270, 271, 273, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 304, 307, 308, 309, 310, 311, 312, 316, 317, 321, 322, 323, 324, 327, 329, 335, 338, 339, 340, 341, 343], "thine": [73, 82], "thing": [2, 21, 73, 74, 82, 88, 89, 229, 234], "think": [1, 2, 12, 17, 73, 74, 75, 76, 78, 82, 84, 88, 89, 90, 94, 229, 235], "third": [6, 41, 73, 74, 82, 88, 89, 136, 140, 163, 175, 239, 260], "tho": [73, 82], "thoma": [73, 82], "thor": [73, 82], "those": [7, 47, 74, 85, 89, 126, 133, 136, 145, 181, 193, 275, 318], "though": [73, 82, 179, 191, 212, 213, 216], "thought": [74, 85], "three": [1, 5, 8, 9, 16, 35, 51, 58, 59, 74, 89, 164, 167, 212, 213, 218, 219, 220, 240, 263, 273, 274, 277, 305, 311, 331], "thriller": [74, 89], "throb": [74, 85], "through": [2, 3, 4, 5, 6, 7, 19, 28, 29, 35, 38, 44, 47, 73, 74, 78, 82, 83, 88, 89, 100, 105, 106, 107, 110, 111, 112, 126, 127, 136, 137, 149, 150, 164, 166, 167, 169, 172, 173, 177, 178, 188, 189, 190, 202, 209, 212, 213, 215, 218, 220, 221, 222, 229, 231, 234, 238, 239, 246, 255, 273, 274, 275, 276, 277, 278, 281, 284, 305, 306, 315, 317, 320, 326, 336], "throughout": [11, 72, 274, 307], "throughput": [7, 9, 48, 66, 73, 78, 83, 169, 171, 172, 212, 213, 216, 273, 274, 275, 293, 305, 314, 318], "throw": [73, 83, 126, 134, 136, 147], "thru": [74, 85], "thu": [4, 31, 107, 110], "thumb": [7, 48], "thursdai": [73, 80, 82, 83], "ti": [4, 5, 32, 36, 273, 282], "ticket": [73, 80, 82], "tidi": [274, 275, 276, 314, 319, 321, 325], "tie": [73, 82], "tiger": [73, 74, 82, 89], "tight": [73, 82], "tight_layout": [273, 274, 276, 277, 278, 279, 292, 307, 312, 321, 323, 325, 327, 329, 331, 335, 338, 340], "tightli": [100, 105], "tild": [278, 332], "tilt": [7, 45], "timber": [73, 82], "time": [1, 2, 3, 4, 5, 6, 7, 9, 10, 12, 16, 17, 18, 19, 20, 21, 22, 24, 25, 28, 30, 32, 34, 36, 40, 46, 48, 61, 68, 73, 74, 76, 78, 82, 85, 88, 89, 99, 107, 109, 164, 168, 169, 171, 172, 179, 181, 185, 191, 193, 198, 212, 213, 215, 216, 217, 221, 224, 228, 229, 234, 237, 238, 239, 244, 245, 248, 251, 252, 257, 258, 260, 273, 274, 275, 277, 278, 279, 280, 281, 293, 294, 301, 306, 313, 317, 319, 321, 323, 325, 326, 332, 335, 336, 337, 340, 342, 343], "time_since_restor": [238, 253], "time_this_iter_": [238, 253], "time_total_": [238, 253], "timedelta": [276, 321], "timelin": [2, 24], "timeseri": [169, 171], "timeseriesbatchpredictor": [276, 325], "timeseriestransform": [276, 322, 323, 325], "timestamp": [4, 31, 173, 238, 247, 253, 273, 276, 279, 281, 321, 338, 342], "timestep": [5, 35, 277, 278, 326, 328, 331, 332, 333, 334, 336], "tini": [274, 275, 276, 277, 278, 308, 316, 325, 328, 331, 334], "tint": [74, 89], "tip": [3, 8, 28, 51, 52, 238, 239, 241, 252, 258, 269], "tip_amount": [3, 8, 28, 51, 52, 169, 171], "tip_percentag": [8, 52], "titan": [73, 82], "titl": [4, 9, 31, 32, 60, 73, 80, 82, 149, 152, 273, 274, 275, 276, 277, 278, 281, 292, 312, 314, 316, 318, 321, 323, 325, 329, 335, 338, 340], "tl": [107, 110], "tlc": [8, 51], "tloss": [76, 97], "tmp": [76, 99, 163, 169, 172, 173, 176, 181, 193, 237, 238, 244, 245, 252], "tmp_checkpoint": [274, 314], "tmpdir": [274, 276, 279, 310, 323, 340], "to_arrow_ref": [275, 317], "to_csv": [8, 53, 276, 279, 321, 338], "to_datetim": [276, 321], "to_json": [237, 244, 245], "to_numpi": [275, 276, 317, 318, 321, 325], "to_panda": [8, 9, 54, 64, 74, 89, 240, 265, 274, 275, 308, 317], "to_parquet": [8, 53, 273, 274, 275, 279, 296, 308, 316, 338], "to_pylist": [276, 321], "to_tensor": [273, 292], "todai": [11, 72, 73, 82], "todo": [169, 172], "togeth": [3, 4, 5, 9, 10, 12, 27, 32, 36, 61, 69, 73, 82, 241, 268, 270, 273, 282, 289, 290], "toke": [221, 224], "token": [84, 85, 94, 95, 99, 173, 183, 195, 212, 213, 215, 216, 217, 218, 219, 221, 224, 225, 226, 227, 229, 232, 233, 234, 235], "tokenization_fn": [74, 89], "tokenize_funct": [76, 97], "toler": [4, 5, 7, 8, 9, 30, 34, 46, 50, 55, 57, 100, 104, 105, 183, 187, 195, 203, 206, 210, 221, 226, 275, 276, 277, 278, 279, 280, 300, 301, 303, 306, 310, 311, 314, 315, 316, 317, 319, 320, 324, 325, 326, 327, 329, 331, 332, 335, 336, 337, 340, 341, 342], "tolist": [10, 70, 169, 172, 241, 269, 270, 273, 276, 279, 292, 296, 321, 342], "toll": [3, 8, 28, 51], "tolls_amount": [3, 8, 28, 51], "tomorrow": [73, 80, 82, 83, 229, 234], "ton": [74, 89], "tone": [229, 232], "tonight": [73, 80, 82, 83], "tonit": [73, 82], "too": [3, 8, 9, 17, 28, 53, 61, 73, 74, 83, 84, 88, 89, 111, 113, 126, 128, 136, 139, 221, 223, 276, 321], "took": [2, 22], "tool": [3, 7, 27, 43, 47, 48, 73, 74, 76, 78, 84, 94, 98, 99, 107, 108, 111, 112, 118, 120, 126, 127, 136, 138, 149, 151, 163, 164, 166, 167, 168, 169, 171, 175, 176, 178, 181, 188, 190, 193, 213, 220, 230, 231, 236, 237, 243, 244, 245, 275, 276, 278, 279, 316, 321, 332, 338], "tool_cal": [229, 234], "tool_call_cli": [229, 234], "tool_call_id": [229, 234], "tool_call_pars": [229, 234], "tool_choic": [229, 234], "top": [1, 3, 8, 13, 17, 21, 24, 27, 50, 73, 75, 82, 90, 177, 184, 189, 196, 229, 232, 237, 243, 274, 275, 276, 278, 314, 318, 323, 336, 337], "top20": [73, 82], "top_item_id": [279, 342], "top_items_df": [279, 342], "top_scor": [279, 342], "topic": [73, 82, 222, 228, 232], "topic_safety_output_restrict": [229, 232], "topk": [279, 342], "torch": [4, 6, 9, 10, 29, 30, 31, 32, 33, 34, 36, 38, 39, 40, 41, 56, 62, 67, 70, 73, 76, 79, 95, 97, 238, 239, 240, 241, 247, 250, 251, 252, 253, 256, 257, 260, 264, 269, 273, 274, 276, 277, 278, 279, 281, 282, 283, 286, 287, 288, 289, 292, 300, 301, 306, 307, 310, 314, 321, 322, 323, 325, 327, 328, 329, 331, 333, 334, 335, 336, 337, 338, 340, 342], "torch_": [4, 31, 238, 247], "torch_config": [76, 98], "torch_d": [9, 59], "torchconfig": [76, 95, 98], "torchmetr": [238, 247, 249, 253, 274, 310], "torchrec": [279, 337], "torchscript": [278, 336], "torchtrain": [4, 29, 32, 33, 76, 94, 95, 98, 253, 276, 279, 281, 283, 284, 285, 293, 296, 301, 303, 305, 306, 307, 314, 320, 321, 323, 325, 326, 327, 331, 332, 333, 337, 338, 340, 341, 342], "torchtrainer_2025": [76, 99], "torchtrainer_4dd7a_00000": [76, 99], "torchtrainer_4dd7a_00000_0_2025": [76, 99], "torchtrainer_d89d0_00000_0_2024": [238, 253], "torchvis": [4, 6, 9, 29, 38, 56, 238, 239, 240, 241, 247, 256, 257, 264, 270, 273, 274, 277, 281, 282, 297, 306, 307, 327], "torqu": [278, 332, 336], "torranc": [73, 82], "total": [2, 3, 4, 8, 25, 28, 32, 51, 76, 98, 212, 213, 217, 221, 227, 237, 238, 239, 244, 245, 247, 258, 260, 273, 277, 278, 283, 284, 327, 333], "total_amount": [8, 51, 52, 169, 171], "total_amt": [6, 41, 239, 258], "totensor": [4, 6, 9, 29, 31, 32, 38, 39, 41, 56, 61, 238, 239, 240, 241, 247, 250, 253, 256, 260, 264, 270, 273, 274, 281, 287, 292, 297, 308, 314], "touch": [74, 85, 275, 279, 316, 342], "tough": [75, 93], "tougher": [75, 93], "tour": [1, 12, 73, 82], "tourism": [73, 82], "tourist": [229, 232], "toward": [7, 45, 75, 77, 93, 278, 279, 332, 338], "tower": [279, 342], "town": [73, 74, 82, 88], "tpot": [212, 213, 217], "tpu": [2, 4, 5, 22, 32, 36, 273, 282], "tqdm": [4, 31, 76, 95, 274, 277, 279, 307, 327, 338], "tr_model": [276, 322], "trace": [4, 5, 7, 30, 34, 46, 74, 85, 163, 170, 175, 273, 280], "traceback": [7, 46], "track": [4, 11, 32, 72, 74, 77, 89, 164, 167, 181, 182, 193, 194, 273, 275, 277, 278, 279, 280, 291, 305, 319, 329, 332, 336, 337, 340, 342], "track_running_stat": [238, 247, 253], "tractabl": [277, 326], "trade": [221, 227, 229, 235], "tradit": [212, 213, 217, 275, 279, 315, 337], "traffic": [75, 90, 93, 100, 105, 136, 146, 149, 156, 169, 172, 183, 187, 195, 203, 206, 210, 212, 213, 217, 218, 221, 226, 276, 320], "trail": [73, 74, 82, 88], "train": [6, 7, 8, 9, 39, 40, 41, 42, 44, 46, 47, 50, 52, 55, 58, 60, 62, 66, 73, 74, 75, 80, 82, 85, 89, 91, 92, 95, 179, 180, 182, 186, 187, 191, 192, 194, 200, 203, 205, 206, 207, 210, 212, 213, 215, 237, 239, 240, 244, 245, 251, 256, 257, 258, 259, 260, 264, 281, 282, 284, 285, 286, 287, 295, 296, 297, 301, 304, 307, 310, 314, 318, 321, 322, 328, 330, 331, 333, 334, 339, 342], "train_arrow": [275, 317], "train_batch": [274, 310], "train_bert": [76, 94, 98, 99], "train_config": [76, 98, 279, 340, 342], "train_count": [277, 327], "train_ctx": [3, 28], "train_d": [273, 275, 277, 278, 279, 296, 297, 298, 316, 317, 327, 329, 333, 335, 338, 340], "train_data": [4, 6, 32, 39, 41, 238, 239, 247, 250, 253, 256, 260, 273, 287], "train_dataload": [5, 35, 36, 277, 278, 329, 335], "train_dataset": [76, 97, 237, 244, 245], "train_df": [275, 316], "train_frac": [279, 338], "train_func": [275, 315, 317, 319], "train_func_per_work": [76, 97, 98], "train_label": [238, 247], "train_linear_model": [6, 41, 239, 259], "train_load": [4, 5, 31, 32, 35, 238, 247, 250, 273, 274, 276, 277, 278, 279, 287, 310, 323, 329, 335, 340], "train_loop": [278, 326, 331, 332, 335], "train_loop_config": [3, 4, 5, 28, 32, 36, 76, 98, 238, 252, 253, 274, 275, 276, 279, 290, 298, 301, 303, 311, 317, 323, 340], "train_loop_per_work": [5, 36, 76, 98, 273, 276, 277, 279, 301, 303, 306, 311, 320, 323, 329, 337, 340], "train_loop_ray_train": [4, 5, 32, 36, 238, 249, 252, 253, 273, 282, 283, 284, 285, 290], "train_loop_ray_train_ray_data": [273, 294, 298], "train_loop_ray_train_with_checkpoint_load": [273, 300, 301, 303], "train_loop_torch": [4, 6, 31, 40, 238, 239, 247, 257], "train_loss": [274, 276, 277, 278, 279, 310, 312, 323, 328, 329, 334, 335, 337, 340], "train_loss_sum": [276, 323], "train_loss_tot": [274, 310], "train_my_simple_model": [6, 41, 239, 258, 259], "train_my_simple_model_2024": [239, 258], "train_my_simple_model_3207e_00000_0_a": [239, 258], "train_my_simple_model_3207e_00000terminated10": [239, 258], "train_my_simple_model_3207e_00001terminated10": [239, 258], "train_my_simple_model_3207e_00002terminated10": [239, 258], "train_my_simple_model_3207e_00003terminated10": [239, 258], "train_my_simple_model_3207e_00004terminated10": [239, 258], "train_parquet": [275, 316], "train_pytorch": [6, 41, 239, 258, 260], "train_pytorch_7cf0c_00000terminated10": [239, 260], "train_pytorch_7cf0c_00001terminated10": [239, 260], "train_record": [276, 321], "train_test_split": [3, 26, 28, 237, 244, 245, 275, 279, 316, 338], "trainabl": [3, 6, 28, 41, 239, 258, 259, 260], "trainbr": [74, 89], "traincontext": [4, 32, 273, 283], "trainer": [3, 4, 5, 28, 32, 35, 36, 76, 95, 98, 237, 238, 244, 245, 249, 252, 253, 274, 276, 277, 278, 279, 281, 282, 291, 298, 301, 302, 303, 305, 306, 311, 313, 314, 319, 323, 324, 329, 330, 335, 337, 340, 341], "training_iter": [238, 253], "training_step": [5, 35, 277, 278, 328, 334], "trainingargu": [76, 95], "trajectori": [278, 336], "transact": [7, 43], "transfer": [1, 2, 7, 8, 9, 13, 19, 43, 54, 61, 64, 164, 168, 240, 265, 274, 314], "transform": [4, 5, 6, 7, 10, 29, 31, 32, 33, 38, 39, 41, 43, 44, 46, 49, 50, 51, 54, 55, 56, 58, 60, 64, 69, 70, 73, 74, 75, 76, 79, 81, 83, 84, 85, 89, 91, 92, 94, 95, 97, 99, 107, 110, 237, 238, 239, 241, 244, 247, 250, 256, 260, 261, 263, 265, 269, 270, 275, 277, 278, 279, 281, 287, 293, 298, 306, 307, 309, 314, 316, 325, 326, 327, 333, 336, 338], "transform_imag": [273, 297], "transient": [273, 274, 301, 302, 310], "transit": [180, 192, 275, 276, 277, 278, 279, 315, 320, 326, 332, 337], "transpar": [277, 329], "transpos": [277, 327], "travel": [73, 82, 229, 232], "treat": [74, 85, 88, 274, 306], "tree": [3, 4, 5, 28, 30, 31, 34, 35, 73, 74, 80, 82, 85, 237, 244, 273, 275, 280, 315, 316], "tree_method": [3, 28, 275, 317], "tremend": [74, 89], "trend": [73, 82], "tri": [74, 85, 275, 315], "trial": [3, 6, 28, 41, 74, 76, 89, 99, 237, 238, 239, 244, 245, 252, 258, 259, 260], "trial_id": [238, 253], "tribul": [74, 89], "trier": [74, 89], "trigger": [9, 60, 63, 73, 83, 275, 278, 316, 319, 333], "trim": [274, 277, 307, 327], "trip": [3, 8, 28, 51, 54, 74, 88, 237, 244, 276, 321, 325], "trip_amount": [3, 28], "trip_dist": [3, 8, 28, 51, 54, 237, 244], "trip_dur": [237, 244], "trivial": [212, 213, 218], "trndnl": [73, 82], "troubleshoot": [163, 176, 178, 181, 190, 193, 229, 236], "truck": [229, 233], "true": [0, 2, 4, 5, 6, 8, 9, 10, 20, 31, 32, 35, 36, 39, 41, 51, 59, 61, 62, 70, 74, 76, 85, 97, 118, 122, 169, 171, 172, 173, 212, 213, 219, 221, 224, 225, 226, 227, 229, 232, 234, 237, 238, 239, 240, 241, 244, 247, 249, 250, 253, 256, 260, 263, 270, 273, 274, 275, 276, 277, 278, 279, 280, 281, 285, 287, 292, 306, 307, 308, 309, 310, 311, 312, 314, 316, 318, 321, 322, 323, 325, 327, 328, 329, 334, 335, 338, 340, 341], "truli": [73, 74, 82, 89], "trump": [73, 82], "truncat": [75, 76, 92, 93, 97, 277, 278, 327, 333], "trust": [73, 82, 100, 102, 105, 276, 325], "truth": [9, 61, 64, 240, 265, 273, 274, 276, 278, 281, 314, 320, 323, 325, 333], "try": [1, 2, 6, 7, 16, 20, 41, 47, 74, 88, 89, 111, 112, 126, 127, 134, 136, 138, 147, 149, 151, 221, 228, 229, 236, 238, 239, 252, 258, 259, 260, 273, 274, 275, 277, 305, 307, 319, 327, 331], "tryna": [73, 82], "tsui": [74, 88], "ttft": [212, 213, 217], "ttm": [74, 88, 89], "tuesdai": [73, 82], "tune": [7, 9, 40, 46, 47, 61, 74, 89, 182, 186, 194, 200, 205, 207, 212, 213, 218, 221, 228, 229, 231, 232, 235, 236, 237, 244, 245, 257, 273, 274, 275, 276, 277, 278, 279, 295, 305, 306, 314, 318, 319, 325, 331, 336, 342, 343], "tune_config": [3, 6, 28, 41, 237, 239, 244, 245, 258, 259, 260], "tuneconfig": [3, 6, 28, 41, 237, 239, 244, 245, 258, 259, 260], "tuner": [3, 6, 28, 41, 237, 239, 244, 245, 258, 259, 260], "tupl": [2, 5, 9, 19, 35, 62, 273, 293, 294], "turn": [1, 6, 8, 14, 41, 53, 73, 74, 82, 89, 275, 315], "tutori": [100, 103, 178, 180, 190, 192, 221, 223, 274, 275, 276, 277, 278, 279, 281, 304, 306, 307, 315, 320, 321, 323, 325, 326, 331, 332, 337, 338, 342], "tv": [73, 74, 82, 88, 89], "tweet": [73, 82], "tweet_ev": [73, 80], "twilight": [73, 82], "twitter": [73, 82], "two": [1, 2, 16, 18, 19, 20, 73, 82, 86, 89, 100, 105, 107, 109, 163, 169, 171, 173, 176, 179, 182, 186, 191, 194, 202, 205, 209, 216, 273, 274, 275, 276, 279, 282, 291, 296, 306, 312, 316, 321, 338, 340, 342], "txt": [0, 11, 72, 77, 173, 180, 192], "type": [2, 4, 5, 6, 7, 8, 9, 10, 20, 32, 33, 35, 38, 43, 46, 55, 59, 61, 62, 67, 73, 74, 76, 79, 85, 95, 101, 105, 107, 109, 111, 115, 118, 124, 126, 133, 136, 145, 149, 154, 160, 164, 168, 169, 172, 173, 177, 179, 185, 189, 191, 198, 212, 213, 217, 221, 224, 234, 235, 239, 241, 258, 269, 315, 318, 319], "typic": [8, 50, 73, 74, 78, 89, 107, 110, 188, 221, 223, 229, 232, 274, 277, 279, 306, 327, 338], "u": [5, 6, 35, 36, 40, 73, 74, 82, 85, 89, 111, 113, 118, 121, 122, 126, 128, 136, 139, 149, 152, 153, 154, 163, 169, 171, 176, 180, 192, 229, 232, 236, 237, 238, 239, 240, 241, 244, 245, 252, 257, 258, 260, 264, 270, 273, 278, 279, 282, 332, 337, 338, 342], "u002c": [73, 82], "u002c000": [73, 82], "u2019": [73, 82], "u2019ll": [73, 82], "u2019m": [73, 82], "u2019r": [73, 82], "u2019t": [73, 82], "u2019v": [73, 82], "u_": [278, 279, 332, 337], "u_k": [278, 332], "uber": [6, 42], "ubj": [3, 28, 237, 244, 245], "ubyt": [238, 239, 252, 258, 260], "udf": [7, 46], "ui": [0, 100, 105, 180, 182, 192, 194, 241, 270], "uid": [279, 338, 342], "uint8": [164, 168, 241, 270, 273, 297], "un": [74, 89], "unabl": [74, 89], "unassoci": [111, 113, 126, 128, 136, 139], "unattach": [111, 113, 126, 128, 136, 139], "unavail": [2, 22], "unavoid": [74, 85], "unbound": [9, 61], "uncas": [75, 92], "uncertainti": [276, 325], "unchang": [273, 297], "uncl": [74, 89], "uncom": [76, 98, 118, 122, 126, 129, 136, 141, 229, 233, 234], "uncondit": [277, 331], "unconnect": [74, 89], "unconnectedbr": [74, 89], "under": [2, 9, 19, 59, 74, 88, 149, 152, 169, 171, 179, 183, 187, 191, 195, 204, 206, 211, 273, 274, 276, 277, 278, 279, 281, 282, 285, 289, 296, 307, 310, 320, 321, 323, 327, 329, 331, 335, 336, 338, 342], "underbrac": [276, 320], "underli": [2, 3, 4, 5, 9, 18, 28, 30, 34, 60, 184, 196, 221, 224, 273, 280], "undersid": [74, 89], "understand": [6, 9, 40, 61, 74, 76, 88, 89, 97, 100, 101, 103, 162, 163, 164, 168, 169, 172, 175, 181, 183, 193, 195, 212, 213, 214, 215, 217, 219, 220, 221, 223, 224, 239, 257, 274, 275, 279, 306, 315, 342], "understat": [74, 89], "understood": [221, 228], "underutil": [212, 213, 215, 216], "uneasy": [73, 82], "unet": [5, 35], "unet2dconditionmodel": [5, 33, 35], "unexpect": [4, 5, 30, 34, 73, 82, 163, 175, 273, 276, 280, 321], "ungat": [221, 224, 229, 232], "unifi": [3, 7, 11, 27, 43, 46, 72, 100, 106, 188, 212, 213, 218], "uniform": [2, 3, 23, 28, 76, 97, 278, 332], "uniformli": [9, 57], "uniniti": [9, 62, 240, 264], "uninstal": [126, 134, 136, 147, 149, 161], "uniqu": [8, 51, 74, 86, 89, 180, 192, 212, 213, 217, 221, 224, 240, 263, 273, 274, 279, 287, 289, 307, 342], "unique_item": [279, 342], "unique_us": [279, 342], "unit": [9, 10, 64, 69, 71, 74, 85, 212, 213, 217, 229, 234, 240, 265, 274, 307], "univari": [276, 322], "univers": [74, 88], "unless": [8, 52, 74, 85, 182, 184, 186, 194, 196, 200, 205, 207, 273, 292, 300], "unlik": [73, 82, 182, 186, 194, 200, 205, 207, 273, 281, 297], "unnecessari": [9, 10, 61, 68, 73, 82, 181, 182, 183, 193, 194, 195, 273, 275, 278, 289, 292, 317, 336], "unnot": [74, 89], "unpredict": [212, 213, 217], "unread": [277, 327], "unregist": [111, 116, 126, 134, 136, 147], "unreleas": [73, 82], "unrelentingli": [74, 88, 89], "unrival": [74, 89], "uns4": [73, 82], "unshuffl": [275, 316], "unsloth": [212, 213, 219, 221, 224, 229, 232], "unsqueez": [4, 31, 32, 238, 247, 253, 273, 276, 278, 292, 321, 322, 323, 325, 336], "unstabl": [277, 326], "unstructur": [7, 43, 57, 343], "until": [2, 4, 7, 8, 18, 32, 48, 51, 52, 73, 74, 82, 89, 164, 168, 212, 213, 216, 217, 238, 240, 252, 264, 273, 275, 290, 317], "untitl": [178, 190], "unus": [111, 113, 126, 128, 136, 139], "unveil": [73, 82], "unwrap": [4, 32, 238, 251, 273, 289, 301], "up": [0, 2, 4, 5, 6, 7, 8, 9, 10, 20, 21, 30, 31, 34, 35, 40, 41, 48, 51, 60, 62, 69, 70, 73, 74, 76, 78, 81, 82, 84, 85, 88, 89, 94, 98, 100, 101, 104, 107, 110, 111, 113, 116, 117, 118, 121, 122, 125, 128, 129, 133, 135, 139, 141, 145, 148, 149, 152, 153, 161, 169, 170, 174, 175, 179, 185, 191, 197, 203, 210, 212, 213, 217, 219, 222, 227, 228, 230, 232, 235, 240, 255, 258, 264, 268, 269, 270, 280, 290, 301, 303, 306, 307, 311, 313, 315, 316, 317, 324, 327, 329, 332, 335, 340, 341], "up_block_typ": [5, 35], "upblock2d": [5, 35], "upcom": 49, "updat": [0, 3, 4, 5, 6, 7, 10, 11, 28, 32, 36, 41, 43, 71, 72, 75, 76, 77, 90, 97, 126, 129, 134, 136, 141, 147, 149, 156, 179, 181, 183, 191, 193, 195, 204, 211, 221, 226, 239, 241, 247, 259, 270, 273, 274, 276, 277, 280, 283, 289, 300, 310, 325, 331], "upgrad": [10, 69, 126, 128, 129, 131, 136, 139, 141, 143, 156, 159, 178, 183, 187, 190, 195, 203, 206, 210, 277, 278, 331, 332, 336], "upload": [4, 32, 180, 192, 229, 232, 238, 251, 275, 319], "upload_fil": [180, 192, 229, 232], "upon": [181, 193], "upper": [178, 190], "upright": [278, 332], "upscal": [241, 270], "upscale_delay_": [241, 270], "upset": [73, 82], "ur": [73, 82], "uri": [180, 192], "url": [7, 48, 182, 194, 276, 321], "urljoin": [213, 219, 221, 225], "urllib": [213, 219, 221, 225], "urmitz": [74, 89], "us": [1, 2, 10, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 33, 35, 36, 37, 40, 42, 43, 44, 45, 46, 48, 49, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 68, 70, 71, 73, 74, 76, 77, 78, 80, 81, 82, 83, 84, 86, 87, 88, 89, 92, 93, 94, 96, 97, 98, 99, 103, 104, 106, 108, 110, 111, 112, 114, 117, 118, 119, 122, 123, 126, 127, 130, 135, 136, 137, 138, 142, 146, 148, 149, 150, 151, 152, 153, 154, 158, 163, 164, 167, 168, 170, 171, 173, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 188, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 212, 213, 215, 216, 217, 218, 219, 222, 223, 225, 226, 228, 231, 236, 238, 243, 245, 249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 261, 263, 264, 266, 269, 271, 281, 282, 283, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 296, 297, 298, 299, 303, 304, 305, 307, 310, 312, 314, 317, 318, 319, 321, 323, 325, 327, 329, 331, 333, 335, 336, 339, 340, 342], "usabl": [229, 233, 273, 297], "usag": [10, 68, 76, 99, 163, 164, 166, 168, 175, 181, 193, 212, 213, 216, 218, 229, 231, 232, 235, 237, 238, 239, 241, 244, 245, 252, 258, 260, 268, 279, 340], "use_gpu": [3, 4, 5, 28, 32, 36, 237, 238, 244, 245, 249, 273, 274, 275, 276, 277, 278, 279, 280, 285, 306, 311, 317, 323, 329, 335, 340], "use_gpu_actor": [274, 314], "usecol": [279, 342], "user": [2, 4, 7, 9, 11, 18, 22, 32, 46, 47, 61, 72, 73, 75, 76, 80, 82, 83, 90, 99, 100, 102, 107, 109, 149, 152, 164, 166, 167, 168, 169, 172, 177, 180, 181, 182, 184, 186, 187, 189, 192, 193, 194, 196, 197, 200, 203, 205, 206, 207, 210, 212, 213, 215, 216, 217, 219, 221, 225, 226, 229, 232, 233, 234, 235, 238, 239, 247, 260, 273, 283, 286, 287, 339, 340], "user2idx": [279, 338, 342], "user_col": [279, 338], "user_embed": [279, 339, 342], "user_id": [173, 279, 338, 342], "user_idx": [279, 337, 338, 339, 340, 342], "user_nam": [149, 152], "user_storag": [180, 192], "user_vec": [279, 339], "user_vector": [279, 342], "userguid": [111, 112, 126, 127, 136, 138], "userservic": [169, 172, 173], "usual": [7, 8, 45, 52, 74, 88, 273, 292], "utc": [238, 247], "util": [2, 4, 5, 6, 9, 10, 22, 31, 32, 33, 35, 38, 39, 41, 57, 61, 66, 68, 69, 76, 94, 95, 97, 169, 172, 181, 183, 187, 193, 195, 203, 206, 210, 212, 213, 215, 216, 217, 218, 238, 239, 241, 247, 250, 251, 256, 270, 273, 274, 276, 277, 278, 279, 280, 281, 287, 289, 293, 305, 306, 307, 321, 327, 333, 336, 338], "uuid": [274, 275, 276, 278, 279, 307, 316, 321, 333, 338], "uv": 77, "ux": [7, 46], "v": [45, 46, 73, 76, 82, 97, 100, 103, 110, 126, 132, 133, 136, 144, 145, 178, 182, 190, 194, 212, 213, 216, 229, 235, 273, 274, 276, 279, 280, 292, 304, 314, 323, 325, 337, 342, 343], "v1": [107, 110, 213, 219, 221, 225, 226, 229, 232, 233, 234, 278, 332, 333, 336], "v2": [73, 81, 273, 275, 276, 277, 278, 279, 280, 315, 320, 326, 329, 332, 337], "v_": [279, 337], "val": [274, 275, 276, 279, 308, 310, 312, 314, 316, 317, 321, 323, 325, 329, 333, 338, 340], "val_batch": [274, 310], "val_d": [275, 277, 278, 279, 316, 317, 318, 319, 327, 329, 333, 335, 338, 340], "val_dataload": [277, 278, 329, 335], "val_df": [275, 316], "val_load": [274, 276, 277, 278, 279, 310, 323, 329, 335, 340], "val_loss": [274, 276, 277, 278, 279, 310, 311, 312, 323, 328, 329, 334, 335, 337, 340], "val_loss_sum": [276, 323], "val_loss_tot": [274, 310], "val_parquet": [275, 316], "val_pd": [275, 317, 318], "val_record": [276, 321], "val_xb": [274, 310], "val_yb": [274, 310], "valid": [3, 28, 76, 97, 162, 183, 195, 229, 233, 237, 241, 244, 245, 270, 277, 278, 306, 310, 311, 314, 317, 318, 319, 328, 329, 332, 333, 334, 335, 337], "valid_dataset": [237, 244, 245], "valid_dataset_featur": [237, 244, 245], "validation_step": [277, 278, 328, 334], "valu": [2, 4, 6, 9, 18, 19, 23, 31, 39, 60, 61, 74, 87, 111, 112, 113, 118, 119, 122, 126, 127, 128, 129, 134, 136, 137, 139, 141, 147, 149, 150, 155, 156, 169, 171, 172, 183, 187, 195, 204, 206, 211, 215, 238, 239, 247, 256, 273, 274, 276, 277, 278, 279, 284, 287, 288, 291, 297, 310, 312, 320, 321, 322, 326, 329, 332, 333, 338, 342], "valuabl": [74, 88], "value_count": [275, 279, 316, 338], "valueerror": [2, 20, 277, 278, 329, 335], "values_nginx": [126, 129, 136, 141], "values_nginx_gke_priv": [149, 156], "values_nginx_gke_publ": [149, 156], "values_nvdp": [126, 129, 136, 141], "vamp": [73, 82], "vampett": [73, 82], "vampir": [73, 82], "van": [73, 82], "vanilla": [5, 26, 36, 38, 212, 213, 216, 276, 320, 321], "var": [100, 105, 149, 161, 273, 274, 275, 276, 277, 278, 279, 281, 307, 316, 321, 327, 333, 338], "varepsilon": [277, 326], "varepsilon_": [278, 332], "varepsilon_k": [278, 332], "vari": [7, 8, 9, 43, 54, 57, 62, 64, 212, 213, 217, 240, 265], "variabl": [2, 21, 22, 111, 113, 118, 122, 126, 128, 136, 139, 149, 152, 179, 180, 181, 191, 192, 193], "variat": [6, 41, 239, 259], "varieti": [9, 59, 64, 74, 84, 240, 265], "variou": [7, 44, 76, 98, 179, 180, 191, 192, 221, 228, 273, 293], "vast": [7, 43], "ve": [73, 74, 82, 85, 89, 111, 117, 212, 213, 220, 221, 228, 229, 231, 236, 274, 314], "vector": [7, 9, 43, 61, 73, 78, 212, 213, 215, 275, 278, 279, 319, 333, 336, 337, 339], "veget": [275, 315], "veloc": [278, 332], "venu": [75, 93], "venv": 0, "verbos": [169, 172], "veri": [4, 5, 6, 7, 8, 31, 32, 35, 36, 41, 47, 52, 73, 74, 82, 89, 221, 223, 238, 239, 240, 248, 259, 264, 273, 276, 281, 325], "verif": 162, "verifi": [5, 9, 35, 61, 111, 116, 118, 121, 134, 146, 147, 149, 152, 155, 156, 157, 179, 181, 182, 191, 193, 194, 276, 316, 321], "vermaelen": [73, 82], "version": [7, 11, 43, 72, 73, 75, 77, 82, 90, 100, 105, 111, 112, 118, 120, 126, 127, 129, 136, 138, 141, 149, 151, 156, 157, 163, 164, 167, 169, 172, 176, 177, 187, 189, 203, 206, 210, 229, 232, 240, 265, 273, 276, 277, 294, 321, 327, 329], "versu": [274, 275, 277, 314, 319, 331], "via": [0, 1, 2, 4, 5, 7, 9, 13, 22, 30, 34, 46, 47, 57, 73, 74, 82, 88, 100, 103, 107, 109, 110, 111, 113, 118, 122, 126, 128, 136, 139, 163, 169, 170, 172, 173, 175, 178, 184, 185, 186, 190, 196, 198, 200, 205, 207, 273, 274, 277, 278, 279, 280, 283, 284, 285, 292, 297, 305, 309, 310, 328, 335, 338, 340], "vicki": [73, 80, 82], "vid": [73, 82], "video": [7, 9, 43, 57, 73, 74, 82, 85, 89], "vietnam": [74, 85], "view": [6, 8, 9, 10, 11, 41, 51, 52, 61, 71, 72, 74, 85, 89, 100, 105, 111, 113, 115, 118, 122, 124, 126, 128, 133, 136, 139, 145, 146, 149, 160, 163, 164, 168, 169, 171, 172, 175, 179, 181, 191, 193, 221, 227, 238, 239, 241, 252, 258, 259, 269, 277, 278, 328, 334], "viewer": [74, 85], "vincent": [74, 85], "violat": [229, 232], "viridi": [275, 318], "virtual": [0, 100, 103, 105, 110, 163, 176, 179, 191, 212, 213, 216], "virtuou": [74, 85], "visibl": [74, 85, 221, 227, 273, 276, 288, 325], "vision": [273, 305, 314, 331, 343], "visit": [6, 41, 163, 176, 239, 241, 259, 270], "visual": [6, 7, 8, 9, 39, 44, 51, 60, 74, 88, 89, 163, 164, 168, 175, 176, 178, 181, 190, 193, 221, 227, 240, 248, 255, 263, 278, 312, 315, 323, 329, 331, 332, 335, 340], "visualis": [276, 321], "vit": [273, 301, 303], "vit_b_16": [274, 314], "vit_l_32": [274, 314], "vllm": [219, 220, 221, 227, 228, 229, 233], "vm": [100, 101, 103, 110, 111, 115, 118, 122, 124, 149, 153, 162, 343], "vocal": [73, 82], "voic": [73, 74, 82, 89], "volatil": [4, 31, 273, 281], "volleybal": [73, 82], "volum": [4, 7, 31, 43, 46, 164, 168, 180, 192, 273, 274, 275, 281, 306, 314, 316], "von": [74, 89], "vpc": [103, 104, 106, 107, 109, 111, 113, 117, 118, 119, 122, 126, 127, 128, 136, 138, 139, 149, 153, 161, 162, 212, 213, 218], "vpc_cidr": [100, 105], "vpc_id": [100, 105, 106, 111, 113], "vpcid": [111, 113, 126, 128, 136, 139], "vram": [73, 82, 221, 224], "vscode": [183, 195], "vtripl": [229, 232], "vulva": [74, 85], "w": [6, 10, 41, 70, 73, 74, 80, 82, 89, 180, 192, 239, 259, 273, 274, 277, 292, 314, 326, 327, 328], "w0": [2, 25], "w1": [2, 25], "wa": [5, 8, 35, 51, 73, 74, 82, 85, 88, 89, 164, 168, 179, 182, 191, 194, 237, 244, 273, 279, 303, 341], "wai": [2, 7, 8, 9, 11, 19, 44, 45, 50, 60, 61, 72, 73, 74, 75, 82, 84, 88, 89, 90, 93, 100, 103, 178, 182, 185, 186, 190, 194, 199, 202, 205, 209, 221, 227, 241, 270, 273, 274, 278, 289, 306, 308, 314, 336], "wait": [1, 15, 16, 17, 23, 73, 82, 149, 156, 179, 191, 212, 213, 216], "wake": [73, 74, 82, 89], "walk": [4, 6, 29, 38, 73, 82, 83, 111, 112, 126, 127, 136, 137, 149, 150, 178, 180, 190, 192, 221, 222, 229, 232, 238, 239, 246, 255, 275, 276, 277, 315, 320, 326], "walter": [74, 88], "wander": [74, 85], "wanna": [73, 82], "want": [0, 1, 2, 4, 5, 7, 8, 9, 10, 15, 18, 19, 24, 25, 32, 35, 44, 51, 53, 54, 61, 62, 64, 70, 71, 73, 74, 76, 80, 81, 82, 85, 88, 89, 99, 100, 105, 118, 122, 125, 136, 139, 149, 152, 161, 178, 181, 190, 193, 212, 213, 217, 237, 240, 241, 244, 264, 265, 269, 273, 276, 281, 287, 288, 292, 320, 323], "war": [73, 74, 82, 85], "warehous": [9, 59], "warm": [5, 35, 73, 82], "warmth": [73, 82], "warn": [73, 74, 82, 88, 89, 277, 278, 329, 335], "warner": [73, 80, 82], "wasn": [74, 88, 276, 321], "wast": [74, 89], "watch": [73, 74, 82, 89, 149, 156], "water": [74, 89], "wave": [74, 89], "wc": [8, 51], "we": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 16, 19, 20, 21, 23, 25, 28, 31, 32, 35, 36, 40, 41, 44, 45, 51, 52, 53, 54, 59, 61, 62, 63, 64, 70, 73, 74, 75, 76, 78, 82, 85, 86, 88, 89, 90, 91, 93, 96, 98, 99, 111, 112, 118, 120, 126, 127, 129, 133, 136, 138, 141, 145, 149, 151, 153, 169, 171, 177, 178, 179, 180, 181, 182, 183, 188, 189, 190, 191, 192, 193, 194, 195, 212, 213, 214, 216, 219, 220, 222, 223, 224, 226, 230, 232, 233, 238, 239, 240, 241, 247, 248, 249, 251, 253, 257, 258, 259, 260, 263, 264, 265, 269, 270, 272, 273, 274, 279, 281, 282, 283, 284, 285, 286, 289, 290, 292, 294, 295, 296, 297, 298, 300, 301, 312, 337, 340], "wealth": [74, 89], "weather": [276, 325], "weaviat": [7, 43], "web": [75, 90, 163, 170, 175, 178, 181, 190, 193, 278, 336], "webservic": 91, "websit": [0, 163, 176], "webster": [74, 88], "wed": [73, 82], "wednesdai": [73, 80, 82], "week": [73, 82, 276, 320, 321], "weight": [2, 25, 212, 213, 217, 221, 224, 238, 247, 273, 274, 275, 276, 277, 278, 279, 280, 283, 289, 292, 301, 305, 314, 316, 319, 325, 331, 336, 340, 342], "weight_decai": [5, 35, 36], "weights_onli": [4, 5, 32, 36, 238, 253, 273, 292], "welbeck": [73, 82], "welcom": [173, 237, 242], "well": [2, 7, 25, 47, 73, 74, 75, 82, 88, 89, 90, 100, 105, 107, 109, 179, 185, 191, 199, 279, 338, 339], "wellcraft": [74, 89], "welllll": [73, 82], "went": [74, 89], "were": [5, 35, 73, 74, 82, 88, 89, 273, 275, 303, 316], "werewolf": [74, 89], "west": [111, 113, 126, 128, 136, 139, 180, 192, 229, 232, 237, 238, 239, 244, 245, 252, 258, 260], "west2": [118, 121, 122, 149, 152, 153, 154], "western": [74, 88, 89], "wget": [279, 338], "what": [1, 3, 5, 6, 9, 15, 26, 28, 35, 41, 49, 61, 73, 74, 82, 85, 89, 101, 118, 119, 149, 153, 164, 165, 167, 168, 174, 177, 189, 214, 219, 232, 234, 237, 239, 242, 244, 245, 258, 285, 305, 338], "whatev": [274, 275, 276, 314, 319, 325], "when": [1, 2, 6, 11, 15, 16, 18, 20, 21, 22, 24, 25, 29, 33, 41, 48, 49, 52, 53, 56, 58, 59, 60, 61, 62, 63, 64, 67, 72, 73, 74, 75, 78, 81, 82, 85, 89, 93, 111, 113, 116, 118, 122, 125, 126, 128, 129, 133, 136, 139, 141, 145, 149, 153, 161, 164, 167, 168, 169, 170, 172, 178, 179, 180, 181, 182, 183, 187, 190, 191, 192, 193, 194, 195, 204, 206, 211, 212, 213, 215, 216, 221, 225, 226, 227, 229, 234, 239, 259, 261, 263, 264, 265, 270, 274, 275, 276, 277, 278, 279, 287, 290, 291, 293, 301, 304, 307, 310, 312, 316, 323, 325, 327, 331, 336, 340, 342], "where": [1, 4, 5, 6, 7, 8, 13, 32, 36, 39, 47, 48, 51, 73, 74, 75, 76, 82, 85, 88, 89, 90, 97, 136, 139, 149, 154, 163, 164, 167, 176, 180, 184, 185, 188, 192, 196, 198, 212, 213, 215, 221, 223, 237, 238, 239, 244, 245, 248, 252, 256, 289, 290, 291, 298, 303, 306, 308, 315, 320, 324, 332, 337, 338, 341], "wherea": [7, 45, 48], "wherev": [73, 82], "whether": [4, 5, 6, 11, 32, 36, 41, 72, 76, 94, 162, 180, 184, 192, 196, 237, 238, 239, 244, 249, 259, 273, 275, 276, 279, 282, 285, 286, 287, 316, 323, 340], "which": [1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 17, 18, 19, 23, 24, 25, 28, 32, 35, 41, 44, 47, 51, 52, 53, 54, 60, 61, 62, 63, 73, 74, 75, 76, 78, 85, 88, 89, 90, 94, 97, 98, 100, 103, 110, 111, 113, 126, 128, 136, 139, 140, 146, 149, 152, 163, 164, 168, 169, 172, 176, 179, 183, 191, 195, 221, 223, 227, 229, 232, 234, 238, 239, 240, 241, 253, 258, 259, 264, 270, 273, 274, 275, 276, 277, 281, 283, 287, 288, 289, 291, 294, 306, 307, 310, 314, 315, 318, 323, 325, 326, 327], "while": [2, 3, 7, 10, 23, 24, 27, 43, 47, 69, 74, 85, 88, 89, 164, 167, 168, 169, 172, 178, 179, 181, 188, 190, 191, 193, 221, 223, 229, 232, 273, 274, 275, 276, 277, 279, 280, 282, 283, 293, 294, 295, 306, 307, 312, 316, 320, 321, 327, 329, 337, 338], "whilst": [74, 89], "white": [6, 39, 73, 74, 82, 89, 239, 256], "who": [73, 74, 76, 82, 85, 88, 89, 99, 100, 105, 162, 184, 196, 212, 213, 217], "whole": [3, 28, 237, 244, 275, 317], "whose": [74, 88, 89], "why": [9, 56, 73, 74, 82, 88, 222, 228, 273, 280, 299], "whyyyyyyi": [73, 82], "wichita": [73, 82], "wide": [7, 46, 73, 74, 75, 82, 89, 90, 149, 156], "widescreen": [74, 88], "width": [9, 61, 164, 168, 240, 241, 264, 270, 277, 327], "wife": [74, 89], "wildlif": [75, 93], "wilki": [74, 88], "willam": [73, 80, 82], "william": [73, 74, 82, 88], "wilmer": [73, 82], "win": [7, 47, 73, 80, 82], "wind": [74, 88], "window": [11, 72, 74, 85, 118, 121, 163, 176, 178, 190, 221, 227, 273, 305, 320, 322, 325], "wire": [273, 277, 305, 329], "wise": [273, 292], "wish": [73, 82], "with_resourc": [6, 41, 239, 259, 260], "within": [2, 7, 22, 43, 100, 102, 105, 107, 109, 163, 176, 178, 180, 182, 184, 185, 186, 190, 192, 194, 196, 198, 202, 205, 209, 278, 279, 336, 338], "without": [2, 7, 10, 18, 19, 22, 24, 43, 69, 74, 85, 89, 100, 105, 163, 176, 178, 179, 188, 190, 191, 212, 213, 216, 229, 232, 233, 273, 274, 275, 276, 277, 278, 279, 280, 299, 301, 302, 306, 313, 317, 318, 319, 320, 321, 325, 326, 331, 332, 337, 338, 340, 341, 342], "woman": [74, 88, 89], "women": [74, 85], "won": [2, 7, 20, 47, 74, 85], "wonder": [6, 41, 73, 80, 82, 107, 109], "wood": [73, 82], "wooden": [74, 88], "word": [74, 89, 212, 213, 215], "work": [1, 2, 4, 6, 8, 10, 11, 16, 18, 31, 32, 40, 53, 54, 56, 69, 72, 73, 74, 76, 78, 81, 82, 88, 89, 98, 107, 109, 110, 178, 180, 181, 183, 184, 188, 190, 192, 193, 195, 196, 212, 213, 215, 221, 224, 228, 229, 231, 238, 249, 274, 275, 278, 279, 286, 296, 305, 306, 308, 313, 314, 315, 316, 330, 332, 337, 338, 340], "worker": [2, 3, 8, 9, 10, 11, 18, 20, 21, 23, 25, 28, 50, 54, 62, 64, 69, 72, 73, 74, 82, 87, 94, 98, 99, 100, 105, 107, 109, 126, 133, 136, 145, 164, 166, 168, 177, 183, 189, 195, 238, 240, 249, 251, 265, 274, 276, 277, 278, 279, 280, 282, 284, 285, 286, 287, 288, 289, 290, 292, 295, 296, 297, 298, 299, 301, 302, 306, 307, 308, 309, 310, 311, 312, 314, 315, 316, 319, 320, 321, 323, 325, 326, 327, 328, 329, 331, 332, 335, 336, 337, 338, 340], "worker_devic": [73, 82], "worker_nod": [126, 133, 136, 145, 164, 168], "worker_rank": [3, 28], "workernodegroupconfig": [126, 133, 136, 145], "workflow": [7, 11, 43, 45, 46, 47, 67, 72, 73, 74, 75, 76, 77, 78, 83, 84, 89, 90, 98, 99, 107, 109, 177, 178, 181, 182, 188, 189, 190, 193, 194, 229, 234, 273, 274, 275, 276, 278, 280, 289, 292, 305, 306, 315, 316, 320, 325, 332], "working_dir": [182, 186, 194, 202, 205, 209, 212, 213, 219, 221, 226], "workload": [2, 3, 4, 5, 7, 8, 9, 11, 22, 27, 28, 32, 35, 43, 46, 47, 55, 57, 72, 75, 90, 100, 105, 108, 110, 111, 117, 126, 135, 136, 148, 163, 164, 167, 168, 170, 176, 177, 179, 180, 181, 182, 186, 187, 188, 189, 191, 192, 193, 194, 199, 200, 203, 205, 206, 207, 210, 212, 213, 217, 221, 223, 228, 237, 240, 243, 262, 266, 273, 287, 293, 305, 314, 325, 331, 336, 342, 343], "workload_identity_pool_provid": [118, 122, 149, 153], "workloadidentitypool": [118, 122, 149, 153], "workloadserviceaccountnam": [126, 128, 131, 136, 139, 143, 149, 159], "workshop": [177, 186, 187, 189, 200, 203, 205, 206, 207, 210], "workspac": [100, 104, 105, 107, 109, 178, 179, 180, 181, 182, 183, 184, 185, 190, 191, 192, 193, 194, 195, 196, 198, 200, 207, 221, 223, 227, 228, 274, 275, 276, 278, 306, 314, 319, 325, 332, 343], "workspace_v2": [178, 190], "world": [4, 32, 73, 74, 76, 82, 89, 97, 169, 172, 178, 182, 186, 190, 194, 201, 202, 205, 208, 209, 229, 236, 273, 274, 277, 279, 283, 306, 329, 337, 342], "world_rank": [4, 32, 273, 288, 290], "world_siz": [4, 32, 273, 283, 294], "worri": [274, 306], "worth": [73, 82], "would": [4, 5, 8, 32, 35, 36, 52, 53, 54, 73, 74, 76, 82, 84, 86, 88, 89, 97, 164, 168, 221, 223, 228, 238, 248, 273, 285], "wound": [74, 88], "wrangl": [273, 281], "wrap": [2, 4, 19, 32, 183, 187, 188, 195, 204, 206, 211, 238, 249, 250, 280, 281, 282, 283, 287, 289, 290, 298, 306, 309, 317, 320, 321, 326, 332, 333, 338], "wright": [73, 82], "write": [1, 2, 3, 4, 5, 6, 7, 9, 13, 16, 25, 28, 32, 36, 41, 43, 49, 50, 52, 56, 58, 60, 61, 65, 100, 105, 164, 168, 173, 178, 180, 190, 192, 238, 239, 240, 253, 259, 264, 266, 273, 274, 276, 277, 279, 289, 296, 301, 306, 307, 308, 310, 321, 327, 337], "write_csv": [8, 53], "write_parquet": [3, 8, 9, 28, 53, 60, 65, 164, 168, 169, 171, 240, 266, 276, 277, 321, 327], "write_t": [274, 276, 307, 321], "writefil": [169, 171], "writer": [4, 31, 238, 247], "writerow": [4, 31, 238, 247], "written": [7, 11, 45, 72, 74, 88, 163, 169, 172, 176, 273, 274, 275, 276, 277, 281, 289, 310, 316, 321, 327], "wrong": [73, 82], "wrote": [8, 53, 73, 82, 274, 275, 277, 279, 307, 316, 327, 338], "wt": [73, 80, 82], "ww2": [74, 89], "wwe": [73, 80, 82], "wyom": [74, 88], "x": [1, 2, 5, 6, 7, 16, 18, 19, 20, 24, 35, 41, 44, 73, 82, 164, 168, 173, 181, 193, 239, 259, 273, 274, 275, 276, 277, 278, 292, 306, 314, 317, 322, 325, 326, 328, 334, 336], "x_": [277, 278, 326, 332], "x_0": [277, 278, 326, 328, 332], "x_t": [277, 278, 326, 328, 332], "x_test": [3, 28], "x_train": [3, 28], "xb": [274, 276, 310, 321], "xgb": [4, 5, 30, 34, 273, 275, 280, 315, 316, 317, 318, 319], "xgb_model": [275, 317], "xgb_param": [275, 317], "xgboost": [6, 41, 237, 244, 245, 316, 318, 319], "xgboost_predict": [3, 28], "xgboosterror": [3, 26], "xgboosttrain": [3, 26, 28, 237, 244, 245, 275, 316, 317], "xgboosttrainer_2024": [237, 244, 245], "xgboosttrainer_81312_00000terminated10": [237, 244, 245], "xgboosttrainer_81312_00001terminated10": [237, 244, 245], "xgboosttrainer_81312_00002terminated10": [237, 244, 245], "xgbpredictor": [275, 318, 319], "xing": [118, 122, 149, 153], "xlabel": [274, 275, 276, 277, 278, 279, 312, 318, 323, 325, 329, 335, 338, 340], "xxx": [118, 122, 136, 139, 149, 152, 153], "xxxx": [118, 122, 149, 153], "xxxxx": [111, 113, 118, 122, 123, 126, 130, 136, 142, 149, 153, 158], "xxxxxx": [111, 113, 126, 128, 136, 139], "xxxxxxx": [126, 128, 136, 139], "xxxxxxxx": [111, 113, 126, 128, 136, 139], "xxxxxxxxx": [111, 113], "xxxxxxxxxx": [111, 113], "xxxxxxxxxxxx": [111, 113, 126, 128, 136, 139], "y": [2, 4, 6, 11, 18, 31, 41, 72, 73, 82, 181, 193, 213, 219, 221, 225, 227, 229, 232, 233, 234, 238, 239, 247, 259, 274, 275, 306, 317], "y_test": [3, 28], "y_train": [3, 28], "ya": [73, 82], "yaml": [10, 71, 126, 129, 136, 141, 149, 156, 164, 168, 173, 183, 195, 212, 213, 219, 221, 226, 229, 233], "yanke": [73, 80, 82], "yann": [238, 239, 252, 258], "yara": [229, 232], "yard": [73, 82], "yb": [274, 276, 310, 321], "ye": [73, 82, 100, 105], "year": [3, 28, 73, 74, 82, 85, 89, 229, 232, 234], "yellow": [3, 28, 74, 85, 212, 213, 216, 237, 244], "yellow_tripdata_": [3, 28], "yellow_tripdata_2011": [8, 51, 54, 169, 171], "yellow_tripdata_2021": [3, 28], "yelp": [76, 94, 99], "yelp_review_ful": [76, 97], "yepo": [73, 82], "yesterdai": [73, 82], "yet": [74, 89, 274, 279, 306, 337, 338], "yield": [2, 23], "ylabel": [274, 275, 276, 277, 278, 279, 312, 316, 318, 321, 323, 325, 329, 335, 338, 340], "yml": 0, "york": [3, 8, 28, 51, 276, 320], "you": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 17, 19, 20, 21, 22, 23, 24, 26, 28, 29, 30, 31, 32, 33, 34, 36, 38, 41, 49, 50, 51, 52, 53, 54, 55, 56, 60, 61, 62, 63, 64, 65, 67, 71, 72, 73, 74, 75, 76, 78, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 93, 94, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 133, 134, 135, 136, 138, 139, 140, 141, 142, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 158, 160, 161, 163, 164, 165, 166, 169, 170, 171, 172, 173, 176, 177, 178, 179, 180, 181, 183, 184, 188, 189, 190, 191, 192, 193, 195, 196, 202, 209, 212, 213, 214, 216, 217, 218, 219, 221, 222, 225, 226, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 246, 250, 253, 255, 259, 264, 265, 266, 267, 268, 281, 284, 285, 286, 287, 288, 289, 290, 291, 292, 298, 303, 304, 305, 307, 308, 309, 312, 316, 317, 321, 322, 323, 327, 329, 333, 334, 335, 338, 340], "young": [74, 85, 89], "your": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 14, 16, 21, 23, 25, 28, 31, 32, 36, 41, 53, 57, 58, 61, 62, 64, 65, 68, 70, 73, 74, 76, 78, 81, 82, 84, 88, 89, 94, 98, 99, 101, 102, 104, 105, 108, 109, 111, 112, 113, 114, 115, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 137, 139, 141, 142, 143, 145, 146, 147, 149, 150, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 169, 171, 172, 173, 177, 178, 179, 180, 184, 185, 189, 190, 191, 192, 196, 198, 203, 210, 212, 213, 216, 217, 218, 219, 221, 224, 225, 226, 227, 229, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 250, 253, 259, 264, 265, 268, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 287, 289, 290, 293, 299, 302, 304, 306, 311, 312, 314, 315, 317, 319, 320, 321, 323, 325, 326, 332, 337, 338, 340, 342], "your_anyscale_org_id": [118, 122], "your_gcp_project_nam": [149, 161], "your_project_id": [118, 121], "yourself": [2, 21, 277, 326], "yr": [73, 82], "ytick": [275, 318], "yunikorn": [107, 109], "z": [73, 82, 276, 321], "zentropa": [74, 88, 89], "zero": [7, 43, 75, 76, 93, 97, 169, 172, 183, 187, 195, 203, 206, 210, 212, 213, 217, 218, 221, 228, 274, 275, 276, 310, 314, 316, 322, 323, 325], "zero_copy_onli": [275, 317], "zero_grad": [4, 6, 31, 32, 40, 41, 76, 97, 238, 239, 247, 249, 253, 257, 260, 273, 274, 276, 279, 283, 294, 300, 310, 323, 340], "zeros_lik": [276, 323], "zilliz": [7, 43], "zip": [6, 39, 239, 256, 274, 275, 277, 279, 307, 318, 327, 331, 338, 342], "zip_ref": [279, 338], "zipfil": [279, 338], "zone": [8, 51, 100, 105, 126, 128, 136, 139, 149, 154], "zprofil": [11, 72], "zsh": [149, 152], "zshrc": [149, 152], "zuoma": [73, 82], "\u03b8": [278, 333, 336], "\u03c0": [181, 193, 278, 333], "\u03f5": [277, 278, 328, 334]}, "titles": ["Ray Enablement Content: Jupyter Book Publishing", "Introduction to Ray Core: Getting Started", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "Introduction to Ray Train + PyTorch", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "Introduction to Ray Tune", "Introduction to Ray Data: Industry Landscape", "Introduction to Ray Data: Ray Data + Structured Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "Introduction to Ray Serve with PyTorch", "Introduction to Ray: Developer", "Introduction to Ray Core: Getting Started", "0. Overview", "1. Creating Remote Functions", "2. Executing Remote Functions", "4. Putting It All Together", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "1. Object store", "2. Chaining Tasks and Passing Data", "3. Task retries", "4. Task Runtime Environments", "5. Resource allocation and management", "6. Nested Tasks", "7. Pattern: Pipeline data processing and waiting for results", "8. Ray Actors", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "1. Overview of the Ray AI Libraries", "2. Quick end-to-end example", "Introduction to Ray Train + PyTorch", "1. When to use Ray Train", "2. Single GPU Training with PyTorch", "3. Distributed Data Parallel Training with Ray Train and PyTorch", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "1. When to use Ray Train", "2. Single GPU Training with PyTorch Lightning", "3. Distributed Training with Ray Train and PyTorch Lightning", "4. Ray Train in Production", "Introduction to Ray Tune", "1. Loading the data", "2. Starting out with vanilla PyTorch", "3. Hyperparameter tuning with Ray Tune", "4. Ray Tune in Production", "Introduction to Ray Data: Industry Landscape", "The Compute Layer", "The Orchestration Layer", "Distributed Computing Frameworks", "Data Processing with Ray Data", "Ray Serve", "Introduction to Ray Data: Ray Data + Structured Data", "0. What is Ray Data?", "2. Loading Data", "3. Transforming Data", "4. Writing Data", "5. Data Operations: Shuffling, Grouping and Aggregation", "6. When to use Ray Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "1. When to Consider Ray Data", "2. How to work with Ray Data", "3. Loading data", "3. Lazy execution mode", "4. Transforming data", "5. Stateful transformations with Ray Actors", "6. Materializing data", "7. Data Operations: grouping, aggregation, and shuffling", "8. Persisting data", "9. Ray Data in production", "Introduction to Ray Serve with PyTorch", "1. When to Consider Ray Serve", "2. Overview of Ray Serve", "3. Implement an image classification service", "4. Development workflow", "Introduction to Ray: Developer", "Batch Inference with Ray Data", "Data Processing with Ray Data", "Online Model Serving with Ray Serve", "Distributed training with Ray Train, PyTorch and Hugging Face", "Data Processing and ML examples with Ray", "Batch Inference with Ray Data", "Architecture", "Load a dataset", "Batch Inference Class", "Create a batch data and call the model", "Run inference on the entire dataset", "Data Processing with Ray Data", "Library Imports", "Convert to Ray Dataset", "Filter Ray Dataset", "Join Two Ray Datasets", "Preprocessing with a Tokenizer", "Online Model Serving with Ray Serve", "Architecture", "FastAPI webservice and deploy a model", "Simulate Client: Send test requests", "Distributed training with Ray Train, PyTorch and Hugging Face", "1. Architecture", "3. Metrics Setup", "4. Training function per worker", "5. Main Training Function", "6. Start Training", "Anyscale Administrator Overview", "Anyscale Administrator Overview", "1. What is an Anyscale Cloud?", "2. Cloud Deployment Types", "3. A Demonstrative Example of Resource Creation with AWS EC2", "3.1 IAM Role Definition", "4. Register Anyscale Cloud to Your Cloud Provider", "Deployment Options: Virtual Machines vs. Kubernetes", "Deployment Options: Virtual Machines vs. Kubernetes", "2. Virtual Machines (VM) vs. Kubernetes (K8s)", "3. (Optional) More Kubernetes Deployments Components", "Introduction: Deploy Anyscale Ray on AWS EC2 Instances", "Introduction: Deploy Anyscale Ray on AWS EC2 Instances", "1. Create Anyscale Resources with Terraform", "2. Register the Anyscale Cloud", "3. Test", "4. Cleanup", "5. Conclusion", "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)", "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)", "Prerequisites", "1. Installation", "2. Create Anyscale Resources with Terraform", "3. Register the Anyscale Cloud", "4. Test", "5. Cleanup", "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster", "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster", "1. Create Anyscale Resources with Terraform", "2. Install Kubernetes Components", "3. Register the Anyscale Cloud", "4. Install the Anyscale Operator", "5. Verify the Installation", "6. Test", "7. Clean up", "8. Conclusion", "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster", "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster", "Prerequisites", "1. Create Anyscale Resources with Terraform", "2. Attach Required IAM Policies to Your existing EKS\u2019s Node Role", "3. Install Kubernetes Components", "4. Register the Anyscale Cloud", "5. Install the Anyscale Operator", "6. Verify the Installation", "7. Test", "8. Troubleshooting", "9. Clean up", "10. Conclusion", "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster", "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster", "Prerequisites", "1. Installation", "2. Create Anyscale Resources with Terraform", "3. Troubleshooting GPU Availability", "4. kubectl Configuration", "5. Install NGINX Ingress Controller", "6. (Optional) Upgrade Anyscale Dependencies", "7. Register the Anyscale Cloud", "8. Install the Anyscale Operator", "8. Test", "9. Cleanup", "Welcome to Anyscale Administration", "Observability Introduction", "Ray and Anyscale Observability Introduction", "Ray and Anyscale Observability Introduction", "Ray Observability", "Anyscale Observability", "Example", "Ray and Anyscale Observability in Detail", "Ray and Anyscale Observability in Detail", "Data Pipeline Observability (Ray Data)", "Web Application Observability (Ray Serve)", "Multi-Actor Ray Serve Tracing Example", "Observability Introduction", "Observability Overview", "Setting Up Local Ray Observability", "101 \u2014 Introduction to Anyscale Workspaces", "101 \u2013 Developing Application with Anyscale", "101 \u2013 Compute Configs and Execution Environments in Anyscale", "101 \u2013 Storage Options in the Anyscale Platform", "101 \u2013 Debug and Monitor Your Anyscale Application", "101 \u2013 Introduction to Anyscale Jobs", "101 \u2013  Introduction to Anyscale Services", "101 \u2013 Collaboration on Anyscale", "101 - Anyscale Organization and Cloud Setup", "Content Used", "Sources", "Last Updated 6/19", "101 \u2014 Introduction to Anyscale Workspaces", "101 \u2013 Developing Application with Anyscale", "101 \u2013 Compute Configs and Execution Environments in Anyscale", "101 \u2013 Storage Options in the Anyscale Platform", "101 \u2013 Debug and Monitor Your Anyscale Application", "101 \u2013 Introduction to Anyscale Jobs", "101 \u2013  Introduction to Anyscale Services", "101 \u2013 Collaboration on Anyscale", "101 - Anyscale Organization and Cloud Setup", "\ud83d\udccc Overview of Structure", "\ud83e\udde0 Summary", "Content Used", "Part 1. Creating and Submitting your first job", "Part 2. Automation and Scheduling", "Sources", "Part 1: Starting your first Anyscale Service", "Content Used", "Sources", "Content Used", "Part 1. Creating and Submitting your first job", "Part 2. Automation and Scheduling", "Sources", "Part 1: Starting your first Anyscale Service", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "What is LLM Serving?", "Key Concepts and Optimizations", "Challenges in LLM Serving", "Ray Serve LLM + Anyscale Architecture", "Getting Started with Ray Serve LLM", "Key Takeaways", "Deploy a Medium-Sized LLM with Ray Serve LLM", "Deploy a Medium-Sized LLM with Ray Serve LLM", "Overview: Why Medium-Sized Models?", "Setting up Ray Serve LLM", "Local Deployment &amp; Inference", "Deploying to Anyscale Services", "Advanced Topics: Monitoring &amp; Optimization", "Summary &amp; Outlook", "Advanced LLM Features with Ray Serve LLM", "Advanced LLM Features with Ray Serve LLM", "Overview: Advanced Features Preview", "Example: Deploying LoRA Adapters", "Example: Getting Structured JSON Output", "Example: Setting up Tool Calling", "How to Choose an LLM?", "Conclusion: Next Steps", "Introduction to the Ray AI Libraries", "Introduction to Ray Train", "Intro to Ray Tune", "Intro to Ray Data", "Intro to Ray Serve", "Introduction to the Ray AI Libraries", "1. Overview of the Ray AI Libraries", "2. End-to-end example: predicting taxi tips in New York", "3. Running an experiment with Ray AI libraries", "Introduction to Ray Train", "1. PyTorch introductory example (single GPU)", "2. Distributed Data Parallel Training with Ray Train and PyTorch (multiple GPUs)", "3. Overview of the training loop in Ray Train", "4. Migrating the model and dataset to Ray Train", "5. Reporting checkpoints and metrics", "6. Launching the distributed training job", "7. Accessing the training results", "8. Ray Train in Production", "Intro to Ray Tune", "1. Loading and visualizing data", "2. Setting up a PyTorch model", "3. Introduction to Ray Tune", "4. Diving deeper into Ray Tune concepts", "5. Hyperparameter tuning the PyTorch model using Ray Tune", "Intro to Ray Data", "1. When to use Ray Data", "2. Loading Data", "3. Transforming Data", "4. Data Operations: Grouping, Aggregation, and Shuffling", "5. Persisting Data", "Intro to Ray Serve", "1. Overview of Ray Serve", "2. Implement an Classifier service", "3. Advanced features of Ray Serve", "4. Ray Serve in Production", "Clean up", "\ud83d\udcda 01 \u00b7 Introduction to Ray Train", "04a Computer-vision pattern with Ray Train", "04b Tabular workload pattern with Ray Train", "04c Time-Series workload pattern with Ray Train", "04-d1 Generative computer-vision pattern with Ray Train", "04-d2 Diffusion-Policy Pattern with Ray Train", "04e Recommendation system pattern with Ray Train", "\ud83d\udcda 01 \u00b7 Introduction to Ray Train", "01 \u00b7 Imports", "04 \u00b7 Define ResNet-18 Model for MNIST", "05 \u00b7 Define the Ray Train Loop (DDP per-worker)", "06 \u00b7 Define <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop_config</span></code>", "07 \u00b7 Configure Scaling with <code class=\"docutils literal notranslate\"><span class=\"pre\">ScalingConfig</span></code>", "08 \u00b7 Wrap the Model with <code class=\"docutils literal notranslate\"><span class=\"pre\">prepare_model()</span></code>", "09 \u00b7 Build the DataLoader with <code class=\"docutils literal notranslate\"><span class=\"pre\">prepare_data_loader()</span></code>", "10 \u00b7 Report Training Metrics", "11 \u00b7 Save Checkpoints and Report Metrics", "14 \u00b7 Create the <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code>", "16 \u00b7 Inspect the Training Results", "18 \u00b7 Load a Checkpoint for Inference", "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data", "01 \u00b7 Define Training Loop with Ray Data", "02 \u00b7 Build DataLoader from Ray Data", "03 \u00b7 Prepare Dataset for Ray Data", "05 \u00b7 Define Image Transformation", "07 \u00b7 Configure <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code> with Ray Data", "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train", "01 \u00b7 Modify Training Loop to Enable Checkpoint Loading", "02 \u00b7 Save Full Checkpoint with Extra State", "04 \u00b7 Launch Fault-Tolerant Training", "05 \u00b7 Manual Restoration from Checkpoints", "07 \u00b7 Clean Up Cluster Storage", "\ud83c\udf89 Wrapping Up &amp; Next Steps", "04a Computer-vision pattern with Ray Train", "1. Imports", "6. Custom <code class=\"docutils literal notranslate\"><span class=\"pre\">Food101Dataset</span></code> for Parquet", "10. Helper: Ray-prepared DataLoaders", "11. <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop_per_worker</span></code>", "12. Launch distributed training with <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code>", "13. Plot training and validation loss curves", "14. Demonstrate fault-tolerant resumption", "15. Batch inference with Ray Data", "04b Tabular workload pattern with Ray Train", "1. Imports", "8. Define the Ray Train worker loop (Arrow-based, memory-efficient)", "12. Confusion matrix visualization", "15. Continue training from the latest checkpoint", "04c Time-Series workload pattern with Ray Train", "1. Imports", "9. PositionalEncoding and Transformer model", "10. Ray Train training loop (with teacher forcing)", "13. Resume training from checkpoint", "14. Inference helper \u2014 Ray Data batch predictor on GPU", "04-d1 Generative computer-vision pattern with Ray Train", "1. Imports and setup", "8. Pixel diffusion LightningModule", "9. Ray Train <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop</span></code> (Lightning + Ray integration)", "12. Resume from latest checkpoint", "13. Reverse diffusion sampler", "04-d2 Diffusion-Policy Pattern with Ray Train", "1. Imports and setup", "4. DiffusionPolicy LightningModule", "5. Distributed Train loop with checkpointing", "8. Reverse diffusion helper", "04e Recommendation system pattern with Ray Train", "1. Imports", "7. Define matrix factorization model", "8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)", "11. Resume training from checkpoint", "12. Inference: recommend top-N items for a user", "Ray Enablement Content"], "titleterms": {"": [136, 140, 163, 176, 179, 191, 275, 276, 278, 315, 320, 332], "0": [1, 8, 13, 50, 100, 101, 273, 289], "01": [273, 280, 281, 292, 294, 300, 305, 343], "02": [273, 281, 293, 295, 301, 305, 343], "02_service_hello_world": [187, 204, 206, 211], "03": [273, 281, 296, 299, 301, 305, 343], "04": [273, 277, 278, 282, 296, 302, 326, 332], "04a": [274, 306], "04b": [275, 315], "04c": [276, 320], "04e": [279, 337], "05": [273, 283, 297, 303], "06": [273, 284, 297, 303], "07": [273, 285, 298, 304], "08": [273, 286, 298], "09": [273, 287], "1": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 16, 18, 21, 22, 24, 27, 28, 30, 31, 32, 34, 35, 36, 39, 50, 57, 61, 62, 64, 68, 72, 76, 95, 100, 102, 105, 107, 108, 109, 111, 113, 118, 121, 122, 126, 128, 129, 136, 139, 141, 149, 152, 153, 163, 173, 176, 180, 182, 183, 186, 187, 188, 192, 194, 195, 201, 202, 204, 205, 206, 208, 209, 211, 212, 213, 216, 217, 218, 219, 221, 223, 227, 229, 235, 237, 238, 239, 240, 241, 243, 247, 256, 262, 268, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 332, 333, 338], "10": [4, 32, 136, 148, 273, 274, 275, 276, 277, 278, 279, 288, 307, 309, 317, 323, 327, 329, 336, 340], "100k": [279, 338], "101": [177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 200, 203, 205, 206, 207, 210, 274, 277, 306, 307, 327, 343], "11": [273, 274, 275, 276, 277, 279, 289, 310, 317, 323, 329, 341], "12": [273, 274, 275, 276, 277, 279, 289, 311, 318, 323, 330, 342], "13": [273, 274, 275, 276, 277, 279, 289, 312, 318, 324, 331, 342], "14": [273, 274, 275, 276, 277, 279, 290, 313, 318, 325, 331, 342], "15": [273, 274, 275, 276, 277, 290, 314, 319, 325, 331], "16": [273, 274, 275, 276, 291, 314, 319, 325], "17": [273, 274, 275, 291, 314, 319], "18": [273, 282, 292], "19": [188, 273, 292], "2": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 15, 16, 19, 22, 24, 28, 31, 32, 35, 36, 40, 51, 58, 59, 61, 62, 64, 69, 72, 76, 95, 100, 103, 105, 107, 109, 111, 114, 118, 121, 122, 126, 129, 136, 140, 141, 149, 152, 153, 163, 173, 176, 180, 182, 186, 187, 188, 192, 194, 201, 202, 204, 205, 206, 208, 209, 211, 212, 213, 216, 217, 218, 219, 221, 227, 229, 235, 237, 238, 239, 240, 241, 244, 248, 257, 263, 269, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 332, 333, 338], "20": [273, 292], "201": 343, "3": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 15, 20, 22, 28, 31, 32, 35, 36, 41, 52, 59, 60, 64, 70, 72, 76, 96, 100, 104, 105, 107, 109, 110, 111, 115, 118, 123, 126, 129, 130, 136, 141, 149, 154, 163, 176, 180, 186, 187, 188, 192, 201, 202, 204, 205, 206, 208, 209, 211, 212, 213, 216, 217, 218, 219, 221, 223, 227, 229, 235, 237, 238, 239, 240, 241, 244, 245, 249, 258, 264, 270, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 332, 333, 338], "30": [276, 321], "4": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 16, 21, 28, 31, 32, 35, 36, 37, 42, 53, 61, 71, 72, 76, 97, 100, 105, 106, 107, 109, 110, 111, 116, 118, 124, 126, 129, 131, 136, 141, 142, 149, 155, 163, 176, 180, 186, 187, 192, 201, 204, 205, 206, 208, 211, 212, 213, 216, 217, 219, 221, 227, 229, 235, 238, 239, 240, 241, 250, 259, 265, 271, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 334, 338], "5": [2, 3, 4, 5, 8, 9, 11, 22, 28, 31, 32, 36, 54, 62, 72, 76, 98, 100, 105, 111, 117, 118, 125, 126, 132, 136, 143, 149, 156, 186, 187, 201, 204, 205, 206, 208, 211, 221, 227, 238, 239, 240, 251, 260, 266, 274, 275, 276, 277, 278, 279, 307, 316, 321, 327, 335, 338], "6": [2, 3, 4, 5, 8, 9, 11, 23, 28, 31, 32, 36, 55, 63, 72, 76, 99, 100, 105, 126, 133, 136, 144, 149, 157, 186, 188, 201, 205, 208, 238, 252, 274, 275, 276, 277, 278, 279, 308, 316, 321, 327, 335, 338], "7": [2, 4, 5, 8, 9, 11, 24, 32, 36, 55, 64, 72, 76, 99, 100, 105, 126, 134, 136, 145, 149, 158, 186, 201, 205, 208, 238, 253, 274, 275, 276, 277, 278, 279, 308, 316, 321, 327, 335, 339], "70b": [221, 223], "8": [2, 4, 8, 9, 11, 25, 32, 55, 65, 72, 76, 99, 100, 105, 126, 135, 136, 146, 149, 159, 160, 238, 254, 274, 275, 276, 277, 278, 279, 308, 317, 321, 323, 328, 336, 340], "9": [4, 9, 32, 66, 136, 147, 149, 161, 274, 275, 276, 277, 278, 279, 308, 317, 322, 329, 336, 340], "A": [100, 104, 126, 127, 149, 150], "For": [186, 202, 205, 209], "In": [7, 11, 43, 72, 186, 187, 201, 204, 205, 206, 208, 211], "It": [1, 11, 16, 72], "On": [7, 9, 47, 61], "The": [7, 43, 44, 45, 182, 186, 194, 200, 201, 205, 207, 208, 212, 213, 215], "These": [229, 231], "To": [107, 109], "about": [1, 2, 16, 21, 240, 263], "access": [4, 5, 32, 36, 107, 109, 238, 253], "accomplish": [221, 228, 229, 236], "action": [278, 332, 336], "activ": [4, 5, 11, 32, 36, 72, 238, 253], "actor": [2, 9, 17, 25, 62, 173, 240, 264, 273, 292], "ad": [0, 277, 326], "adapt": [229, 232], "add": [11, 72], "addit": [163, 176], "admin": 343, "administr": [100, 101, 162], "advanc": [2, 17, 221, 227, 229, 230, 231, 236, 241, 270], "after": [186, 201, 205, 208], "again": [186, 202, 205, 209], "aggreg": [8, 9, 54, 64, 240, 265], "ai": [3, 7, 26, 27, 44, 237, 242, 243, 244, 245], "alert": [169, 172], "align": [229, 235], "all": [1, 8, 9, 16, 54, 64, 186, 202, 205, 209, 276, 325], "alloc": [2, 22], "also": [186, 201, 205, 208], "altern": [212, 213, 216], "an": [2, 3, 10, 18, 26, 28, 70, 100, 102, 136, 137, 179, 186, 191, 201, 205, 208, 221, 226, 229, 235, 237, 241, 244, 245, 269, 278, 336], "annot": [239, 259], "anti": [1, 16], "anyscal": [100, 101, 102, 105, 106, 107, 108, 109, 111, 112, 113, 114, 118, 119, 122, 123, 126, 127, 128, 130, 131, 136, 137, 139, 142, 143, 149, 150, 153, 157, 158, 159, 162, 164, 165, 167, 169, 170, 172, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 218, 221, 226, 274, 275, 276, 277, 278, 279, 306, 315, 320, 326, 332, 337, 343], "apach": [7, 43], "api": [118, 121, 149, 152, 173], "appli": [273, 297], "applic": [7, 10, 46, 69, 169, 172, 178, 181, 188, 190, 193], "approach": [229, 231], "ar": [100, 103, 274, 275, 276, 277, 278, 279, 306, 315, 320, 326, 332, 337], "architectur": [73, 75, 76, 79, 91, 95, 100, 105, 173, 212, 213, 218], "area": [238, 253], "argument": [2, 18, 186, 202, 205, 209], "arm": [11, 72], "arrow": [7, 43, 275, 317], "artifact": [276, 325], "assist": [229, 232, 234], "assumpt": [163, 176], "attach": [136, 140], "auroc": [238, 253], "authent": [118, 121, 149, 152], "auto": [187, 204, 206, 211], "autom": [182, 186, 194, 202, 205, 209], "automat": [273, 301], "autosc": [9, 62, 241, 270], "autoscal": [126, 129, 136, 141], "avail": [2, 22, 149, 154, 173, 186, 202, 205, 209], "aw": [100, 104, 111, 112, 126, 127, 129, 136, 137, 141], "awai": [273, 274, 275, 276, 277, 278, 279, 280, 293, 299, 306, 315, 320, 326, 332, 337], "backend": [107, 109], "balanc": [126, 129, 136, 141, 275, 316], "base": [8, 9, 54, 64, 240, 265, 275, 279, 317, 337], "basic": [186, 201, 205, 208], "batch": [2, 3, 7, 8, 9, 24, 28, 46, 54, 64, 73, 78, 81, 82, 212, 213, 216, 240, 265, 274, 275, 276, 308, 314, 316, 318, 321, 325], "below": [186, 187, 201, 204, 205, 206, 208, 211], "benchmark": [229, 235], "benefit": [187, 203, 206, 210, 229, 232, 233, 234], "best": [277, 331], "block": [8, 9, 54, 59, 64, 240, 265], "book": 0, "bound": [2, 22], "breakdown": [221, 224], "build": [0, 4, 31, 238, 247, 273, 275, 287, 295, 317], "cach": [212, 213, 216], "california": [275, 316], "call": [1, 16, 73, 82, 229, 234], "can": [186, 201, 205, 208, 274, 275, 276, 277, 278, 279, 314, 319, 325, 331, 336, 342], "car": [229, 233], "case": [11, 72, 229, 235], "caus": [2, 24], "chain": [2, 19], "challeng": [7, 46, 212, 213, 217], "characterist": [183, 195], "check": [186, 187, 201, 202, 204, 205, 206, 208, 209, 211, 274, 276, 277, 307, 321, 327], "checkout": [187, 204, 206, 211], "checkpoint": [4, 5, 31, 32, 36, 238, 247, 251, 273, 275, 276, 277, 278, 279, 289, 292, 300, 301, 303, 319, 324, 330, 331, 335, 340, 341], "choos": [221, 223, 229, 235], "class": [73, 81, 275, 316], "classif": [10, 70, 274, 275, 306, 315], "classifi": [241, 269], "clean": [3, 11, 28, 72, 126, 134, 136, 147, 181, 182, 183, 193, 194, 195, 241, 272, 273, 274, 275, 277, 278, 279, 292, 304, 314, 319, 331, 336, 342], "cleanup": [111, 116, 118, 125, 149, 161, 186, 201, 205, 208, 276, 325], "cli": [186, 188, 202, 205, 209], "client": [75, 93], "clone": [184, 187, 188, 196, 204, 206, 211], "cloud": [100, 102, 103, 106, 111, 114, 118, 121, 123, 126, 130, 136, 142, 149, 152, 158, 180, 185, 192, 197, 198], "cluster": [2, 22, 73, 75, 76, 83, 93, 99, 126, 127, 129, 136, 137, 141, 149, 150, 163, 176, 180, 192, 273, 304], "code": [3, 28, 186, 187, 201, 204, 205, 206, 208, 211, 229, 232], "collabor": [184, 188, 196], "command": [118, 122, 186, 187, 201, 204, 205, 206, 208, 211], "comparison": [221, 223], "compon": [107, 110, 126, 129, 136, 141, 221, 224], "compos": [241, 270], "comput": [7, 11, 44, 46, 72, 118, 119, 179, 191, 238, 253, 274, 277, 306, 326], "concept": [4, 5, 6, 32, 36, 41, 212, 213, 216, 239, 259], "conclus": [111, 117, 126, 135, 136, 148, 229, 236], "concurr": [9, 61, 221, 227], "conda": [11, 72], "config": [179, 191], "configur": [2, 4, 5, 22, 32, 36, 118, 121, 149, 152, 155, 169, 172, 173, 179, 187, 191, 204, 206, 211, 212, 213, 219, 221, 224, 226, 229, 232, 238, 249, 252, 273, 275, 285, 289, 298, 301, 317], "confus": [275, 318], "consid": [9, 10, 57, 68], "consider": [212, 213, 216, 229, 235], "consol": [186, 201, 205, 208], "contain": [179, 187, 191, 204, 206, 211], "content": [0, 186, 200, 205, 207, 343], "context": [212, 213, 216, 229, 235], "continu": [212, 213, 216, 275, 319], "control": [100, 105, 107, 109, 126, 129, 136, 141, 149, 156], "convert": [74, 86, 89], "core": [1, 2, 7, 12, 17, 47], "cost": [212, 213, 217, 229, 235], "count": [276, 321], "cours": [0, 11, 72, 273, 305], "cover": [182, 183, 194, 195, 229, 231, 275, 315, 316], "cpu": [275, 318], "creat": [1, 4, 5, 11, 14, 31, 35, 36, 72, 73, 74, 82, 86, 100, 105, 111, 113, 118, 122, 126, 128, 136, 139, 149, 153, 179, 182, 186, 191, 194, 201, 202, 205, 208, 209, 273, 279, 290, 338], "creation": [100, 104], "cursor": [178, 190], "curv": [238, 253, 274, 277, 279, 312, 329, 340], "custom": [0, 8, 9, 54, 64, 100, 105, 240, 241, 265, 270, 274, 308], "d1": [277, 326], "d2": [278, 332], "dashboard": [169, 171, 181, 193], "data": [2, 3, 4, 5, 6, 7, 8, 9, 19, 24, 26, 28, 32, 36, 39, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 63, 64, 65, 66, 73, 74, 77, 78, 82, 84, 107, 109, 169, 171, 238, 239, 240, 247, 248, 256, 261, 262, 263, 264, 265, 266, 273, 274, 275, 276, 277, 279, 280, 289, 293, 294, 295, 296, 297, 298, 305, 314, 318, 325, 327, 338], "databas": [7, 43], "datafram": [74, 89, 273, 291], "dataload": [4, 5, 31, 35, 76, 97, 273, 274, 276, 287, 295, 308, 309, 321], "dataset": [4, 8, 31, 32, 51, 73, 74, 80, 83, 85, 86, 87, 88, 237, 238, 240, 244, 250, 263, 273, 275, 276, 278, 279, 281, 296, 316, 321, 332, 333, 338], "ddp": [273, 280, 283], "de": [277, 326], "debug": [181, 193], "decod": [212, 213, 215, 277, 327], "deeper": [6, 41, 239, 259], "default": [187, 204, 206, 211, 239, 259], "defin": [5, 35, 100, 103, 238, 247, 273, 275, 279, 282, 283, 284, 294, 297, 317, 339, 340], "definit": [100, 105], "demand": [212, 213, 217, 276, 320], "demonstr": [100, 104, 274, 313], "depend": [2, 11, 21, 72, 107, 109, 149, 157], "deploi": [73, 75, 82, 92, 107, 108, 110, 111, 112, 118, 119, 126, 127, 136, 137, 149, 150, 153, 187, 188, 203, 204, 206, 210, 211, 221, 222, 226, 229, 232], "deploy": [10, 69, 75, 92, 100, 103, 107, 108, 110, 162, 173, 212, 213, 219, 221, 225, 241, 268, 270], "descript": [229, 233], "detail": [169, 170], "develop": [10, 11, 71, 72, 173, 178, 187, 188, 190, 204, 206, 211, 343], "devic": [126, 129, 136, 141], "diagnost": [275, 318], "differ": [221, 228], "diffus": [5, 35, 277, 278, 326, 328, 331, 332, 336], "diffusionpolici": [278, 334], "digit": [273, 281], "directori": [238, 247], "disabl": 0, "displai": [186, 201, 205, 208, 277, 331], "distribut": [3, 4, 5, 7, 28, 32, 36, 46, 76, 77, 94, 238, 248, 252, 273, 274, 275, 276, 277, 278, 279, 280, 289, 306, 311, 315, 317, 320, 325, 326, 329, 335, 337, 340], "dive": [6, 41, 239, 259], "document": [186, 202, 205, 209], "doe": [274, 306], "domain": [229, 235], "down": [11, 72, 221, 225, 226], "download": [187, 204, 206, 211, 273, 281], "dual": [100, 105], "duplic": [184, 196], "each": [107, 109], "ec2": [100, 104, 111, 112], "editor": [186, 201, 205, 208], "ef": [100, 105], "effici": [275, 317], "ek": [126, 127, 136, 137, 140], "els": [187, 204, 206, 211], "embed": [279, 337], "enabl": [0, 118, 121, 149, 152, 221, 227, 273, 300, 343], "encod": [274, 277, 279, 307, 327, 338], "end": [3, 28, 237, 244, 273, 292], "endpoint": [173, 187, 204, 206, 211], "engin": [7, 44, 118, 119, 149, 150, 212, 213, 218], "ensembl": [3, 28], "enter": [187, 204, 206, 211], "entir": [73, 83], "environ": [2, 11, 21, 72, 179, 187, 191, 204, 206, 211, 278, 332], "error": [73, 83], "evalu": [275, 317], "everyth": [187, 204, 206, 211], "exampl": [0, 3, 11, 26, 28, 72, 77, 100, 104, 107, 110, 162, 164, 168, 173, 186, 187, 201, 204, 205, 206, 208, 211, 221, 223, 228, 229, 232, 233, 234, 237, 238, 244, 247, 343], "execut": [0, 1, 8, 9, 15, 52, 60, 107, 109, 179, 186, 191, 201, 205, 208, 240, 264], "exercis": [6, 41, 239, 259], "exist": [100, 105, 136, 137, 140], "expect": [229, 233], "experi": [237, 239, 244, 245, 259], "explor": [181, 193], "extern": [107, 109], "extra": [273, 301], "face": [76, 94], "factor": [279, 337, 339], "failur": [2, 24], "failureconfig": [273, 301], "fastapi": [75, 90, 92, 241, 270], "fault": [273, 274, 299, 302, 305, 313], "featur": [0, 8, 55, 186, 200, 205, 207, 229, 230, 231, 237, 241, 244, 268, 270, 275, 318], "fetch": [2, 24], "file": [8, 9, 54, 64, 180, 186, 187, 192, 201, 204, 205, 206, 208, 211, 221, 226, 240, 265, 275, 316], "filter": [74, 87], "find": [186, 201, 205, 208], "first": [182, 183, 186, 187, 194, 195, 200, 201, 204, 205, 206, 207, 208, 211], "fit": [5, 36, 273, 290], "flask": [75, 90], "flow": [7, 48, 173], "folder": [186, 202, 205, 209], "follow": [182, 183, 186, 187, 194, 195, 200, 201, 204, 205, 206, 207, 208, 211], "food": [274, 277, 306, 307, 327], "food101dataset": [274, 308], "forc": [276, 323], "forecast": [276, 320], "forest": [275, 315], "format": [7, 43], "forward": [277, 326], "foundat": [212, 213, 214], "fraction": [2, 22, 241, 270], "framework": [7, 46, 229, 235], "from": [186, 201, 205, 208, 273, 275, 276, 277, 278, 279, 295, 303, 319, 324, 330, 331, 336, 338, 341], "full": [186, 202, 205, 209, 273, 301], "function": [1, 7, 14, 15, 44, 76, 97, 98, 100, 102], "gce": [118, 119], "gcp": [118, 119], "gener": [0, 4, 5, 7, 31, 32, 36, 46, 212, 213, 215, 277, 278, 326, 331, 333], "get": [1, 2, 6, 12, 15, 16, 24, 41, 173, 188, 212, 213, 219, 229, 233, 239, 258], "gettingstart": 343, "github": [187, 204, 206, 211], "give": [187, 204, 206, 211], "gke": [149, 150], "global": [8, 9, 54, 64, 240, 265], "go": [273, 305], "googl": [118, 121, 149, 150, 152], "gpu": [4, 5, 31, 32, 35, 36, 149, 154, 238, 241, 247, 248, 249, 270, 276, 323, 325], "grafana": [163, 176], "group": [8, 9, 54, 64, 100, 105, 240, 265], "groupbi": [8, 9, 54, 64, 240, 265], "guid": [187, 203, 206, 210], "handl": [186, 201, 205, 208], "hardwar": [221, 227, 229, 235], "harm": [1, 16], "head": [163, 176, 186, 187, 201, 204, 205, 206, 208, 211], "hello_world": [186, 201, 205, 208], "helper": [274, 276, 278, 309, 325, 336], "hourli": [276, 321], "how": [0, 8, 9, 50, 58, 100, 103, 107, 109, 221, 228, 229, 235, 237, 244, 273, 274, 275, 276, 277, 278, 279, 280, 306, 315, 320, 326, 332, 337], "hug": [76, 94], "huggingfac": 77, "hyperparamet": [3, 6, 28, 41, 239, 260], "i": [7, 8, 47, 48, 50, 75, 90, 100, 102, 186, 201, 205, 208, 212, 213, 215, 221, 226], "iam": [100, 105, 136, 140], "id": [1, 16, 100, 105, 178, 190, 279, 338, 342], "imag": [10, 70, 179, 191, 273, 274, 277, 297, 306, 307, 308, 326, 327], "implement": [3, 10, 26, 70, 241, 269], "import": [73, 74, 75, 76, 79, 85, 91, 95, 273, 274, 275, 276, 277, 278, 279, 281, 307, 316, 318, 321, 327, 333, 338], "improv": [221, 227], "industri": [7, 43], "infer": [3, 28, 73, 78, 81, 83, 212, 213, 215, 218, 221, 225, 226, 273, 274, 275, 276, 279, 292, 314, 318, 319, 325, 337, 342], "infrastructur": [100, 103, 107, 110, 149, 153, 212, 213, 218], "ingress": [126, 129, 136, 141, 149, 156], "initi": [74, 85], "input": [274, 277, 279, 306, 326, 337], "inspect": [187, 204, 206, 211, 237, 244, 273, 274, 275, 276, 291, 308, 316, 321], "instal": [0, 11, 72, 77, 118, 121, 126, 129, 131, 132, 136, 141, 143, 144, 149, 152, 156, 159, 163, 173, 176, 188], "instanc": [75, 93, 100, 105, 111, 112, 118, 119], "instruct": [182, 186, 194, 200, 205, 207], "integr": [241, 270, 273, 277, 293, 305, 329], "intro": [6, 9, 41, 56, 239, 240, 241, 255, 261, 267, 343], "introduct": [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 17, 26, 29, 33, 38, 43, 49, 67, 72, 111, 112, 118, 119, 126, 127, 136, 137, 149, 150, 163, 164, 165, 174, 177, 178, 179, 180, 181, 182, 183, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 200, 203, 205, 206, 207, 210, 212, 213, 214, 237, 238, 239, 242, 246, 258, 273, 280, 292, 305], "introductori": [238, 247], "invert": [278, 332], "io": [2, 22], "irvin": [275, 316], "item": [279, 337, 338, 342], "job": [4, 32, 182, 186, 188, 194, 200, 201, 202, 205, 207, 208, 209, 238, 252], "join": [74, 88, 279, 342], "json": [229, 233], "jupyt": [0, 11, 72], "just": [75, 90], "jvm": [7, 46], "k8": [107, 109], "kei": [4, 5, 32, 36, 100, 102, 183, 186, 195, 200, 205, 207, 212, 213, 216, 220, 221, 224, 228, 229, 232, 233, 234, 236, 238, 241, 249, 268], "kubectl": [149, 155], "kubernet": [107, 108, 109, 110, 126, 129, 136, 141, 149, 150, 212, 213, 217], "kv": [212, 213, 216], "label": [274, 306], "lake": [7, 43], "lakehous": [7, 43], "landscap": [7, 43], "languag": [212, 213, 214], "larg": [2, 22, 212, 213, 214], "last": [188, 273, 303], "latenc": [212, 213, 217], "latest": [275, 277, 319, 330], "launch": [4, 11, 32, 72, 163, 169, 172, 176, 177, 189, 221, 225, 226, 238, 252, 273, 274, 276, 277, 278, 279, 290, 298, 302, 311, 323, 329, 335, 340], "layer": [7, 43, 44, 45, 107, 109], "lazi": [9, 60], "learn": [7, 44, 162, 188, 229, 231, 232, 233, 234, 273, 274, 275, 276, 277, 278, 279, 280, 293, 299, 306, 315, 320, 326, 332, 337], "leav": [187, 204, 206, 211], "legend": [11, 72], "level": [2, 18], "librari": [3, 26, 27, 73, 74, 75, 76, 79, 85, 91, 95, 237, 242, 243, 244, 245], "lifecycl": [4, 32, 187, 203, 206, 210, 273, 289], "lightn": [5, 33, 35, 36, 277, 329], "lightningmodul": [277, 278, 328, 334], "limit": [9, 61], "list": [186, 202, 205, 209], "lite": [274, 306], "ll": [162, 229, 231, 273, 280, 293, 299], "llama": [221, 223], "llm": [212, 213, 214, 215, 217, 218, 219, 221, 222, 224, 227, 229, 230, 232, 235, 343], "load": [4, 5, 6, 8, 9, 31, 36, 39, 51, 59, 73, 74, 80, 85, 126, 129, 136, 141, 238, 239, 240, 247, 256, 263, 273, 274, 275, 276, 277, 279, 292, 296, 300, 307, 316, 321, 327, 338], "loader": [238, 247], "local": [0, 11, 72, 163, 173, 176, 178, 180, 190, 192, 221, 225, 238, 247], "log": [169, 171, 172, 181, 186, 193, 201, 205, 208], "loop": [1, 4, 5, 16, 31, 32, 35, 238, 247, 249, 253, 273, 275, 276, 278, 279, 283, 294, 300, 317, 323, 335, 340], "lora": [229, 232], "loss": [274, 276, 277, 278, 279, 312, 323, 329, 335, 340], "mac": [11, 72], "machin": [7, 44, 107, 108, 109], "main": [76, 98, 186, 187, 201, 204, 205, 206, 208, 211], "manag": [2, 11, 22, 72, 107, 109, 187, 203, 206, 210, 212, 213, 217, 279, 340], "mani": [2, 24], "manual": [273, 303], "materi": [9, 63, 240, 264], "matrix": [275, 279, 318, 337, 339], "matter": [229, 231, 233, 234], "max_model_len": [221, 227], "medium": [221, 222, 223, 224], "memori": [7, 43, 73, 83, 212, 213, 217, 275, 317], "memorydb": [100, 105], "metric": [4, 31, 32, 76, 96, 169, 171, 172, 181, 193, 238, 247, 251, 273, 279, 288, 289, 291, 340], "migrat": [4, 5, 32, 36, 238, 250, 274, 275, 276, 277, 279, 306, 315, 320, 326, 337], "min": [276, 321], "mini": [275, 316], "miniforg": [11, 72], "ml": 77, "mnist": [238, 247, 273, 281, 282], "mode": [8, 9, 52, 60, 240, 264], "model": [3, 4, 5, 6, 26, 28, 31, 32, 35, 36, 41, 73, 75, 82, 90, 92, 212, 213, 214, 216, 221, 223, 224, 227, 229, 235, 238, 239, 247, 250, 257, 260, 273, 274, 275, 276, 279, 282, 286, 306, 317, 322, 337, 339], "modifi": [273, 300], "modul": [273, 292, 305], "monitor": [181, 193, 221, 227], "more": [4, 5, 32, 36, 107, 110, 221, 227, 229, 232, 233, 234, 236, 240, 263], "movi": [279, 342], "movielen": [279, 338], "multi": [173, 276, 320], "multipl": [238, 248], "n": [279, 342], "name": [186, 187, 201, 204, 205, 206, 208, 211], "navig": [0, 186, 201, 205, 208], "need": [107, 109, 187, 204, 206, 211], "nest": [2, 23], "new": [0, 11, 72, 100, 105, 126, 127, 149, 150, 173, 186, 201, 205, 208, 237, 244, 277, 326], "next": [163, 176, 186, 201, 205, 208, 212, 213, 220, 221, 228, 229, 236, 273, 274, 275, 276, 277, 278, 279, 305, 314, 319, 325, 331, 336, 342], "nginx": [126, 129, 136, 141, 149, 156], "node": [136, 140, 163, 176, 179, 187, 191, 204, 206, 211, 276, 320], "nois": [277, 326], "normal": [276, 278, 321, 333], "note": [1, 2, 4, 5, 9, 16, 21, 22, 24, 32, 36, 59, 62, 273, 289], "notebook": [0, 11, 72, 74, 84, 178, 182, 183, 186, 190, 194, 195, 202, 205, 209, 237, 242], "now": [163, 176, 186, 201, 205, 208], "nvidia": [126, 129, 136, 141], "nyc": [237, 244, 276, 320, 321], "o": [11, 72], "object": [2, 17, 18, 24, 107, 109, 180, 188, 192, 277, 278, 279, 326, 332, 337], "observ": [163, 164, 165, 166, 167, 169, 170, 171, 172, 174, 175, 176, 343], "onc": [2, 24, 186, 201, 205, 208], "one": [276, 321], "onli": [169, 172, 273, 289], "onlin": [75, 90], "open": [186, 201, 205, 208], "oper": [8, 9, 54, 64, 107, 108, 126, 131, 136, 143, 149, 159, 240, 265], "optim": [212, 213, 216, 217, 221, 227], "option": [11, 72, 100, 105, 107, 108, 110, 126, 129, 136, 141, 149, 157, 163, 176, 180, 188, 192, 275, 316], "orchestr": [7, 45, 212, 213, 218], "order": [8, 9, 54, 64, 240, 265], "organ": [185, 197, 198], "other": [221, 228], "our": [186, 201, 205, 208, 221, 223, 238, 247], "out": [6, 40, 73, 83, 186, 201, 202, 205, 208, 209], "outlin": [73, 74, 75, 76, 78, 84, 90, 94], "outlook": [100, 101, 107, 110, 221, 228, 240, 266], "output": [0, 229, 233], "over": [7, 47, 186, 187, 201, 204, 205, 206, 208, 211, 276, 321], "overview": [1, 3, 4, 5, 10, 11, 13, 27, 31, 32, 35, 69, 72, 100, 101, 163, 173, 175, 185, 188, 198, 221, 223, 229, 231, 237, 238, 241, 243, 249, 268], "packag": [163, 176], "panda": [74, 89], "parallel": [1, 4, 5, 16, 32, 36, 212, 213, 216, 221, 227, 238, 248, 273, 280, 289], "parquet": [274, 275, 276, 277, 279, 307, 308, 316, 321, 327, 338], "part": [182, 183, 186, 187, 194, 195, 201, 202, 204, 205, 206, 208, 209, 211], "pass": [2, 18, 19, 187, 204, 206, 211], "passeng": [276, 321], "past": [186, 201, 205, 208], "path": 188, "pattern": [1, 2, 16, 18, 24, 274, 275, 276, 277, 278, 279, 306, 315, 320, 326, 332, 337], "pendulum": [278, 332, 333], "per": [76, 97, 273, 279, 283, 337], "persist": [9, 65, 238, 240, 252, 266, 273, 274, 277, 289, 307, 327], "phase": [212, 213, 215], "pip": [2, 21], "pipelin": [2, 24, 169, 171, 188, 221, 227], "pixel": [277, 328], "plane": [100, 105], "platform": [180, 192], "plot": [274, 276, 277, 278, 279, 312, 323, 329, 335, 340], "plugin": [126, 129, 136, 141], "point": [238, 249, 279, 338], "polici": [136, 140, 277, 278, 326, 332, 336], "positionalencod": [276, 322], "post": [275, 319], "practic": [229, 235], "predict": [4, 5, 31, 32, 36, 237, 244, 273, 292], "predictor": [276, 325], "prefil": [212, 213, 215], "prepar": [273, 274, 276, 296, 309, 321], "prepare_data_load": [273, 287], "prepare_model": [273, 286], "preprocess": [74, 89], "prerequisit": [11, 72, 111, 112, 118, 120, 126, 127, 136, 138, 149, 151, 162, 163, 169, 170, 173, 176, 221, 225], "preview": [229, 231], "problem": [274, 275, 276, 277, 278, 279, 306, 315, 320, 326, 332, 337], "process": [2, 7, 24, 46, 47, 74, 77, 84, 212, 213, 215, 229, 235, 277, 326], "product": [4, 5, 6, 8, 9, 32, 37, 42, 55, 66, 238, 240, 241, 254, 266, 271], "profil": 173, "project": [184, 185, 196, 198], "prometheu": [163, 176], "provid": [100, 106, 126, 127], "publish": [0, 187, 204, 206, 211], "purpos": [7, 46, 100, 102], "put": [1, 16], "py": [186, 187, 201, 204, 205, 206, 208, 211], "python": [186, 201, 202, 205, 208, 209], "pytorch": [4, 5, 6, 10, 29, 31, 32, 33, 35, 36, 40, 41, 67, 76, 77, 94, 238, 239, 247, 248, 257, 260, 276, 321], "qualiti": [229, 235], "quantiz": [221, 227], "queri": [212, 213, 219], "quick": [3, 28, 276, 321], "rai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 16, 17, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 36, 37, 38, 41, 42, 43, 47, 48, 49, 50, 55, 56, 57, 58, 62, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 99, 107, 109, 111, 112, 118, 119, 126, 127, 136, 137, 149, 150, 163, 164, 165, 166, 169, 170, 171, 172, 173, 176, 181, 186, 188, 193, 201, 205, 208, 212, 213, 214, 218, 219, 221, 222, 224, 225, 229, 230, 232, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 254, 255, 258, 259, 260, 261, 262, 266, 267, 268, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 283, 289, 292, 293, 294, 295, 296, 297, 298, 299, 305, 306, 309, 314, 315, 316, 317, 318, 320, 321, 323, 325, 326, 327, 329, 332, 335, 337, 338, 340, 343], "random": [276, 321], "rank": [273, 279, 289, 337], "rate": [279, 337, 338], "read": [8, 9, 54, 64, 240, 265], "real": [278, 333], "recap": [237, 244, 245], "recommend": [11, 72, 188, 229, 235, 279, 337, 342], "reduc": [221, 227], "regist": [100, 106, 111, 114, 118, 123, 126, 130, 136, 142, 149, 158, 173], "registr": 173, "regress": [3, 26], "relat": [221, 223, 228], "remot": [1, 4, 14, 15, 32], "remov": [276, 325], "replica": [10, 69, 221, 227], "report": [4, 32, 238, 247, 251, 273, 288, 289], "repositori": [187, 188, 204, 206, 211], "request": [2, 22, 75, 93, 173, 221, 225], "requir": [11, 72, 118, 121, 126, 127, 136, 140, 149, 152, 163, 176, 212, 213, 217, 229, 235], "resampl": [276, 321], "resiz": [274, 277, 307, 327], "resnet": [273, 282], "resourc": [2, 9, 22, 61, 62, 100, 103, 104, 111, 113, 118, 122, 126, 128, 136, 139, 149, 153, 184, 196, 212, 213, 220, 221, 228, 229, 236], "restor": [273, 303], "result": [1, 2, 4, 5, 15, 24, 32, 36, 238, 253, 273, 276, 291, 325], "resum": [273, 276, 277, 279, 303, 324, 330, 341], "resumpt": [274, 313], "retri": [2, 20, 273, 301], "retriev": 173, "revers": [277, 278, 326, 331, 332, 336], "roc": [238, 253], "role": [100, 105, 136, 140, 185, 198], "row": [8, 9, 54, 64, 240, 265], "run": [4, 5, 31, 32, 36, 73, 83, 107, 109, 118, 122, 169, 171, 182, 186, 187, 194, 200, 201, 204, 205, 206, 207, 208, 211, 221, 226, 237, 242, 244, 245, 273, 274, 276, 292, 314, 325], "runconfig": [273, 289], "runtim": [2, 21], "s3": [100, 105], "same": [186, 202, 205, 209], "sampl": [173, 273, 277, 278, 281, 326, 331, 332, 336], "sampler": [277, 331], "saniti": [274, 276, 277, 307, 321, 327], "save": [238, 247, 273, 289, 301], "scalabl": [212, 213, 217], "scale": [4, 5, 32, 36, 73, 75, 82, 92, 221, 227, 238, 249, 273, 278, 285, 332], "scalingconfig": [273, 285], "schedul": [182, 186, 194, 202, 205, 209, 238, 247], "script": [186, 201, 205, 208], "sdk": [186, 202, 205, 209], "second": [74, 86], "secur": [100, 105], "select": [187, 204, 206, 211, 229, 235], "send": [75, 93, 221, 225], "sequenc": [276, 320], "seri": [276, 320], "serv": [0, 3, 7, 10, 26, 28, 48, 67, 68, 69, 75, 90, 93, 169, 172, 173, 212, 213, 214, 215, 217, 218, 219, 221, 222, 224, 225, 229, 230, 232, 241, 267, 268, 270, 271, 343], "server": [11, 72], "servic": [10, 70, 183, 186, 187, 188, 195, 200, 203, 204, 205, 206, 207, 210, 211, 221, 226, 241, 269], "set": [11, 72, 163, 176, 221, 224, 226, 229, 234, 239, 257, 259], "setup": [76, 96, 163, 173, 176, 185, 197, 274, 275, 276, 277, 278, 279, 306, 315, 320, 326, 327, 333, 337], "share": [180, 192, 277, 279, 331, 342], "shuffl": [8, 9, 54, 64, 240, 265, 277, 327], "shut": [11, 72, 221, 225, 226], "shutdown": [73, 74, 75, 76, 83, 89, 93, 99, 213, 219], "sign": 188, "simpl": [11, 72, 169, 171], "simul": [75, 93], "singl": [4, 5, 31, 35, 238, 247], "size": [221, 222, 223, 224, 228, 275, 316], "slide": [276, 321], "solv": [274, 275, 276, 277, 278, 279, 306, 315, 320, 326, 332, 337], "sourc": [187, 203, 206, 210], "spark": [7, 47], "specif": [1, 9, 16, 61, 62, 186, 201, 205, 208], "spin": [187, 204, 206, 211], "split": [0, 274, 275, 277, 278, 279, 308, 316, 327, 333, 338], "stabl": [5, 35], "start": [1, 6, 12, 40, 41, 76, 99, 163, 176, 183, 187, 188, 195, 204, 206, 211, 212, 213, 219, 239, 258, 275, 317], "starter": [187, 204, 206, 211], "state": [9, 62, 240, 264, 273, 278, 301, 332], "statu": [186, 201, 205, 208], "step": [163, 176, 212, 213, 219, 220, 221, 228, 229, 236, 237, 244, 245, 273, 274, 275, 276, 277, 278, 279, 305, 314, 319, 325, 331, 336, 342], "storag": [4, 32, 107, 109, 180, 192, 238, 252, 273, 277, 279, 289, 304, 331, 342], "store": [2, 17, 18, 107, 109, 180, 192], "strategi": [221, 227], "stream": [7, 46], "structur": [7, 8, 43, 49, 162, 173, 185, 198, 229, 233], "style": [278, 332], "submit": [182, 186, 194, 201, 202, 205, 208, 209], "subnet": [100, 105], "successfulli": [186, 201, 205, 208], "summari": [73, 74, 75, 76, 83, 89, 93, 99, 100, 105, 185, 199, 221, 228], "support": [100, 103], "system": [279, 337], "tab": [181, 186, 187, 193, 201, 204, 205, 206, 208, 211], "tabl": [239, 259], "tabular": [275, 315], "take": [273, 274, 275, 276, 277, 278, 279, 280, 293, 299, 306, 314, 315, 319, 320, 325, 326, 331, 332, 336, 337, 342], "takeawai": [212, 213, 220, 221, 228, 229, 236], "task": [2, 17, 19, 20, 21, 22, 23, 229, 235, 237, 244], "taxi": [237, 244, 276, 320, 321], "teacher": [276, 323], "team": 188, "templat": [187, 204, 206, 211, 221, 228], "tensor": [277, 326], "termin": [163, 176, 186, 187, 201, 204, 205, 206, 208, 211], "terraform": [111, 113, 118, 122, 126, 128, 136, 139, 149, 153], "test": [0, 75, 93, 111, 115, 118, 124, 126, 133, 136, 145, 149, 160, 179, 191], "text": [212, 213, 215], "tfvar": [118, 122, 149, 153], "thi": [11, 72, 182, 183, 187, 194, 195, 203, 204, 206, 210, 211, 237, 242, 244, 274, 275, 276, 277, 278, 279, 306, 314, 315, 319, 320, 325, 326, 331, 332, 336, 337, 342], "through": [182, 186, 187, 194, 200, 203, 205, 206, 207, 210], "time": [276, 320], "tip": [237, 244], "titl": [279, 342], "togeth": [1, 16], "token": [74, 76, 89, 97], "toler": [273, 274, 299, 302, 305, 313], "too": [2, 24], "tool": [229, 234], "top": [2, 18, 279, 342], "topic": [221, 227, 229, 236], "torch": [5, 35], "torchtrain": [5, 36, 238, 252, 273, 274, 277, 278, 290, 298, 311, 329, 335], "trace": [169, 172, 173], "track": [186, 201, 205, 208], "train": [3, 4, 5, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 76, 77, 94, 97, 98, 99, 238, 246, 247, 248, 249, 250, 252, 253, 254, 273, 274, 275, 276, 277, 278, 279, 280, 283, 288, 289, 290, 291, 292, 293, 294, 298, 299, 300, 302, 303, 305, 306, 308, 311, 312, 315, 316, 317, 319, 320, 323, 324, 325, 326, 327, 329, 332, 335, 336, 337, 338, 340, 341, 343], "train_loop": [277, 329], "train_loop_config": [273, 284], "train_loop_per_work": [274, 310], "trainer": [273, 275, 290, 317], "transform": [8, 9, 52, 61, 62, 240, 264, 273, 274, 276, 297, 308, 320, 322], "tripl": [279, 337], "troubleshoot": [136, 146, 149, 154], "tune": [3, 6, 26, 28, 38, 41, 42, 239, 255, 258, 259, 260], "tupl": [278, 332], "tutori": [273, 305], "two": [74, 88, 212, 213, 215], "type": [100, 103, 229, 233, 275, 316], "uci": [275, 316], "under": [238, 253], "univers": [275, 316], "unstructur": [9, 56], "up": [3, 11, 28, 72, 126, 134, 136, 147, 163, 176, 181, 182, 183, 187, 188, 193, 194, 195, 204, 206, 211, 221, 224, 226, 229, 234, 239, 241, 257, 272, 273, 274, 275, 276, 277, 278, 279, 292, 304, 305, 314, 319, 325, 331, 336, 342], "upcom": [8, 55], "updat": [187, 188, 203, 206, 210, 238, 253], "upgrad": [149, 157, 221, 227], "uri": [279, 338], "url": [186, 201, 205, 208], "us": [3, 4, 5, 6, 7, 8, 9, 11, 26, 30, 31, 32, 34, 41, 47, 50, 54, 55, 64, 72, 75, 90, 100, 105, 107, 109, 169, 172, 186, 187, 200, 202, 203, 204, 205, 206, 207, 209, 210, 211, 221, 227, 229, 232, 233, 234, 235, 237, 239, 240, 241, 244, 260, 262, 265, 268, 270, 273, 274, 275, 276, 277, 278, 279, 280, 306, 315, 320, 326, 332, 337, 338], "usag": 0, "user": [173, 185, 198, 279, 337, 338, 342], "uv": [11, 72], "v": [7, 47, 48, 107, 108, 109, 186, 201, 205, 208], "val": [277, 278, 327, 335], "valid": [274, 275, 276, 279, 308, 312, 316, 323, 338, 340], "valu": [212, 213, 216], "vanilla": [3, 6, 28, 40], "verifi": [11, 72, 126, 132, 136, 144, 186, 201, 205, 208, 275, 319], "view": [273, 291], "viewer": [181, 193], "virtual": [107, 108, 109], "vision": [274, 277, 306, 326], "visual": [238, 239, 247, 256, 273, 274, 275, 276, 277, 279, 281, 292, 307, 314, 316, 318, 321, 325, 327, 338], "vllm": [212, 213, 218], "vm": [107, 109], "vpc": [100, 105], "vscode": [178, 187, 190, 204, 206, 211], "wa": [186, 201, 205, 208], "wait": [2, 24], "walk": [182, 186, 187, 194, 200, 203, 205, 206, 207, 210], "warehous": [7, 43], "we": [11, 72, 186, 187, 201, 204, 205, 206, 208, 211, 221, 228, 229, 231, 236, 237, 244], "weather": [229, 234], "web": [169, 172], "webservic": [75, 92], "welcom": [11, 72, 162], "what": [7, 8, 47, 48, 50, 75, 90, 100, 102, 107, 109, 162, 163, 176, 212, 213, 215, 221, 226, 228, 229, 231, 236, 273, 274, 275, 276, 277, 278, 279, 280, 293, 299, 306, 315, 320, 326, 332, 337], "when": [4, 5, 7, 8, 9, 10, 30, 34, 47, 55, 57, 68, 107, 109, 240, 241, 262, 268, 273, 280], "where": [273, 274, 275, 276, 277, 278, 279, 305, 314, 319, 325, 331, 336, 342], "which": [107, 109], "why": [7, 11, 47, 48, 72, 75, 90, 188, 212, 213, 217, 221, 223, 229, 231, 232, 233, 234, 277, 326], "window": [212, 213, 216, 229, 235, 276, 321], "work": [0, 9, 58, 273, 277, 280, 326], "worker": [4, 5, 32, 36, 76, 97, 163, 176, 179, 187, 191, 204, 206, 211, 273, 275, 283, 317], "workflow": [0, 10, 71, 179, 186, 191, 201, 205, 208], "workload": [107, 109, 169, 171, 185, 198, 274, 275, 276, 277, 278, 279, 306, 315, 320, 326, 332, 337], "workspac": [177, 186, 187, 188, 189, 201, 202, 204, 205, 206, 208, 209, 211], "wrap": [273, 274, 275, 276, 277, 278, 279, 286, 305, 314, 319, 325, 331, 336, 342], "write": [8, 53, 275, 316], "xgboost": [3, 26, 28, 275, 315, 317], "yaml": [187, 204, 206, 211], "york": [237, 244], "you": [162, 182, 186, 187, 194, 200, 201, 203, 205, 206, 207, 208, 210, 273, 274, 275, 276, 277, 278, 279, 280, 293, 299, 306, 314, 315, 319, 320, 325, 326, 331, 332, 336, 337, 342], "your": [11, 72, 100, 106, 107, 110, 136, 140, 181, 182, 183, 186, 187, 188, 193, 194, 195, 200, 201, 204, 205, 206, 207, 208, 211]}})