Search.setIndex({"alltitles": {"0. Overview": [[1, "overview"], [13, null], [73, "overview"], [74, "overview"]], "0. What is Ray Data?": [[8, "what-is-ray-data"], [50, null]], "01 \u00b7 Define Training Loop with Ray Data": [[178, "define-training-loop-with-ray-data"], [193, null]], "01 \u00b7 Imports": [[178, "imports"], [180, null]], "01 \u00b7 Modify Training Loop to Enable Checkpoint Loading": [[178, "modify-training-loop-to-enable-checkpoint-loading"], [199, null]], "02 Anyscale Admin": [[339, "anyscale-admin"]], "02 \u00b7 Build DataLoader from Ray Data": [[178, "build-dataloader-from-ray-data"], [194, null]], "02 \u00b7 Download MNIST Dataset": [[178, "download-mnist-dataset"], [180, "download-mnist-dataset"]], "02 \u00b7 Save Full Checkpoint with Extra State": [[178, "save-full-checkpoint-with-extra-state"], [200, null]], "03 Observability": [[339, "observability"]], "03 \u00b7 Configure Automatic Retries with FailureConfig": [[178, "configure-automatic-retries-with-failureconfig"], [200, "configure-automatic-retries-with-failureconfig"]], "03 \u00b7 Prepare Dataset for Ray Data": [[178, "prepare-dataset-for-ray-data"], [195, null]], "03 \u00b7 Visualize Sample Digits": [[178, "visualize-sample-digits"], [180, "visualize-sample-digits"]], "04 \u00b7 Define ResNet-18 Model for MNIST": [[178, "define-resnet-18-model-for-mnist"], [181, null]], "04 \u00b7 Launch Fault-Tolerant Training": [[178, "launch-fault-tolerant-training"], [201, null]], "04 \u00b7 Load Dataset into Ray Data": [[178, "load-dataset-into-ray-data"], [195, "load-dataset-into-ray-data"]], "04-d1 Generative computer-vision pattern with Ray Train": [[235, null], [236, null]], "04-d2 Diffusion-Policy Pattern with Ray Train": [[242, null], [243, null]], "04a Computer-vision pattern with Ray Train": [[268, null], [269, null]], "04b Tabular workload pattern with Ray Train": [[255, null], [256, null]], "04c Time-Series workload pattern with Ray Train": [[261, null], [262, null]], "04e Recommendation system pattern with Ray Train": [[248, null], [249, null]], "05 \u00b7 Define Image Transformation": [[178, "define-image-transformation"], [196, null]], "05 \u00b7 Define the Ray Train Loop (DDP per-worker)": [[178, "define-the-ray-train-loop-ddp-per-worker"], [182, null]], "05 \u00b7 Manual Restoration from Checkpoints": [[178, "manual-restoration-from-checkpoints"], [202, null]], "06 \u00b7 Apply Transformations with Ray Data": [[178, "apply-transformations-with-ray-data"], [196, "apply-transformations-with-ray-data"]], "06 \u00b7 Define train_loop_config": [[178, "define-train-loop-config"], [183, null]], "06 \u00b7 Resume Training from the Last Checkpoint": [[178, "resume-training-from-the-last-checkpoint"], [202, "resume-training-from-the-last-checkpoint"]], "07 \u00b7 Clean Up Cluster Storage": [[178, "clean-up-cluster-storage"], [203, null]], "07 \u00b7 Configure Scaling with ScalingConfig": [[178, "configure-scaling-with-scalingconfig"], [184, null]], "07 \u00b7 Configure TorchTrainer with Ray Data": [[178, "configure-torchtrainer-with-ray-data"], [197, null]], "08 \u00b7 Launch Training with Ray Data": [[178, "launch-training-with-ray-data"], [197, "launch-training-with-ray-data"]], "08 \u00b7 Wrap the Model with prepare_model()": [[178, "wrap-the-model-with-prepare-model"], [185, null]], "09 \u00b7 Build the DataLoader with prepare_data_loader()": [[178, "build-the-dataloader-with-prepare-data-loader"], [186, null]], "1. Architecture": [[221, "architecture"], [224, null]], "1. Cloud Object Store": [[153, "cloud-object-store"], [165, "cloud-object-store"]], "1. Create Anyscale Resources with Terraform": [[84, "create-anyscale-resources-with-terraform"], [86, null], [99, "create-anyscale-resources-with-terraform"], [101, null], [109, "create-anyscale-resources-with-terraform"], [112, null]], "1. Creating Remote Functions": [[1, "creating-remote-functions"], [14, null]], "1. Dataset tuples": [[242, "dataset-tuples"], [243, "dataset-tuples"]], "1. Deploy to Kubernetes with Anyscale Operator": [[80, "deploy-to-kubernetes-with-anyscale-operator"], [81, "deploy-to-kubernetes-with-anyscale-operator"]], "1. How to Use Ray Data?": [[8, "how-to-use-ray-data"], [50, "how-to-use-ray-data"]], "1. Imports": [[248, "imports"], [250, null], [255, "imports"], [257, null], [261, "imports"], [263, null], [268, "imports"], [270, null]], "1. Imports and setup": [[235, "imports-and-setup"], [237, null], [242, "imports-and-setup"], [244, null]], "1. In the Anyscale Console, open (or create) a Workspace.": [[159, "in-the-anyscale-console-open-or-create-a-workspace"], [174, "in-the-anyscale-console-open-or-create-a-workspace"]], "1. Installation": [[91, "installation"], [94, null], [122, "installation"], [125, null]], "1. Key-Value (KV) Caching": [[278, "key-value-kv-caching"], [279, "key-value-kv-caching"], [282, "key-value-kv-caching"]], "1. Loading and visualizing data": [[305, "loading-and-visualizing-data"], [322, null]], "1. Loading the data": [[6, "loading-the-data"], [39, null]], "1. Memory Management": [[278, "memory-management"], [279, "memory-management"], [283, "memory-management"]], "1. Model Quality Benchmarks": [[295, "model-quality-benchmarks"], [301, "model-quality-benchmarks"]], "1. Object store": [[2, "object-store"], [18, null]], "1. Overview of Ray Serve": [[307, "overview-of-ray-serve"], [334, null]], "1. Overview of the Ray AI Libraries": [[3, "overview-of-the-ray-ai-libraries"], [27, null], [303, "overview-of-the-ray-ai-libraries"], [309, null]], "1. PyTorch introductory example (single GPU)": [[304, "pytorch-introductory-example-single-gpu"], [313, null]], "1. Ray Serve for Orchestration": [[278, "ray-serve-for-orchestration"], [279, "ray-serve-for-orchestration"], [284, "ray-serve-for-orchestration"]], "1. Reduce max_model_len": [[287, "reduce-max-model-len"], [293, "reduce-max-model-len"]], "1. Sign Up for Anyscale": [[161, "sign-up-for-anyscale"]], "1. Spin up a Anyscale Workspace, we will use this as the environment to develop and publish the Anyscale Service. Give this workspace a name, check the Auto-Select Worker Nodes and leave everything else as default.": [[160, "spin-up-a-anyscale-workspace-we-will-use-this-as-the-environment-to-develop-and-publish-the-anyscale-service-give-this-workspace-a-name-check-the-auto-select-worker-nodes-and-leave-everything-else-as-default"], [177, "spin-up-a-anyscale-workspace-we-will-use-this-as-the-environment-to-develop-and-publish-the-anyscale-service-give-this-workspace-a-name-check-the-auto-select-worker-nodes-and-leave-everything-else-as-default"]], "1. Split Notebooks and Generate Navigation": [[0, "split-notebooks-and-generate-navigation"]], "1. User Profile Retrieval": [[146, "user-profile-retrieval"]], "1. Using the same workspace, create a notebook folder": [[159, "using-the-same-workspace-create-a-notebook-folder"], [175, "using-the-same-workspace-create-a-notebook-folder"]], "1. What is an Anyscale Cloud?": [[73, "what-is-an-anyscale-cloud"], [75, null]], "1. When to Consider Ray Data": [[9, "when-to-consider-ray-data"], [57, null]], "1. When to Consider Ray Serve": [[10, "when-to-consider-ray-serve"], [68, null]], "1. When to use Ray Data": [[306, "when-to-use-ray-data"], [328, null]], "1. When to use Ray Train": [[4, "when-to-use-ray-train"], [5, "when-to-use-ray-train"], [30, null], [34, null]], "1.1 Configure Google Cloud Authentication": [[91, "configure-google-cloud-authentication"], [94, "configure-google-cloud-authentication"]], "1.1. Configure Google Cloud Authentication": [[122, "configure-google-cloud-authentication"], [125, "configure-google-cloud-authentication"]], "1.1. Pattern: pass an object as a top-level argument": [[2, "pattern-pass-an-object-as-a-top-level-argument"], [18, "pattern-pass-an-object-as-a-top-level-argument"]], "1.2 Enable Required APIs": [[91, "enable-required-apis"], [94, "enable-required-apis"]], "1.2: Enable Required APIs": [[122, "enable-required-apis"], [125, "enable-required-apis"]], "10 \u00b7 Report Training Metrics": [[178, "report-training-metrics"], [187, null]], "10. Clean up": [[242, "clean-up"], [247, "clean-up"]], "10. Conclusion": [[109, "conclusion"], [121, null]], "10. Helper: Ray-prepared DataLoaders": [[268, "helper-ray-prepared-dataloaders"], [272, null]], "10. Launch distributed Training with TorchTrainer": [[235, "launch-distributed-training-with-torchtrainer"], [239, "launch-distributed-training-with-torchtrainer"]], "10. Plot train and validation loss curves": [[248, "plot-train-and-validation-loss-curves"], [252, "plot-train-and-validation-loss-curves"]], "10. Ray Train training loop (with teacher forcing)": [[261, "ray-train-training-loop-with-teacher-forcing"], [265, null]], "10. Start distributed training": [[255, "start-distributed-training"], [258, "start-distributed-training"]], "101 - Anyscale Organization and Cloud Setup": [[158, null], [170, null]], "101 Introduction to Anyscale Services": [[159, "introduction-to-anyscale-services"], [160, "introduction-to-anyscale-services"], [173, "introduction-to-anyscale-services"], [176, "introduction-to-anyscale-services"]], "101 \u2013  Introduction to Anyscale Services": [[156, null], [168, null]], "101 \u2013 Collaboration on Anyscale": [[157, null], [169, null]], "101 \u2013 Compute Configs and Execution Environments in Anyscale": [[152, null], [164, null]], "101 \u2013 Debug and Monitor Your Anyscale Application": [[154, null], [166, null]], "101 \u2013 Developing Application with Anyscale": [[151, null], [163, null]], "101 \u2013 Introduction to Anyscale Jobs": [[155, null], [167, null]], "101 \u2013 Storage Options in the Anyscale Platform": [[153, null], [165, null]], "101 \u2014 Introduction to Anyscale Workspaces": [[150, null], [162, null]], "11 \u00b7 Save Checkpoints and Report Metrics": [[178, "save-checkpoints-and-report-metrics"], [188, null]], "11. Evaluate the trained model": [[255, "evaluate-the-trained-model"], [258, "evaluate-the-trained-model"]], "11. Launch training on 8 GPUs": [[261, "launch-training-on-8-gpus"], [265, "launch-training-on-8-gpus"]], "11. Plot loss curves": [[235, "plot-loss-curves"], [239, "plot-loss-curves"]], "11. Resume training from checkpoint": [[248, "resume-training-from-checkpoint"], [253, null]], "11. train_loop_per_worker": [[268, "train-loop-per-worker"], [273, null]], "12 \u00b7 Save Checkpoints on Rank-0 Only": [[178, "save-checkpoints-on-rank-0-only"], [188, "save-checkpoints-on-rank-0-only"]], "12. Confusion matrix visualization": [[255, "confusion-matrix-visualization"], [259, null]], "12. Inference: recommend top-N items for a user": [[248, "inference-recommend-top-n-items-for-a-user"], [254, null]], "12. Launch distributed training with TorchTrainer": [[268, "launch-distributed-training-with-torchtrainer"], [274, null]], "12. Plot training and validation loss": [[261, "plot-training-and-validation-loss"], [265, "plot-training-and-validation-loss"]], "12. Resume from latest checkpoint": [[235, "resume-from-latest-checkpoint"], [240, null]], "13 \u00b7 Configure Persistent Storage with RunConfig": [[178, "configure-persistent-storage-with-runconfig"], [188, "configure-persistent-storage-with-runconfig"]], "13. CPU batch inference with Ray Data": [[255, "cpu-batch-inference-with-ray-data"], [259, "cpu-batch-inference-with-ray-data"]], "13. Join top-N item IDs with movie titles": [[248, "join-top-n-item-ids-with-movie-titles"], [254, "join-top-n-item-ids-with-movie-titles"]], "13. Plot training and validation loss curves": [[268, "plot-training-and-validation-loss-curves"], [275, null]], "13. Resume training from checkpoint": [[261, "resume-training-from-checkpoint"], [266, null]], "13. Reverse diffusion sampler": [[235, "reverse-diffusion-sampler"], [241, null]], "14 \u00b7 Create the TorchTrainer": [[178, "create-the-torchtrainer"], [189, null]], "14. Clean up shared storage": [[248, "clean-up-shared-storage"], [254, "clean-up-shared-storage"]], "14. Demonstrate fault-tolerant resumption": [[268, "demonstrate-fault-tolerant-resumption"], [276, null]], "14. Feature-importance diagnostics": [[255, "feature-importance-diagnostics"], [259, "feature-importance-diagnostics"]], "14. Generate and display samples from the best checkpoint": [[235, "generate-and-display-samples-from-the-best-checkpoint"], [241, "generate-and-display-samples-from-the-best-checkpoint"]], "14. Inference helper \u2014 Ray Data batch predictor on GPU": [[261, "inference-helper-ray-data-batch-predictor-on-gpu"], [267, null]], "15 \u00b7 Launch Training with trainer.fit()": [[178, "launch-training-with-trainer-fit"], [189, "launch-training-with-trainer-fit"]], "15. Batch inference with Ray Data": [[268, "batch-inference-with-ray-data"], [277, null]], "15. Clean up shared storage": [[235, "clean-up-shared-storage"], [241, "clean-up-shared-storage"]], "15. Continue training from the latest checkpoint": [[255, "continue-training-from-the-latest-checkpoint"], [260, null]], "15. Run distributed inference and visualize results": [[261, "run-distributed-inference-and-visualize-results"], [267, "run-distributed-inference-and-visualize-results"]], "16 \u00b7 Inspect the Training Results": [[178, "inspect-the-training-results"], [190, null]], "16. Cleanup: remove all training artifacts": [[261, "cleanup-remove-all-training-artifacts"], [267, "cleanup-remove-all-training-artifacts"]], "16. Run and visualize Ray Data inference": [[268, "run-and-visualize-ray-data-inference"], [277, "run-and-visualize-ray-data-inference"]], "16. Verify post-training inference": [[255, "verify-post-training-inference"], [260, "verify-post-training-inference"]], "17 \u00b7 View Metrics as a DataFrame": [[178, "view-metrics-as-a-dataframe"], [190, "view-metrics-as-a-dataframe"]], "17. Clean up": [[255, "clean-up"], [260, "clean-up"], [268, "clean-up"], [277, "clean-up"]], "18 \u00b7 Load a Checkpoint for Inference": [[178, "load-a-checkpoint-for-inference"], [191, null]], "19 \u00b7 Run Inference and Visualize Predictions": [[178, "run-inference-and-visualize-predictions"], [191, "run-inference-and-visualize-predictions"]], "2. Attach Required IAM Policies to Your existing EKS\u2019s Node Role": [[109, "attach-required-iam-policies-to-your-existing-eks-s-node-role"], [113, null]], "2. Build the Book": [[0, "build-the-book"]], "2. Chaining Tasks and Passing Data": [[2, "chaining-tasks-and-passing-data"], [19, null]], "2. Clone the Repository (Optional)": [[161, "clone-the-repository-optional"]], "2. Cloud Deployment Types": [[73, "cloud-deployment-types"], [76, null]], "2. Continuous Batching": [[278, "continuous-batching"], [279, "continuous-batching"], [282, "continuous-batching"]], "2. Create Anyscale Resources with Terraform": [[91, "create-anyscale-resources-with-terraform"], [95, null], [122, "create-anyscale-resources-with-terraform"], [126, null]], "2. Distributed Data Parallel Training with Ray Train and PyTorch (multiple GPUs)": [[304, "distributed-data-parallel-training-with-ray-train-and-pytorch-multiple-gpus"], [314, null]], "2. End-to-end example: predicting taxi tips in New York": [[303, "end-to-end-example-predicting-taxi-tips-in-new-york"], [310, null]], "2. Executing Remote Functions": [[1, "executing-remote-functions"], [15, null]], "2. Generate a real pendulum dataset": [[242, "generate-a-real-pendulum-dataset"], [244, "generate-a-real-pendulum-dataset"]], "2. How to work with Ray Data": [[9, "how-to-work-with-ray-data"], [58, null]], "2. Implement an Classifier service": [[307, "implement-an-classifier-service"], [335, null]], "2. Install Kubernetes Components": [[99, "install-kubernetes-components"], [102, null]], "2. Latency Requirements": [[278, "latency-requirements"], [279, "latency-requirements"], [283, "latency-requirements"]], "2. Library Imports": [[221, "library-imports"], [224, "library-imports"]], "2. Load 10 % of Food-101": [[235, "load-10-of-food-101"], [237, "load-10-of-food-101"], [268, "load-10-of-food-101"], [270, "load-10-of-food-101"]], "2. Load MovieLens 100K dataset": [[248, "load-movielens-100k-dataset"], [250, "load-movielens-100k-dataset"]], "2. Load NYC taxi passenger counts (30-min)": [[261, "load-nyc-taxi-passenger-counts-30-min"], [263, "load-nyc-taxi-passenger-counts-30-min"]], "2. Load the University of California, Irvine (UCI) Cover type dataset": [[255, "load-the-university-of-california-irvine-uci-cover-type-dataset"], [257, "load-the-university-of-california-irvine-uci-cover-type-dataset"]], "2. Loading Data": [[8, "loading-data"], [51, null], [306, "loading-data"], [329, null]], "2. Once in the workspace, navigate to the VS Code Editor": [[159, "once-in-the-workspace-navigate-to-the-vs-code-editor"], [174, "once-in-the-workspace-navigate-to-the-vs-code-editor"]], "2. Overview of Ray Serve": [[10, "overview-of-ray-serve"], [69, null]], "2. Quick end-to-end example": [[3, "quick-end-to-end-example"], [28, null]], "2. Register the Anyscale Cloud": [[84, "register-the-anyscale-cloud"], [87, null]], "2. Setting up a PyTorch model": [[305, "setting-up-a-pytorch-model"], [323, null]], "2. Shared File Storage": [[153, "shared-file-storage"], [165, "shared-file-storage"]], "2. Single GPU Training with PyTorch": [[4, "single-gpu-training-with-pytorch"], [31, null]], "2. Single GPU Training with PyTorch Lightning": [[5, "single-gpu-training-with-pytorch-lightning"], [35, null]], "2. Starting out with vanilla PyTorch": [[6, "starting-out-with-vanilla-pytorch"], [40, null]], "2. Task and Domain Alignment": [[295, "task-and-domain-alignment"], [301, "task-and-domain-alignment"]], "2. Training objective": [[242, "training-objective"], [243, "training-objective"]], "2. Use Quantized Models": [[287, "use-quantized-models"], [293, "use-quantized-models"]], "2. Use the Anyscale CLI to submit the Anyscale Job. For a full list of all available arguments, check out the Anyscale Job CLI documentation.": [[159, "use-the-anyscale-cli-to-submit-the-anyscale-job-for-a-full-list-of-all-available-arguments-check-out-the-anyscale-job-cli-documentation"], [175, "use-the-anyscale-cli-to-submit-the-anyscale-job-for-a-full-list-of-all-available-arguments-check-out-the-anyscale-job-cli-documentation"]], "2. User Registration": [[146, "user-registration"]], "2. Virtual Machines (VM) vs. Kubernetes (K8s)": [[80, "virtual-machines-vm-vs-kubernetes-k8s"], [82, null]], "2. vLLM as the inference engine": [[278, "vllm-as-the-inference-engine"], [279, "vllm-as-the-inference-engine"], [284, "vllm-as-the-inference-engine"]], "2.1 Control Layer: What Anyscale Manages or Needs Access To": [[80, "control-layer-what-anyscale-manages-or-needs-access-to"], [82, "control-layer-what-anyscale-manages-or-needs-access-to"]], "2.1 Create terraform.tfvars": [[91, "create-terraform-tfvars"], [95, "create-terraform-tfvars"]], "2.1 Install the Cluster Autoscaler": [[99, "install-the-cluster-autoscaler"], [102, "install-the-cluster-autoscaler"]], "2.1 Overview": [[5, "overview"], [35, "overview"]], "2.1 Vanilla XGboost code": [[3, "vanilla-xgboost-code"], [28, "vanilla-xgboost-code"]], "2.1. Overview": [[4, "overview"], [31, "overview"]], "2.1: Create terraform.tfvars": [[122, "create-terraform-tfvars"], [126, "create-terraform-tfvars"]], "2.2 Data Layer: Storage, Object Stores, and External Dependencies": [[80, "data-layer-storage-object-stores-and-external-dependencies"], [82, "data-layer-storage-object-stores-and-external-dependencies"]], "2.2 Hyperparameter tuning with Ray Tune": [[3, "hyperparameter-tuning-with-ray-tune"], [28, "hyperparameter-tuning-with-ray-tune"]], "2.2 Install the AWS Load Balancer Controller": [[99, "install-the-aws-load-balancer-controller"], [102, "install-the-aws-load-balancer-controller"]], "2.2 Note on blocks": [[9, "note-on-blocks"], [59, "note-on-blocks"]], "2.2 Run Terraform Commands": [[91, "run-terraform-commands"], [95, "run-terraform-commands"]], "2.2. Build model and load it on the GPU": [[4, "build-model-and-load-it-on-the-gpu"], [31, "build-model-and-load-it-on-the-gpu"]], "2.2. Create a torch dataloader": [[5, "create-a-torch-dataloader"], [35, "create-a-torch-dataloader"]], "2.2: Deploy Infrastructure": [[122, "deploy-infrastructure"], [126, "deploy-infrastructure"]], "2.3 Define a stable diffusion model": [[5, "define-a-stable-diffusion-model"], [35, "define-a-stable-diffusion-model"]], "2.3 Install the Nginx Ingress Controller": [[99, "install-the-nginx-ingress-controller"], [102, "install-the-nginx-ingress-controller"]], "2.3 Workload Execution Layer: How Ray Runs on Each Backend": [[80, "workload-execution-layer-how-ray-runs-on-each-backend"], [82, "workload-execution-layer-how-ray-runs-on-each-backend"]], "2.3. Create Dataset and DataLoader": [[4, "create-dataset-and-dataloader"], [31, "create-dataset-and-dataloader"]], "2.3. Distributed training with Ray Train": [[3, "distributed-training-with-ray-train"], [28, "distributed-training-with-ray-train"]], "2.4 (Optional) Install the Nvidia Device Plugin": [[99, "optional-install-the-nvidia-device-plugin"], [102, "optional-install-the-nvidia-device-plugin"]], "2.4 Serving an ensemble model with Ray Serve": [[3, "serving-an-ensemble-model-with-ray-serve"], [28, "serving-an-ensemble-model-with-ray-serve"]], "2.4 When to use which": [[80, "when-to-use-which"], [82, "when-to-use-which"]], "2.4. Create metrics and checkpointing": [[4, "create-metrics-and-checkpointing"], [31, "create-metrics-and-checkpointing"]], "2.4. Define a PyTorch Lightning training loop": [[5, "define-a-pytorch-lightning-training-loop"], [35, "define-a-pytorch-lightning-training-loop"]], "2.5 Batch inference with Ray Data": [[3, "batch-inference-with-ray-data"], [28, "batch-inference-with-ray-data"]], "2.5. Run the training loop": [[4, "run-the-training-loop"], [31, "run-the-training-loop"]], "2.6 Clean up": [[3, "clean-up"], [28, "clean-up"]], "2.6. Use checkpointed model to generate predictions": [[4, "use-checkpointed-model-to-generate-predictions"], [31, "use-checkpointed-model-to-generate-predictions"]], "2.Download starter template. Clone a github repository containing the files needed to deploy a Anyscale Service. Head over to the VSCode Tab (In Anyscale Workspace) and enter the following command into the terminal.": [[160, "download-starter-template-clone-a-github-repository-containing-the-files-needed-to-deploy-a-anyscale-service-head-over-to-the-vscode-tab-in-anyscale-workspace-and-enter-the-following-command-into-the-terminal"], [177, "download-starter-template-clone-a-github-repository-containing-the-files-needed-to-deploy-a-anyscale-service-head-over-to-the-vscode-tab-in-anyscale-workspace-and-enter-the-following-command-into-the-terminal"]], "20 \u00b7 Clean Up the Ray Actor": [[178, "clean-up-the-ray-actor"], [191, "clean-up-the-ray-actor"]], "3. (Optional) More Kubernetes Deployments Components": [[80, "optional-more-kubernetes-deployments-components"], [83, null]], "3. A Demonstrative Example of Resource Creation with AWS EC2": [[73, "a-demonstrative-example-of-resource-creation-with-aws-ec2"], [77, null]], "3. Advanced features of Ray Serve": [[307, "advanced-features-of-ray-serve"], [336, null]], "3. Anyscale for Infrastructure": [[278, "anyscale-for-infrastructure"], [279, "anyscale-for-infrastructure"], [284, "anyscale-for-infrastructure"]], "3. Context Window Requirements": [[295, "context-window-requirements"], [301, "context-window-requirements"]], "3. Distributed Data Parallel Training with Ray Train and PyTorch": [[4, "distributed-data-parallel-training-with-ray-train-and-pytorch"], [32, null]], "3. Distributed Training with Ray Train and PyTorch Lightning": [[5, "distributed-training-with-ray-train-and-pytorch-lightning"], [36, null]], "3. Enable Pipeline Parallelism": [[287, "enable-pipeline-parallelism"], [293, "enable-pipeline-parallelism"]], "3. Getting Results": [[1, "getting-results"], [15, "getting-results"]], "3. Hyperparameter tuning with Ray Tune": [[6, "hyperparameter-tuning-with-ray-tune"], [41, null]], "3. Implement an image classification service": [[10, "implement-an-image-classification-service"], [70, null]], "3. Inspect the code for the Service Endpoint (./examples/02_service_hello_world/main.py)": [[160, "inspect-the-code-for-the-service-endpoint-examples-02-service-hello-world-main-py"], [177, "inspect-the-code-for-the-service-endpoint-examples-02-service-hello-world-main-py"]], "3. Install Kubernetes Components": [[109, "install-kubernetes-components"], [114, null]], "3. Install Ray and the Anyscale CLI (Recommended)": [[161, "install-ray-and-the-anyscale-cli-recommended"]], "3. Introduction to Ray Tune": [[305, "introduction-to-ray-tune"], [324, null]], "3. Lazy execution mode": [[9, "lazy-execution-mode"], [60, null]], "3. Loading data": [[9, "loading-data"], [59, null]], "3. Local Cluster Storage": [[153, "local-cluster-storage"], [165, "local-cluster-storage"]], "3. Metrics Setup": [[221, "metrics-setup"], [225, null]], "3. Model parallelization or alternatives": [[278, "model-parallelization-or-alternatives"], [279, "model-parallelization-or-alternatives"], [282, "model-parallelization-or-alternatives"]], "3. Next, create a new file. You can name it hello_world.py": [[159, "next-create-a-new-file-you-can-name-it-hello-world-py"], [174, "next-create-a-new-file-you-can-name-it-hello-world-py"]], "3. Normalize and split": [[242, "normalize-and-split"], [244, "normalize-and-split"]], "3. Overview of the training loop in Ray Train": [[304, "overview-of-the-training-loop-in-ray-train"], [315, null]], "3. Point to Parquet dataset URI": [[248, "point-to-parquet-dataset-uri"], [250, "point-to-parquet-dataset-uri"]], "3. Register the Anyscale Cloud": [[91, "register-the-anyscale-cloud"], [96, null], [99, "register-the-anyscale-cloud"], [103, null]], "3. Resample to hourly, then normalize": [[261, "resample-to-hourly-then-normalize"], [263, "resample-to-hourly-then-normalize"]], "3. Resize and encode images": [[235, "resize-and-encode-images"], [237, "resize-and-encode-images"], [268, "resize-and-encode-images"], [270, "resize-and-encode-images"]], "3. Reverse diffusion (sampling)": [[242, "reverse-diffusion-sampling"], [243, "reverse-diffusion-sampling"]], "3. Running an experiment with Ray AI libraries": [[303, "running-an-experiment-with-ray-ai-libraries"], [310, "running-an-experiment-with-ray-ai-libraries"], [311, null]], "3. Scalability Demands": [[278, "scalability-demands"], [279, "scalability-demands"], [283, "scalability-demands"]], "3. Serve Locally for Testing": [[0, "serve-locally-for-testing"]], "3. Submit the job again using the Anyscale Python SDK": [[159, "submit-the-job-again-using-the-anyscale-python-sdk"], [175, "submit-the-job-again-using-the-anyscale-python-sdk"]], "3. Task retries": [[2, "task-retries"], [20, null]], "3. Test": [[84, "test"], [88, null]], "3. Transforming Data": [[8, "transforming-data"], [52, null], [306, "transforming-data"], [330, null]], "3. Troubleshooting GPU Availability": [[122, "troubleshooting-gpu-availability"], [127, null]], "3. Visualize class balance": [[255, "visualize-class-balance"], [257, "visualize-class-balance"]], "3.1 Distributed Data Parallel Training": [[5, "distributed-data-parallel-training"], [36, "distributed-data-parallel-training"]], "3.1 IAM Role Definition": [[73, "iam-role-definition"], [78, null]], "3.1 Install the Cluster Autoscaler": [[109, "install-the-cluster-autoscaler"], [114, "install-the-cluster-autoscaler"]], "3.1. Overview of the training loop in Ray Train": [[4, "overview-of-the-training-loop-in-ray-train"], [32, "overview-of-the-training-loop-in-ray-train"]], "3.1.1\u202f\u202fAnyscale Control Plane Role (anyscale-iam-role-id)": [[73, "anyscale-control-plane-role-anyscale-iam-role-id"], [78, "anyscale-control-plane-role-anyscale-iam-role-id"]], "3.1.2\u202f\u202fInstance Role (instance-iam-role-id)": [[73, "instance-role-instance-iam-role-id"], [78, "instance-role-instance-iam-role-id"]], "3.10. Activity: Run the distributed training with more workers": [[4, "activity-run-the-distributed-training-with-more-workers"], [32, "activity-run-the-distributed-training-with-more-workers"]], "3.2 Install the AWS Load Balancer Controller": [[109, "install-the-aws-load-balancer-controller"], [114, "install-the-aws-load-balancer-controller"]], "3.2 Ray Train Migration": [[5, "ray-train-migration"], [36, "ray-train-migration"]], "3.2. Configure scale and GPUs": [[4, "configure-scale-and-gpus"], [32, "configure-scale-and-gpus"]], "3.2.1. Note on Ray Train key concepts": [[4, "note-on-ray-train-key-concepts"], [32, "note-on-ray-train-key-concepts"]], "3.2\u202fVPC": [[73, "vpc"], [78, "vpc"]], "3.3 Install the Nginx Ingress Controller": [[109, "install-the-nginx-ingress-controller"], [114, "install-the-nginx-ingress-controller"]], "3.3 Subnets": [[73, "subnets"], [78, "subnets"]], "3.3. Configure scale and GPUs": [[5, "configure-scale-and-gpus"], [36, "configure-scale-and-gpus"]], "3.3. Migrating the model to Ray Train": [[4, "migrating-the-model-to-ray-train"], [32, "migrating-the-model-to-ray-train"]], "3.3.1. Note on Ray Train key concepts": [[5, "note-on-ray-train-key-concepts"], [36, "note-on-ray-train-key-concepts"]], "3.4 (Optional) Install the Nvidia Device Plugin": [[109, "optional-install-the-nvidia-device-plugin"], [114, "optional-install-the-nvidia-device-plugin"]], "3.4 Create and fit a Ray Train TorchTrainer": [[5, "create-and-fit-a-ray-train-torchtrainer"], [36, "create-and-fit-a-ray-train-torchtrainer"]], "3.4. Migrating the dataset to Ray Train": [[4, "migrating-the-dataset-to-ray-train"], [32, "migrating-the-dataset-to-ray-train"]], "3.4\u202fSecurity Groups": [[73, "security-groups"], [78, "security-groups"]], "3.5. Access the training results": [[5, "access-the-training-results"], [36, "access-the-training-results"]], "3.5. Reporting checkpoints and metrics": [[4, "reporting-checkpoints-and-metrics"], [32, "reporting-checkpoints-and-metrics"]], "3.5.1. Note on the checkpoint lifecycle": [[4, "note-on-the-checkpoint-lifecycle"], [32, "note-on-the-checkpoint-lifecycle"]], "3.5\u202fS3": [[73, "s3"], [78, "s3"]], "3.6. Configure remote storage": [[4, "configure-remote-storage"], [32, "configure-remote-storage"]], "3.6. Load the checkpointed model to generate predictions": [[5, "load-the-checkpointed-model-to-generate-predictions"], [36, "load-the-checkpointed-model-to-generate-predictions"]], "3.6\u202fEFS (Optional)": [[73, "efs-optional"], [78, "efs-optional"]], "3.7. Activity: Run the distributed training with more workers": [[5, "activity-run-the-distributed-training-with-more-workers"], [36, "activity-run-the-distributed-training-with-more-workers"]], "3.7. Launching the distributed training job": [[4, "launching-the-distributed-training-job"], [32, "launching-the-distributed-training-job"]], "3.7\u202fMemoryDB (Optional)": [[73, "memorydb-optional"], [78, "memorydb-optional"]], "3.8 Summary": [[73, "summary"], [78, "summary"]], "3.8. Access the training results": [[4, "access-the-training-results"], [32, "access-the-training-results"]], "3.9. Use checkpointed model to generate predictions": [[4, "id1"], [32, "use-checkpointed-model-to-generate-predictions"]], "4. Checkout the service.yaml file.": [[160, "checkout-the-service-yaml-file"], [177, "checkout-the-service-yaml-file"]], "4. Cleanup": [[84, "cleanup"], [89, null]], "4. Context Window Considerations": [[278, "context-window-considerations"], [279, "context-window-considerations"], [282, "context-window-considerations"]], "4. Cost Optimization": [[278, "cost-optimization"], [279, "cost-optimization"], [283, "cost-optimization"]], "4. Data Operations: Grouping, Aggregation, and Shuffling": [[306, "data-operations-grouping-aggregation-and-shuffling"], [331, null]], "4. Development workflow": [[10, "development-workflow"], [71, null]], "4. DiffusionPolicy LightningModule": [[242, "diffusionpolicy-lightningmodule"], [245, null]], "4. Diving deeper into Ray Tune concepts": [[305, "diving-deeper-into-ray-tune-concepts"], [325, null]], "4. Examples Outlook: Deploying to Your Infrastructure": [[80, "examples-outlook-deploying-to-your-infrastructure"], [83, "examples-outlook-deploying-to-your-infrastructure"]], "4. Hardware and Cost Considerations": [[295, "hardware-and-cost-considerations"], [301, "hardware-and-cost-considerations"]], "4. Install the Anyscale Operator": [[99, "install-the-anyscale-operator"], [104, null]], "4. Local File Store": [[153, "local-file-store"], [165, "local-file-store"]], "4. Migrating the model and dataset to Ray Train": [[304, "migrating-the-model-and-dataset-to-ray-train"], [316, null]], "4. Paste the basic Ray example below into the file.": [[159, "paste-the-basic-ray-example-below-into-the-file"], [174, "paste-the-basic-ray-example-below-into-the-file"]], "4. Putting It All Together": [[1, "putting-it-all-together"], [16, null]], "4. Quick visual sanity-check": [[261, "quick-visual-sanity-check"], [263, "quick-visual-sanity-check"]], "4. Ray Serve in Production": [[307, "ray-serve-in-production"], [337, null]], "4. Ray Train in Production": [[4, "ray-train-in-production"], [5, "ray-train-in-production"], [32, "ray-train-in-production"], [37, null]], "4. Ray Tune in Production": [[6, "ray-tune-in-production"], [42, null]], "4. Register Anyscale Cloud to Your Cloud Provider": [[73, "register-anyscale-cloud-to-your-cloud-provider"], [79, null]], "4. Register the Anyscale Cloud": [[109, "register-the-anyscale-cloud"], [115, null]], "4. Scale with More Replicas": [[287, "scale-with-more-replicas"], [293, "scale-with-more-replicas"]], "4. Task Runtime Environments": [[2, "task-runtime-environments"], [21, null]], "4. Test": [[91, "test"], [97, null]], "4. Training function per worker": [[221, "training-function-per-worker"], [226, null]], "4. Transforming data": [[9, "transforming-data"], [61, null]], "4. Visual sanity check": [[235, "visual-sanity-check"], [237, "visual-sanity-check"], [268, "visual-sanity-check"], [270, "visual-sanity-check"]], "4. Visualize dataset: ratings, users, and items": [[248, "visualize-dataset-ratings-users-and-items"], [250, "visualize-dataset-ratings-users-and-items"]], "4. Write train / validation Parquet files": [[255, "write-train-validation-parquet-files"], [257, "write-train-validation-parquet-files"]], "4. Writing Data": [[8, "writing-data"], [53, null]], "4. kubectl Configuration": [[122, "kubectl-configuration"], [128, null]], "4.1 On resource specification": [[9, "on-resource-specification"], [61, "on-resource-specification"]], "4.1. Note about Ray ID Specification": [[1, "note-about-ray-id-specification"], [16, "note-about-ray-id-specification"]], "4.1. Note about pip dependencies": [[2, "note-about-pip-dependencies"], [21, "note-about-pip-dependencies"]], "4.2 On concurrency limiting": [[9, "on-concurrency-limiting"], [61, "on-concurrency-limiting"]], "4.2. Anti-pattern: Calling ray.get in a loop harms parallelism": [[1, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"], [16, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"]], "5. Cleanup": [[91, "cleanup"], [98, null]], "5. Conclusion": [[84, "conclusion"], [90, null]], "5. Create Ray Dataset from Parquet and encode IDs": [[248, "create-ray-dataset-from-parquet-and-encode-ids"], [250, "create-ray-dataset-from-parquet-and-encode-ids"]], "5. Data Operations: Shuffling, Grouping and Aggregation": [[8, "data-operations-shuffling-grouping-and-aggregation"], [54, null]], "5. Deploy the Anyscale Service by running the command below in the terminal and passing in the service configuration yaml file.": [[160, "deploy-the-anyscale-service-by-running-the-command-below-in-the-terminal-and-passing-in-the-service-configuration-yaml-file"], [177, "deploy-the-anyscale-service-by-running-the-command-below-in-the-terminal-and-passing-in-the-service-configuration-yaml-file"]], "5. Distributed Train loop with checkpointing": [[242, "distributed-train-loop-with-checkpointing"], [246, null]], "5. Hyperparameter tuning the PyTorch model using Ray Tune": [[305, "hyperparameter-tuning-the-pytorch-model-using-ray-tune"], [326, null]], "5. Install NGINX Ingress Controller": [[122, "install-nginx-ingress-controller"], [129, null]], "5. Install the Anyscale Operator": [[109, "install-the-anyscale-operator"], [116, null]], "5. Load the train and validation splits as Ray Datasets": [[255, "load-the-train-and-validation-splits-as-ray-datasets"], [257, "load-the-train-and-validation-splits-as-ray-datasets"]], "5. Main Training Function": [[221, "main-training-function"], [227, null]], "5. Open the terminal and run the following command to submit the Ray workflow as an Anyscale Job.": [[159, "open-the-terminal-and-run-the-following-command-to-submit-the-ray-workflow-as-an-anyscale-job"], [174, "open-the-terminal-and-run-the-following-command-to-submit-the-ray-workflow-as-an-anyscale-job"]], "5. Persist to Parquet": [[235, "persist-to-parquet"], [237, "persist-to-parquet"], [268, "persist-to-parquet"], [270, "persist-to-parquet"]], "5. Persisting Data": [[306, "persisting-data"], [332, null]], "5. Reporting checkpoints and metrics": [[304, "reporting-checkpoints-and-metrics"], [317, null]], "5. Resource allocation and management": [[2, "resource-allocation-and-management"], [22, null]], "5. Sliding-window dataset to Parquet": [[261, "sliding-window-dataset-to-parquet"], [263, "sliding-window-dataset-to-parquet"]], "5. Stateful transformations with Ray Actors": [[9, "stateful-transformations-with-ray-actors"], [62, null]], "5. Upgrade Hardware": [[287, "upgrade-hardware"], [293, "upgrade-hardware"]], "5. Verify the Installation": [[99, "verify-the-installation"], [105, null]], "5.1 Resource specification for stateful transformations": [[9, "resource-specification-for-stateful-transformations"], [62, "resource-specification-for-stateful-transformations"]], "5.1. Note on resources requests, available resources, configuring large clusters": [[2, "note-on-resources-requests-available-resources-configuring-large-clusters"], [22, "note-on-resources-requests-available-resources-configuring-large-clusters"]], "5.2 Note on autoscaling for stateful transformations": [[9, "note-on-autoscaling-for-stateful-transformations"], [62, "note-on-autoscaling-for-stateful-transformations"]], "5.2. Fractional resources": [[2, "fractional-resources"], [22, "fractional-resources"]], "5.3. IO bound tasks and fractional resources": [[2, "io-bound-tasks-and-fractional-resources"], [22, "io-bound-tasks-and-fractional-resources"]], "6. (Optional) Upgrade Anyscale Dependencies": [[122, "optional-upgrade-anyscale-dependencies"], [130, null]], "6. Custom Food101Dataset for Parquet": [[268, "custom-food101dataset-for-parquet"], [271, null]], "6. Inspect dataset sizes (optional)": [[255, "inspect-dataset-sizes-optional"], [257, "inspect-dataset-sizes-optional"]], "6. Launch Ray TorchTrainer": [[242, "launch-ray-torchtrainer"], [246, "launch-ray-torchtrainer"]], "6. Launching the distributed training job": [[304, "launching-the-distributed-training-job"], [318, null]], "6. Load and decode with Ray Data": [[235, "load-and-decode-with-ray-data"], [237, "load-and-decode-with-ray-data"]], "6. Materializing data": [[9, "materializing-data"], [63, null]], "6. Nested Tasks": [[2, "nested-tasks"], [23, null]], "6. PyTorch Dataset over Parquet": [[261, "pytorch-dataset-over-parquet"], [263, "pytorch-dataset-over-parquet"]], "6. Start Training": [[221, "start-training"], [228, null]], "6. Test": [[99, "test"], [106, null]], "6. Track the status of the job, head over to the Jobs tab and find the submitted Anyscale Job. The url is also displayed in the terminal.": [[159, "track-the-status-of-the-job-head-over-to-the-jobs-tab-and-find-the-submitted-anyscale-job-the-url-is-also-displayed-in-the-terminal"], [174, "track-the-status-of-the-job-head-over-to-the-jobs-tab-and-find-the-submitted-anyscale-job-the-url-is-also-displayed-in-the-terminal"]], "6. Train/validation split using Ray Data": [[248, "train-validation-split-using-ray-data"], [250, "train-validation-split-using-ray-data"]], "6. Verify the Installation": [[109, "verify-the-installation"], [117, null]], "6. When to use Ray Data": [[8, "when-to-use-ray-data"], [55, null]], "7. Accessing the training results": [[304, "accessing-the-training-results"], [319, null]], "7. Clean up": [[99, "clean-up"], [107, null]], "7. Data Operations: grouping, aggregation, and shuffling": [[9, "data-operations-grouping-aggregation-and-shuffling"], [64, null]], "7. Define matrix factorization model": [[248, "define-matrix-factorization-model"], [251, null]], "7. Image transform": [[268, "image-transform"], [271, "image-transform"]], "7. In the Anyscale Jobs console, we can check out the status of the submitted job. From the logs, we can verify that our job was successfully executed and Anyscale will now handle the cleanup.": [[159, "in-the-anyscale-jobs-console-we-can-check-out-the-status-of-the-submitted-job-from-the-logs-we-can-verify-that-our-job-was-successfully-executed-and-anyscale-will-now-handle-the-cleanup"], [174, "in-the-anyscale-jobs-console-we-can-check-out-the-status-of-the-submitted-job-from-the-logs-we-can-verify-that-our-job-was-successfully-executed-and-anyscale-will-now-handle-the-cleanup"]], "7. Inspect a mini-batch": [[255, "inspect-a-mini-batch"], [257, "inspect-a-mini-batch"]], "7. Inspect one random batch": [[261, "inspect-one-random-batch"], [263, "inspect-one-random-batch"]], "7. Pattern: Pipeline data processing and waiting for results": [[2, "pattern-pipeline-data-processing-and-waiting-for-results"], [24, null]], "7. Plot train / val loss": [[242, "plot-train-val-loss"], [246, "plot-train-val-loss"]], "7. Ray Data in Production": [[8, "ray-data-in-production"], [55, "ray-data-in-production"]], "7. Register the Anyscale Cloud": [[122, "register-the-anyscale-cloud"], [131, null]], "7. Shuffle and Train/Val split": [[235, "shuffle-and-train-val-split"], [237, "shuffle-and-train-val-split"]], "7. Shutdown Ray Cluster": [[221, "shutdown-ray-cluster"], [228, "shutdown-ray-cluster"]], "7. Test": [[109, "test"], [118, null]], "7.1 Batch Processing Pattern": [[2, "batch-processing-pattern"], [24, "batch-processing-pattern"]], "7.1. Custom batching using groupby.": [[9, "custom-batching-using-groupby"], [64, "custom-batching-using-groupby"]], "7.2 Note on fetching too many objects at once with ray.get causes failure": [[2, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"], [24, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"]], "7.2. Aggregations": [[9, "aggregations"], [64, "aggregations"]], "7.3. Shuffling data": [[9, "shuffling-data"], [64, "shuffling-data"]], "7.3.1. File based shuffle on read": [[9, "file-based-shuffle-on-read"], [64, "file-based-shuffle-on-read"]], "7.3.2. Shuffling block order": [[9, "shuffling-block-order"], [64, "shuffling-block-order"]], "7.3.3. Shuffle all rows globally": [[9, "shuffle-all-rows-globally"], [64, "shuffle-all-rows-globally"]], "8. Conclusion": [[99, "conclusion"], [108, null]], "8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)": [[248, "define-ray-train-loop-with-validation-checkpointing-and-ray-managed-metrics"], [252, null]], "8. Define the Ray Train worker loop (Arrow-based, memory-efficient)": [[255, "define-the-ray-train-worker-loop-arrow-based-memory-efficient"], [258, null]], "8. Install the Anyscale Operator": [[122, "install-the-anyscale-operator"], [132, null]], "8. Persisting data": [[9, "persisting-data"], [65, null]], "8. Pixel diffusion LightningModule": [[235, "pixel-diffusion-lightningmodule"], [238, null]], "8. Ray Actors": [[2, "ray-actors"], [25, null]], "8. Ray Train in Production": [[304, "ray-train-in-production"], [320, null]], "8. Ray-prepared DataLoader": [[261, "ray-prepared-dataloader"], [263, "ray-prepared-dataloader"]], "8. Reverse diffusion helper": [[242, "reverse-diffusion-helper"], [247, null]], "8. Summary": [[221, "summary"], [228, "summary"]], "8. Test": [[122, "test"], [133, null]], "8. Train/validation split": [[268, "train-validation-split"], [271, "train-validation-split"]], "8. Troubleshooting": [[109, "troubleshooting"], [119, null]], "8. Upcoming Features in Ray Data": [[8, "upcoming-features-in-ray-data"], [55, "upcoming-features-in-ray-data"]], "9. Clean up": [[109, "clean-up"], [120, null]], "9. Cleanup": [[122, "cleanup"], [134, null]], "9. Configure XGBoost and build the Trainer": [[255, "configure-xgboost-and-build-the-trainer"], [258, "configure-xgboost-and-build-the-trainer"]], "9. Inspect a DataLoader batch": [[268, "inspect-a-dataloader-batch"], [271, "inspect-a-dataloader-batch"]], "9. Launch distributed training with Ray Train": [[248, "launch-distributed-training-with-ray-train"], [252, "launch-distributed-training-with-ray-train"]], "9. PositionalEncoding and Transformer model": [[261, "positionalencoding-and-transformer-model"], [264, null]], "9. Ray Data in production": [[9, "ray-data-in-production"], [66, null]], "9. Ray Train train_loop (Lightning + Ray integration)": [[235, "ray-train-train-loop-lightning-ray-integration"], [239, null]], "9. Sample an action from the trained policy": [[242, "sample-an-action-from-the-trained-policy"], [247, "sample-an-action-from-the-trained-policy"]], "API Endpoints": [[146, "api-endpoints"]], "Activity: Update the training loop to compute the area under the curve of ROC (AUROC)": [[304, "activity-update-the-training-loop-to-compute-the-area-under-the-curve-of-roc-auroc"], [319, "activity-update-the-training-loop-to-compute-the-area-under-the-curve-of-roc-auroc"]], "Adding New Notebooks or Courses": [[0, "adding-new-notebooks-or-courses"]], "Additional Setup (Optional)": [[136, "additional-setup-optional"], [149, "additional-setup-optional"]], "Advanced LLM Features with Ray Serve LLM": [[295, null], [296, null]], "Advanced Topics: Monitoring & Optimization": [[287, "advanced-topics-monitoring-optimization"], [293, null]], "After navigating to a specific Anyscale Workspace, you can submit your main python script as a Anyscale Job.": [[159, "after-navigating-to-a-specific-anyscale-workspace-you-can-submit-your-main-python-script-as-a-anyscale-job"], [174, "after-navigating-to-a-specific-anyscale-workspace-you-can-submit-your-main-python-script-as-a-anyscale-job"]], "Aggregations": [[306, "aggregations"], [331, "aggregations"]], "Annotated experiment table": [[305, "annotated-experiment-table"], [325, "annotated-experiment-table"]], "Anyscale 101 Learning Path": [[161, "anyscale-101-learning-path"]], "Anyscale Administrator Overview": [[73, null], [74, null]], "Anyscale Observability": [[137, "anyscale-observability"], [140, null]], "Anyscale Projects": [[157, "anyscale-projects"], [169, "anyscale-projects"]], "Anyscale Ray Serve Observability": [[142, "anyscale-ray-serve-observability"], [145, "anyscale-ray-serve-observability"]], "Anyscale Service Lifecycle": [[160, "anyscale-service-lifecycle"], [176, "anyscale-service-lifecycle"]], "Apache Arrow": [[7, "apache-arrow"], [43, "apache-arrow"]], "Applications": [[10, "applications"], [69, "applications"]], "Architecture": [[146, "architecture"], [205, "architecture"], [208, null], [229, "architecture"], [232, null]], "Available Endpoints": [[146, "available-endpoints"]], "Batch Inference Class": [[205, "batch-inference-class"], [210, null]], "Batch Inference with Ray Data": [[205, null], [207, null]], "Batch Processing": [[7, "batch-processing"], [46, "batch-processing"]], "Benefits of using Anyscale Services": [[160, "benefits-of-using-anyscale-services"], [176, "benefits-of-using-anyscale-services"]], "Build and load our model on a single GPU": [[304, "build-and-load-our-model-on-a-single-gpu"], [313, "build-and-load-our-model-on-a-single-gpu"]], "Challenges in LLM Serving": [[278, "challenges-in-llm-serving"], [279, "challenges-in-llm-serving"], [283, null]], "Challenges with JVM": [[7, "challenges-with-jvm"], [46, "challenges-with-jvm"]], "Clean Up": [[154, "clean-up"], [155, "clean-up"], [156, "clean-up"], [166, "clean-up"], [167, "clean-up"], [168, "clean-up"]], "Clean up": [[307, "clean-up"], [338, null]], "Cloning/Duplicating Resources": [[157, "cloning-duplicating-resources"], [169, "cloning-duplicating-resources"]], "Cloud": [[158, "cloud"], [171, "cloud"]], "Collaborating with Your Team": [[161, "collaborating-with-your-team"]], "Composing Deployments": [[307, "composing-deployments"], [336, "composing-deployments"]], "Compute by Function": [[7, "compute-by-function"], [44, "compute-by-function"]], "Conclusion: Next Steps": [[295, "conclusion-next-steps"], [302, null]], "Concurrency Optimization Strategies": [[287, "concurrency-optimization-strategies"], [293, "concurrency-optimization-strategies"]], "Configuration Breakdown": [[287, "configuration-breakdown"], [290, "configuration-breakdown"]], "Configuration for Medium-Sized Models": [[287, "configuration-for-medium-sized-models"], [290, "configuration-for-medium-sized-models"]], "Configure Ray Serve LLM with LoRA": [[295, "configure-ray-serve-llm-with-lora"], [298, "configure-ray-serve-llm-with-lora"]], "Configure persistent storage": [[304, "configure-persistent-storage"], [318, "configure-persistent-storage"]], "Configure scale and GPUs": [[304, "configure-scale-and-gpus"], [315, "configure-scale-and-gpus"]], "Configure the Worker Node(s)": [[152, "configure-the-worker-node-s"], [164, "configure-the-worker-node-s"]], "Content Used": [[159, null], [173, null]], "Convert to Pandas DataFrame": [[213, "convert-to-pandas-dataframe"], [220, "convert-to-pandas-dataframe"]], "Convert to Ray Dataset": [[213, "convert-to-ray-dataset"], [217, null]], "Course Welcome and Overview": [[11, "course-welcome-and-overview"], [72, "course-welcome-and-overview"]], "Create a Second Dataset": [[213, "create-a-second-dataset"], [217, "create-a-second-dataset"]], "Create a batch data and call the model": [[205, "create-a-batch-data-and-call-the-model"], [211, null]], "Create custom security group": [[73, "create-custom-security-group"], [78, "create-custom-security-group"]], "Creating Anyscale Resources": [[84, "creating-anyscale-resources"], [86, "creating-anyscale-resources"], [91, "creating-anyscale-resources"], [95, "creating-anyscale-resources"], [99, "creating-anyscale-resources"], [101, "creating-anyscale-resources"]], "Creating a Compute Config": [[152, "creating-a-compute-config"], [164, "creating-a-compute-config"]], "Creating a Container Image": [[152, "creating-a-container-image"], [164, "creating-a-container-image"]], "Custom batching using groupby": [[306, "custom-batching-using-groupby"], [331, "custom-batching-using-groupby"]], "Custom batching using groupby and aggregations": [[8, "custom-batching-using-groupby-and-aggregations"], [54, "custom-batching-using-groupby-and-aggregations"]], "Customization": [[0, "customization"]], "Customizing autoscaling": [[307, "customizing-autoscaling"], [336, "customizing-autoscaling"]], "Data Engineering Compute": [[7, "data-engineering-compute"], [44, "data-engineering-compute"]], "Data Pipeline Observability (Ray Data)": [[142, "data-pipeline-observability-ray-data"], [144, null]], "Data Processing and ML examples with Ray": [[206, null], [214, null], [222, null], [230, null]], "Data Processing with Ray Data": [[7, "data-processing-with-ray-data"], [47, null], [213, null], [215, null]], "Data flow": [[7, "data-flow"], [48, "data-flow"]], "Data lakes": [[7, "data-lakes"], [43, "data-lakes"]], "Data warehouses": [[7, "data-warehouses"], [43, "data-warehouses"]], "Databases": [[7, "databases"], [43, "databases"]], "Dataloaders": [[221, "dataloaders"], [226, "dataloaders"]], "Dataset": [[8, "dataset"], [51, "dataset"]], "Decode Phase": [[278, "decode-phase"], [279, "decode-phase"], [281, "decode-phase"]], "Default settings for Ray Tune": [[305, "default-settings-for-ray-tune"], [325, "default-settings-for-ray-tune"]], "Defining a data loader": [[304, "defining-a-data-loader"], [313, "defining-a-data-loader"]], "Deploy a Medium-Sized LLM with Ray Serve LLM": [[287, null], [288, null]], "Deploy the model": [[229, "deploy-the-model"], [233, "deploy-the-model"]], "Deploying Applications with Services": [[161, "deploying-applications-with-services"]], "Deploying Pipelines with Jobs": [[161, "deploying-pipelines-with-jobs"]], "Deploying at scale": [[205, "deploying-at-scale"], [211, "deploying-at-scale"]], "Deploying to Anyscale Services": [[287, "deploying-to-anyscale-services"], [292, null]], "Deployment": [[146, "deployment"]], "Deployment Example Structure": [[135, "deployment-example-structure"]], "Deployment Options: Virtual Machines vs. Kubernetes": [[80, null], [81, null]], "Deployments": [[10, "deployments"], [69, "deployments"], [307, "deployments"], [334, "deployments"]], "Developer Intro to Ray": [[339, "developer-intro-to-ray"]], "Developing in Anyscale Workspaces": [[161, "developing-in-anyscale-workspaces"]], "Disabling Notebook Execution and Outputs": [[0, "disabling-notebook-execution-and-outputs"]], "Distributed Computing Frameworks": [[7, "distributed-computing-frameworks"], [46, null]], "Distributed Data-Parallel Training with Ray Train": [[178, "distributed-data-parallel-training-with-ray-train"], [188, "distributed-data-parallel-training-with-ray-train"]], "Distributed training with Ray Train, PyTorch and Hugging Face": [[221, null], [223, null]], "Distributed training with Ray Train, PyTorch and HuggingFace": [[206, "distributed-training-with-ray-train-pytorch-and-huggingface"], [214, "distributed-training-with-ray-train-pytorch-and-huggingface"], [222, "distributed-training-with-ray-train-pytorch-and-huggingface"], [230, "distributed-training-with-ray-train-pytorch-and-huggingface"]], "Diving deeper into Ray Tune concepts": [[6, "diving-deeper-into-ray-tune-concepts"], [41, "diving-deeper-into-ray-tune-concepts"]], "Dual-Subnet Architecture": [[73, "dual-subnet-architecture"], [78, "dual-subnet-architecture"]], "Enabling LLM Monitoring": [[287, "enabling-llm-monitoring"], [293, "enabling-llm-monitoring"]], "End of Module 01 \u00b7 Introduction to Ray Train": [[178, "end-of-module-01-introduction-to-ray-train"], [191, "end-of-module-01-introduction-to-ray-train"]], "Environment state and action": [[242, "environment-state-and-action"], [243, "environment-state-and-action"]], "Example": [[137, "example"], [141, null]], "Example Workflow": [[0, "example-workflow"]], "Example: Car type description": [[295, "example-car-type-description"], [299, "example-car-type-description"]], "Example: Code Assistant LoRA": [[295, "example-code-assistant-lora"], [298, "example-code-assistant-lora"]], "Example: Deploying LoRA Adapters": [[295, "example-deploying-lora-adapters"], [298, null]], "Example: Getting Structured JSON Output": [[295, "example-getting-structured-json-output"], [299, null]], "Example: Setting up Tool Calling": [[295, "example-setting-up-tool-calling"], [300, null]], "Example: Weather Assistant with Tool Calling": [[295, "example-weather-assistant-with-tool-calling"], [300, "example-weather-assistant-with-tool-calling"]], "Execution mode": [[8, "execution-mode"], [52, "execution-mode"], [306, "execution-mode"], [330, "execution-mode"]], "Exercise": [[6, "exercise"], [41, "exercise"], [305, "exercise"], [325, "exercise"]], "Expected Output": [[295, "expected-output"], [299, "expected-output"]], "Exploring the Anyscale Log Viewer": [[154, "exploring-the-anyscale-log-viewer"], [166, "exploring-the-anyscale-log-viewer"]], "Exploring the Anyscale Metrics Tab": [[154, "exploring-the-anyscale-metrics-tab"], [166, "exploring-the-anyscale-metrics-tab"]], "Exploring the Ray Dashboard": [[154, "exploring-the-ray-dashboard"], [166, "exploring-the-ray-dashboard"]], "FastAPI webservice and deploy a model": [[229, "fastapi-webservice-and-deploy-a-model"], [233, null]], "Features": [[0, "features"]], "File based shuffle on read": [[8, "file-based-shuffle-on-read"], [54, "file-based-shuffle-on-read"], [306, "file-based-shuffle-on-read"], [331, "file-based-shuffle-on-read"]], "Filter Ray Dataset": [[213, "filter-ray-dataset"], [218, null]], "Forward process: adding noise": [[235, "forward-process-adding-noise"], [236, "forward-process-adding-noise"]], "General-Purpose Distributed Computing": [[7, "general-purpose-distributed-computing"], [46, "general-purpose-distributed-computing"]], "Get User Profile": [[146, "get-user-profile"]], "Getting Started": [[161, "getting-started"]], "Getting Started with Ray Serve LLM": [[278, "getting-started-with-ray-serve-llm"], [279, "getting-started-with-ray-serve-llm"], [285, null]], "Getting started": [[6, "getting-started"], [41, "getting-started"], [305, "getting-started"], [324, "getting-started"]], "Gettingstarted": [[339, "gettingstarted"]], "How Navigation Works": [[0, "how-navigation-works"]], "How Other Sizes Differ": [[287, "how-other-sizes-differ"], [294, "how-other-sizes-differ"]], "How Resources are defined": [[73, "how-resources-are-defined"], [76, "how-resources-are-defined"]], "How to Choose an LLM?": [[295, "how-to-choose-an-llm"], [301, null]], "How to migrate this computer vision workload to a distributed setup using Ray on Anyscale": [[268, "how-to-migrate-this-computer-vision-workload-to-a-distributed-setup-using-ray-on-anyscale"], [269, "how-to-migrate-this-computer-vision-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this diffusion-policy workload to a distributed setup using Ray on Anyscale": [[235, "how-to-migrate-this-diffusion-policy-workload-to-a-distributed-setup-using-ray-on-anyscale"], [236, "how-to-migrate-this-diffusion-policy-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this recommendation system workload to a distributed setup using Ray on Anyscale": [[248, "how-to-migrate-this-recommendation-system-workload-to-a-distributed-setup-using-ray-on-anyscale"], [249, "how-to-migrate-this-recommendation-system-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this tabular workload to a distributed setup using Ray on Anyscale": [[255, "how-to-migrate-this-tabular-workload-to-a-distributed-setup-using-ray-on-anyscale"], [256, "how-to-migrate-this-tabular-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this time-series workload to a distributed multi-node setup using Ray on Anyscale": [[261, "how-to-migrate-this-time-series-workload-to-a-distributed-multi-node-setup-using-ray-on-anyscale"], [262, "how-to-migrate-this-time-series-workload-to-a-distributed-multi-node-setup-using-ray-on-anyscale"]], "How to scale this policy learning workload using Ray on Anyscale": [[242, "how-to-scale-this-policy-learning-workload-using-ray-on-anyscale"], [243, "how-to-scale-this-policy-learning-workload-using-ray-on-anyscale"]], "How we use Ray AI Libraries for this task": [[303, "how-we-use-ray-ai-libraries-for-this-task"], [310, "how-we-use-ray-ai-libraries-for-this-task"]], "Hyperparameter tune the PyTorch model using Ray Tune": [[6, "hyperparameter-tune-the-pytorch-model-using-ray-tune"], [41, "hyperparameter-tune-the-pytorch-model-using-ray-tune"]], "Import Libraries": [[205, "import-libraries"], [208, "import-libraries"]], "Import libraries": [[229, "import-libraries"], [232, "import-libraries"]], "Improving Concurrency": [[287, "improving-concurrency"], [293, "improving-concurrency"]], "In-memory data formats": [[7, "in-memory-data-formats"], [43, "in-memory-data-formats"]], "Inference: ranking items per user": [[248, "inference-ranking-items-per-user"], [249, "inference-ranking-items-per-user"]], "Initialize Ray and Load a Dataset": [[213, "initialize-ray-and-load-a-dataset"], [216, "initialize-ray-and-load-a-dataset"]], "Input: Images as tensors": [[235, "input-images-as-tensors"], [236, "input-images-as-tensors"]], "Input: user\u2013item\u2013rating triples": [[248, "input-useritemrating-triples"], [249, "input-useritemrating-triples"]], "Inputs": [[268, "inputs"], [269, "inputs"]], "Inspecting the features of the NYC taxi dataset": [[303, "inspecting-the-features-of-the-nyc-taxi-dataset"], [310, "inspecting-the-features-of-the-nyc-taxi-dataset"]], "Installation": [[0, "installation"], [206, "installation"], [214, "installation"], [222, "installation"], [230, "installation"]], "Integrating with FastAPI": [[307, "integrating-with-fastapi"], [336, "integrating-with-fastapi"]], "Intro to Ray Data": [[306, null], [327, null]], "Intro to Ray Data:  Ray Data + Unstructured Data": [[9, null], [56, null]], "Intro to Ray Serve": [[307, null], [333, null]], "Intro to Ray Tune": [[6, "intro-to-ray-tune"], [41, "intro-to-ray-tune"], [305, null], [321, null]], "Introduction": [[151, "introduction"], [152, "introduction"], [153, "introduction"], [154, "introduction"], [163, "introduction"], [164, "introduction"], [165, "introduction"], [166, "introduction"]], "Introduction to Ray Core (Advancement): Object store, Tasks, Actors": [[2, null], [17, null]], "Introduction to Ray Core: Getting Started": [[1, null], [12, null]], "Introduction to Ray Data: Industry Landscape": [[7, null], [43, null]], "Introduction to Ray Data: Ray Data + Structured Data": [[8, null], [49, null]], "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving": [[278, null], [279, null], [280, null]], "Introduction to Ray Serve with PyTorch": [[10, null], [67, null]], "Introduction to Ray Train": [[304, null], [312, null]], "Introduction to Ray Train + PyTorch": [[4, null], [29, null]], "Introduction to Ray Train: Ray Train + PyTorch Lightning": [[5, null], [33, null]], "Introduction to Ray Tune": [[6, null], [38, null]], "Introduction to Ray: Developer": [[11, null], [72, null]], "Introduction to the Ray AI Libraries": [[303, null], [308, null]], "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model": [[3, null], [26, null]], "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster": [[99, null], [100, null]], "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster": [[122, null], [123, null]], "Introduction: Deploy Anyscale Ray on AWS EC2 Instances": [[84, null], [85, null]], "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster": [[109, null], [110, null]], "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)": [[91, null], [92, null]], "Introduction: Why Anyscale?": [[161, "introduction-why-anyscale"]], "Join Two Ray Datasets": [[213, "join-two-ray-datasets"], [219, null]], "Key Benefits": [[295, "key-benefits"], [295, "id1"], [295, "id3"], [298, "key-benefits"], [299, "key-benefits"], [300, "key-benefits"]], "Key Components": [[287, "key-components"], [290, "key-components"]], "Key Concepts and Optimizations": [[278, "key-concepts-and-optimizations"], [279, "key-concepts-and-optimizations"], [282, null]], "Key Features": [[159, "key-features"], [173, "key-features"]], "Key Functions": [[73, "key-functions"], [75, "key-functions"]], "Key Ray Serve Features": [[307, "key-ray-serve-features"], [334, "key-ray-serve-features"]], "Key Takeaways": [[278, "key-takeaways"], [279, "key-takeaways"], [286, null], [287, "key-takeaways"], [294, "key-takeaways"], [295, "key-takeaways"], [302, "key-takeaways"]], "Key characteristics of Anyscale Services": [[156, "key-characteristics-of-anyscale-services"], [168, "key-characteristics-of-anyscale-services"]], "Key points": [[304, "key-points"], [315, "key-points"]], "Labels": [[268, "labels"], [269, "labels"]], "Lakehouses": [[7, "lakehouses"], [43, "lakehouses"]], "Last Updated 6/19": [[161, null]], "Launch Grafana": [[136, "launch-grafana"], [149, "launch-grafana"]], "Launching Ray Serve": [[287, "launching-ray-serve"], [291, "launching-ray-serve"]], "Launching a Anyscale Workspace": [[150, "launching-a-anyscale-workspace"], [162, "launching-a-anyscale-workspace"]], "Launching a Web Application using Ray Serve": [[142, "launching-a-web-application-using-ray-serve"], [145, "launching-a-web-application-using-ray-serve"]], "Launching a distributed training job with a TorchTrainer.": [[304, "launching-a-distributed-training-job-with-a-torchtrainer"], [318, "launching-a-distributed-training-job-with-a-torchtrainer"]], "Launching the Service": [[287, "launching-the-service"], [292, "launching-the-service"]], "Learn More": [[295, "learn-more"], [295, "id2"], [295, "id4"], [298, "learn-more"], [299, "learn-more"], [300, "learn-more"]], "Learning Approach": [[295, "learning-approach"], [297, "learning-approach"]], "Learning Path Overview and Objectives": [[161, "learning-path-overview-and-objectives"]], "Library Imports": [[213, "library-imports"], [216, null]], "Llm Serving": [[339, "llm-serving"]], "Load a dataset": [[205, "load-a-dataset"], [209, null]], "Loading and visualizing MNIST data": [[304, "loading-and-visualizing-mnist-data"], [313, "loading-and-visualizing-mnist-data"]], "Local Deployment & Inference": [[287, "local-deployment-inference"], [291, null]], "Local Development": [[146, "local-development"]], "Local IDE (VSCode / Cursor)": [[151, "local-ide-vscode-cursor"], [163, "local-ide-vscode-cursor"]], "Logging Configuration": [[142, "logging-configuration"], [145, "logging-configuration"]], "Machine Learning and AI Compute": [[7, "machine-learning-and-ai-compute"], [44, "machine-learning-and-ai-compute"]], "Managing Dependencies": [[11, "managing-dependencies"], [72, "managing-dependencies"]], "Materializing Data": [[306, "materializing-data"], [330, "materializing-data"]], "Model Recommendations by Use Case": [[295, "model-recommendations-by-use-case"], [301, "model-recommendations-by-use-case"]], "Model Selection Framework": [[295, "model-selection-framework"], [301, "model-selection-framework"]], "Model Size Comparison": [[287, "model-size-comparison"], [289, "model-size-comparison"]], "Model: embedding-based matrix factorization": [[248, "model-embedding-based-matrix-factorization"], [249, "model-embedding-based-matrix-factorization"]], "More Advanced Topics": [[295, "more-advanced-topics"], [302, "more-advanced-topics"]], "More about Datasets": [[306, "more-about-datasets"], [329, "more-about-datasets"]], "Multi-Actor Ray Serve Tracing Example": [[146, null]], "Next Steps": [[278, "next-steps"], [279, "next-steps"], [286, "next-steps"], [287, "next-steps"], [294, "next-steps"], [295, "next-steps"], [302, "next-steps"]], "Note on the Checkpoint Lifecycle": [[178, "note-on-the-checkpoint-lifecycle"], [188, "note-on-the-checkpoint-lifecycle"]], "Notebook": [[151, "notebook"], [163, "notebook"]], "Now launch a Ray worker node in the terminal:": [[136, "now-launch-a-ray-worker-node-in-the-terminal"], [149, "now-launch-a-ray-worker-node-in-the-terminal"]], "Observability Introduction": [[136, null], [147, null]], "Observability Overview": [[136, "observability-overview"], [148, null]], "On Ray Data vs Spark": [[7, "on-ray-data-vs-spark"], [47, "on-ray-data-vs-spark"]], "Online Model Serving with Ray Serve": [[229, null], [231, null]], "Option 1: Create a New VPC": [[73, "option-1-create-a-new-vpc"], [78, "option-1-create-a-new-vpc"]], "Option 2: Use Existing VPC": [[73, "option-2-use-existing-vpc"], [78, "option-2-use-existing-vpc"]], "Organization": [[158, "organization"], [171, "organization"]], "Our Example: Llama-3.1-70B": [[287, "our-example-llama-3-1-70b"], [289, "our-example-llama-3-1-70b"]], "Out of memory errors": [[205, "out-of-memory-errors"], [212, "out-of-memory-errors"]], "Outline": [[205, "outline"], [207, "outline"], [221, "outline"], [223, "outline"], [229, "outline"], [231, "outline"]], "Outline of the notebook": [[213, "outline-of-the-notebook"], [215, "outline-of-the-notebook"]], "Outlook": [[73, "outlook"], [74, "outlook"]], "Outlook:  Ray Data in Production": [[306, "outlook-ray-data-in-production"], [332, "outlook-ray-data-in-production"]], "Overview": [[146, "overview"]], "Overview: Advanced Features Preview": [[295, "overview-advanced-features-preview"], [297, null]], "Overview: Why Medium-Sized Models?": [[287, "overview-why-medium-sized-models"], [289, null]], "Part 1. Creating and Submitting your first job": [[155, "part-1-creating-and-submitting-your-first-job"], [159, "part-1-creating-and-submitting-your-first-job"], [167, "part-1-creating-and-submitting-your-first-job"], [174, null]], "Part 1: Starting your first Anyscale Service": [[156, "part-1-starting-your-first-anyscale-service"], [160, "part-1-starting-your-first-anyscale-service"], [168, "part-1-starting-your-first-anyscale-service"], [177, null]], "Part 2. Automation and Scheduling": [[155, "part-2-automation-and-scheduling"], [159, "part-2-automation-and-scheduling"], [167, "part-2-automation-and-scheduling"], [175, null]], "Practical Selection Process": [[295, "practical-selection-process"], [301, "practical-selection-process"]], "Prefill Phase": [[278, "prefill-phase"], [279, "prefill-phase"], [281, "prefill-phase"]], "Preprocessing with a Tokenizer": [[213, "preprocessing-with-a-tokenizer"], [220, null]], "Prerequisites": [[11, "prerequisites"], [72, "prerequisites"], [84, "prerequisites"], [85, "prerequisites"], [91, "prerequisites"], [93, null], [99, "prerequisites"], [100, "prerequisites"], [109, "prerequisites"], [111, null], [122, "prerequisites"], [124, null], [135, "prerequisites"], [142, "prerequisites"], [143, "prerequisites"], [146, "prerequisites"], [287, "prerequisites"], [291, "prerequisites"]], "Prerequisites and Assumptions": [[136, "prerequisites-and-assumptions"], [149, "prerequisites-and-assumptions"]], "Projects": [[158, "projects"], [171, "projects"]], "Providers": [[99, "providers"], [100, "providers"]], "Publishing": [[0, "publishing"]], "Purpose": [[73, "purpose"], [75, "purpose"]], "Ray 101": [[339, "ray-101"]], "Ray Data Logs": [[142, "ray-data-logs"], [144, "ray-data-logs"]], "Ray Data Metrics": [[142, "ray-data-metrics"], [144, "ray-data-metrics"]], "Ray Enablement Content": [[339, null]], "Ray Enablement Content: Jupyter Book Publishing": [[0, null]], "Ray Observability": [[137, "ray-observability"], [139, null]], "Ray Serve": [[7, "ray-serve"], [48, null]], "Ray Serve Alerts": [[142, "ray-serve-alerts"], [145, "ray-serve-alerts"]], "Ray Serve LLM + Anyscale Architecture": [[278, "ray-serve-llm-anyscale-architecture"], [279, "ray-serve-llm-anyscale-architecture"], [284, null]], "Ray Serve Logs": [[142, "ray-serve-logs"], [145, "ray-serve-logs"]], "Ray Serve Metrics": [[142, "ray-serve-metrics"], [145, "ray-serve-metrics"]], "Ray Serve Tracing (Anyscale Only)": [[142, "ray-serve-tracing-anyscale-only"], [145, "ray-serve-tracing-anyscale-only"]], "Ray Serve vs Ray Data": [[7, "ray-serve-vs-ray-data"], [48, "ray-serve-vs-ray-data"]], "Ray Train Foundation": [[339, "ray-train-foundation"]], "Ray Workloads Data Dashboard": [[142, "ray-workloads-data-dashboard"], [144, "ray-workloads-data-dashboard"]], "Ray and Anyscale Observability Introduction": [[137, null], [138, null]], "Ray and Anyscale Observability in Detail": [[142, null], [143, null]], "Recap": [[303, "recap"], [310, "recap"], [311, "recap"]], "Register New User": [[146, "register-new-user"]], "Related Examples": [[287, "related-examples"], [289, "related-examples"]], "Related Examples & Templates": [[287, "related-examples-templates"], [294, "related-examples-templates"]], "Replicas": [[10, "replicas"], [69, "replicas"]], "Reporting metrics": [[304, "reporting-metrics"], [313, "reporting-metrics"]], "Request Flow": [[146, "request-flow"]], "Requirements": [[99, "requirements"], [100, "requirements"]], "Resources": [[278, "resources"], [279, "resources"], [286, "resources"], [287, "resources"], [294, "resources"], [295, "resources"], [302, "resources"]], "Reverse diffusion: sampling new images": [[235, "reverse-diffusion-sampling-new-images"], [236, "reverse-diffusion-sampling-new-images"]], "Run a simple Data Pipeline": [[142, "run-a-simple-data-pipeline"], [144, "run-a-simple-data-pipeline"]], "Run inference on the entire dataset": [[205, "run-inference-on-the-entire-dataset"], [212, null]], "Running Inference on Anyscale": [[287, "running-inference-on-anyscale"], [292, "running-inference-on-anyscale"]], "Running this notebook": [[303, "running-this-notebook"], [308, "running-this-notebook"]], "Sample Requests": [[146, "sample-requests"]], "Saving a checkpoint in a local directory": [[304, "saving-a-checkpoint-in-a-local-directory"], [313, "saving-a-checkpoint-in-a-local-directory"]], "Scaling deployment": [[229, "scaling-deployment"], [233, "scaling-deployment"]], "Scheduling the training loop on a single GPU": [[304, "scheduling-the-training-loop-on-a-single-gpu"], [313, "scheduling-the-training-loop-on-a-single-gpu"]], "Sending Requests": [[287, "sending-requests"], [291, "sending-requests"]], "Setting Up Local Ray Observability": [[136, "setting-up-local-ray-observability"], [149, null]], "Setting Up a Local Ray server using Jupyter Notebook": [[11, "setting-up-a-local-ray-server-using-jupyter-notebook"], [72, "setting-up-a-local-ray-server-using-jupyter-notebook"]], "Setting up Ray Serve LLM": [[287, "setting-up-ray-serve-llm"], [290, null]], "Setting up the Configuration File": [[287, "setting-up-the-configuration-file"], [292, "setting-up-the-configuration-file"]], "Setup and Installation": [[146, "setup-and-installation"]], "Shuffle all rows globally": [[8, "shuffle-all-rows-globally"], [54, "shuffle-all-rows-globally"]], "Shuffle rows globally": [[306, "shuffle-rows-globally"], [331, "shuffle-rows-globally"]], "Shuffling block order": [[8, "shuffling-block-order"], [54, "shuffling-block-order"], [306, "shuffling-block-order"], [331, "shuffling-block-order"]], "Shuffling data": [[8, "shuffling-data"], [54, "shuffling-data"], [306, "shuffling-data"], [331, "shuffling-data"]], "Shutdown Ray": [[213, "shutdown-ray"], [220, "shutdown-ray"]], "Shutdown Ray cluster": [[205, "shutdown-ray-cluster"], [212, "shutdown-ray-cluster"]], "Shutdown the Ray Serve instances and Ray Cluster": [[229, "shutdown-the-ray-serve-instances-and-ray-cluster"], [234, "shutdown-the-ray-serve-instances-and-ray-cluster"]], "Shutting Down": [[287, "shutting-down"], [291, "shutting-down"]], "Shutting Down the Service": [[287, "shutting-down-the-service"], [292, "shutting-down-the-service"]], "Simulate Client: Send test requests": [[229, "simulate-client-send-test-requests"], [234, null]], "Sources": [[160, null], [176, null]], "Start by launching the Ray head node in the terminal:": [[136, "start-by-launching-the-ray-head-node-in-the-terminal"], [149, "start-by-launching-the-ray-head-node-in-the-terminal"]], "Stateful transformations with actors": [[306, "stateful-transformations-with-actors"], [330, "stateful-transformations-with-actors"]], "Step 1: Configuration": [[278, "step-1-configuration"], [279, "step-1-configuration"], [285, "step-1-configuration"]], "Step 1: Install Required Packages": [[136, "step-1-install-required-packages"], [149, "step-1-install-required-packages"]], "Step 2: Deployment": [[278, "step-2-deployment"], [279, "step-2-deployment"], [285, "step-2-deployment"]], "Step 2: Launch Prometheus": [[136, "step-2-launch-prometheus"], [149, "step-2-launch-prometheus"]], "Step 3: Launch Ray Cluster": [[136, "step-3-launch-ray-cluster"], [149, "step-3-launch-ray-cluster"]], "Step 3: Querying": [[278, "step-3-querying"], [279, "step-3-querying"], [285, "step-3-querying"]], "Step 4: Install and Launch Grafana": [[136, "step-4-install-and-launch-grafana"], [149, "step-4-install-and-launch-grafana"]], "Step 4: Shutdown": [[279, "step-4-shutdown"], [285, "step-4-shutdown"]], "Steps to run:": [[303, "steps-to-run"], [310, "steps-to-run"], [311, "steps-to-run"]], "Streaming Applications": [[7, "streaming-applications"], [46, "streaming-applications"]], "Structure of a data lake": [[7, "structure-of-a-data-lake"], [43, "structure-of-a-data-lake"]], "Summary": [[205, "summary"], [212, "summary"], [213, "summary"], [220, "summary"], [229, "summary"], [234, "summary"]], "Summary & Outlook": [[287, "summary-outlook"], [294, null]], "Supported Infrastructure Types": [[73, "supported-infrastructure-types"], [76, "supported-infrastructure-types"]], "Testing the Container Image and Compute Config with an Anyscale Workflow": [[152, "testing-the-container-image-and-compute-config-with-an-anyscale-workflow"], [164, "testing-the-container-image-and-compute-config-with-an-anyscale-workflow"]], "The Compute Layer": [[7, "the-compute-layer"], [44, null]], "The LLM Text Generation Process": [[278, "the-llm-text-generation-process"], [279, "the-llm-text-generation-process"], [281, "the-llm-text-generation-process"]], "The Orchestration Layer": [[7, "the-orchestration-layer"], [45, null]], "The data layer": [[7, "the-data-layer"], [43, "the-data-layer"]], "The following instructions will walk you through running your first job.": [[159, "the-following-instructions-will-walk-you-through-running-your-first-job"], [173, "the-following-instructions-will-walk-you-through-running-your-first-job"]], "The following instructions will walk you through running your first job. This notebook covers the following:": [[155, "the-following-instructions-will-walk-you-through-running-your-first-job-this-notebook-covers-the-following"], [167, "the-following-instructions-will-walk-you-through-running-your-first-job-this-notebook-covers-the-following"]], "This guide will walk you through deploying, updating, and managing Anyscale Services": [[160, "this-guide-will-walk-you-through-deploying-updating-and-managing-anyscale-services"], [176, "this-guide-will-walk-you-through-deploying-updating-and-managing-anyscale-services"]], "This notebook covers the following:": [[156, "this-notebook-covers-the-following"], [168, "this-notebook-covers-the-following"]], "Tokenizer": [[221, "tokenizer"], [226, "tokenizer"]], "Trace Structure": [[146, "trace-structure"]], "Tracing Configuration": [[146, "tracing-configuration"]], "Training objective": [[235, "training-objective"], [236, "training-objective"], [248, "training-objective"], [249, "training-objective"]], "Two Phases of LLM Inference": [[278, "two-phases-of-llm-inference"], [279, "two-phases-of-llm-inference"], [281, "two-phases-of-llm-inference"]], "Usage": [[0, "usage"]], "Users and Roles": [[158, "users-and-roles"], [171, "users-and-roles"]], "Using LoRA Adapters": [[295, "using-lora-adapters"], [298, "using-lora-adapters"]], "Using Structured Output": [[295, "using-structured-output"], [299, "using-structured-output"]], "Using Tool Calling": [[295, "using-tool-calling"], [300, "using-tool-calling"]], "Using fractions of a GPU": [[307, "using-fractions-of-a-gpu"], [336, "using-fractions-of-a-gpu"]], "VSCode": [[151, "vscode"], [163, "vscode"]], "Web Application Observability (Ray Serve)": [[142, "web-application-observability-ray-serve"], [145, null]], "Welcome to Anyscale Administration": [[135, null]], "What We Accomplished": [[287, "what-we-accomplished"], [294, "what-we-accomplished"], [295, "what-we-accomplished"], [302, "what-we-accomplished"]], "What We\u2019ll Cover": [[295, "what-we-ll-cover"], [297, "what-we-ll-cover"]], "What You\u2019ll Learn": [[135, "what-you-ll-learn"]], "What does the model learn?": [[268, "what-does-the-model-learn"], [269, "what-does-the-model-learn"]], "What is LLM Serving?": [[278, "what-is-llm-serving"], [279, "what-is-llm-serving"], [281, null]], "What is Ray Data ?": [[7, "what-is-ray-data"], [47, "what-is-ray-data"]], "What is Ray Serve ?": [[7, "what-is-ray-serve"], [48, "what-is-ray-serve"]], "What is Ray Serve?": [[229, "what-is-ray-serve"], [231, "what-is-ray-serve"]], "What is an Anyscale Service?": [[287, "what-is-an-anyscale-service"], [292, "what-is-an-anyscale-service"]], "What problem are you solving? (Diffusion as image de-noising)": [[235, "what-problem-are-you-solving-diffusion-as-image-de-noising"], [236, "what-problem-are-you-solving-diffusion-as-image-de-noising"]], "What problem are you solving? (Forest cover classification with XGBoost)": [[255, "what-problem-are-you-solving-forest-cover-classification-with-xgboost"], [256, "what-problem-are-you-solving-forest-cover-classification-with-xgboost"]], "What problem are you solving? (Inverted Pendulum, Diffusion-Style)": [[242, "what-problem-are-you-solving-inverted-pendulum-diffusion-style"], [243, "what-problem-are-you-solving-inverted-pendulum-diffusion-style"]], "What problem are you solving? (NYC taxi demand forecasting with a Transformer)": [[261, "what-problem-are-you-solving-nyc-taxi-demand-forecasting-with-a-transformer"], [262, "what-problem-are-you-solving-nyc-taxi-demand-forecasting-with-a-transformer"]], "What problem are you solving? (image classification with Food-101-Lite)": [[268, "what-problem-are-you-solving-image-classification-with-food-101-lite"], [269, "what-problem-are-you-solving-image-classification-with-food-101-lite"]], "What problem are you solving? (matrix factorization for recommendations)": [[248, "what-problem-are-you-solving-matrix-factorization-for-recommendations"], [249, "what-problem-are-you-solving-matrix-factorization-for-recommendations"]], "What you learn and take away": [[235, "what-you-learn-and-take-away"], [236, "what-you-learn-and-take-away"], [242, "what-you-learn-and-take-away"], [243, "what-you-learn-and-take-away"], [248, "what-you-learn-and-take-away"], [249, "what-you-learn-and-take-away"], [255, "what-you-learn-and-take-away"], [256, "what-you-learn-and-take-away"], [261, "what-you-learn-and-take-away"], [262, "what-you-learn-and-take-away"], [268, "what-you-learn-and-take-away"], [269, "what-you-learn-and-take-away"]], "What you\u2019ll learn & take away": [[178, "what-youll-learn-take-away"], [178, "id1"], [178, "id3"], [179, "what-youll-learn-take-away"], [192, "what-youll-learn-take-away"], [198, "what-youll-learn-take-away"]], "What\u2019s Next": [[136, "what-s-next"], [149, "what-s-next"]], "What\u2019s XGBoost?": [[255, "what-s-xgboost"], [256, "what-s-xgboost"]], "What\u2019s a policy?": [[242, "what-s-a-policy"], [243, "what-s-a-policy"]], "What\u2019s a sequence-to-sequence Transformer?": [[261, "what-s-a-sequence-to-sequence-transformer"], [262, "what-s-a-sequence-to-sequence-transformer"]], "When to use Ray Core over Ray Data ?": [[7, "when-to-use-ray-core-over-ray-data"], [47, "when-to-use-ray-core-over-ray-data"]], "When to use Ray Serve?": [[307, "when-to-use-ray-serve"], [334, "when-to-use-ray-serve"]], "Where can you take this next?": [[235, "where-can-you-take-this-next"], [241, "where-can-you-take-this-next"], [242, "where-can-you-take-this-next"], [247, "where-can-you-take-this-next"], [248, "where-can-you-take-this-next"], [254, "where-can-you-take-this-next"], [255, "where-can-you-take-this-next"], [260, "where-can-you-take-this-next"], [261, "where-can-you-take-this-next"], [267, "where-can-you-take-this-next"], [268, "where-can-you-take-this-next"], [277, "where-can-you-take-this-next"]], "Why Choose Medium-Sized Models?": [[287, "why-choose-medium-sized-models"], [289, "why-choose-medium-sized-models"]], "Why Ray Data ?": [[7, "why-ray-data"], [47, "why-ray-data"]], "Why Ray Serve ?": [[7, "why-ray-serve"], [48, "why-ray-serve"]], "Why Ray?": [[11, "why-ray"], [72, "why-ray"]], "Why Structured Output Matters": [[295, "why-structured-output-matters"], [299, "why-structured-output-matters"]], "Why These Features Matter": [[295, "why-these-features-matter"], [297, "why-these-features-matter"]], "Why Tool Calling Matters": [[295, "why-tool-calling-matters"], [300, "why-tool-calling-matters"]], "Why Use LoRA Adapters?": [[295, "why-use-lora-adapters"], [298, "why-use-lora-adapters"]], "Why not Kubernetes ?": [[278, "why-not-kubernetes"], [279, "why-not-kubernetes"], [283, "why-not-kubernetes"]], "Why not use just FastAPI or Flask?": [[229, "why-not-use-just-fastapi-or-flask"], [231, "why-not-use-just-fastapi-or-flask"]], "Why this works": [[235, "why-this-works"], [236, "why-this-works"]], "Wl Ray Data Batch Inference": [[339, "wl-ray-data-batch-inference"]], "Wl Ray Data Processing": [[339, "wl-ray-data-processing"]], "Wl Ray Distributed Training": [[339, "wl-ray-distributed-training"]], "Wl Ray Serve Online Serving": [[339, "wl-ray-serve-online-serving"]], "Wl Train Generative Cv": [[339, "wl-train-generative-cv"]], "Wl Train Policy Learning": [[339, "wl-train-policy-learning"]], "Wl Train Rec Sys": [[339, "wl-train-rec-sys"]], "Wl Train Tabular": [[339, "wl-train-tabular"]], "Wl Train Time Series": [[339, "wl-train-time-series"]], "Wl Train Vision Pattern": [[339, "wl-train-vision-pattern"]], "Workloads": [[158, "workloads"], [171, "workloads"]], "Wrap up and next steps": [[235, "wrap-up-and-next-steps"], [241, "wrap-up-and-next-steps"], [242, "wrap-up-and-next-steps"], [247, "wrap-up-and-next-steps"], [248, "wrap-up-and-next-steps"], [254, "wrap-up-and-next-steps"], [255, "wrap-up-and-next-steps"], [260, "wrap-up-and-next-steps"], [261, "wrap-up-and-next-steps"], [267, "wrap-up-and-next-steps"], [268, "wrap-up-and-next-steps"], [277, "wrap-up-and-next-steps"]], "\u25b6\ufe0f 3. Activate the Environment": [[11, "activate-the-environment"], [72, "activate-the-environment"]], "\u2705 1. Install Conda": [[11, "install-conda"], [72, "install-conda"]], "\u2705 7. Verify Ray Installation with a Simple Example": [[11, "verify-ray-installation-with-a-simple-example"], [72, "verify-ray-installation-with-a-simple-example"]], "\u2705 Module 01 \u00b7 Introduction to Ray Train": [[178, "module-01-introduction-to-ray-train"], [204, "module-01-introduction-to-ray-train"]], "\u2705 Module 02 \u00b7 Integrating Ray Train with Ray Data": [[178, "module-02-integrating-ray-train-with-ray-data"], [204, "module-02-integrating-ray-train-with-ray-data"]], "\u2705 Module 03 \u00b7 Fault Tolerance in Ray Train": [[178, "module-03-fault-tolerance-in-ray-train"], [204, "module-03-fault-tolerance-in-ray-train"]], "\ud83c\udf89 Wrapping Up & Next Steps": [[178, "wrapping-up-next-steps"], [204, null]], "\ud83d\udccb Notebook Compute Requirements Legend": [[11, "notebook-compute-requirements-legend"], [72, "notebook-compute-requirements-legend"]], "\ud83d\udccc Overview of Structure": [[158, "overview-of-structure"], [171, null]], "\ud83d\udcda 01 \u00b7 Introduction to Ray Train": [[178, null], [179, null]], "\ud83d\udcda Next Tutorials in the Course": [[178, "next-tutorials-in-the-course"], [204, "next-tutorials-in-the-course"]], "\ud83d\udce6 4. Install UV and Dependencies": [[11, "install-uv-and-dependencies"], [72, "install-uv-and-dependencies"]], "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data": [[178, "integrating-ray-train-with-ray-data"], [192, null]], "\ud83d\udd0e Integrating Ray Train with Ray Data": [[178, "id2"], [192, "id1"]], "\ud83d\udd0e When to use Ray Train": [[178, "when-to-use-ray-train"], [179, "when-to-use-ray-train"]], "\ud83d\udda5\ufe0f 5. (Optional but Recommended) Add Your Conda Environment to Jupyter": [[11, "optional-but-recommended-add-your-conda-environment-to-jupyter"], [72, "optional-but-recommended-add-your-conda-environment-to-jupyter"]], "\ud83d\udda5\ufe0f How Distributed Data Parallel (DDP) Works": [[178, "how-distributed-data-parallel-ddp-works"], [179, "how-distributed-data-parallel-ddp-works"]], "\ud83d\ude80 6. Launch Jupyter Notebook": [[11, "launch-jupyter-notebook"], [72, "launch-jupyter-notebook"]], "\ud83d\ude80 Where to go next": [[178, "where-to-go-next"], [204, "where-to-go-next"]], "\ud83d\udee0\ufe0f 2. Create a New Conda Environment": [[11, "create-a-new-conda-environment"], [72, "create-a-new-conda-environment"]], "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train": [[178, "fault-tolerance-in-ray-train"], [198, null]], "\ud83e\udde0 Summary": [[158, "summary"], [172, null]], "\ud83e\udde9 Miniforge Installation (It depends on your OS. In this case, we use ARM Macs)": [[11, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"], [72, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"]], "\ud83e\uddf9 8. Shut Down and Clean Up": [[11, "shut-down-and-clean-up"], [72, "shut-down-and-clean-up"]]}, "docnames": ["README", "courses/00_Developer_Intro_to_Ray/00_Intro_Ray_Core_Basics", "courses/00_Developer_Intro_to_Ray/00a_Intro_Ray_Core_Advancement", "courses/00_Developer_Intro_to_Ray/01_Intro_Ray_AI_Libs_Overview", "courses/00_Developer_Intro_to_Ray/02a_Intro_Ray_Train_with_PyTorch", "courses/00_Developer_Intro_to_Ray/02b_Intro_Ray_Train_with_PyTorch_Lightning", "courses/00_Developer_Intro_to_Ray/03_Intro_Ray_Tune", "courses/00_Developer_Intro_to_Ray/04a_Intro_Ray_Data_Industry_Landscape", "courses/00_Developer_Intro_to_Ray/04b_Intro_Ray_Data_Structured", "courses/00_Developer_Intro_to_Ray/04c_Intro_Ray_Data_Unstructured", "courses/00_Developer_Intro_to_Ray/05_Intro_Ray_Serve_PyTorch", "courses/00_Developer_Intro_to_Ray/README", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_01", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_02", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_03", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_04", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_05", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_01", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_02", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_03", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_04", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_05", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_06", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_07", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_08", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_09", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_01", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_02", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_03", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_01", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_02", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_03", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_04", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_01", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_02", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_03", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_04", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_05", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_01", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_02", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_03", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_04", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_05", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_06", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_01", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_02", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_03", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_04", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_05", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_06", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_07", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_01", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_02", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_03", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_04", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_05", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_06", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_07", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_08", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_09", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_10", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_11", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_01", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_02", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_03", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_04", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_05", "courses/00_Developer_Intro_to_Ray/output/README_01", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/anyscale_administrator_overview", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/anyscale_vm_vs_k8s", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/deploy_to_ec2", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/deploy_to_GCE", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/deploy_to_a_new_EKS", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/deploy_to_an_existing_EKS", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/deploy_to_new_GKE", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12", "courses/02_Anyscale_Admin/README", "courses/03_Observability/01_general_intro_and_setup", "courses/03_Observability/02_Ray_Anyscale_Introduction/2_Ray_Anyscale_Observability_Overview", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/03_Ray_Anyscale_Observability_in_Details", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/tracing_example/README", "courses/03_Observability/output/01_general_intro_and_setup_01", "courses/03_Observability/output/01_general_intro_and_setup_02", "courses/03_Observability/output/01_general_intro_and_setup_03", "courses/GettingStarted/101_01_anyscale_intro_workspace", "courses/GettingStarted/101_02_anyscale_development_intro", "courses/GettingStarted/101_03_anyscale_compute_runtime_intro", "courses/GettingStarted/101_04_anyscale_storage_options", "courses/GettingStarted/101_05_anyscale_logging_metrics", "courses/GettingStarted/101_06_anyscale_intro_jobs", "courses/GettingStarted/101_07_anyscale_intro_services", "courses/GettingStarted/101_08_anyscale_collaboration", "courses/GettingStarted/101_09_anyscale_org_setup", "courses/GettingStarted/101_anyscale_intro_jobs", "courses/GettingStarted/101_anyscale_intro_services", "courses/GettingStarted/README", "courses/GettingStarted/output/101_01_anyscale_intro_workspace_01", "courses/GettingStarted/output/101_02_anyscale_development_intro_01", "courses/GettingStarted/output/101_03_anyscale_compute_runtime_intro_01", "courses/GettingStarted/output/101_04_anyscale_storage_options_01", "courses/GettingStarted/output/101_05_anyscale_logging_metrics_01", "courses/GettingStarted/output/101_06_anyscale_intro_jobs_01", "courses/GettingStarted/output/101_07_anyscale_intro_services_01", "courses/GettingStarted/output/101_08_anyscale_collaboration_01", "courses/GettingStarted/output/101_09_anyscale_org_setup_01", "courses/GettingStarted/output/101_09_anyscale_org_setup_02", "courses/GettingStarted/output/101_09_anyscale_org_setup_03", "courses/GettingStarted/output/101_anyscale_intro_jobs_01", "courses/GettingStarted/output/101_anyscale_intro_jobs_02", "courses/GettingStarted/output/101_anyscale_intro_jobs_03", "courses/GettingStarted/output/101_anyscale_intro_services_01", "courses/GettingStarted/output/101_anyscale_intro_services_02", "courses/Ray_Train_Foundation/01_02_03_intro_to_ray_train", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_01", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_02", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_03", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_04", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_05", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_06", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_07", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_08", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_09", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_10", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_11", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_12", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_13", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_14", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_15", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_16", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_17", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_18", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_19", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_20", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_21", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_22", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_23", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_24", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_25", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_26", "courses/WL_Ray_Data_Batch_Inference/01_Ray_Data_batch_inference", "courses/WL_Ray_Data_Batch_Inference/README", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_01", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_02", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_03", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_04", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_05", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_06", "courses/WL_Ray_Data_Processing/02_Ray_Data_data_processing", "courses/WL_Ray_Data_Processing/README", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_01", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_02", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_03", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_04", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_05", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_06", "courses/WL_Ray_Distributed_Training/04_Ray_Train_distributed_training", "courses/WL_Ray_Distributed_Training/README", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_01", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_02", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_03", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_04", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_05", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_06", "courses/WL_Ray_Serve_Online_Serving/03_Ray_Serve_online_serving", "courses/WL_Ray_Serve_Online_Serving/README", "courses/WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_01", "courses/WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_02", "courses/WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_03", "courses/WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_04", "courses/WL_Train_Generative_CV/04d1_generative_cv_pattern", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_01", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_02", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_03", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_04", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_05", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_06", "courses/WL_Train_Policy_Learning/04d2_policy_learning_pattern", "courses/WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_01", "courses/WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_02", "courses/WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_03", "courses/WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_04", "courses/WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_05", "courses/WL_Train_Rec_sys/04e_rec_sys_workload_pattern", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_01", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_02", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_03", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_04", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_05", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_06", "courses/WL_Train_Tabular/04b_tabular_workload_pattern", "courses/WL_Train_Tabular/output/04b_tabular_workload_pattern_01", "courses/WL_Train_Tabular/output/04b_tabular_workload_pattern_02", "courses/WL_Train_Tabular/output/04b_tabular_workload_pattern_03", "courses/WL_Train_Tabular/output/04b_tabular_workload_pattern_04", "courses/WL_Train_Tabular/output/04b_tabular_workload_pattern_05", "courses/WL_Train_Time_Series/04c_time_series_workload_pattern", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_01", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_02", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_03", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_04", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_05", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_06", "courses/WL_Train_Vision_Pattern/04a_vision_pattern", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_01", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_02", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_03", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_04", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_05", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_06", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_07", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_08", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_09", "courses/llm_serving/00_intro_serve_llm/README", "courses/llm_serving/00_intro_serve_llm/notebook", "courses/llm_serving/00_intro_serve_llm/output/notebook_01", "courses/llm_serving/00_intro_serve_llm/output/notebook_02", "courses/llm_serving/00_intro_serve_llm/output/notebook_03", "courses/llm_serving/00_intro_serve_llm/output/notebook_04", "courses/llm_serving/00_intro_serve_llm/output/notebook_05", "courses/llm_serving/00_intro_serve_llm/output/notebook_06", "courses/llm_serving/00_intro_serve_llm/output/notebook_07", "courses/llm_serving/01_deploy_medium_llm/notebook", "courses/llm_serving/01_deploy_medium_llm/output/notebook_01", "courses/llm_serving/01_deploy_medium_llm/output/notebook_02", "courses/llm_serving/01_deploy_medium_llm/output/notebook_03", "courses/llm_serving/01_deploy_medium_llm/output/notebook_04", "courses/llm_serving/01_deploy_medium_llm/output/notebook_05", "courses/llm_serving/01_deploy_medium_llm/output/notebook_06", "courses/llm_serving/01_deploy_medium_llm/output/notebook_07", "courses/llm_serving/02_advanced_llm_features/notebook", "courses/llm_serving/02_advanced_llm_features/output/notebook_01", "courses/llm_serving/02_advanced_llm_features/output/notebook_02", "courses/llm_serving/02_advanced_llm_features/output/notebook_03", "courses/llm_serving/02_advanced_llm_features/output/notebook_04", "courses/llm_serving/02_advanced_llm_features/output/notebook_05", "courses/llm_serving/02_advanced_llm_features/output/notebook_06", "courses/llm_serving/02_advanced_llm_features/output/notebook_07", "courses/ray-101/1_AI_Libs_Intro", "courses/ray-101/2_Intro_Train", "courses/ray-101/3_Intro_Tune", "courses/ray-101/4_Intro_Data", "courses/ray-101/5_Intro_Serve", "courses/ray-101/output/1_AI_Libs_Intro_01", "courses/ray-101/output/1_AI_Libs_Intro_02", "courses/ray-101/output/1_AI_Libs_Intro_03", "courses/ray-101/output/1_AI_Libs_Intro_04", "courses/ray-101/output/2_Intro_Train_01", "courses/ray-101/output/2_Intro_Train_02", "courses/ray-101/output/2_Intro_Train_03", "courses/ray-101/output/2_Intro_Train_04", "courses/ray-101/output/2_Intro_Train_05", "courses/ray-101/output/2_Intro_Train_06", "courses/ray-101/output/2_Intro_Train_07", "courses/ray-101/output/2_Intro_Train_08", "courses/ray-101/output/2_Intro_Train_09", "courses/ray-101/output/3_Intro_Tune_01", "courses/ray-101/output/3_Intro_Tune_02", "courses/ray-101/output/3_Intro_Tune_03", "courses/ray-101/output/3_Intro_Tune_04", "courses/ray-101/output/3_Intro_Tune_05", "courses/ray-101/output/3_Intro_Tune_06", "courses/ray-101/output/4_Intro_Data_01", "courses/ray-101/output/4_Intro_Data_02", "courses/ray-101/output/4_Intro_Data_03", "courses/ray-101/output/4_Intro_Data_04", "courses/ray-101/output/4_Intro_Data_05", "courses/ray-101/output/4_Intro_Data_06", "courses/ray-101/output/5_Intro_Serve_01", "courses/ray-101/output/5_Intro_Serve_02", "courses/ray-101/output/5_Intro_Serve_03", "courses/ray-101/output/5_Intro_Serve_04", "courses/ray-101/output/5_Intro_Serve_05", "courses/ray-101/output/5_Intro_Serve_06", "index"], "envversion": {"sphinx": 62, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["README.md", "courses/00_Developer_Intro_to_Ray/00_Intro_Ray_Core_Basics.ipynb", "courses/00_Developer_Intro_to_Ray/00a_Intro_Ray_Core_Advancement.ipynb", "courses/00_Developer_Intro_to_Ray/01_Intro_Ray_AI_Libs_Overview.ipynb", "courses/00_Developer_Intro_to_Ray/02a_Intro_Ray_Train_with_PyTorch.ipynb", "courses/00_Developer_Intro_to_Ray/02b_Intro_Ray_Train_with_PyTorch_Lightning.ipynb", "courses/00_Developer_Intro_to_Ray/03_Intro_Ray_Tune.ipynb", "courses/00_Developer_Intro_to_Ray/04a_Intro_Ray_Data_Industry_Landscape.ipynb", "courses/00_Developer_Intro_to_Ray/04b_Intro_Ray_Data_Structured.ipynb", "courses/00_Developer_Intro_to_Ray/04c_Intro_Ray_Data_Unstructured.ipynb", "courses/00_Developer_Intro_to_Ray/05_Intro_Ray_Serve_PyTorch.ipynb", "courses/00_Developer_Intro_to_Ray/README.ipynb", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/00_Intro_Ray_Core_Basics_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_06.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_07.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_08.ipynb", "courses/00_Developer_Intro_to_Ray/output/00a_Intro_Ray_Core_Advancement_09.ipynb", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/01_Intro_Ray_AI_Libs_Overview_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/02a_Intro_Ray_Train_with_PyTorch_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/03_Intro_Ray_Tune_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/04a_Intro_Ray_Data_Industry_Landscape_06.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_06.ipynb", "courses/00_Developer_Intro_to_Ray/output/04b_Intro_Ray_Data_Structured_07.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_06.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_07.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_08.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_09.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_10.ipynb", "courses/00_Developer_Intro_to_Ray/output/04c_Intro_Ray_Data_Unstructured_11.ipynb", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_01.ipynb", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_02.ipynb", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_03.ipynb", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_04.ipynb", "courses/00_Developer_Intro_to_Ray/output/05_Intro_Ray_Serve_PyTorch_05.ipynb", "courses/00_Developer_Intro_to_Ray/output/README_01.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/anyscale_administrator_overview.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05.ipynb", "courses/02_Anyscale_Admin/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06.ipynb", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/anyscale_vm_vs_k8s.ipynb", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01.ipynb", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02.ipynb", "courses/02_Anyscale_Admin/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/deploy_to_ec2.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05.ipynb", "courses/02_Anyscale_Admin/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/deploy_to_GCE.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06.ipynb", "courses/02_Anyscale_Admin/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/deploy_to_a_new_EKS.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08.ipynb", "courses/02_Anyscale_Admin/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/deploy_to_an_existing_EKS.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11.ipynb", "courses/02_Anyscale_Admin/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/deploy_to_new_GKE.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11.ipynb", "courses/02_Anyscale_Admin/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12.ipynb", "courses/02_Anyscale_Admin/README.md", "courses/03_Observability/01_general_intro_and_setup.ipynb", "courses/03_Observability/02_Ray_Anyscale_Introduction/2_Ray_Anyscale_Observability_Overview.ipynb", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01.ipynb", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02.ipynb", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03.ipynb", "courses/03_Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04.ipynb", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/03_Ray_Anyscale_Observability_in_Details.ipynb", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01.ipynb", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02.ipynb", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03.ipynb", "courses/03_Observability/03_Ray_Anyscale_Observability_in_Detail/tracing_example/README.md", "courses/03_Observability/output/01_general_intro_and_setup_01.ipynb", "courses/03_Observability/output/01_general_intro_and_setup_02.ipynb", "courses/03_Observability/output/01_general_intro_and_setup_03.ipynb", "courses/GettingStarted/101_01_anyscale_intro_workspace.ipynb", "courses/GettingStarted/101_02_anyscale_development_intro.ipynb", "courses/GettingStarted/101_03_anyscale_compute_runtime_intro.ipynb", "courses/GettingStarted/101_04_anyscale_storage_options.ipynb", "courses/GettingStarted/101_05_anyscale_logging_metrics.ipynb", "courses/GettingStarted/101_06_anyscale_intro_jobs.ipynb", "courses/GettingStarted/101_07_anyscale_intro_services.ipynb", "courses/GettingStarted/101_08_anyscale_collaboration.ipynb", "courses/GettingStarted/101_09_anyscale_org_setup.ipynb", "courses/GettingStarted/101_anyscale_intro_jobs.ipynb", "courses/GettingStarted/101_anyscale_intro_services.ipynb", "courses/GettingStarted/README.md", "courses/GettingStarted/output/101_01_anyscale_intro_workspace_01.ipynb", "courses/GettingStarted/output/101_02_anyscale_development_intro_01.ipynb", "courses/GettingStarted/output/101_03_anyscale_compute_runtime_intro_01.ipynb", "courses/GettingStarted/output/101_04_anyscale_storage_options_01.ipynb", "courses/GettingStarted/output/101_05_anyscale_logging_metrics_01.ipynb", "courses/GettingStarted/output/101_06_anyscale_intro_jobs_01.ipynb", "courses/GettingStarted/output/101_07_anyscale_intro_services_01.ipynb", "courses/GettingStarted/output/101_08_anyscale_collaboration_01.ipynb", "courses/GettingStarted/output/101_09_anyscale_org_setup_01.ipynb", "courses/GettingStarted/output/101_09_anyscale_org_setup_02.ipynb", "courses/GettingStarted/output/101_09_anyscale_org_setup_03.ipynb", "courses/GettingStarted/output/101_anyscale_intro_jobs_01.ipynb", "courses/GettingStarted/output/101_anyscale_intro_jobs_02.ipynb", "courses/GettingStarted/output/101_anyscale_intro_jobs_03.ipynb", "courses/GettingStarted/output/101_anyscale_intro_services_01.ipynb", "courses/GettingStarted/output/101_anyscale_intro_services_02.ipynb", "courses/Ray_Train_Foundation/01_02_03_intro_to_ray_train.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_01.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_02.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_03.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_04.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_05.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_06.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_07.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_08.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_09.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_10.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_11.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_12.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_13.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_14.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_15.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_16.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_17.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_18.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_19.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_20.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_21.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_22.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_23.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_24.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_25.ipynb", "courses/Ray_Train_Foundation/output/01_02_03_intro_to_ray_train_26.ipynb", "courses/WL_Ray_Data_Batch_Inference/01_Ray_Data_batch_inference.ipynb", "courses/WL_Ray_Data_Batch_Inference/README.md", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_01.ipynb", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_02.ipynb", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_03.ipynb", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_04.ipynb", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_05.ipynb", "courses/WL_Ray_Data_Batch_Inference/output/01_Ray_Data_batch_inference_06.ipynb", "courses/WL_Ray_Data_Processing/02_Ray_Data_data_processing.ipynb", "courses/WL_Ray_Data_Processing/README.md", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_01.ipynb", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_02.ipynb", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_03.ipynb", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_04.ipynb", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_05.ipynb", "courses/WL_Ray_Data_Processing/output/02_Ray_Data_data_processing_06.ipynb", "courses/WL_Ray_Distributed_Training/04_Ray_Train_distributed_training.ipynb", "courses/WL_Ray_Distributed_Training/README.md", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_01.ipynb", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_02.ipynb", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_03.ipynb", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_04.ipynb", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_05.ipynb", "courses/WL_Ray_Distributed_Training/output/04_Ray_Train_distributed_training_06.ipynb", "courses/WL_Ray_Serve_Online_Serving/03_Ray_Serve_online_serving.ipynb", "courses/WL_Ray_Serve_Online_Serving/README.md", "courses/WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_01.ipynb", "courses/WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_02.ipynb", "courses/WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_03.ipynb", "courses/WL_Ray_Serve_Online_Serving/output/03_Ray_Serve_online_serving_04.ipynb", "courses/WL_Train_Generative_CV/04d1_generative_cv_pattern.ipynb", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_01.ipynb", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_02.ipynb", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_03.ipynb", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_04.ipynb", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_05.ipynb", "courses/WL_Train_Generative_CV/output/04d1_generative_cv_pattern_06.ipynb", "courses/WL_Train_Policy_Learning/04d2_policy_learning_pattern.ipynb", "courses/WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_01.ipynb", "courses/WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_02.ipynb", "courses/WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_03.ipynb", "courses/WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_04.ipynb", "courses/WL_Train_Policy_Learning/output/04d2_policy_learning_pattern_05.ipynb", "courses/WL_Train_Rec_sys/04e_rec_sys_workload_pattern.ipynb", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_01.ipynb", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_02.ipynb", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_03.ipynb", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_04.ipynb", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_05.ipynb", "courses/WL_Train_Rec_sys/output/04e_rec_sys_workload_pattern_06.ipynb", "courses/WL_Train_Tabular/04b_tabular_workload_pattern.ipynb", "courses/WL_Train_Tabular/output/04b_tabular_workload_pattern_01.ipynb", "courses/WL_Train_Tabular/output/04b_tabular_workload_pattern_02.ipynb", "courses/WL_Train_Tabular/output/04b_tabular_workload_pattern_03.ipynb", "courses/WL_Train_Tabular/output/04b_tabular_workload_pattern_04.ipynb", "courses/WL_Train_Tabular/output/04b_tabular_workload_pattern_05.ipynb", "courses/WL_Train_Time_Series/04c_time_series_workload_pattern.ipynb", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_01.ipynb", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_02.ipynb", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_03.ipynb", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_04.ipynb", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_05.ipynb", "courses/WL_Train_Time_Series/output/04c_time_series_workload_pattern_06.ipynb", "courses/WL_Train_Vision_Pattern/04a_vision_pattern.ipynb", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_01.ipynb", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_02.ipynb", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_03.ipynb", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_04.ipynb", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_05.ipynb", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_06.ipynb", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_07.ipynb", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_08.ipynb", "courses/WL_Train_Vision_Pattern/output/04a_vision_pattern_09.ipynb", "courses/llm_serving/00_intro_serve_llm/README.md", "courses/llm_serving/00_intro_serve_llm/notebook.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_01.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_02.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_03.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_04.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_05.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_06.ipynb", "courses/llm_serving/00_intro_serve_llm/output/notebook_07.ipynb", "courses/llm_serving/01_deploy_medium_llm/notebook.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_01.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_02.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_03.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_04.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_05.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_06.ipynb", "courses/llm_serving/01_deploy_medium_llm/output/notebook_07.ipynb", "courses/llm_serving/02_advanced_llm_features/notebook.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_01.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_02.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_03.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_04.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_05.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_06.ipynb", "courses/llm_serving/02_advanced_llm_features/output/notebook_07.ipynb", "courses/ray-101/1_AI_Libs_Intro.ipynb", "courses/ray-101/2_Intro_Train.ipynb", "courses/ray-101/3_Intro_Tune.ipynb", "courses/ray-101/4_Intro_Data.ipynb", "courses/ray-101/5_Intro_Serve.ipynb", "courses/ray-101/output/1_AI_Libs_Intro_01.ipynb", "courses/ray-101/output/1_AI_Libs_Intro_02.ipynb", "courses/ray-101/output/1_AI_Libs_Intro_03.ipynb", "courses/ray-101/output/1_AI_Libs_Intro_04.ipynb", "courses/ray-101/output/2_Intro_Train_01.ipynb", "courses/ray-101/output/2_Intro_Train_02.ipynb", "courses/ray-101/output/2_Intro_Train_03.ipynb", "courses/ray-101/output/2_Intro_Train_04.ipynb", "courses/ray-101/output/2_Intro_Train_05.ipynb", "courses/ray-101/output/2_Intro_Train_06.ipynb", "courses/ray-101/output/2_Intro_Train_07.ipynb", "courses/ray-101/output/2_Intro_Train_08.ipynb", "courses/ray-101/output/2_Intro_Train_09.ipynb", "courses/ray-101/output/3_Intro_Tune_01.ipynb", "courses/ray-101/output/3_Intro_Tune_02.ipynb", "courses/ray-101/output/3_Intro_Tune_03.ipynb", "courses/ray-101/output/3_Intro_Tune_04.ipynb", "courses/ray-101/output/3_Intro_Tune_05.ipynb", "courses/ray-101/output/3_Intro_Tune_06.ipynb", "courses/ray-101/output/4_Intro_Data_01.ipynb", "courses/ray-101/output/4_Intro_Data_02.ipynb", "courses/ray-101/output/4_Intro_Data_03.ipynb", "courses/ray-101/output/4_Intro_Data_04.ipynb", "courses/ray-101/output/4_Intro_Data_05.ipynb", "courses/ray-101/output/4_Intro_Data_06.ipynb", "courses/ray-101/output/5_Intro_Serve_01.ipynb", "courses/ray-101/output/5_Intro_Serve_02.ipynb", "courses/ray-101/output/5_Intro_Serve_03.ipynb", "courses/ray-101/output/5_Intro_Serve_04.ipynb", "courses/ray-101/output/5_Intro_Serve_05.ipynb", "courses/ray-101/output/5_Intro_Serve_06.ipynb", "index.md"], "indexentries": {}, "objects": {}, "objnames": {}, "objtypes": {}, "terms": {"": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 16, 18, 19, 20, 22, 24, 25, 26, 28, 30, 31, 32, 34, 35, 36, 39, 40, 41, 43, 47, 48, 51, 52, 53, 54, 57, 59, 60, 62, 64, 68, 69, 70, 72, 73, 78, 84, 86, 87, 90, 91, 95, 96, 99, 101, 102, 103, 112, 114, 115, 122, 131, 137, 140, 141, 142, 144, 145, 151, 153, 154, 156, 157, 161, 163, 165, 166, 168, 169, 178, 179, 180, 181, 182, 186, 187, 188, 191, 192, 194, 204, 205, 207, 209, 211, 212, 213, 216, 219, 220, 221, 223, 225, 226, 227, 235, 236, 237, 239, 241, 246, 247, 248, 249, 250, 251, 252, 253, 254, 257, 258, 259, 260, 263, 265, 267, 268, 269, 270, 271, 272, 273, 275, 277, 278, 279, 281, 283, 284, 285, 286, 287, 289, 290, 291, 292, 293, 294, 295, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 313, 314, 315, 316, 317, 318, 322, 323, 324, 326, 329, 331, 335, 336, 338], "0": [2, 3, 4, 5, 6, 9, 11, 20, 22, 24, 25, 28, 31, 32, 35, 36, 39, 40, 41, 49, 53, 60, 61, 62, 72, 78, 84, 85, 86, 91, 93, 96, 99, 100, 101, 102, 103, 109, 111, 112, 114, 115, 122, 124, 131, 136, 137, 141, 146, 149, 150, 152, 154, 162, 164, 166, 179, 180, 181, 182, 186, 187, 189, 191, 195, 196, 199, 200, 204, 205, 209, 211, 213, 216, 217, 220, 221, 226, 228, 229, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244, 246, 247, 248, 250, 252, 254, 255, 256, 257, 258, 261, 263, 264, 265, 267, 268, 269, 270, 271, 273, 277, 278, 279, 285, 287, 291, 292, 295, 298, 299, 300, 303, 304, 305, 306, 307, 308, 310, 311, 312, 313, 316, 317, 318, 319, 321, 322, 323, 324, 325, 326, 327, 329, 330, 331, 333, 335, 336], "00": [146, 153, 165, 205, 211, 221, 228, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 303, 304, 305, 306, 310, 311, 318, 324, 326, 331, 339], "000": [6, 39, 178, 180, 242, 244, 248, 250, 255, 257, 305, 322], "0000000000000": [84, 86], "00000000000000000": [84, 86], "0001": [5, 9, 36, 62], "0003573892": [305, 324], "0003590581": [305, 324], "0003788471": [305, 324], "0003824231": [305, 324], "0004189012": [305, 324], "00046369e": [205, 212], "00087994e": [205, 212], "00129196e": [205, 212], "00217544e": [205, 212], "00403815e": [205, 212], "0059": [303, 310, 311], "00596860e": [205, 212], "00641076e": [205, 212], "006742": [304, 319], "00719017e": [205, 212], "00724374e": [205, 212], "00728178e": [205, 212], "00749106": [205, 211], "00753223": [205, 211], "007877049646500664": [305, 326], "00787705": [305, 326], "00844238": [205, 211], "00926834e": [205, 212], "0092816": [205, 211], "00958297e": [205, 212], "00974117e": [205, 212], "00982723e": [205, 212], "00994138e": [205, 212], "00_developer_intro_to_rai": [206, 214, 222, 230], "00a": 339, "00z": 146, "01": [3, 5, 28, 36, 205, 212, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 304, 318, 339], "01000227e": [205, 212], "01103884e": [205, 212], "01104600e": [205, 212], "01141734e": [205, 212], "01148352e": [205, 212], "01190887": [205, 211], "01222771": [205, 211], "01231135": [205, 211], "01273207e": [205, 212], "01347007e": [205, 212], "01351717e": [205, 212], "01387227e": [205, 212], "01402104e": [205, 212], "01455652": [205, 211], "01504247": [205, 211], "01505721e": [205, 212], "01544438e": [205, 212], "01616676e": [205, 212], "01646197e": [205, 212], "01848297e": [205, 212], "01875377e": [205, 212], "01946776e": [205, 212], "01951000e": [205, 212], "01958193e": [205, 212], "02": [3, 28, 205, 212, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270], "02095584": [205, 211], "02202111": [205, 211], "02316421e": [205, 212], "02338964e": [205, 212], "02352677e": [205, 212], "0242": [303, 310, 311], "02448604e": [205, 212], "02480531e": [205, 212], "02481507e": [205, 212], "02496293": [205, 211], "02508835e": [205, 212], "02556132": [205, 211], "02750473e": [205, 212], "02791084": [205, 211], "02842702e": [205, 212], "02_service_hello_world": [156, 168], "02a": 339, "02b": 339, "02d": [248, 250], "03": [3, 28, 205, 212, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 303, 305, 310, 311, 324], "03000000000000000": [84, 86], "03162946e": [205, 212], "03241184e": [205, 212], "03302041": [205, 211], "03319024e": [205, 212], "03347346e": [205, 212], "03351809e": [205, 212], "03357503": [205, 211], "03620186e": [205, 212], "03722222e": [205, 212], "03901269e": [205, 212], "03915609e": [205, 212], "03924675": [205, 211], "03974594e": [205, 212], "03d": [235, 239, 242, 246], "04": [205, 212, 237, 245, 248, 250, 255, 257, 261, 263, 268, 270, 339], "04023737e": [205, 212], "04127836e": [205, 212], "04148921": [205, 211], "04188204e": [205, 212], "04267104e": [205, 212], "04279362e": [205, 212], "04313433e": [205, 212], "04401015e": [205, 212], "04419766e": [205, 212], "04514116e": [205, 212], "04760937e": [205, 212], "04781413e": [205, 212], "04831458": [205, 211], "04871886e": [205, 212], "04a": 339, "04b": 339, "04c": 339, "04d1": 339, "04d2": 339, "04e": 339, "05": [5, 8, 35, 51, 54, 142, 144, 221, 228, 235, 237, 242, 246, 248, 250, 255, 257, 261, 263, 268, 270, 303, 304, 310, 311, 313, 318, 319, 339], "050227": [205, 211], "05029851e": [205, 212], "05031021e": [205, 212], "05048873e": [205, 212], "05110919e": [205, 212], "05117615e": [205, 212], "05286286": [205, 211], "05403318e": [205, 212], "05412337e": [205, 212], "0564279": [305, 326], "05699580e": [205, 212], "0582891": [305, 326], "05838008e": [205, 212], "05897461e": [205, 212], "05951829e": [205, 212], "05955295e": [205, 212], "05962829": [205, 211], "05964907e": [205, 212], "05_069833_2422": [304, 318], "06": [11, 72, 153, 165, 235, 237, 242, 246, 248, 250, 261, 263, 268, 271, 304, 318, 339], "06012297e": [205, 212], "06039613e": [205, 212], "06096685": [205, 211], "06099542": [205, 211], "06128380e": [205, 212], "06155156e": [205, 212], "06164196e": [205, 212], "06194988": [205, 211], "06202352e": [205, 212], "06234232e": [205, 212], "06241195e": [205, 212], "06251295": [205, 211], "06282867e": [205, 212], "06317782e": [205, 212], "06359579e": [205, 212], "06367093324661255": [304, 318, 319], "063671": [304, 319], "06383444e": [205, 212], "06465332e": [205, 212], "06659506e": [205, 212], "06678507e": [205, 212], "06857569e": [205, 212], "06872221": [205, 211], "06887527e": [205, 212], "07": [221, 228, 235, 237, 242, 246, 248, 251, 255, 257, 261, 263, 268, 271, 295, 300, 339], "07005756e": [205, 212], "07039157e": [205, 212], "07176238": [205, 211], "07316985e": [205, 212], "07334603e": [205, 212], "07348490e": [205, 212], "07420641e": [205, 212], "07510021": [205, 211], "07512747e": [205, 212], "07565679e": [205, 212], "07582638e": [205, 212], "07590961e": [205, 212], "07614997e": [205, 212], "07735191e": [205, 212], "07769895e": [205, 212], "07796153e": [205, 212], "07813133e": [205, 212], "07829855": [205, 211], "08": [146, 205, 212, 235, 238, 242, 247, 248, 252, 261, 263, 268, 271, 287, 293, 339], "08080895": [205, 211], "08113792": [205, 211], "08117312e": [205, 212], "08142687": [205, 211], "08161136e": [205, 212], "08306534e": [205, 212], "08318681e": [205, 212], "08386130e": [205, 212], "08393911e": [205, 212], "08423311e": [205, 212], "08562492e": [205, 212], "08593434e": [205, 212], "08834168e": [205, 212], "08847059e": [205, 212], "08849846e": [205, 212], "08855490e": [205, 212], "08866049e": [205, 212], "08888834e": [205, 212], "08900222e": [205, 212], "08926150e": [205, 212], "08967713e": [205, 212], "09": [235, 239, 242, 247, 248, 252, 255, 258, 261, 264, 268, 271, 304, 305, 313, 319, 324, 326, 339], "09058516e": [205, 212], "09158831e": [205, 212], "09305708e": [205, 212], "09318195e": [205, 212], "09376505e": [205, 212], "09640113e": [205, 212], "09668531e": [205, 212], "09694359e": [205, 212], "09729558e": [205, 212], "09788750e": [205, 212], "09841380e": [205, 212], "09954223e": [205, 212], "09_200164_18044": [221, 228], "0a000000000000000": [84, 86], "0bd7bde3f2c914b3": [84, 86], "0f8bb12ddf9a451e9": [84, 86, 99, 101, 109, 112], "0m": [221, 228, 304, 305, 307, 318, 324, 326, 335], "0x72c6d85fc9d0": [307, 336], "0x72c6d85fcf90": [307, 336], "0x72c6d85fd3d0": [307, 336], "0x72c6d85fd550": [307, 336], "0x72c6d85fe590": [307, 336], "0x72c6d85ff250": [307, 336], "0x72c6d85ff3d0": [307, 336], "0x72c6d8608750": [307, 336], "0x72c6d8609050": [307, 336], "0x72c6d860b6d0": [307, 336], "0x72c6d860b7d0": [307, 336], "0x72c6d8610490": [307, 336], "0x72c6d8610a50": [307, 336], "0x72c6d8611310": [307, 336], "0x72c6d8611ad0": [307, 336], "0x72c6d8611b90": [307, 336], "0x72c6d8612050": [307, 336], "0x72c6d8613690": [307, 336], "0x72c6d8620a90": [307, 336], "0x72c6d8620e10": [307, 336], "0x72c6d86218d0": [307, 336], "0x72c6d8621b90": [307, 336], "0x72c6d8622ad0": [307, 336], "0x72c6d8623590": [307, 336], "0x72c6d86281d0": [307, 336], "0x72c6d8628710": [307, 336], "0x72c6d862a1d0": [307, 336], "0x72c6d862ac50": [307, 336], "0x72c6d862b790": [307, 336], "0x72c6d862b7d0": [307, 336], "0x72c6d862c690": [307, 336], "0x72c6d872db50": [307, 336], "0x72c6d8747390": [307, 336], "0x72c6d87500d0": [307, 336], "0x72c6d8752290": [307, 336], "0x72c6d8752e50": [307, 336], "0x72c6d8757dd0": [307, 336], "0x72c6d87793d0": [307, 336], "0x72c6d8779b90": [307, 336], "0x72c6d877a010": [307, 336], "0x72c6d877a6d0": [307, 336], "0x72c6d877b010": [307, 336], "0x72c6d877bc10": [307, 336], "0x72c6d8785e50": [307, 336], "0x72c6d8785fd0": [307, 336], "0x72c6d8786a50": [307, 336], "0x72c6d8787c90": [307, 336], "0x72c6d8794350": [307, 336], "0x72c6d8795110": [307, 336], "0x72c6d8796b50": [307, 336], "0x72c6d8797150": [307, 336], "0x72c6d8797ed0": [307, 336], "0x72c6d87a0690": [307, 336], "0x72c6d87a14d0": [307, 336], "0x72c6d87a1b50": [307, 336], "0x72c6d87a1f50": [307, 336], "0x72c6d87a2c10": [307, 336], "0x72c6d87a3d50": [307, 336], "0x72c6d87a3ed0": [307, 336], "0x72c6d87a8690": [307, 336], "0x72c6d87a96d0": [307, 336], "0x72c6d87a9cd0": [307, 336], "0x72c6d87aa0d0": [307, 336], "0x72c6d87aa4d0": [307, 336], "0x72c6d87aad50": [307, 336], "0x72c6d87ab690": [307, 336], "0x72c6d87ac4d0": [307, 336], "0x72c6d87adb90": [307, 336], "0x72c6d87ae4d0": [307, 336], "0x72c6d87ae710": [307, 336], "0x72c6d87c0110": [307, 336], "0x72c6d87c1110": [307, 336], "0x72c6d87c1250": [307, 336], "0x72c6d87c18d0": [307, 336], "0x72c6d87c2350": [307, 336], "0x72c6d87c3a50": [307, 336], "0x72c6d87d0690": [307, 336], "0x72c6d87d0d90": [307, 336], "0x72c6d87d1c50": [307, 336], "0x72c6d87d1cd0": [307, 336], "0x72c6d87d3190": [307, 336], "0x72c6d87d3fd0": [307, 336], "0x72c6d87d8250": [307, 336], "0x72c6d87d8e90": [307, 336], "0x72c6d87d96d0": [307, 336], "0x72c6d87da1d0": [307, 336], "0x72c6d87e4810": [307, 336], "0x72c6d87e4f90": [307, 336], "0x72c6d87e62d0": [307, 336], "0x72c6d87e64d0": [307, 336], "0x72c6e01cbd50": [307, 336], "0x72c6e034a510": [307, 336], "0x72c6e034b950": [307, 336], "0x72c6e0351e10": [307, 336], "0x72c6e0353410": [307, 336], "0x72c6e035ca50": [307, 336], "0x72c6e035d5d0": [307, 336], "0x72c6e03660d0": [307, 336], "0x72c72000ddd0": [307, 336], "0x72c73032a850": [307, 336], "0xxxxxxxx": [84, 86], "0xxxxxxxxx": [84, 86], "0xxxxxxxxxx": [84, 86], "1": [15, 19, 23, 38, 40, 41, 49, 67, 70, 85, 93, 98, 100, 106, 111, 118, 124, 129, 134, 137, 141, 142, 143, 145, 154, 166, 178, 179, 180, 181, 184, 185, 189, 190, 191, 196, 197, 199, 200, 202, 205, 209, 211, 212, 213, 216, 218, 219, 220, 225, 227, 228, 236, 238, 239, 241, 245, 246, 247, 249, 251, 252, 254, 256, 258, 259, 260, 262, 264, 265, 267, 269, 271, 273, 274, 277, 290, 291, 292, 294, 298, 299, 300, 308, 310, 311, 312, 318, 319, 321, 323, 324, 325, 326, 327, 329, 330, 331, 333, 335, 336, 339], "10": [2, 3, 6, 9, 10, 11, 18, 20, 24, 25, 28, 31, 39, 41, 57, 61, 70, 72, 73, 78, 91, 95, 99, 100, 101, 111, 112, 122, 124, 126, 137, 141, 142, 144, 146, 153, 154, 165, 166, 181, 205, 209, 211, 213, 219, 220, 221, 228, 236, 240, 241, 244, 246, 250, 254, 256, 257, 260, 263, 267, 269, 303, 304, 305, 306, 307, 310, 311, 313, 315, 318, 319, 322, 324, 326, 330, 335, 336, 339], "100": [2, 6, 9, 25, 41, 61, 62, 146, 154, 166, 178, 185, 213, 217, 221, 226, 242, 247, 248, 250, 295, 298, 305, 306, 307, 325, 330, 335, 336], "1000": [8, 9, 51, 59, 137, 141, 154, 166, 235, 238, 242, 244, 245, 306, 329], "10000": [261, 264], "100000000000": [5, 35], "100k": [249, 254], "100th": [213, 217], "101": [73, 78, 236, 241, 273, 277], "1010": [213, 220], "10129036e": [205, 212], "101_01_anyscale_intro_workspac": 161, "101_02_anyscale_development_intro": 161, "101_03_anycale_compute_runtime_intro": 161, "101_04_anyscale_storage_opt": 161, "101_05_anyscale_logging_and_metr": 161, "101_06_anyscale_intro_job": 161, "101_07_anyscale_intro_servic": 161, "101_08_anyscale_collaboration_intro": 161, "101_09_anyscale_org_setup": 161, "102": [73, 78], "1024": [2, 5, 8, 9, 18, 35, 51, 52, 61, 91, 95, 261, 264], "10279503e": [205, 212], "10307": [305, 326], "10526211e": [205, 212], "10536157": [205, 211], "10537948e": [205, 212], "105m": [142, 145, 146], "10776436e": [205, 212], "10807291e": [205, 212], "10863163e": [205, 212], "10879738e": [205, 212], "108934": [304, 313], "10893423855304718": [304, 313], "10954670e": [205, 212], "10956261e": [205, 212], "10_000": [5, 35, 36, 242, 244], "10am": [205, 211], "10m": [248, 254], "11": [11, 72, 205, 211, 221, 228, 303, 304, 305, 310, 311, 318, 319, 324, 326, 339], "11016287e": [205, 212], "11058047e": [205, 212], "11085677e": [205, 212], "110m": [142, 145, 146], "11493243e": [205, 212], "11712754e": [205, 212], "11721872": [205, 211], "11745796e": [205, 212], "11788076e": [205, 212], "11897744e": [205, 212], "11_10": [221, 228], "11th": [205, 211], "12": [3, 11, 28, 72, 99, 102, 109, 114, 122, 129, 153, 165, 206, 214, 222, 230, 250, 263, 303, 304, 310, 311, 318, 339], "12014441e": [205, 212], "12174596e": [205, 212], "12183236e": [205, 212], "12234001e": [205, 212], "123": 146, "123456": [91, 94], "12468980e": [205, 212], "12480514e": [205, 212], "12500": [213, 216, 219, 220], "12501": [213, 220], "12502": [213, 220], "12503": [213, 220], "12504": [213, 220], "12587933e": [205, 212], "12685782e": [205, 212], "127": [11, 72, 136, 149], "128": [4, 6, 31, 32, 40, 41, 178, 183, 205, 211, 242, 245, 261, 265, 267, 304, 305, 313, 318, 319, 323, 326], "12821269e": [205, 212], "12832280e": [205, 212], "12841654e": [205, 212], "128k": [278, 279, 282, 295, 301], "12907687e": [205, 212], "12912727e": [205, 212], "12939501e": [205, 212], "129887": [304, 318], "12th": [205, 211], "12x": [305, 326], "12xlarg": [137, 141], "13": [99, 102, 109, 114, 257, 303, 305, 310, 311, 324, 339], "13000": [213, 219, 220], "13086134e": [205, 212], "13095595e": [205, 212], "13100": [213, 219, 220], "13238472e": [205, 212], "13547181e": [205, 212], "13586960e": [205, 212], "13600": [213, 219, 220], "13700": [213, 219, 220], "138": [304, 318, 319], "13803817e": [205, 212], "13828215e": [205, 212], "13841531e": [205, 212], "13b": [287, 289, 294, 295, 301], "14": [205, 211, 221, 228, 237, 242, 244, 250, 257, 263, 270, 287, 289, 339], "140": [287, 289], "14019522e": [205, 212], "140gb": [287, 289, 291], "14159100e": [205, 212], "14268738e": [205, 212], "14443852e": [205, 212], "14533243e": [205, 212], "14656349e": [205, 212], "14703774e": [205, 212], "14777484e": [205, 212], "14787792e": [205, 212], "14892137e": [205, 212], "14971709e": [205, 212], "14gb": [278, 279, 283], "14th": [205, 211], "15": [2, 4, 24, 31, 84, 85, 91, 95, 99, 100, 101, 109, 111, 112, 122, 126, 136, 149, 205, 209, 211, 221, 228, 259, 295, 300, 303, 304, 310, 311, 318, 339], "150": [205, 209, 211], "15072963e": [205, 212], "150m": 146, "15157820e": [205, 212], "15247765e": [205, 212], "15391724e": [205, 212], "15394783": [205, 211], "15428728e": [205, 212], "15451038e": [205, 212], "155": [8, 51], "15531293e": [205, 212], "15553670e": [205, 212], "15556864e": [205, 212], "15585802e": [205, 212], "155m": 146, "156": [303, 310, 311], "15658525e": [205, 212], "15747452e": [205, 212], "15786707e": [205, 212], "15844142e": [205, 212], "15874708e": [205, 212], "15_08": [303, 310, 311], "15_15": [303, 310, 311], "15x": [304, 318], "16": [183, 205, 211, 221, 228, 235, 237, 256, 258, 270, 271, 287, 293, 303, 304, 305, 310, 311, 318, 324, 326, 339], "160": [5, 35, 287, 289], "16129310e": [205, 212], "16142738e": [205, 212], "16249922e": [205, 212], "16273381e": [205, 212], "16315296e": [205, 212], "163491": [303, 310, 311], "163492": [303, 310, 311], "16707636e": [205, 212], "168": [261, 262, 263], "16848189e": [205, 212], "1693310400": 146, "16946062e": [205, 212], "16970104e": [205, 212], "16th": [205, 209, 211], "16xlarg": [137, 141], "17": [73, 78, 99, 102, 103, 109, 114, 115, 122, 131, 153, 165, 205, 211, 213, 216, 287, 293, 303, 310, 311, 339], "17145060e": [205, 212], "172": [73, 78], "17218718e": [205, 212], "17278847e": [205, 212], "17306670e": [205, 212], "1732276209": [304, 319], "1732276227": [304, 319], "17342269e": [205, 212], "17458829e": [205, 212], "17503238e": [205, 212], "17549804e": [205, 212], "175m": [142, 145, 146], "17605399e": [205, 212], "17623755e": [205, 212], "17677706e": [205, 212], "17716263e": [205, 212], "17952654e": [205, 212], "17982033e": [205, 212], "17th": [205, 211], "18": [91, 96, 99, 103, 109, 115, 122, 131, 153, 165, 179, 180, 185, 205, 211, 268, 269, 287, 293, 304, 319, 339], "18025970e": [205, 212], "180m": [142, 145, 146], "18165373e": [205, 212], "18200418e": [205, 212], "18252768e": [205, 212], "18264270e": [205, 212], "18500029e": [205, 212], "18707759e": [205, 212], "1879": [11, 72], "18899436e": [205, 212], "18926680e": [205, 212], "18932852e": [205, 212], "18th": [205, 211], "19": [153, 165, 205, 211, 235, 237, 261, 263, 268, 270, 287, 293, 303, 305, 310, 311, 324, 326, 339], "19172417e": [205, 212], "19192507e": [205, 212], "19254841e": [205, 212], "19275388e": [205, 212], "19408388e": [205, 212], "19601890e": [205, 212], "19630387e": [205, 212], "1967": [213, 216], "19670653e": [205, 212], "19684739e": [205, 212], "19716156e": [205, 212], "19767813e": [205, 212], "19797611e": [205, 212], "19835320e": [205, 212], "19861914e": [205, 212], "19884178e": [205, 212], "1995": [205, 211], "19986786e": [205, 212], "1_000_000": [2, 24], "1d": [242, 245], "1e": [4, 5, 6, 31, 32, 35, 40, 41, 178, 182, 193, 199, 221, 227, 242, 245, 248, 252, 261, 265, 268, 274, 304, 305, 313, 315, 319, 323, 325, 326], "1f": [261, 263], "1gb": [8, 51], "1h34m20": [304, 318], "1m": [248, 254], "1m10": [307, 335], "1mb": [137, 141], "1pb": [137, 141], "1st": [2, 19, 205, 209, 211], "1xt4": [303, 304, 305, 310, 311, 318, 324, 326], "2": [20, 23, 38, 39, 41, 49, 53, 67, 70, 85, 86, 98, 100, 101, 111, 112, 134, 137, 141, 142, 143, 145, 150, 156, 162, 168, 178, 179, 181, 183, 191, 194, 205, 209, 211, 212, 213, 218, 219, 220, 227, 228, 229, 233, 236, 241, 249, 252, 254, 258, 264, 265, 267, 269, 272, 273, 277, 289, 292, 294, 298, 299, 300, 308, 311, 312, 313, 315, 318, 319, 321, 322, 324, 325, 326, 327, 331, 333, 336, 339], "20": [6, 11, 39, 41, 72, 73, 78, 153, 165, 221, 228, 235, 237, 242, 244, 248, 250, 252, 255, 257, 261, 265, 287, 293, 303, 304, 305, 310, 311, 313, 318, 319, 322, 324, 325, 326, 339], "200": [205, 209, 211, 213, 217], "20093006e": [205, 212], "200m": 146, "20103974e": [205, 212], "20113872e": [205, 212], "2012": [73, 78, 205, 211], "2014": [261, 262, 263], "2015": [205, 211], "20152864e": [205, 212], "2017": [205, 209, 211], "20175812e": [205, 212], "2021": [3, 28, 303, 310], "2023": [8, 55], "2024": [8, 9, 55, 66, 146, 303, 304, 305, 306, 310, 311, 318, 319, 324, 326, 332], "2025": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 17, 26, 29, 33, 38, 43, 49, 56, 72, 84, 85, 91, 92, 99, 100, 109, 110, 122, 123, 125, 136, 137, 138, 142, 143, 147, 153, 159, 160, 161, 165, 173, 176, 205, 207, 213, 215, 221, 223, 228, 229, 231, 278, 279, 280, 287, 288, 295, 296, 300], "20302368700504303": [305, 323], "20312032e": [205, 212], "20370263e": [205, 212], "20407227e": [205, 212], "20567256e": [205, 212], "20587808e": [205, 212], "205m": 146, "20716389e": [205, 212], "20yr": [205, 211], "21": [73, 78, 221, 228, 304, 318, 339], "210m": 146, "21206143e": [205, 212], "21334969e": [205, 212], "21377383e": [205, 212], "21380231e": [205, 212], "21405767e": [205, 212], "2147483648": [2, 22], "21503563e": [205, 212], "21508265e": [205, 212], "21584712e": [205, 212], "21600205e": [205, 212], "21637220e": [205, 212], "21745682e": [205, 212], "21776196e": [205, 212], "21796946e": [205, 212], "21971425e": [205, 212], "21st": [205, 211], "22": [205, 211, 221, 228, 304, 313, 318, 339], "22192677e": [205, 212], "222": [305, 324, 326], "22234142e": [205, 212], "22277103e": [205, 212], "22332841e": [205, 212], "224": [235, 237, 241, 268, 269, 270, 271], "22458421e": [205, 212], "225": [268, 271], "22552105e": [205, 212], "22804798e": [205, 212], "22867932e": [205, 212], "22891478e": [205, 212], "229": [268, 271], "22918031e": [205, 212], "22967193e": [205, 212], "22_06": [304, 318], "22_11": [304, 318, 319], "22nd": [205, 211], "23": [205, 211, 235, 237, 268, 270, 303, 310, 339], "23053056e": [205, 212], "23069037e": [205, 212], "23083012e": [205, 212], "23107583e": [205, 212], "23113478e": [205, 212], "23204021e": [205, 212], "23302741e": [205, 212], "23314434e": [205, 212], "23478852e": [205, 212], "235m": [142, 145, 146], "23639209e": [205, 212], "23694149e": [205, 212], "23694418e": [205, 212], "23702966e": [205, 212], "23793101e": [205, 212], "23815991e": [205, 212], "23960292e": [205, 212], "23982316e": [205, 212], "23x": [221, 228], "24": [2, 22, 73, 78, 261, 262, 263, 303, 310, 311, 339], "24085984e": [205, 212], "240m": [142, 145, 146], "24192730e": [205, 212], "24219281e": [205, 212], "24473451e": [205, 212], "245m": [142, 145, 146], "24615501e": [205, 212], "24854468e": [205, 212], "24885803e": [205, 212], "24984046e": [205, 212], "24xlarg": [137, 141], "25": [99, 100, 109, 111, 122, 124, 221, 228, 303, 304, 305, 310, 311, 318, 326, 339], "250": [213, 217], "25000": [213, 216, 217, 218], "25175510e": [205, 212], "25248020e": [205, 212], "25258800e": [205, 212], "25268223e": [205, 212], "25294994e": [205, 212], "25387060e": [205, 212], "25401214e": [205, 212], "255": [6, 39, 235, 237, 305, 306, 322, 330], "256": [5, 35, 36, 137, 141, 235, 237, 268, 270, 304, 313, 319], "25669474e": [205, 212], "25707304e": [205, 212], "25723777e": [205, 212], "25795130e": [205, 212], "25806283e": [205, 212], "25912409e": [205, 212], "25934454e": [205, 212], "25_924022_2383": [303, 310, 311], "25th": [205, 211], "26": [221, 228, 287, 289, 304, 305, 313, 318, 324, 339], "26000811e": [205, 212], "26009388e": [205, 212], "26156314e": [205, 212], "26208720e": [205, 212], "26244992e": [205, 212], "26258478e": [205, 212], "26309279e": [205, 212], "26314560e": [205, 212], "26320729e": [205, 212], "26351237e": [205, 212], "26371822e": [205, 212], "264131": [305, 326], "26429361e": [205, 212], "26502474e": [205, 212], "26816351e": [205, 212], "26948217e": [205, 212], "26990714e": [205, 212], "27": [205, 209, 211, 303, 304, 310, 311, 319], "27106623e": [205, 212], "27159020e": [205, 212], "27211": [305, 324], "27212": [305, 324], "27213278e": [205, 212], "27219909e": [205, 212], "27394149e": [205, 212], "27486494e": [205, 212], "27514724e": [205, 212], "27598614e": [205, 212], "27818017e": [205, 212], "2784426808357239": [304, 313], "278443": [304, 313], "27902825e": [205, 212], "27904241e": [205, 212], "27935463e": [205, 212], "27x": [221, 228], "28": [9, 10, 61, 70, 142, 145, 178, 180, 221, 228, 304, 305, 306, 307, 318, 324, 330, 335, 336], "28050": [305, 326], "28076579e": [205, 212], "28086493e": [205, 212], "28125295e": [205, 212], "28131025e": [205, 212], "28160176e": [205, 212], "28330866e": [205, 212], "28415197e": [205, 212], "28529142e": [205, 212], "28545947e": [205, 212], "28572544e": [205, 212], "28628640e": [205, 212], "28712449e": [205, 212], "28749549e": [205, 212], "28796948e": [205, 212], "28858958e": [205, 212], "28869668e": [205, 212], "28947487e": [205, 212], "28947702e": [205, 212], "28th": [205, 211], "28x28": [6, 39, 305, 322], "29": [295, 300, 303, 304, 305, 310, 311, 318, 319, 324, 326], "29297644e": [205, 212], "29473785e": [205, 212], "29535252e": [205, 212], "29715446e": [205, 212], "29792884e": [205, 212], "29807210e": [205, 212], "29825398e": [205, 212], "29964035e": [205, 212], "29_09": [305, 324], "29t10": 146, "2a": [99, 101, 109, 112], "2b": [99, 101, 109, 112], "2cpu": [99, 106, 109, 118], "2d": [6, 39, 248, 254, 305, 322], "2e": [235, 238], "2f": [2, 18, 154, 166, 248, 254, 255, 259], "2nd": [2, 19, 205, 211], "2ykut_ijz8q8gwt5vphvitzshksddol6msszjxzwe5a": [287, 292], "3": [16, 24, 37, 38, 40, 49, 61, 62, 67, 87, 98, 100, 107, 111, 120, 124, 134, 137, 141, 142, 145, 146, 152, 164, 178, 179, 180, 181, 191, 193, 199, 200, 205, 206, 211, 212, 213, 214, 216, 220, 222, 227, 228, 230, 236, 238, 241, 245, 246, 247, 249, 252, 254, 258, 265, 267, 269, 274, 290, 291, 292, 297, 298, 299, 302, 308, 312, 313, 319, 320, 321, 323, 325, 326, 327, 329, 331, 333, 339], "30": [205, 211, 221, 228, 248, 250, 262, 267, 295, 300, 304, 318], "30478994e": [205, 212], "30517557e": [205, 212], "30540405e": [205, 212], "30551": [304, 319], "30557770e": [205, 212], "30565623e": [205, 212], "30582720e": [205, 212], "30603": [304, 318], "30618355e": [205, 212], "30638674e": [205, 212], "30715715e": [205, 212], "30746688e": [205, 212], "30834863e": [205, 212], "308870": [304, 319], "30887049436569214": [304, 318], "30980236e": [205, 212], "30981060e": [205, 212], "30999158e": [205, 212], "30min": [261, 263], "30th": [205, 211], "31": [11, 72, 221, 228, 303, 304, 305, 310, 311, 318, 324, 326], "31019164e": [205, 212], "31141504e": [205, 212], "31172134e": [205, 212], "31209707e": [205, 212], "31449399e": [205, 212], "31499174e": [205, 212], "31562141e": [205, 212], "31659403e": [205, 212], "31913936e": [205, 212], "31973000e": [205, 212], "31st": [205, 211], "32": [2, 5, 9, 18, 25, 35, 61, 235, 238, 239, 242, 246, 255, 258, 287, 293, 295, 298], "320": [5, 35], "32145682e": [205, 212], "32192443e": [205, 212], "32244647e": [205, 212], "32266051e": [205, 212], "32296453e": [205, 212], "32373542e": [205, 212], "32401919e": [205, 212], "32454751e": [205, 212], "32564947e": [205, 212], "326001912355423": [305, 323], "32635012e": [205, 212], "32654747e": [205, 212], "3266499161421599": [305, 324], "32665": [305, 324], "32722983e": [205, 212], "32756231e": [205, 212], "32768": [287, 290, 293, 295, 300], "32768166e": [205, 212], "32890965e": [205, 212], "32902160e": [205, 212], "32918817e": [205, 212], "32b": [295, 300, 301], "32gb": [305, 324, 326], "32k": [278, 279, 282, 295, 301], "32m": [221, 228, 304, 305, 318, 326], "33": [205, 209, 211, 212], "33023707e": [205, 212], "33163792e": [205, 212], "33167297e": [205, 212], "33281359e": [205, 212], "33300245e": [205, 212], "33315668e": [205, 212], "33572224e": [205, 212], "33688403e": [205, 212], "33728483e": [205, 212], "33760041e": [205, 212], "33768886e": [205, 212], "33827804e": [205, 212], "33832851e": [205, 212], "33951919e": [205, 212], "34": [205, 212, 304, 305, 306, 318, 324, 331], "34206163e": [205, 212], "34323069e": [205, 212], "34348310e": [205, 212], "34449054e": [205, 212], "34561165e": [205, 212], "34613437e": [205, 212], "34668782e": [205, 212], "34740751e": [205, 212], "34842591e": [205, 212], "34999743e": [205, 212], "35": [205, 212, 221, 228, 304, 305, 318, 319, 324, 326], "35016027e": [205, 212], "35024523e": [205, 212], "35026570e": [205, 212], "35185423e": [205, 212], "35189386e": [205, 212], "35268092e": [205, 212], "35481167e": [205, 212], "35665376e": [205, 212], "35665385e": [205, 212], "35833579e": [205, 212], "35873899e": [205, 212], "35890651e": [205, 212], "36": [221, 228, 287, 293, 305, 326], "36150215e": [205, 212], "36240765e": [205, 212], "365191": [304, 319], "36595646e": [205, 212], "36710434e": [205, 212], "36786141e": [205, 212], "36829392e": [205, 212], "36855166e": [205, 212], "36857450e": [205, 212], "36868265e": [205, 212], "36873224e": [205, 212], "36m": [221, 228, 304, 305, 307, 318, 324, 326, 335], "37": [287, 293, 304, 305, 318, 326], "37080820e": [205, 212], "37142141e": [205, 212], "37153175e": [205, 212], "37196398e": [205, 212], "37391533e": [205, 212], "37605290e": [205, 212], "37751494e": [205, 212], "37764162e": [205, 212], "37784477e": [205, 212], "37850042e": [205, 212], "37964366e": [205, 212], "38": [221, 228], "38109175e": [205, 212], "38115362e": [205, 212], "38281021e": [205, 212], "384": [205, 211, 212, 287, 293], "38448": [305, 324, 326], "384480": [305, 326], "38451725e": [205, 212], "38496622e": [205, 212], "38554320e": [205, 212], "38563488e": [205, 212], "38687068e": [205, 212], "38798335e": [205, 212], "38858718e": [205, 212], "38910706e": [205, 212], "39053045e": [205, 212], "39066431e": [205, 212], "39268537e": [205, 212], "39287962e": [205, 212], "39421923e": [205, 212], "39532143e": [205, 212], "39561000e": [205, 212], "39590132e": [205, 212], "39660065e": [205, 212], "39745891e": [205, 212], "39747020e": [205, 212], "39796034e": [205, 212], "39855982e": [205, 212], "39905545e": [205, 212], "3b": [295, 299], "3d": [242, 245], "3f": [255, 258, 259, 260], "3rd": [205, 211], "3x3": [178, 180, 191], "4": [20, 26, 29, 33, 38, 41, 49, 62, 67, 98, 129, 134, 142, 145, 146, 154, 166, 178, 179, 205, 211, 212, 213, 220, 238, 239, 246, 249, 252, 254, 258, 265, 267, 271, 288, 289, 290, 291, 298, 299, 300, 303, 310, 311, 312, 318, 319, 321, 324, 326, 327, 329, 333, 336, 339], "40": [146, 205, 211, 213, 216, 304, 318], "4000": [2, 22], "40064341e": [205, 212], "40084913e": [205, 212], "400b": [287, 289, 294, 295, 301], "40222309e": [205, 212], "40240113e": [205, 212], "40254933e": [205, 212], "403": [304, 305, 318, 324, 326], "40336857e": [205, 212], "40409318e": [205, 212], "40456108e": [205, 212], "40510444e": [205, 212], "40537590e": [205, 212], "406": [268, 271], "40600796e": [205, 212], "40880044e": [205, 212], "40937243e": [205, 212], "40g": [287, 290, 291], "41": [11, 72, 221, 228], "41169238e": [205, 212], "41280317e": [205, 212], "41516277e": [205, 212], "41520910e": [205, 212], "41526775e": [205, 212], "41575071e": [205, 212], "41598": [221, 228], "41599": [221, 228], "415m": 146, "41709536e": [205, 212], "41786465e": [205, 212], "41920993e": [205, 212], "41922843e": [205, 212], "41933542e": [205, 212], "41968962e": [205, 212], "41985670e": [205, 212], "42": [3, 28, 248, 250, 255, 257, 268, 271], "420m": 146, "42177847e": [205, 212], "422321": [221, 228], "42241838e": [205, 212], "42471355e": [205, 212], "42482564e": [205, 212], "42548003e": [205, 212], "42604055e": [205, 212], "42662169e": [205, 212], "42837034e": [205, 212], "42856956e": [205, 212], "42857780e": [205, 212], "42904809e": [205, 212], "42943636e": [205, 212], "43057": [303, 310, 311], "43168001e": [205, 212], "43248991e": [205, 212], "43267226e": [205, 212], "43328887e": [205, 212], "43732": [304, 313], "43747482e": [205, 212], "43779554e": [205, 212], "43806068e": [205, 212], "43821533e": [205, 212], "43902507e": [205, 212], "43916206e": [205, 212], "43940079e": [205, 212], "43996522e": [205, 212], "44": [303, 305, 310, 311, 326], "44072895e": [205, 212], "44087312e": [205, 212], "44237953e": [205, 212], "44269560e": [205, 212], "44299744e": [205, 212], "443": [73, 78], "44326049e": [205, 212], "44500265e": [205, 212], "44628939e": [205, 212], "44769201e": [205, 212], "44773570": [304, 313], "44875658e": [205, 212], "44877301e": [205, 212], "44888324e": [205, 212], "44945610e": [205, 212], "45": [84, 87, 205, 211, 304, 318], "45237213e": [205, 212], "45387161e": [205, 212], "45463008e": [205, 212], "45558545e": [205, 212], "45565206e": [205, 212], "45596695e": [205, 212], "456": [146, 268, 271], "45615": [205, 209, 212], "45630976e": [205, 212], "45667797e": [205, 212], "45803327e": [205, 212], "45804777e": [205, 212], "45845979e": [205, 212], "45928478e": [205, 212], "45977615e": [205, 212], "45981687e": [205, 212], "46": [99, 102, 109, 114, 221, 228], "46038486e": [205, 212], "46128924e": [205, 212], "46165411e": [205, 212], "46190545e": [205, 212], "46210968e": [205, 212], "46212946e": [205, 212], "46281177e": [205, 212], "46350823e": [205, 212], "46354072e": [205, 212], "46371266e": [205, 212], "46490431e": [205, 212], "46654081e": [205, 212], "46736982e": [205, 212], "46787928e": [205, 212], "46938870e": [205, 212], "46954274e": [205, 212], "47110615e": [205, 212], "47293536e": [205, 212], "47399181e": [205, 212], "47439016e": [205, 212], "47477984e": [205, 212], "475m": 146, "47602344e": [205, 212], "47635157e": [205, 212], "47678024e": [205, 212], "47783903e": [205, 212], "47834991e": [205, 212], "47896763e": [205, 212], "47925606e": [205, 212], "47997355e": [205, 212], "48": [137, 141, 261, 262, 263, 287, 293, 304, 319], "48048115e": [205, 212], "480m": 146, "485": [268, 271], "485m": 146, "48663167e": [205, 212], "48813944e": [205, 212], "48855758e": [205, 212], "48856053e": [205, 212], "49": [221, 228, 278, 279, 285, 287, 292, 304, 318, 319], "49026504e": [205, 212], "49106100e": [205, 212], "49187856e": [205, 212], "49249397e": [205, 212], "49257514e": [205, 212], "49307863e": [205, 212], "49331436e": [205, 212], "49361154e": [205, 212], "49425897e": [205, 212], "49631714e": [205, 212], "49642932e": [205, 212], "49779003e": [205, 212], "49808554e": [205, 212], "49876371e": [205, 212], "49959813e": [205, 212], "4f": [248, 252], "4k": [278, 279, 282, 295, 301], "4m37": [305, 324], "4th": [205, 211], "5": [1, 6, 16, 19, 20, 25, 39, 40, 41, 49, 61, 85, 93, 100, 111, 124, 142, 145, 146, 178, 182, 186, 191, 196, 205, 211, 212, 213, 219, 220, 226, 239, 241, 244, 249, 252, 265, 267, 274, 275, 277, 295, 298, 299, 301, 303, 307, 310, 311, 312, 313, 315, 316, 319, 321, 322, 323, 324, 325, 327, 329, 330, 331, 336, 339], "50": [9, 59, 84, 86, 213, 219, 229, 233, 234, 235, 241, 242, 247, 255, 258, 260, 287, 293, 304, 306, 318, 319, 329], "500": [6, 41, 137, 141, 235, 237, 268, 270, 271], "50099332e": [205, 212], "50164117e": [205, 212], "50424745e": [205, 212], "50576949e": [205, 212], "50653918e": [205, 212], "50785267e": [205, 212], "50892793e": [205, 212], "50946071e": [205, 212], "50_per_index": [9, 59, 61, 64, 306, 307, 329, 331, 336], "51002133e": [205, 212], "51042950e": [205, 212], "51119480e": [205, 212], "512": [178, 197, 200, 202, 248, 252, 304, 313, 319], "51281480e": [205, 212], "51315774e": [205, 212], "51320172e": [205, 212], "51383309e": [205, 212], "51503164e": [205, 212], "51518283e": [205, 212], "51617597e": [205, 212], "51739728e": [205, 212], "51786283e": [205, 212], "51865722e": [205, 212], "52096841e": [205, 212], "52135583e": [205, 212], "52154651e": [205, 212], "52324782e": [205, 212], "525325868955": [73, 78], "52539432e": [205, 212], "52647242e": [205, 212], "53": [304, 318], "53193595e": [205, 212], "53377999e": [205, 212], "53381238e": [205, 212], "53534813e": [205, 212], "53647423e": [205, 212], "53716086e": [205, 212], "53754605e": [205, 212], "53959617e": [205, 212], "53998228e": [205, 212], "54": [221, 228, 255, 256, 257, 303, 310, 311], "5404948": [303, 310, 311], "54113623e": [205, 212], "54230055e": [205, 212], "54291149e": [205, 212], "54304842e": [205, 212], "54450669e": [205, 212], "54546804e": [205, 212], "54573391e": [205, 212], "54621774e": [205, 212], "54645248e": [205, 212], "54685497e": [205, 212], "55": [304, 318], "55025972e": [205, 212], "55099240e": [205, 212], "550_000": [5, 36], "5529555": [303, 310, 311], "5552995": [303, 310, 311], "55601753e": [205, 212], "55613178e": [205, 212], "55635225e": [205, 212], "55699139e": [205, 212], "5588189": [303, 310, 311], "55900936e": [205, 212], "55923614e": [205, 212], "56011016e": [205, 212], "56041365e": [205, 212], "56115811e": [205, 212], "56188577e": [205, 212], "56274483e": [205, 212], "56389399e": [205, 212], "56662523e": [205, 212], "56688479e": [205, 212], "56718080e": [205, 212], "56896329e": [205, 212], "56896693e": [205, 212], "56920916e": [205, 212], "57": [287, 293], "57008413e": [205, 212], "57156566e": [205, 212], "57160601e": [205, 212], "57175567e": [205, 212], "57226282e": [205, 212], "57393692e": [205, 212], "57440994e": [205, 212], "57502690e": [205, 212], "57536250e": [205, 212], "57737350e": [205, 212], "57877821e": [205, 212], "57970206e": [205, 212], "58": [303, 310, 311], "580": [137, 141, 255, 256, 257], "58003160e": [205, 212], "58086956e": [205, 212], "580k": [255, 257], "580x580x3": [137, 141], "58155808e": [205, 212], "58189368e": [205, 212], "58261052e": [205, 212], "58301930e": [205, 212], "58452298e": [205, 212], "58531007e": [205, 212], "58614498e": [205, 212], "58806413e": [205, 212], "58818898e": [205, 212], "58824509e": [205, 212], "58904049e": [205, 212], "59": [295, 300, 303, 310, 311], "59060508e": [205, 212], "59105931e": [205, 212], "59260547e": [205, 212], "59349391e": [205, 212], "59438765e": [205, 212], "59552705e": [205, 212], "59555081e": [205, 212], "59728657e": [205, 212], "59733031e": [205, 212], "59831755e": [205, 212], "59980466e": [205, 212], "5_anyscalejob": [159, 173], "6": [49, 136, 142, 145, 146, 149, 205, 211, 212, 244, 256, 265, 270, 305, 306, 307, 312, 324, 326, 329, 331, 335, 336], "60": [6, 39, 142, 145, 178, 180, 305, 322], "60254669e": [205, 212], "60417205": [303, 310, 311], "60559933e": [205, 212], "60700288e": [205, 212], "60769555e": [205, 212], "60900429e": [205, 212], "60926636e": [205, 212], "60929009e": [205, 212], "61124960e": [205, 212], "61173157e": [205, 212], "61210120e": [205, 212], "61377022e": [205, 212], "61495838e": [205, 212], "61626707e": [205, 212], "61644816e": [205, 212], "61783993e": [205, 212], "61808310e": [205, 212], "62014505e": [205, 212], "62131321e": [205, 212], "62209135e": [205, 212], "62317707e": [205, 212], "62408248e": [205, 212], "62470581e": [205, 212], "62676229e": [205, 212], "62782574e": [205, 212], "6290559": [303, 310, 311], "629055917263031": [303, 310, 311], "63": [221, 228], "63008086e": [205, 212], "63077107e": [205, 212], "63363028e": [205, 212], "63391770e": [205, 212], "63410094e": [205, 212], "63496330e": [205, 212], "63529071e": [205, 212], "63559180e": [205, 212], "63716504e": [205, 212], "63769671e": [205, 212], "6379": [136, 149], "64": [4, 5, 6, 31, 35, 40, 41, 178, 181, 205, 211, 248, 251, 252, 261, 264, 268, 274, 277, 304, 305, 313, 319, 323, 326], "640": [5, 35], "64042781e": [205, 212], "641551": [304, 319], "64353992e": [205, 212], "64574068e": [205, 212], "64824829e": [205, 212], "64927173e": [205, 212], "65101083e": [205, 212], "65288996e": [205, 212], "65297012e": [205, 212], "65683129e": [205, 212], "65831": [305, 324], "65908886e": [205, 212], "66008": [303, 310, 311], "66033891e": [205, 212], "66034375e": [205, 212], "660569": [303, 310, 311], "662196": [303, 310, 311], "66246349e": [205, 212], "662499": [303, 310, 311], "66510823e": [205, 212], "66672035e": [205, 212], "66712171e": [205, 212], "66808441e": [205, 212], "66871315e": [205, 212], "66888866e": [205, 212], "67074795e": [205, 212], "67096058e": [205, 212], "67168414e": [205, 212], "67200039e": [205, 212], "67249476e": [205, 212], "67394597e": [205, 212], "67530221e": [205, 212], "67625724e": [205, 212], "67702921e": [205, 212], "68044616e": [205, 212], "682": [248, 250], "68261507e": [205, 212], "68279577e": [205, 212], "68410710e": [205, 212], "68623075e": [205, 212], "68863142e": [205, 212], "68928802e": [205, 212], "68990344e": [205, 212], "69230062e": [205, 212], "69485952e": [205, 212], "69655108e": [205, 212], "69689246e": [205, 212], "69690510e": [205, 212], "69849765e": [205, 212], "69882979e": [205, 212], "69974936e": [205, 212], "6_000108_000000": [5, 35], "6_2024": [305, 324], "6_anyscaleservic": [160, 176], "6f": [154, 166, 221, 226], "6th": [205, 211], "6x": [305, 326], "7": [6, 31, 40, 41, 49, 142, 145, 146, 178, 181, 205, 211, 212, 239, 252, 258, 265, 270, 303, 305, 306, 310, 312, 313, 323, 324, 326, 329, 331], "70": [287, 289], "70078446e": [205, 212], "70130879e": [205, 212], "70160577e": [205, 212], "70176884e": [205, 212], "70207208e": [205, 212], "70480572e": [205, 212], "70483862e": [205, 212], "70502144e": [205, 212], "70606668e": [205, 212], "70623609e": [205, 212], "70640138e": [205, 212], "70758316e": [205, 212], "70849383e": [205, 212], "70991046e": [205, 212], "70b": [278, 279, 282, 290, 291, 292, 293, 294, 295, 301], "71": [303, 310, 311], "71107422e": [205, 212], "71276686e": [205, 212], "71327189e": [205, 212], "71361454e": [205, 212], "71367359e": [205, 212], "71599907e": [205, 212], "71614686e": [205, 212], "71677093e": [205, 212], "71802861e": [205, 212], "71831726e": [205, 212], "72043703e": [205, 212], "72060728e": [205, 212], "72087287e": [205, 212], "72232352e": [205, 212], "72505680e": [205, 212], "72602186e": [205, 212], "72664893e": [205, 212], "72793153e": [205, 212], "72918749e": [205, 212], "72964428e": [205, 212], "73005784e": [205, 212], "73056117e": [205, 212], "73160497e": [205, 212], "73338448e": [205, 212], "73339425e": [205, 212], "73386657e": [205, 212], "73471044e": [205, 212], "73475703e": [205, 212], "73528048e": [205, 212], "73541475e": [205, 212], "73842654e": [205, 212], "73909385e": [205, 212], "73954112e": [205, 212], "73962507e": [205, 212], "74006203e": [205, 212], "74092592e": [205, 212], "74106744e": [205, 212], "74198601e": [205, 212], "74435990e": [205, 212], "74639919e": [205, 212], "74725649e": [205, 212], "75": [268, 277, 287, 293], "75087003e": [205, 212], "75092961e": [205, 212], "75342406e": [205, 212], "75359203e": [205, 212], "75361482e": [205, 212], "75378935e": [205, 212], "75653571e": [205, 212], "75755769e": [205, 212], "75899668e": [205, 212], "76055871e": [205, 212], "76067518e": [205, 212], "76138058e": [205, 212], "76263633e": [205, 212], "76323075e": [205, 212], "76346910e": [205, 212], "76389585e": [205, 212], "76404205e": [205, 212], "76448804e": [205, 212], "76463524e": [205, 212], "764641": [221, 228], "76568236e": [205, 212], "76702512e": [205, 212], "76715726e": [205, 212], "76780378e": [205, 212], "76795331e": [205, 212], "768": [287, 293], "76875392e": [205, 212], "76900255e": [205, 212], "76936167e": [205, 212], "76972622e": [205, 212], "77": [5, 35], "77095975e": [205, 212], "7721": [305, 324], "7722": [305, 324], "7725": [305, 324], "77374096e": [205, 212], "774": [11, 72], "77459675e": [205, 212], "77555919e": [205, 212], "77606642e": [205, 212], "77784879e": [205, 212], "77830246e": [205, 212], "78041089e": [205, 212], "78072055e": [205, 212], "78197911e": [205, 212], "78284839e": [205, 212], "7834368348121643": [304, 318], "783437": [304, 319], "78439620e": [205, 212], "78474542e": [205, 212], "78715137e": [205, 212], "78778227e": [205, 212], "78793629e": [205, 212], "78836194e": [205, 212], "78927866e": [205, 212], "79144512e": [205, 212], "79223316e": [205, 212], "79288665e": [205, 212], "79409343e": [205, 212], "79455946e": [205, 212], "79656491e": [205, 212], "79875319e": [205, 212], "79880509e": [205, 212], "79888725e": [205, 212], "799808": [221, 228], "79x": [287, 293], "7am": [205, 211], "7b": [278, 279, 283, 287, 289, 294, 295, 301], "7th": [205, 209, 211, 212], "7x": [4, 5, 32, 37, 304, 318, 320], "8": [5, 18, 22, 31, 35, 36, 49, 54, 146, 178, 180, 183, 184, 191, 205, 211, 212, 213, 219, 227, 236, 237, 239, 243, 244, 246, 249, 250, 262, 264, 269, 270, 274, 275, 287, 288, 289, 290, 291, 293, 294, 295, 301, 303, 306, 308, 329, 331], "80": [235, 237, 242, 244, 248, 250, 255, 257, 287, 293], "800": [287, 289], "8000": [0, 3, 10, 28, 70, 142, 145, 229, 234, 278, 279, 285, 287, 291, 295, 298, 299, 300, 303, 307, 310, 311, 335, 336], "80058852e": [205, 212], "80134752e": [205, 212], "80346647e": [205, 212], "80356482e": [205, 212], "80584863e": [205, 212], "80600139e": [205, 212], "80770779e": [205, 212], "80772168e": [205, 212], "80778313e": [205, 212], "80779386e": [205, 212], "8080": [136, 149], "80b": [287, 289, 295, 301], "81101489e": [205, 212], "81150190e": [205, 212], "81153491e": [205, 212], "81209888e": [205, 212], "81385726e": [205, 212], "8147535920143127": [304, 313], "814754": [304, 313], "81605020e": [205, 212], "81611199e": [205, 212], "81677291e": [205, 212], "81750199e": [205, 212], "8192": [278, 279, 285, 295, 298, 299], "81969379e": [205, 212], "81974494e": [205, 212], "81992236e": [205, 212], "82237032e": [205, 212], "82248098e": [205, 212], "82377563e": [205, 212], "82634446e": [205, 212], "8265": [11, 72, 136, 149], "82670507e": [205, 212], "82704625e": [205, 212], "82709087e": [205, 212], "82714777e": [205, 212], "82736081e": [205, 212], "82819171e": [205, 212], "82876396e": [205, 212], "82900697e": [205, 212], "82929088e": [205, 212], "82934299e": [205, 212], "82985464e": [205, 212], "83": [304, 313], "83045536e": [205, 212], "83186028e": [205, 212], "83414386e": [205, 212], "83462293e": [205, 212], "83483610e": [205, 212], "83486040e": [205, 212], "837": [287, 293], "83987024e": [205, 212], "84016372e": [205, 212], "84025085e": [205, 212], "84072098e": [205, 212], "84077434e": [205, 212], "84100178e": [205, 212], "84200376e": [205, 212], "84204574e": [205, 212], "84257627e": [205, 212], "84342591e": [205, 212], "84438897e": [205, 212], "84560393e": [205, 212], "84609653e": [205, 212], "84649059e": [205, 212], "84724203e": [205, 212], "84787306e": [205, 212], "84826868e": [205, 212], "85": [295, 298], "85011083e": [205, 212], "85030222e": [205, 212], "85491417e": [205, 212], "85630648e": [205, 212], "85674404e": [205, 212], "85712914e": [205, 212], "85737485e": [205, 212], "86": [84, 85, 91, 93, 99, 100, 109, 111, 122, 124, 295, 300], "86025219e": [205, 212], "86067504e": [205, 212], "86110076e": [205, 212], "86166140e": [205, 212], "86302094e": [205, 212], "86359452e": [205, 212], "86543170e": [205, 212], "86763499e": [205, 212], "87068461e": [205, 212], "87136489e": [205, 212], "87145798e": [205, 212], "87155861e": [205, 212], "87222108e": [205, 212], "87272584e": [205, 212], "87305135e": [205, 212], "87395632e": [205, 212], "87503409e": [205, 212], "87622940e": [205, 212], "87687856e": [205, 212], "87722724e": [205, 212], "87823978e": [205, 212], "87826851e": [205, 212], "88060308e": [205, 212], "88125470e": [205, 212], "88136353e": [205, 212], "88193573e": [205, 212], "88231084e": [205, 212], "88282757e": [205, 212], "88339034e": [205, 212], "88483773e": [205, 212], "88547347e": [205, 212], "88624009e": [205, 212], "88834351e": [205, 212], "88852262e": [205, 212], "89050034e": [205, 212], "89146360e": [205, 212], "89227986e": [205, 212], "89326443e": [205, 212], "89384127e": [205, 212], "89393270e": [205, 212], "89414667e": [205, 212], "89564747e": [205, 212], "89613281e": [205, 212], "89739510e": [205, 212], "89740060e": [205, 212], "89891556e": [205, 212], "89910729e": [205, 212], "8b": [278, 279, 285, 295, 298, 301], "8cpu": [305, 324, 326], "8gb": [99, 106, 109, 118], "8k": [278, 279, 282, 295, 301], "8th": [205, 209, 211], "8xl40": [287, 293], "9": [2, 6, 25, 31, 39, 40, 84, 85, 91, 93, 99, 100, 102, 103, 111, 114, 115, 124, 131, 137, 141, 146, 178, 180, 181, 195, 205, 212, 221, 226, 237, 241, 263, 269, 270, 303, 305, 306, 310, 311, 322, 323, 329, 331], "90": [99, 100, 137, 141, 205, 211, 295, 299], "90151963e": [205, 212], "90165711e": [205, 212], "90290013e": [205, 212], "90392175e": [205, 212], "90476887e": [205, 212], "90511677e": [205, 212], "90528610e": [205, 212], "90640": [305, 326], "90656148e": [205, 212], "90659517e": [205, 212], "90668219e": [205, 212], "90833239e": [205, 212], "91": [304, 318], "91010717e": [205, 212], "91011913e": [205, 212], "91205712e": [205, 212], "91437262e": [205, 212], "91462375e": [205, 212], "91614705e": [205, 212], "91802080e": [205, 212], "91938744e": [205, 212], "92106171e": [205, 212], "92230538e": [205, 212], "92250460e": [205, 212], "92267558e": [205, 212], "92452052e": [205, 212], "92608377e": [205, 212], "92633970e": [205, 212], "92704949e": [205, 212], "92848936e": [205, 212], "92968845e": [205, 212], "93108886e": [205, 212], "93218452e": [205, 212], "93256009e": [205, 212], "93267390e": [205, 212], "93274853e": [205, 212], "93358018e": [205, 212], "93396618e": [205, 212], "93505753e": [205, 212], "93599756e": [205, 212], "93615": [303, 310, 311], "93653901e": [205, 212], "93655512e": [205, 212], "93872128e": [205, 212], "93877090e": [205, 212], "93991698e": [205, 212], "94008095e": [205, 212], "94048835e": [205, 212], "94202918e": [205, 212], "942508": [304, 319], "9425080418586731": [304, 318, 319], "943": [248, 250], "94449548e": [205, 212], "94474315e": [205, 212], "94507216e": [205, 212], "94511395e": [205, 212], "94559568e": [205, 212], "94806529e": [205, 212], "94919703e": [205, 212], "94921815e": [205, 212], "94926137e": [205, 212], "949393": [221, 228], "94980036e": [205, 212], "950654": [304, 313], "9506543874740601": [304, 313], "95128240e": [205, 212], "95309842e": [205, 212], "95409912e": [205, 212], "95493992e": [205, 212], "95530374e": [205, 212], "95612733e": [205, 212], "95642184e": [205, 212], "95678936e": [205, 212], "95717700e": [205, 212], "95755831e": [205, 212], "95777296e": [205, 212], "95807119e": [205, 212], "9590334296226501": [229, 234], "959243851260": [84, 86], "96": [306, 331], "96016713e": [205, 212], "96078059e": [205, 212], "96112816e": [205, 212], "96223371e": [205, 212], "96230914e": [205, 212], "96423475e": [205, 212], "96519734e": [205, 212], "96568063e": [205, 212], "96672040e": [205, 212], "96707787e": [205, 212], "96917129e": [205, 212], "97122377e": [205, 212], "97128675e": [205, 212], "97161290e": [205, 212], "97234564e": [205, 212], "97312343e": [205, 212], "97468323e": [205, 212], "97478577e": [205, 212], "97603959e": [205, 212], "97606084e": [205, 212], "97651729e": [205, 212], "97699012e": [205, 212], "97741865e": [205, 212], "97773625e": [205, 212], "97899076e": [205, 212], "97899440e": [205, 212], "97910169e": [205, 212], "97926176e": [205, 212], "97952076e": [205, 212], "97973699e": [205, 212], "98": [306, 331], "98070179e": [205, 212], "98221380e": [205, 212], "98234466e": [205, 212], "98250581e": [205, 212], "98440845e": [205, 212], "98496": [305, 324], "98530062e": [205, 212], "98645657e": [205, 212], "98712543e": [205, 212], "988": [306, 331], "99006638e": [205, 212], "99074054e": [205, 212], "99094741e": [205, 212], "99340957e": [205, 212], "99445761e": [205, 212], "99487356e": [205, 212], "99530393e": [205, 212], "99537045e": [205, 212], "999": [242, 244], "99955577e": [205, 212], "9996353387832642": [229, 234], "9998507499694824": [229, 234], "A": [1, 2, 3, 4, 5, 6, 8, 9, 10, 14, 18, 22, 25, 26, 28, 29, 32, 33, 36, 38, 41, 49, 51, 56, 59, 67, 69, 80, 82, 91, 93, 97, 109, 118, 146, 151, 152, 158, 163, 164, 171, 178, 181, 188, 190, 205, 209, 210, 211, 213, 219, 220, 221, 223, 235, 238, 241, 242, 243, 245, 247, 248, 250, 251, 252, 254, 255, 257, 261, 262, 265, 278, 279, 280, 282, 283, 287, 288, 289, 295, 296, 298, 301, 304, 305, 306, 307, 315, 324, 325, 329, 334], "AND": [156, 168], "And": [205, 211, 213, 216], "As": [11, 72, 137, 141, 151, 155, 159, 163, 167, 175, 213, 216, 220, 261, 267, 295, 298, 306, 331], "At": [1, 2, 4, 13, 20, 31, 73, 78, 142, 145, 205, 211, 213, 220, 235, 236, 242, 243, 268, 269, 304, 313], "Be": [205, 211], "But": [6, 40, 205, 211, 213, 216, 219, 220, 305, 323], "By": [2, 7, 9, 22, 46, 59, 61, 142, 145, 155, 159, 161, 167, 173, 178, 179, 180, 188, 255, 258, 261, 262, 268, 269, 270, 271, 275, 277], "For": [1, 2, 3, 7, 8, 9, 10, 11, 13, 21, 23, 28, 48, 52, 53, 54, 55, 62, 64, 71, 72, 73, 74, 76, 80, 81, 82, 83, 99, 103, 109, 115, 122, 125, 131, 136, 142, 144, 145, 149, 150, 151, 152, 153, 154, 155, 156, 160, 162, 163, 164, 165, 166, 167, 168, 177, 178, 188, 192, 205, 206, 211, 213, 214, 215, 220, 221, 222, 224, 226, 230, 242, 243, 248, 249, 255, 259, 268, 269, 278, 279, 283, 287, 290, 291, 292, 295, 298, 299, 300, 301, 303, 304, 306, 309, 317, 330, 331], "IN": [142, 145], "IT": [205, 211], "If": [1, 2, 3, 4, 6, 7, 8, 11, 15, 21, 22, 26, 31, 32, 41, 44, 47, 48, 53, 72, 84, 85, 91, 98, 99, 100, 106, 109, 111, 118, 119, 122, 124, 127, 134, 151, 152, 153, 154, 161, 163, 164, 165, 166, 178, 184, 187, 199, 201, 202, 205, 211, 212, 213, 216, 218, 219, 220, 221, 227, 235, 241, 248, 252, 254, 261, 265, 268, 269, 275, 276, 295, 298, 305, 325], "In": [2, 3, 4, 8, 9, 10, 19, 22, 23, 28, 31, 47, 48, 54, 60, 61, 62, 64, 71, 80, 83, 84, 86, 99, 101, 109, 112, 137, 139, 150, 151, 152, 153, 154, 155, 156, 158, 161, 162, 163, 164, 165, 166, 167, 168, 171, 178, 179, 180, 183, 188, 190, 192, 195, 198, 204, 205, 207, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 226, 229, 231, 232, 233, 234, 235, 241, 242, 243, 247, 248, 254, 255, 256, 258, 261, 262, 265, 268, 269, 278, 279, 286, 287, 289, 295, 297, 298, 306, 330, 331], "It": [0, 3, 4, 5, 6, 7, 8, 9, 12, 28, 31, 32, 35, 41, 47, 50, 51, 58, 61, 62, 73, 75, 78, 84, 85, 99, 100, 103, 109, 110, 115, 122, 123, 131, 142, 145, 151, 152, 158, 163, 164, 170, 178, 182, 183, 186, 205, 211, 212, 213, 216, 220, 221, 223, 226, 227, 228, 229, 231, 242, 245, 248, 251, 255, 256, 261, 264, 287, 289, 295, 299, 306, 307, 329, 336], "Its": [7, 47, 213, 220], "NOT": [205, 211], "No": [0, 8, 53, 80, 82, 136, 149, 178, 193, 205, 211, 235, 241, 242, 243, 247, 248, 249, 268, 275, 277, 287, 294], "Not": [2, 4, 5, 25, 29, 33, 35, 73, 74, 178, 203, 205, 209, 211, 268, 277], "OR": [91, 94, 122, 125], "Of": [213, 219], "On": [73, 76, 80, 83, 122, 125, 152, 157, 164, 169, 248, 252, 268, 269, 278, 279, 282, 287, 293, 304, 317], "One": [151, 158, 163, 171, 213, 216, 220, 268, 274, 295, 298], "Or": [0, 153, 165, 278, 287, 290, 295, 298], "Such": [213, 220], "THE": [205, 211, 213, 219], "TO": [205, 211], "That": [2, 18, 161, 205, 211, 213, 219, 220], "The": [0, 1, 2, 3, 4, 5, 6, 8, 9, 14, 15, 19, 25, 27, 28, 31, 32, 36, 39, 41, 42, 46, 48, 51, 52, 54, 60, 61, 63, 73, 76, 78, 79, 80, 81, 82, 83, 84, 87, 91, 95, 96, 99, 102, 103, 109, 114, 115, 119, 122, 125, 131, 136, 137, 141, 142, 144, 145, 146, 148, 149, 151, 152, 153, 154, 156, 157, 158, 160, 161, 163, 164, 165, 166, 168, 169, 172, 175, 177, 178, 179, 181, 183, 184, 188, 189, 190, 191, 192, 193, 195, 196, 199, 202, 205, 207, 211, 212, 213, 216, 219, 220, 221, 223, 225, 226, 227, 228, 235, 236, 242, 243, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 261, 264, 267, 268, 269, 277, 282, 283, 284, 285, 287, 290, 292, 293, 295, 298, 299, 300, 303, 304, 305, 306, 310, 315, 322, 326, 329], "Their": [205, 211], "Then": [0, 7, 10, 11, 47, 70, 72, 84, 87, 91, 96, 99, 103, 109, 113, 115, 122, 131, 205, 209, 211, 212, 213, 219, 248, 254], "There": [8, 9, 51, 53, 54, 64, 155, 159, 167, 175, 205, 211, 213, 219, 220, 229, 231, 233, 234, 306, 331], "These": [0, 2, 3, 7, 9, 18, 22, 27, 46, 60, 73, 78, 91, 95, 136, 148, 150, 152, 154, 162, 164, 166, 178, 180, 189, 206, 213, 214, 220, 222, 230, 248, 250, 278, 279, 282, 287, 289, 306, 331], "To": [2, 3, 4, 6, 7, 8, 9, 10, 11, 19, 23, 28, 32, 41, 45, 52, 54, 60, 61, 62, 63, 64, 71, 72, 73, 78, 81, 84, 86, 99, 101, 109, 112, 136, 142, 144, 145, 148, 149, 151, 153, 154, 161, 163, 165, 166, 178, 188, 190, 196, 199, 200, 205, 209, 210, 211, 221, 224, 227, 248, 254, 268, 271, 276, 278, 279, 285, 304, 305, 306, 307, 316, 317, 318, 319, 324, 325, 328, 330, 331, 335, 336], "With": [4, 11, 32, 72, 137, 141, 158, 172, 178, 179, 191, 192, 197, 198, 201, 204, 213, 215, 220, 229, 231, 248, 249, 255, 256, 268, 269, 278, 279, 282, 295, 301, 307, 336], "_": [2, 6, 20, 41, 154, 166, 205, 211, 235, 236, 238, 241, 242, 243, 244, 248, 249, 261, 262, 268, 269, 277, 295, 298, 305, 307, 325, 335, 336], "_2": [235, 236, 242, 243], "__call__": [3, 9, 10, 28, 62, 70, 142, 145, 205, 207, 210, 255, 259, 261, 267, 268, 277, 303, 306, 307, 310, 311, 330, 335, 336], "__file__": [153, 165], "__getitem__": [5, 35, 261, 263, 268, 271], "__index_level_0__": [255, 258], "__init__": [2, 3, 5, 9, 10, 25, 28, 35, 62, 70, 153, 165, 178, 191, 205, 210, 229, 233, 235, 238, 242, 245, 248, 251, 255, 259, 261, 263, 264, 267, 268, 271, 277, 303, 306, 307, 310, 311, 330, 335, 336], "__len__": [5, 35, 261, 263, 268, 271], "__main__": [137, 141, 154, 166], "__name__": [137, 141, 154, 166], "_arrow_table_from_shard": [255, 258], "_build": 0, "_class_nam": [5, 35], "_config": 0, "_diffusers_vers": [5, 35], "_dmat_from_arrow": [255, 258], "_k": [242, 243], "_model": [3, 28, 303, 310, 311], "_sample_timestep": [5, 35], "_shared_step": [235, 238, 242, 245], "_static": 0, "_toc": 0, "a10": [268, 269], "a100": [268, 269, 287, 290, 291, 293], "a10g": [242, 243, 246, 247], "a_random_job_nam": [91, 97, 109, 118], "abandon": [213, 220], "abil": [2, 4, 5, 23, 30, 34, 137, 139, 150, 152, 162, 164, 178, 179, 213, 216], "abl": [4, 7, 8, 32, 47, 54, 84, 85, 99, 100, 109, 111, 119, 122, 124, 136, 149, 229, 234], "abortmultipartupload": [73, 78], "about": [4, 6, 12, 13, 17, 19, 22, 23, 24, 31, 32, 41, 80, 82, 84, 90, 137, 139, 141, 152, 153, 154, 164, 165, 166, 178, 180, 182, 190, 205, 207, 211, 213, 215, 216, 219, 220, 221, 223, 229, 231, 268, 269, 278, 279, 283, 287, 292, 295, 298, 300, 303, 304, 305, 307, 310, 311, 316, 319, 325, 336], "abov": [2, 4, 5, 22, 32, 36, 136, 137, 141, 142, 145, 149, 178, 179, 180, 181, 188, 190, 205, 211, 213, 220, 261, 265, 278, 279, 283, 284, 295, 298], "absenc": [142, 145, 205, 211, 255, 260], "absent": [213, 220], "absolut": [4, 32, 261, 263], "abstract": [3, 7, 27, 43, 73, 75, 161, 242, 243, 255, 256, 261, 267, 278, 279, 283, 284, 287, 290, 306, 328], "absurd": [213, 220], "academ": [261, 262, 295, 301], "academi": [213, 220], "acc": [2, 25, 255, 258, 304, 313, 315, 319], "acc_metr": [268, 273], "acceler": [1, 2, 5, 7, 9, 10, 11, 13, 22, 35, 36, 43, 62, 68, 72, 84, 90, 122, 127, 152, 161, 164, 235, 239, 242, 246], "accelerator_shap": [303, 304, 305, 310, 311, 318, 324, 326], "accelerator_typ": [9, 62, 229, 233, 278, 279, 285, 287, 290, 293, 295, 298, 299, 300, 303, 304, 305, 310, 311, 318, 324, 326], "accept": [1, 6, 9, 10, 11, 16, 41, 61, 62, 68, 72, 178, 181, 191, 229, 231, 233, 261, 264, 305, 306, 324, 326, 330], "access": [2, 7, 18, 25, 31, 43, 73, 76, 77, 78, 83, 109, 119, 136, 137, 141, 142, 145, 146, 149, 151, 153, 154, 157, 158, 161, 163, 165, 166, 169, 170, 171, 178, 183, 188, 190, 197, 221, 226, 228, 229, 231, 255, 257, 268, 269, 270, 287, 288, 290, 291, 295, 296, 298, 299, 300, 312], "accident": [255, 258], "acclaim": [213, 220], "accomplish": [178, 204], "accord": [1, 8, 9, 16, 54, 64, 152, 164, 178, 189, 197, 306, 331], "accordingli": [221, 226, 242, 247, 295, 301], "account": [2, 7, 25, 47, 73, 77, 78, 80, 82, 84, 85, 86, 91, 92, 95, 99, 100, 101, 109, 111, 112, 122, 124, 126, 134, 161, 213, 216], "account_id": [73, 78], "accross": [278, 279, 282, 283], "accumul": [4, 32, 178, 182, 304, 315], "accur": [261, 262, 278, 279, 282], "accuraci": [6, 9, 40, 64, 221, 223, 225, 227, 255, 256, 258, 259, 260, 268, 273, 277, 287, 289, 295, 301, 304, 305, 306, 313, 315, 317, 318, 319, 323, 331], "accuracy_scor": [255, 257, 258], "achiev": [2, 4, 5, 8, 9, 10, 19, 24, 30, 34, 54, 62, 64, 68, 178, 179, 278, 279, 282, 306, 331], "acid": [7, 43], "acl": [158, 171], "across": [1, 2, 3, 4, 5, 7, 8, 9, 11, 13, 18, 21, 23, 28, 32, 36, 43, 46, 54, 55, 57, 59, 63, 64, 66, 72, 73, 75, 78, 80, 82, 135, 137, 139, 141, 146, 152, 153, 154, 158, 164, 165, 166, 172, 178, 179, 180, 181, 182, 183, 185, 188, 189, 190, 192, 195, 196, 197, 204, 213, 217, 220, 221, 223, 227, 228, 229, 231, 235, 236, 237, 241, 242, 243, 244, 247, 248, 249, 250, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 267, 268, 269, 273, 275, 277, 278, 279, 283, 284, 287, 290, 293, 294, 295, 298, 301, 303, 304, 305, 306, 308, 315, 317, 318, 326, 328, 330, 331], "act": [73, 78, 178, 183, 213, 220], "act_dim": [242, 245, 247], "act_fn": [5, 35], "action": [73, 78, 137, 141, 213, 219, 220, 244, 245, 295, 298, 300, 307, 336], "action_spac": [242, 244], "activ": [0, 1, 2, 6, 8, 9, 16, 25, 41, 54, 61, 146, 150, 155, 159, 162, 167, 173, 248, 250, 278, 279, 283, 305, 325], "actor": [4, 5, 7, 8, 10, 18, 22, 32, 36, 47, 51, 56, 68, 69, 137, 139, 181, 204, 205, 207, 210, 255, 259, 260, 261, 262, 267, 268, 277, 278, 279, 283, 304, 307, 315, 329, 334], "actorpoolmapoper": [9, 63], "actorpoolstrategi": [255, 257, 259, 260], "actress": [213, 216], "actual": [1, 6, 9, 15, 41, 62, 84, 86, 91, 94, 99, 101, 104, 109, 112, 116, 122, 125, 128, 132, 178, 188, 205, 211, 213, 220, 242, 244, 248, 249, 295, 300, 305, 306, 324, 330], "ad": [5, 36, 73, 78, 84, 86, 90, 152, 157, 161, 164, 169, 178, 192, 204, 205, 211, 242, 243, 255, 258, 259, 307, 335], "adam": [4, 6, 29, 31, 32, 38, 40, 41, 178, 180, 182, 193, 199, 235, 238, 242, 245, 248, 252, 261, 265, 268, 273, 304, 305, 313, 315, 319, 323, 326], "adamw": [5, 35], "adapt": [178, 180, 185, 221, 227, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277, 278, 279, 282, 296, 297, 302], "adapter_config": [295, 298], "adapter_model": [295, 298], "adapter_nam": [295, 298], "adaptiveavgpool2d": [304, 313, 319], "add": [1, 2, 4, 5, 7, 8, 9, 10, 14, 15, 16, 18, 19, 21, 25, 32, 35, 36, 43, 52, 53, 61, 70, 73, 78, 91, 95, 99, 102, 104, 107, 109, 114, 116, 119, 120, 122, 125, 129, 132, 137, 141, 142, 144, 150, 151, 152, 156, 160, 162, 163, 164, 168, 177, 178, 180, 181, 183, 186, 191, 204, 206, 213, 214, 217, 222, 230, 235, 236, 241, 242, 244, 247, 248, 254, 255, 260, 261, 265, 267, 268, 277, 278, 279, 284, 295, 298, 300, 304, 307, 316, 319, 335], "add_label": [9, 61, 64, 306, 331], "add_nois": [5, 35], "add_ref": [2, 23], "add_subplot": [4, 31, 32, 178, 180, 191], "addit": [4, 8, 32, 52, 73, 76, 137, 140, 150, 151, 152, 154, 161, 162, 163, 164, 166, 178, 185, 198, 199, 229, 231, 232, 248, 249, 254, 255, 260, 268, 277, 295, 298, 302], "addition": [8, 9, 54, 61, 73, 78, 84, 90, 137, 141, 221, 227, 248, 249, 307, 336], "addr": [235, 239], "address": [1, 7, 13, 43, 73, 78, 84, 86, 99, 101, 109, 112, 136, 149, 178, 189, 205, 211], "adebayor": [205, 211], "adher": [213, 220], "adjust": [5, 8, 35, 53, 178, 181, 205, 211, 213, 219, 221, 228, 255, 258, 268, 277, 295, 301], "adjust_total_amount": [8, 52], "adjusted_data": [8, 53, 55], "adjusted_data_rai": [8, 53, 55], "adjusted_total_amount": [8, 52, 142, 144], "admin": [73, 74, 78, 158, 171], "administr": [213, 220, 339], "admittedli": [213, 216], "adopt": [8, 51], "adv": [205, 209, 211], "advanc": [5, 6, 7, 35, 40, 43, 47, 80, 82, 83, 84, 90, 142, 145, 152, 164, 268, 277, 278, 279, 286, 288, 294, 298, 305, 323, 333, 339], "advantag": [7, 47, 287, 289], "adventureland": [205, 211], "adversari": [235, 236], "affect": [10, 69], "affin": [304, 313, 319], "afford": [205, 211, 287, 289], "after": [1, 2, 3, 4, 5, 10, 11, 16, 18, 24, 28, 31, 32, 35, 71, 72, 99, 103, 106, 109, 113, 115, 118, 122, 131, 137, 141, 142, 145, 146, 151, 152, 154, 160, 163, 164, 166, 176, 178, 180, 182, 188, 191, 205, 207, 211, 212, 213, 219, 221, 223, 228, 235, 236, 241, 242, 243, 248, 249, 250, 252, 254, 255, 257, 260, 261, 263, 265, 266, 268, 269, 273, 275, 278, 279, 282, 304, 307, 319, 336], "afterward": [268, 269], "again": [2, 3, 8, 22, 26, 52, 84, 89, 152, 155, 164, 167, 213, 220, 235, 240, 248, 253, 255, 256, 268, 276], "against": [178, 201, 205, 209, 211, 213, 219, 220, 242, 247, 248, 249, 252, 254, 261, 267, 268, 275, 287, 292, 295, 298], "agent": [242, 243, 247, 278, 279, 282, 295, 301], "aggreg": [7, 46, 49, 56, 60, 178, 179, 235, 238, 239, 255, 259, 260, 268, 273, 304, 315, 327], "aggress": [9, 61, 142, 145], "agil": [242, 243], "agnost": [261, 263], "ago": [213, 216], "agre": [213, 219], "aguero": [205, 211], "ahead": [213, 219, 287, 294, 295, 302], "ai": [2, 8, 9, 10, 11, 25, 55, 59, 61, 62, 64, 70, 72, 153, 155, 157, 159, 160, 161, 165, 167, 169, 173, 176, 278, 279, 285, 287, 294, 295, 297, 298, 300, 306, 307, 329, 330, 331, 332, 335, 336, 339], "air": [303, 310, 311], "aj": [205, 209, 211], "ak": [73, 76], "ako": [205, 211], "alaska": [213, 219], "alb": [80, 83], "alberta": [213, 219], "album": [205, 211], "alciato": [205, 209, 211], "ald": [205, 211], "alert": [143, 156, 159, 160, 168, 173, 176, 178, 204], "algorithm": [6, 41, 80, 83, 161, 221, 228, 255, 256, 278, 279, 284, 303, 304, 305, 310, 311, 318, 324, 325, 326], "alic": 146, "align": [7, 46, 178, 180, 235, 239, 268, 270, 273], "alik": [205, 207], "all": [0, 2, 3, 4, 5, 6, 7, 11, 12, 17, 18, 23, 24, 26, 27, 29, 32, 33, 36, 38, 41, 43, 49, 51, 55, 56, 61, 72, 73, 76, 78, 80, 82, 84, 85, 91, 92, 94, 99, 100, 105, 106, 107, 109, 110, 117, 118, 120, 122, 123, 125, 127, 136, 137, 138, 141, 142, 143, 146, 147, 149, 151, 152, 153, 154, 155, 157, 158, 161, 163, 164, 165, 166, 167, 169, 171, 178, 179, 180, 182, 183, 187, 188, 190, 197, 200, 201, 202, 205, 206, 207, 210, 211, 213, 214, 215, 216, 219, 220, 221, 222, 223, 227, 229, 230, 231, 235, 239, 242, 244, 247, 248, 249, 250, 252, 254, 255, 257, 258, 259, 260, 262, 263, 268, 269, 272, 273, 275, 277, 278, 279, 280, 281, 282, 287, 288, 295, 296, 298, 299, 302, 303, 304, 305, 306, 309, 317, 325, 326, 331], "all_fil": [153, 165], "all_results_at_onc": [2, 24], "alleg": [213, 216], "allegori": [213, 220], "alli": [213, 219], "allobjectact": [73, 78], "alloc": [6, 7, 9, 17, 41, 48, 57, 61, 73, 78, 80, 81, 84, 86, 99, 101, 109, 112, 178, 184, 191, 205, 207, 255, 258, 261, 262, 305, 325], "allow": [2, 3, 4, 5, 7, 18, 22, 27, 28, 30, 32, 34, 48, 73, 78, 109, 119, 142, 145, 150, 152, 154, 157, 158, 162, 164, 166, 169, 171, 178, 179, 184, 185, 188, 197, 200, 202, 205, 207, 211, 213, 220, 221, 223, 226, 227, 228, 229, 231, 233, 235, 239, 248, 253, 255, 258, 261, 265, 267, 268, 269, 287, 293, 295, 297, 298, 306, 307, 329, 330, 336], "allowedhead": [73, 78, 109, 119], "allowedmethod": [73, 78, 109, 119], "allowedorigin": [73, 78, 109, 119], "allreduc": [178, 179], "alltoallapi": [9, 64], "allus": [213, 220], "almost": [213, 220, 255, 257], "alon": [8, 9, 54, 64, 306, 331], "along": [152, 164, 178, 187, 205, 211, 268, 269], "alongsid": [178, 198, 204, 255, 257, 268, 277], "alonso": [205, 211], "alphas_cumprod": [5, 35], "alreadi": [8, 9, 10, 11, 51, 64, 68, 72, 99, 102, 109, 114, 137, 141, 151, 153, 161, 163, 165, 178, 180, 182, 191, 202, 205, 209, 211, 248, 250, 254, 255, 259, 261, 263, 268, 276, 306, 331], "also": [2, 4, 8, 9, 10, 19, 31, 32, 55, 64, 70, 84, 85, 91, 93, 99, 100, 106, 109, 111, 118, 122, 124, 137, 141, 142, 143, 145, 151, 153, 154, 155, 156, 158, 160, 163, 165, 166, 167, 168, 170, 176, 178, 180, 190, 200, 205, 209, 211, 212, 213, 215, 216, 219, 220, 229, 231, 232, 235, 239, 248, 250, 251, 252, 255, 257, 261, 265, 278, 279, 282, 304, 306, 307, 313, 319, 331, 335, 336], "altern": [11, 72, 84, 85, 99, 100, 109, 111, 122, 124, 221, 226, 255, 260], "alwai": [0, 2, 22, 137, 140, 255, 260, 261, 263, 295, 298, 299], "am": [213, 216], "amaz": [205, 211, 213, 219, 295, 302], "amazon": [7, 43, 73, 76, 84, 85, 99, 100, 109, 111, 153, 154, 165, 166, 205, 211], "amazonaw": [73, 78, 304, 305, 318, 324, 326], "amazonelasticfilesystemclientreadwriteaccess": [109, 113, 119], "ambush": [213, 219], "america": [213, 216], "american": [213, 216, 219, 220], "ami": [80, 82, 205, 211], "amnt": [205, 211], "among": [8, 9, 54, 64, 213, 219, 220, 287, 290, 306, 331], "amount": [2, 3, 4, 5, 7, 8, 9, 25, 28, 30, 34, 43, 51, 61, 153, 165, 178, 179, 261, 265, 267, 268, 269, 303, 310], "amp": [205, 211, 235, 241, 268, 277], "amus": [213, 220], "an": [0, 1, 4, 7, 8, 9, 13, 15, 17, 19, 20, 21, 25, 27, 31, 43, 47, 48, 49, 53, 54, 55, 56, 58, 59, 60, 61, 62, 67, 69, 71, 74, 77, 78, 79, 80, 83, 84, 85, 91, 97, 99, 100, 108, 111, 121, 136, 137, 138, 141, 142, 144, 145, 149, 151, 153, 155, 156, 157, 158, 161, 163, 165, 167, 168, 169, 170, 171, 173, 178, 179, 180, 205, 206, 209, 210, 211, 213, 214, 216, 218, 219, 220, 221, 222, 227, 229, 230, 231, 233, 235, 236, 241, 243, 248, 249, 255, 256, 258, 259, 260, 261, 262, 268, 269, 271, 278, 279, 282, 284, 285, 290, 291, 293, 296, 298, 304, 306, 308, 313, 317, 327, 329, 330, 334, 336, 339], "anal": [213, 220], "analys": [213, 220], "analysi": [7, 46, 135, 154, 166, 213, 220, 229, 231, 233, 234, 278, 279, 282, 295, 298, 301], "analyt": [7, 43, 44, 46], "analyz": [7, 43, 136, 148, 178, 190, 295, 298], "anatom": [213, 216], "anatomi": [205, 211], "angel": [213, 219, 220], "angelbr": [213, 220], "anger": [205, 211], "anggrek": [205, 211], "angl": [242, 243], "angular": [242, 243], "ani": [1, 2, 3, 6, 8, 9, 10, 11, 13, 14, 22, 28, 38, 40, 41, 51, 62, 65, 67, 70, 72, 84, 86, 91, 95, 98, 99, 101, 109, 112, 122, 134, 136, 137, 141, 142, 144, 149, 150, 151, 157, 161, 162, 163, 169, 178, 183, 196, 203, 205, 210, 213, 215, 216, 219, 220, 229, 233, 234, 235, 236, 239, 242, 247, 248, 249, 252, 254, 255, 256, 257, 258, 260, 261, 267, 268, 269, 277, 295, 298, 305, 306, 307, 323, 324, 329, 330, 332, 335, 336], "anniversari": [205, 211], "annot": [6, 41, 80, 83, 255, 259], "anon": [5, 35], "anonym": [142, 144, 303, 310, 311], "anoth": [8, 9, 52, 60, 80, 83, 152, 155, 159, 164, 167, 175, 205, 209, 211, 213, 217, 219, 220, 235, 241], "answer": [205, 211, 213, 216, 278, 279, 282], "ant": [205, 211], "anthoni": [213, 219, 220], "anti": [2, 12, 19, 24, 205, 211, 213, 220], "antiwoman": [213, 220], "anymor": [235, 241, 248, 254], "anyon": [205, 211, 213, 216], "anyscal": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 17, 26, 28, 29, 31, 32, 33, 37, 38, 43, 49, 51, 54, 56, 59, 61, 62, 64, 67, 70, 72, 76, 77, 83, 88, 89, 90, 93, 94, 97, 98, 102, 105, 106, 107, 108, 111, 113, 114, 117, 118, 119, 120, 121, 124, 125, 128, 133, 134, 136, 141, 144, 146, 147, 148, 149, 171, 172, 178, 179, 180, 204, 205, 207, 209, 213, 215, 221, 223, 229, 231, 241, 247, 254, 260, 267, 274, 280, 285, 286, 288, 289, 293, 294, 295, 296, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 318, 320, 324, 326, 329, 330, 331, 335, 336], "anyscale_101": [159, 160, 173, 176], "anyscale_artifact_storag": [5, 35, 137, 141, 153, 165], "anyscale_cloud_id": [91, 95], "anyscale_cloud_nam": [73, 79, 84, 85, 86, 87, 88, 89, 91, 92, 95, 96, 97, 98, 99, 100, 101, 103, 106, 107, 109, 110, 112, 115, 118, 120, 122, 123, 125, 126, 131, 133, 134], "anyscale_cloud_storage_bucket": [153, 165], "anyscale_cloud_storage_bucket_region": [153, 165], "anyscale_iam_rol": [109, 113], "anyscale_iam_role_arn": [73, 79], "anyscale_iam_s3_policy_arn": [109, 113], "anyscale_registration_command": [84, 86, 91, 95, 99, 101, 109, 112, 122, 126], "anyscale_s3_bucket_nam": [84, 86, 89, 99, 101, 107, 109, 112, 120], "anyscale_security_group": [73, 78], "anyscale_vpc": [73, 78], "anyscale_vpc_nam": [73, 78], "anyscalerai": [99, 107, 109, 120], "anyscaleuserdata": [287, 292], "anyth": [80, 83, 153, 165, 178, 183, 213, 216, 220, 268, 276], "anywher": [2, 18, 155, 167], "apach": 46, "aperitif": [205, 211], "api": [2, 7, 8, 9, 10, 25, 45, 46, 47, 48, 50, 55, 61, 62, 68, 80, 81, 82, 83, 93, 124, 134, 155, 156, 158, 159, 160, 161, 167, 168, 171, 173, 175, 177, 205, 207, 209, 210, 212, 213, 218, 229, 231, 235, 239, 248, 250, 255, 260, 261, 267, 268, 269, 277, 278, 279, 284, 285, 287, 290, 295, 297, 299, 300, 302, 304, 306, 307, 316, 330, 334], "api_kei": [278, 279, 285, 287, 291, 292, 295, 298, 299, 300], "apigatewai": [142, 145, 146], "app": [1, 3, 10, 13, 28, 71, 109, 119, 142, 145, 146, 154, 156, 160, 166, 168, 176, 177, 229, 231, 233, 235, 241, 278, 279, 283, 285, 287, 290, 291, 292, 293, 295, 298, 299, 300, 307, 336], "app1": [10, 71, 142, 145], "app_build": [10, 71], "apparatu": [242, 247], "appear": [205, 211, 213, 216, 220, 278, 279, 283], "append": [1, 2, 9, 16, 18, 24, 61, 153, 165, 235, 237, 242, 244, 248, 252, 261, 263, 268, 270, 277, 295, 300], "appl": [11, 72, 205, 207, 211, 221, 223, 226], "appli": [0, 2, 3, 4, 5, 8, 9, 10, 25, 28, 31, 35, 50, 51, 52, 54, 58, 62, 64, 70, 84, 86, 87, 91, 95, 96, 98, 99, 101, 103, 109, 112, 115, 122, 126, 131, 134, 179, 180, 182, 186, 189, 191, 205, 207, 211, 212, 213, 215, 218, 220, 221, 226, 229, 233, 235, 237, 242, 243, 244, 248, 249, 250, 261, 265, 268, 269, 271, 303, 306, 307, 310, 311, 330, 331, 335], "applic": [2, 3, 8, 11, 20, 27, 43, 47, 48, 50, 67, 70, 71, 72, 80, 83, 84, 90, 91, 94, 122, 125, 136, 137, 139, 140, 143, 146, 148, 150, 152, 155, 156, 157, 159, 160, 162, 164, 167, 168, 169, 173, 176, 177, 229, 231, 233, 234, 248, 254, 278, 279, 283, 285, 287, 289, 290, 292, 294, 295, 297, 299, 300, 302, 303, 306, 307, 309, 328, 333, 334, 335, 336, 338], "application_log": [229, 233], "approach": [2, 7, 19, 48, 135, 205, 207, 213, 215, 220, 221, 223, 227, 235, 236, 248, 249, 250, 255, 257, 268, 269], "appropri": [99, 106, 109, 118, 278, 279, 283, 287, 290], "approv": [84, 86, 89, 91, 95, 98, 99, 101, 107, 109, 120, 122, 126, 134], "approx": [2, 18, 242, 243], "approxim": [248, 249, 250, 278, 279, 283], "april": [205, 211], "ar": [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 22, 24, 25, 31, 32, 36, 37, 41, 42, 43, 45, 46, 47, 48, 51, 52, 53, 54, 55, 59, 60, 61, 62, 64, 68, 69, 72, 78, 80, 82, 83, 84, 85, 86, 89, 91, 94, 95, 99, 100, 101, 105, 106, 107, 109, 111, 112, 113, 117, 118, 119, 120, 122, 124, 125, 127, 134, 136, 137, 141, 142, 143, 144, 145, 146, 149, 150, 152, 153, 154, 155, 157, 158, 159, 162, 164, 165, 166, 167, 169, 171, 175, 178, 179, 180, 181, 182, 183, 185, 186, 188, 189, 190, 191, 193, 196, 198, 202, 205, 206, 207, 210, 211, 213, 214, 216, 219, 220, 221, 222, 226, 229, 230, 231, 233, 234, 237, 241, 244, 247, 252, 254, 260, 267, 270, 273, 277, 278, 279, 283, 287, 289, 290, 292, 293, 294, 295, 298, 299, 300, 302, 304, 305, 306, 307, 315, 316, 320, 326, 328, 330, 331, 334, 336], "arang": [261, 264, 267], "arbitrari": [1, 13, 295, 299], "architectur": [4, 5, 10, 32, 35, 36, 69, 77, 80, 82, 135, 178, 181, 207, 223, 231, 235, 241, 248, 251, 261, 262, 264, 268, 277, 280, 286, 304, 307, 315, 334], "archuleta": [205, 211], "area": [8, 54, 295, 302], "aree": 278, "aren": [109, 119, 261, 265], "arena": [295, 301], "arg": [261, 264, 268, 277, 295, 299], "argmax": [4, 9, 10, 31, 32, 62, 70, 178, 191, 221, 225, 255, 258, 259, 268, 273, 277, 304, 306, 307, 313, 319, 330, 335], "argu": [213, 220], "arguabl": [213, 216], "argument": [1, 4, 5, 6, 9, 10, 16, 17, 19, 22, 32, 36, 41, 62, 70, 155, 167, 178, 183, 185, 205, 211, 213, 220, 221, 226, 295, 298, 300, 305, 306, 307, 324, 326, 330, 335], "arm": [213, 219], "arm64": [11, 72], "arn": [73, 78, 84, 86, 99, 101, 109, 112, 113], "around": [4, 5, 32, 36, 156, 160, 168, 176, 178, 181, 213, 216, 219, 220, 268, 277, 295, 298], "arr": [178, 191, 235, 237], "arrai": [6, 8, 9, 10, 39, 41, 52, 61, 70, 178, 180, 191, 195, 196, 205, 211, 212, 295, 299, 303, 305, 307, 310, 311, 322, 324, 325, 335, 336], "arrang": [2, 23], "array_equ": [2, 18], "array_split": [248, 250], "arriv": [7, 46, 242, 247], "arrow_ref": [255, 258], "arsen": [205, 211], "art": [213, 219, 278, 279, 284], "articl": [307, 337], "artifact": [4, 32, 153, 165, 178, 180, 188, 189, 203, 221, 228, 255, 260, 268, 277, 303, 304, 310, 311, 318], "artifact_dir": [255, 260], "artifact_storag": [153, 165], "artifact_storage_path": [5, 35], "artifici": [242, 243], "artist": [213, 216], "as_directori": [4, 5, 32, 36, 178, 191, 199, 235, 239, 241, 242, 246, 247, 248, 252, 254, 255, 258, 261, 265, 267, 268, 273, 277, 304, 319], "as_fram": [255, 257], "as_index": [248, 252, 261, 265, 268, 275], "as_tensor": [5, 35, 178, 191], "asarrai": [235, 237], "asgi": 146, "ask": [2, 25, 213, 216, 235, 239, 268, 274], "aspen": [255, 256], "assert": [2, 9, 24, 61, 235, 241, 242, 247, 255, 257, 306, 307, 330, 336], "assess": [261, 265, 267, 268, 275], "assign": [8, 9, 53, 59, 122, 129, 157, 169, 178, 184, 189, 194, 248, 249, 255, 258, 268, 269, 306, 329], "assist": [205, 211, 301], "associ": [7, 48, 137, 139, 151, 163, 178, 190, 248, 254], "assum": [2, 18, 73, 78, 80, 82, 136, 149, 178, 180, 255, 259], "assumerol": [73, 78], "astral": [11, 72], "astyp": [5, 10, 35, 70, 242, 244, 248, 250, 255, 259, 261, 267, 268, 277], "async": [3, 10, 28, 70, 142, 145, 303, 307, 310, 311, 335, 336], "asynchron": [6, 40], "asyncio": [3, 7, 26, 28, 48], "athen": [205, 211], "atom": [7, 43, 205, 211], "attach": [80, 82, 83, 119, 178, 188, 191, 200, 248, 252, 261, 265, 268, 273], "attempt": [9, 60, 61, 178, 199, 261, 265], "attend": [213, 220], "attent": [213, 216, 221, 226, 261, 262, 267, 278, 279, 286], "attention_head_dim": [5, 35], "attribut": [235, 239, 255, 260], "audienc": [213, 219, 220, 248, 254], "audio": [205, 211], "audit": [278, 279, 284], "augment": [178, 192, 204, 268, 277], "august": [205, 211], "auschwitz": [213, 220], "auth": [91, 94, 122, 125, 128, 261, 263], "authent": [73, 78, 80, 83, 92, 93, 124, 151, 153, 163, 165, 287, 291, 292], "author": [146, 287, 291, 295, 298], "auto": [5, 35, 36, 80, 82, 84, 86, 89, 91, 95, 98, 99, 101, 107, 109, 120, 122, 126, 134, 142, 144, 156, 168, 235, 239, 242, 243, 246, 248, 252, 268, 269, 277, 295, 300, 307, 336], "auto_select_worker_config": [278, 279, 285, 287, 292], "autocal": [80, 82], "autocomplet": [295, 301], "autodiscoveri": [99, 102, 109, 114], "autograd": [178, 191], "autom": [80, 81, 82, 151, 163, 173, 255, 256, 261, 267, 295, 300, 302], "automat": [0, 2, 4, 5, 8, 9, 20, 30, 32, 34, 35, 51, 57, 62, 73, 78, 80, 82, 83, 91, 95, 151, 155, 156, 159, 160, 163, 167, 168, 173, 176, 179, 180, 181, 182, 183, 185, 186, 188, 190, 193, 194, 195, 197, 198, 201, 204, 205, 207, 209, 210, 213, 216, 221, 223, 226, 229, 233, 235, 236, 239, 241, 242, 243, 244, 246, 247, 248, 249, 252, 253, 254, 255, 256, 258, 260, 261, 262, 265, 266, 267, 268, 269, 272, 273, 274, 275, 276, 277, 278, 279, 284, 287, 292, 304, 307, 316, 336], "automodelforsequenceclassif": [221, 224, 226], "autosc": [7, 10, 48, 57, 69, 80, 82, 84, 90, 91, 98, 152, 154, 156, 160, 164, 166, 168, 176, 177, 178, 179, 229, 231, 278, 279, 283, 285], "autoscal": [2, 4, 5, 9, 22, 30, 34, 62, 80, 82, 83, 84, 90, 105, 106, 107, 108, 117, 118, 119, 120, 121, 137, 139, 150, 152, 162, 164, 229, 234, 304, 305, 307, 318, 324, 335, 336], "autoscaling_config": [278, 279, 285, 287, 290, 293, 295, 300, 307, 336], "autotoken": [221, 224, 226], "autotun": [6, 42], "auxiliari": [242, 247], "avail": [4, 5, 7, 8, 9, 11, 17, 18, 19, 23, 24, 31, 36, 48, 54, 61, 62, 72, 73, 78, 84, 86, 99, 101, 109, 112, 136, 137, 140, 141, 142, 145, 149, 151, 155, 156, 160, 163, 167, 168, 176, 178, 180, 181, 183, 188, 197, 199, 205, 211, 212, 213, 218, 221, 223, 226, 229, 231, 235, 241, 242, 246, 248, 250, 252, 261, 266, 267, 268, 273, 277, 278, 279, 281, 284, 287, 291, 295, 298, 301], "available_resourc": [2, 22], "availi": [150, 156, 160, 162, 168, 176], "avalanch": [213, 219], "averag": [4, 5, 8, 32, 36, 54, 178, 179, 182, 213, 216, 248, 252, 255, 259, 268, 273], "avg_loss": [4, 31], "avg_train_loss": [248, 252, 261, 265], "avg_val_loss": [248, 252, 261, 265], "avgpool": [304, 313, 319], "avoid": [1, 2, 4, 5, 7, 8, 9, 10, 16, 23, 30, 34, 35, 46, 52, 53, 57, 61, 68, 142, 145, 154, 155, 156, 166, 167, 168, 178, 179, 188, 191, 200, 203, 205, 212, 213, 216, 219, 242, 243, 247, 248, 250, 255, 257, 258, 259, 261, 263, 268, 277, 278, 279, 285], "aw": [7, 8, 9, 10, 43, 51, 59, 62, 70, 74, 75, 76, 78, 79, 80, 82, 83, 86, 89, 90, 101, 104, 106, 107, 108, 111, 112, 113, 116, 119, 120, 121, 122, 132, 135, 153, 157, 158, 165, 169, 171, 213, 220, 295, 298, 303, 304, 305, 306, 307, 310, 311, 318, 324, 326, 329, 330, 335], "awai": [9, 60, 151, 161, 163, 205, 211, 213, 216, 220, 267], "await": [3, 10, 28, 70, 303, 307, 310, 311, 335, 336], "awak": [213, 220], "awar": [242, 243, 248, 254, 261, 263], "award": [205, 211, 213, 220], "aws_region": [73, 79, 84, 86, 99, 101, 102, 104, 109, 112, 114, 116, 295, 298], "aws_role_nam": [91, 95], "awsregion": [99, 102, 109, 114], "ax": [6, 39, 178, 180, 235, 237, 241, 268, 270, 304, 305, 313, 319, 322], "axi": [4, 6, 9, 10, 31, 32, 39, 60, 62, 70, 178, 180, 191, 221, 225, 235, 237, 241, 255, 258, 259, 261, 267, 268, 270, 277, 304, 305, 306, 307, 313, 319, 322, 330, 335], "axvlin": [261, 267], "aydin": [295, 298], "azur": [73, 76, 178, 188, 204, 268, 277], "b": [1, 2, 10, 14, 16, 19, 22, 70, 178, 191, 205, 211, 235, 237, 238, 248, 254, 261, 264, 265, 267, 268, 277], "babi": [205, 209, 211], "back": [2, 3, 25, 28, 142, 145, 156, 168, 178, 194, 195, 205, 209, 211, 212, 213, 219, 220, 235, 241, 242, 243, 247, 248, 250, 255, 258, 259, 268, 277, 303, 307, 310, 311, 336], "backbon": [178, 181, 235, 241], "backdrop": [213, 220], "backend": [137, 140, 141, 142, 143, 221, 226, 227, 268, 277, 287, 293, 295, 298], "background": [1, 15, 213, 220], "backpressur": [7, 47, 48, 142, 144, 278, 279, 283], "backpropag": [221, 226], "backward": [4, 5, 6, 31, 32, 36, 40, 41, 178, 179, 182, 193, 199, 213, 220, 221, 226, 235, 236, 248, 252, 261, 265, 268, 273, 304, 305, 313, 315, 319, 323, 326], "bad": [205, 211, 213, 220], "bai": [213, 220], "bake": [2, 21], "balanc": [2, 10, 25, 68, 73, 78, 80, 83, 84, 90, 107, 108, 120, 121, 156, 160, 168, 176, 177, 229, 231, 233, 242, 243, 278, 279, 284, 287, 289, 290, 292, 295, 301], "bale": [205, 211], "ball": [205, 211], "band": [205, 211], "bandwidth": [278, 279, 281], "bank": [213, 220], "bar": [248, 250, 255, 257, 268, 270], "barca": [205, 211], "barcelona": [205, 211, 295, 298], "barh": [255, 259], "barr": [213, 220], "barrier": [235, 239, 242, 246], "base": [3, 4, 5, 6, 7, 11, 28, 30, 34, 35, 36, 41, 43, 45, 46, 48, 53, 58, 59, 62, 72, 80, 81, 82, 83, 99, 100, 109, 110, 122, 123, 135, 137, 141, 142, 145, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 171, 176, 178, 179, 188, 191, 205, 211, 213, 215, 219, 221, 226, 228, 229, 233, 235, 236, 237, 242, 243, 244, 254, 256, 257, 259, 261, 267, 268, 269, 275, 277, 278, 279, 283, 284, 295, 297, 298, 301, 302, 305, 307, 325, 329, 330, 336], "base_dir": [268, 277], "base_model_id": [295, 298], "base_s3_path": [295, 298], "base_url": [156, 168, 278, 279, 285, 287, 291, 292, 295, 298, 299, 300], "baselin": [6, 40, 178, 180, 242, 247, 248, 251, 305, 323], "basemodel": [3, 26, 28, 295, 299], "bash": [11, 72, 136, 149], "bash_profil": [11, 72], "basic": [1, 4, 6, 9, 10, 12, 29, 31, 38, 40, 41, 64, 71, 73, 77, 80, 83, 135, 136, 142, 143, 149, 155, 161, 167, 261, 263, 295, 296, 297, 302, 304, 305, 306, 312, 321, 323, 325, 331, 339], "basicblock": [304, 313, 319], "basicvariantgener": [6, 41, 305, 325, 326], "bastion": [73, 78], "batch": [4, 5, 6, 10, 17, 22, 26, 31, 32, 35, 36, 39, 47, 48, 52, 55, 57, 60, 61, 62, 68, 70, 142, 144, 154, 155, 158, 159, 166, 167, 171, 173, 178, 179, 182, 183, 186, 188, 191, 192, 193, 194, 197, 199, 204, 206, 212, 214, 221, 222, 223, 224, 226, 227, 230, 235, 237, 238, 242, 244, 245, 246, 247, 248, 249, 250, 252, 256, 258, 260, 262, 269, 283, 284, 286, 303, 304, 305, 307, 310, 311, 315, 316, 322, 330, 334, 335, 336], "batch_df": [235, 237], "batch_first": [261, 264], "batch_format": [8, 52, 235, 237, 242, 244, 248, 250, 255, 257, 259, 260, 261, 267, 268, 277], "batch_idx": [5, 35, 235, 238, 242, 245], "batch_pr": [9, 62, 306, 330], "batch_siz": [2, 4, 5, 6, 8, 9, 24, 31, 32, 35, 36, 39, 40, 41, 52, 60, 61, 62, 178, 182, 186, 193, 194, 199, 205, 211, 212, 221, 226, 235, 239, 242, 246, 248, 252, 255, 257, 261, 263, 265, 267, 268, 271, 272, 273, 274, 277, 304, 305, 306, 307, 313, 315, 316, 319, 322, 323, 326, 330, 335], "batch_size_per_work": [5, 36, 221, 226, 227], "batchnorm2d": [304, 313, 319], "bathtub": [213, 220], "bathtuby": [213, 220], "batman": [205, 211], "batteri": [303, 308], "battl": [205, 209, 211, 212, 213, 219, 220], "battleship": [213, 220], "bayesian": [6, 41, 305, 325], "bbc": [205, 211], "bc": [242, 247], "bd1": [205, 211], "beach": [295, 298], "bearer": 146, "beast": [205, 211], "beauti": [205, 211, 213, 219, 220, 229, 234], "bebr": [213, 220], "becaus": [2, 6, 8, 20, 41, 53, 80, 82, 84, 89, 99, 107, 109, 119, 120, 137, 141, 151, 163, 213, 216, 219, 220, 235, 237, 239, 240, 255, 260, 268, 270, 275, 278, 279, 282, 285, 305, 324], "becom": [2, 7, 8, 18, 24, 48, 51, 137, 141, 178, 183, 191, 205, 211, 235, 236], "bee": [205, 209, 211], "been": [7, 47, 151, 152, 157, 163, 164, 169, 213, 219, 220, 255, 258], "befor": [2, 3, 4, 5, 6, 8, 9, 24, 26, 28, 32, 36, 40, 41, 54, 64, 84, 85, 91, 93, 99, 100, 109, 111, 122, 124, 125, 134, 142, 143, 150, 161, 162, 178, 180, 188, 192, 193, 196, 197, 199, 200, 202, 213, 216, 229, 233, 235, 237, 248, 250, 255, 257, 261, 263, 268, 269, 270, 271, 278, 279, 282, 303, 304, 305, 306, 310, 311, 317, 323, 326, 331], "beforehand": [151, 163, 295, 298], "begin": [84, 85, 91, 93, 99, 100, 109, 111, 122, 124, 142, 143, 151, 161, 163, 205, 207, 213, 220, 248, 252, 295, 301], "beginn": [206, 214, 222, 230], "behalf": [136, 148], "behavior": [2, 19, 136, 137, 139, 142, 145, 148, 152, 164, 178, 198, 242, 247, 248, 250, 268, 270, 273, 275, 295, 297, 298], "behaviour": [235, 237, 255, 259], "behind": [80, 83, 213, 219, 220, 248, 249, 278, 279, 283], "being": [4, 7, 32, 47, 152, 154, 164, 166, 213, 216, 219, 220, 229, 234, 235, 237, 268, 270, 304, 307, 317, 336], "believ": [213, 219], "belong": [268, 269], "below": [1, 2, 4, 6, 7, 16, 20, 25, 31, 32, 41, 43, 80, 82, 142, 144, 151, 152, 154, 155, 156, 161, 163, 164, 166, 167, 168, 175, 178, 182, 190, 235, 241, 255, 260, 268, 277, 278, 279, 283, 287, 293, 305, 325], "ben": [205, 209, 211, 212, 213, 219], "benchmark": [255, 260, 268, 269, 299], "benefit": [2, 3, 6, 9, 11, 19, 27, 40, 61, 72, 73, 78, 302, 303, 305, 309, 323], "bergman": [213, 216], "berni": [205, 211], "bert": [206, 214, 221, 222, 223, 226, 227, 228, 230], "besok": [205, 211], "best": [3, 4, 6, 8, 9, 28, 32, 41, 54, 64, 73, 76, 91, 95, 137, 140, 152, 164, 178, 179, 188, 190, 195, 204, 205, 211, 212, 213, 219, 220, 221, 223, 242, 247, 248, 254, 255, 258, 260, 261, 265, 267, 268, 269, 275, 277, 278, 295, 298, 301, 303, 305, 306, 310, 311, 324, 326, 331], "best_ckpt": [235, 239, 241, 242, 246, 247, 255, 258, 259, 260, 261, 265, 268, 274], "best_ckpt_path": [261, 267, 268, 277], "best_result": [6, 41, 305, 324, 326], "better": [2, 6, 7, 9, 22, 40, 46, 61, 154, 156, 166, 168, 178, 204, 205, 211, 213, 216, 218, 219, 229, 231, 235, 241, 242, 247, 248, 254, 255, 259, 295, 297, 305, 323], "between": [2, 6, 7, 8, 9, 10, 19, 39, 41, 43, 45, 46, 51, 54, 57, 61, 64, 68, 73, 74, 75, 78, 80, 82, 109, 119, 135, 137, 140, 141, 153, 165, 178, 183, 213, 216, 217, 248, 249, 255, 257, 261, 263, 278, 279, 283, 287, 289, 295, 297, 298, 305, 306, 322, 325, 329, 331], "beyonc": [205, 211], "beyond": [157, 169, 205, 211, 213, 219, 295, 296, 300], "bf16": [5, 35, 36], "bfloat16": [268, 277], "bia": [4, 6, 31, 40, 41, 178, 181, 255, 257, 304, 305, 313, 319, 323, 326], "bias": [178, 204, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277], "bidder": [213, 219], "bieber": [205, 211], "big": [7, 43, 47, 213, 219, 221, 223, 235, 236, 242, 243, 248, 249], "bigger": [205, 211], "bigl": [268, 269], "bigqueri": [7, 43], "bigr": [268, 269], "bill": [91, 93, 122, 124, 158, 171], "billion": [137, 141, 287, 289], "bin": [0, 122, 125, 248, 250], "binari": [255, 256, 260, 303, 310, 311], "bind": [3, 7, 10, 28, 43, 70, 142, 145, 156, 160, 168, 177, 229, 233, 303, 307, 310, 311, 335, 336], "birthdai": [205, 211], "bit": [2, 24, 213, 219, 220], "bitten": [213, 219], "bjork": [213, 220], "bjp": [205, 211], "black": [6, 39, 205, 211, 213, 220, 248, 250, 305, 322], "blair": [205, 211], "blank": [150, 162], "bless": [205, 211], "blind": [213, 220], "blob": [178, 188, 248, 250, 268, 277], "block": [1, 2, 4, 6, 10, 15, 16, 17, 23, 24, 32, 40, 51, 52, 53, 57, 60, 61, 62, 69, 70, 71, 99, 107, 109, 120, 142, 144, 145, 178, 189, 192, 193, 205, 207, 209, 213, 218, 235, 237, 248, 249, 250, 255, 258, 278, 279, 285, 287, 291, 293, 295, 298, 299, 300, 304, 307, 318, 329, 335, 336], "block_out_channel": [5, 35], "blockbust": [213, 220], "blog": [4, 5, 6, 32, 37, 42, 304, 320], "blow": [205, 211], "blue": [205, 209, 211, 235, 236, 268, 269, 278, 279, 282], "bn1": [304, 313, 319], "bn2": [304, 313, 319], "board": [213, 220], "boat": [213, 219], "bob": [205, 211], "bodi": [10, 70, 213, 216, 220, 307, 335], "boi": [213, 216, 220], "boilerpl": [178, 185, 188, 235, 236, 242, 243, 255, 256, 268, 277], "bomb": [213, 220], "bon": [205, 211], "book": [205, 209, 211, 212, 213, 219], "bookkeep": [268, 272], "bool": [268, 277], "boolean": [303, 310], "boost": [3, 28, 255, 256, 257, 258, 260, 303, 310], "booster": [3, 28, 255, 258, 259, 260, 303, 310, 311], "boot": [205, 211], "booth": [205, 211], "bootstrap": [80, 82], "border": [213, 219], "bore": [213, 219], "both": [0, 7, 9, 43, 48, 57, 73, 78, 80, 82, 109, 113, 136, 137, 139, 142, 145, 148, 158, 161, 171, 178, 188, 190, 192, 195, 198, 202, 205, 211, 221, 223, 227, 235, 237, 255, 257, 258, 287, 288, 295, 299], "boto3": [153, 165, 295, 298], "bottleneck": [136, 137, 141, 148, 255, 257], "bottom": [2, 24, 157, 169], "bound": [17, 73, 78], "boundari": [7, 46], "bouquet": [205, 211], "bout": [205, 209, 211, 212], "box": [4, 32, 154, 157, 166, 169, 178, 179], "br": [213, 216, 219, 220], "brain": [213, 219], "branch": 0, "brand": [295, 299], "braun": [205, 211], "break": [5, 6, 35, 39, 154, 166, 213, 220, 268, 271, 305, 322], "breakdown": [7, 43], "breakneck": [213, 220], "breakpoint": [221, 226], "breez": [213, 216], "brennan": [213, 219], "brew": [3, 26, 84, 85, 99, 100, 109, 111, 122, 124, 136, 149], "bridg": [7, 43, 178, 183, 213, 220, 306, 307, 328, 337], "brief": [295, 298], "bring": [3, 27, 178, 189, 205, 209, 211, 213, 219, 220, 255, 257], "brit": [205, 211], "british": [205, 211, 213, 220], "broadcast": [178, 179], "broader": [7, 47], "brock": [205, 211], "brought": [213, 219], "brown": [205, 211, 213, 216], "brows": 146, "browser": [0, 11, 72, 91, 94, 151, 163], "bryant": [205, 211], "bst": [3, 28], "bubbl": [205, 211], "bucket": [73, 77, 78, 79, 84, 86, 89, 90, 91, 92, 95, 98, 99, 101, 107, 109, 112, 119, 120, 122, 126, 134, 153, 165, 295, 298], "bucket_nam": [91, 95, 295, 298], "budget": [295, 301], "buf": [235, 237, 268, 270], "buffalo": [205, 211], "buffer": [7, 43, 178, 194, 278, 279, 283], "bug": [136, 148], "bui": [205, 211, 213, 219], "build": [1, 2, 3, 5, 7, 8, 9, 10, 11, 16, 17, 27, 28, 32, 35, 43, 46, 48, 50, 57, 60, 67, 69, 70, 71, 72, 73, 77, 150, 152, 161, 162, 164, 180, 182, 185, 193, 195, 213, 220, 235, 236, 237, 242, 243, 248, 249, 250, 254, 256, 257, 261, 263, 267, 268, 271, 273, 277, 287, 294, 295, 297, 300, 302, 303, 307, 309, 317, 333, 334, 336], "build_app": [10, 71], "build_data_load": [6, 39, 40, 305, 322, 323], "build_data_loader_ray_train": [4, 32, 178, 182, 186, 304, 315, 316, 319], "build_data_loader_ray_train_ray_data": [178, 193, 194, 199], "build_data_loader_torch": [4, 31, 304, 313], "build_dataload": [261, 263, 265, 268, 272, 273], "build_inference_dataset": [268, 277], "build_openai_app": [278, 279, 285, 287, 290, 293, 295, 298, 299, 300], "build_resnet18": [4, 31, 32, 178, 181, 185, 191, 304, 313, 316, 319], "builder": [10, 71], "built": [0, 1, 3, 4, 5, 7, 8, 13, 27, 30, 32, 34, 36, 43, 50, 80, 83, 84, 90, 91, 95, 136, 137, 139, 146, 149, 151, 152, 154, 163, 164, 166, 178, 179, 181, 193, 229, 231, 242, 243, 246, 248, 253, 255, 257, 258, 259, 260, 261, 267, 268, 269, 273, 277, 278, 279, 283, 303, 307, 309, 335, 337], "bulk": [155, 159, 167, 173], "bundl": [152, 164], "bunni": [213, 216], "burden": 161, "burrito": [268, 269], "bursti": [150, 152, 162, 164, 278, 279, 283], "busi": [7, 10, 43, 69, 229, 233, 295, 300], "button": [151, 152, 156, 157, 163, 164, 168, 169], "bx1": [235, 238], "bx3xhxw": [235, 238], "bypass": [2, 19], "bystand": [213, 220], "byte": [9, 61, 142, 144, 235, 237, 241, 268, 270], "bytesio": [235, 237, 268, 270, 271, 277], "byth": [153, 165], "c": [2, 10, 11, 22, 70, 72, 152, 153, 164, 165, 178, 191, 205, 211, 213, 219, 248, 252, 255, 257, 258, 261, 265, 268, 275, 277], "cab": [3, 8, 28, 51, 54, 142, 144, 303, 310], "cabin": [213, 220], "cabl": [213, 216], "cach": [2, 18, 73, 77, 152, 164, 178, 204, 235, 237, 242, 244, 248, 250, 254, 255, 257, 261, 263, 268, 270, 271, 277, 281, 283, 286, 287, 293, 306, 330], "cactu": [205, 209, 211], "caesar": [268, 269], "cahse": [205, 211], "calcul": [4, 8, 32, 52, 154, 166, 178, 182, 221, 225, 226, 304, 315], "call": [2, 3, 4, 5, 6, 8, 9, 12, 15, 19, 24, 25, 28, 32, 36, 41, 54, 55, 60, 61, 64, 142, 145, 146, 156, 168, 178, 180, 182, 185, 188, 189, 193, 197, 199, 200, 201, 202, 212, 213, 220, 221, 228, 235, 237, 239, 240, 248, 252, 255, 257, 260, 268, 270, 274, 279, 286, 296, 297, 302, 304, 305, 306, 317, 318, 324, 326, 331], "call_id": [295, 300], "callabl": [9, 10, 62, 71, 205, 207, 210, 212, 295, 300, 306, 330], "callback": [5, 36, 235, 238, 239, 242, 246, 247, 255, 258, 260], "caller": [205, 211], "came": [205, 211], "camera": [213, 219, 220], "campu": [205, 211], "can": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 28, 30, 31, 32, 34, 35, 36, 38, 40, 41, 44, 46, 47, 48, 49, 51, 52, 53, 54, 56, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 76, 78, 80, 82, 83, 84, 85, 86, 88, 89, 90, 91, 95, 97, 98, 99, 100, 101, 106, 107, 108, 109, 111, 112, 113, 118, 119, 120, 121, 122, 124, 133, 134, 136, 137, 139, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 172, 173, 175, 177, 178, 179, 180, 183, 184, 187, 189, 190, 191, 192, 195, 196, 198, 199, 200, 202, 204, 205, 207, 209, 210, 211, 212, 213, 215, 216, 218, 219, 220, 221, 223, 226, 227, 228, 229, 231, 233, 236, 237, 238, 239, 243, 245, 249, 250, 256, 257, 262, 263, 266, 269, 270, 271, 278, 279, 280, 281, 282, 283, 284, 285, 287, 288, 289, 291, 292, 293, 295, 296, 297, 298, 300, 303, 304, 305, 306, 307, 308, 313, 317, 319, 323, 325, 326, 329, 330, 331, 332, 334, 335, 336], "canadian": [213, 219], "cancel": [213, 219], "candid": [235, 239, 242, 246, 247, 295, 298], "cannon": [213, 220], "cannot": [2, 11, 18, 72, 84, 87, 89, 91, 96, 99, 103, 109, 115, 122, 131, 152, 164, 213, 216], "canopi": [255, 260], "cant": [213, 220], "canva": [4, 5, 32, 37, 304, 320], "capabl": [7, 10, 43, 47, 69, 80, 83, 137, 138, 140, 146, 151, 153, 163, 165, 221, 227, 229, 231, 287, 289, 294, 295, 296, 297, 300, 301, 302], "capac": [84, 86, 99, 101, 109, 112, 152, 164, 248, 249, 278, 279, 281, 282], "capit": [278, 279, 285, 295, 298], "captain": [213, 219], "caption_lat": [5, 35], "captur": [142, 145, 157, 169, 235, 239, 248, 252, 261, 262, 267], "car_typ": [295, 299], "card": [8, 51, 287, 290], "cardescript": [295, 299], "cardiffnlp": [205, 209], "care": [178, 188, 213, 219, 220, 261, 263], "carli": [205, 211], "carriag": [213, 220], "carrow": [205, 211], "cartograph": [255, 256], "cartpol": [242, 247], "cartyp": [295, 299], "case": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 19, 25, 27, 28, 32, 36, 37, 41, 42, 46, 54, 55, 62, 64, 71, 80, 82, 84, 86, 99, 101, 109, 112, 137, 140, 152, 154, 164, 166, 178, 200, 205, 211, 212, 213, 220, 221, 226, 248, 249, 255, 256, 261, 265, 278, 279, 282, 287, 289, 294, 297, 298, 302, 303, 304, 305, 306, 309, 310, 314, 320, 324, 330, 331], "cash": [8, 51], "castl": [213, 219], "casual": [205, 211], "cat": [235, 238, 242, 245, 261, 265], "catalog": [248, 249], "catch": [205, 211], "categor": [7, 44, 46, 154, 166], "categori": [9, 60, 80, 82, 221, 223, 268, 269], "cattl": [213, 219], "cattleman": [213, 219], "caus": [4, 5, 17, 30, 34, 84, 86, 99, 101, 109, 112, 137, 141, 178, 179], "cd": [0, 10, 71, 84, 88, 91, 97, 99, 106, 109, 118, 122, 133, 137, 141, 142, 145, 155, 156, 159, 160, 161, 167, 168, 173, 177, 268, 277, 295, 302], "cdot": [235, 236, 242, 243], "ceil_mod": [304, 313, 319], "cell": [3, 4, 5, 6, 8, 9, 10, 28, 29, 32, 33, 37, 42, 55, 66, 71, 151, 152, 153, 155, 159, 163, 164, 165, 167, 175, 178, 180, 191, 248, 250, 261, 263, 304, 320], "celsiu": [2, 25, 295, 300], "cena": [205, 209, 211], "center": [178, 186, 205, 211, 213, 216, 235, 237, 268, 269, 270], "center_input_sampl": [5, 35], "centercrop": [235, 237, 268, 270], "central": [137, 140, 178, 188, 295, 298], "centric": [7, 46], "ceph": [7, 43], "cerebr": [213, 219], "cert": [80, 83], "certain": [4, 5, 6, 8, 29, 33, 41, 52, 213, 216, 229, 233, 305, 325], "certif": [80, 83], "chain": [17, 306, 331], "chair": [205, 211], "chalk": [213, 220], "challeng": [4, 5, 9, 10, 30, 34, 57, 68, 161, 178, 179, 192, 280, 286], "chanc": [2, 20, 205, 211], "chang": [2, 4, 5, 6, 7, 9, 10, 18, 30, 34, 36, 41, 43, 65, 68, 71, 84, 86, 91, 95, 99, 101, 109, 112, 119, 137, 139, 142, 145, 153, 154, 156, 160, 165, 166, 168, 177, 178, 179, 205, 211, 213, 219, 221, 227, 235, 236, 239, 241, 242, 243, 248, 249, 255, 256, 261, 267, 268, 269, 276, 277, 287, 292, 294, 295, 298, 305, 326], "channel": [9, 61, 137, 141, 142, 145, 178, 180, 181, 191, 213, 220, 235, 236, 237, 238, 261, 267, 268, 269, 304, 306, 307, 313, 330, 336], "chao": [213, 220], "chap": [84, 85, 99, 100, 109, 111], "charact": [213, 220, 229, 234, 278, 279, 281], "characterist": [7, 8, 10, 43, 51, 69, 278, 279, 281, 306, 329], "charg": [8, 51], "charli": [205, 211], "charm": [205, 211, 213, 219], "chart": [99, 102, 104, 109, 114, 116, 122, 132, 255, 257], "chase": [205, 209, 211], "chat": [278, 279, 282, 285, 287, 291, 292, 295, 298, 299, 300, 301], "chatbot": [295, 300, 301], "cheap": [205, 211], "cheaper": [7, 45], "cheapli": [213, 216], "cheat": [213, 219], "check": [2, 4, 5, 8, 9, 10, 22, 31, 32, 36, 51, 53, 61, 62, 70, 84, 88, 91, 97, 99, 105, 106, 109, 117, 118, 119, 122, 127, 133, 142, 144, 146, 153, 154, 155, 156, 157, 165, 166, 167, 168, 169, 176, 178, 180, 188, 189, 199, 205, 211, 221, 226, 239, 242, 246, 248, 250, 255, 257, 260, 278, 279, 283, 295, 300, 304, 319], "check_cal": [235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270], "check_val_every_n_epoch": [235, 239, 242, 246], "checkout": [154, 156, 166, 168], "checkpoint": [8, 9, 30, 34, 35, 55, 57, 153, 165, 179, 180, 181, 182, 189, 190, 193, 197, 198, 201, 203, 204, 221, 228, 236, 237, 239, 243, 244, 247, 249, 250, 254, 256, 258, 262, 263, 265, 267, 268, 269, 270, 273, 274, 275, 276, 277, 303, 305, 310, 311, 312, 315, 318, 319, 324], "checkpoint_": [268, 277], "checkpoint_000000": [304, 319], "checkpoint_000001": [304, 319], "checkpoint_at_end": [261, 265], "checkpoint_config": [235, 239, 242, 246, 248, 252, 255, 258, 261, 265, 268, 274], "checkpoint_dir": [268, 277], "checkpoint_dir_nam": [304, 319], "checkpoint_frequ": [235, 239, 242, 246, 255, 258, 268, 274], "checkpoint_nam": [255, 258], "checkpoint_path": [4, 5, 31, 36, 261, 267, 268, 277, 304, 313], "checkpoint_root": [268, 277], "checkpoint_score_": [261, 265], "checkpoint_score_attribut": [235, 239, 242, 246, 255, 258, 261, 265, 268, 274], "checkpoint_score_ord": [235, 239, 242, 246, 255, 258, 261, 265, 268, 274], "checkpointconfig": [235, 236, 237, 239, 242, 243, 244, 246, 248, 249, 250, 252, 255, 256, 257, 258, 261, 263, 265, 268, 270, 274], "cheekbon": [205, 211], "chelsea": [205, 211], "cheri": [205, 211], "chill": [205, 211], "chip": [11, 72], "chloe": [213, 216], "chmod": [136, 149], "choic": [7, 8, 48, 55, 80, 82, 229, 231, 261, 262, 278, 279, 285, 287, 291, 292, 295, 298, 299, 300], "choos": [9, 11, 63, 72, 73, 74, 76, 109, 119, 122, 129, 150, 151, 152, 162, 163, 164, 213, 220, 248, 254, 278, 279, 283, 296, 302, 306, 330], "choreo": [205, 211], "chose": [4, 32, 304, 317], "chown": [136, 149], "chri": [205, 211], "chrisbrown": [205, 211], "christian": [205, 211], "chromadb": [7, 43], "chronolog": [137, 139], "chuck": [205, 211], "chunk": [7, 9, 46, 64, 154, 166, 278, 279, 285, 287, 291, 292, 295, 298, 306, 331], "church": [205, 211], "churn": [261, 267], "chw": [235, 237], "ci": [155, 159, 167, 173, 178, 204, 268, 277, 295, 302], "ciara": [205, 211], "cidr": [73, 78], "cidr_block": [73, 78], "cif": [205, 211], "cifar": [178, 197, 200, 202], "cifar10": [178, 195, 203], "cinema": [213, 216, 219, 220], "cinemat": [213, 220], "cinematograph": [213, 220], "cinematographi": [213, 219, 220], "cineworld": [205, 211], "citi": [3, 8, 28, 51, 205, 211, 213, 220, 261, 262, 295, 300, 303, 310], "citizenship": [205, 211], "ckpt": [4, 5, 32, 36, 235, 236, 239, 241, 242, 243, 246, 247, 248, 252, 255, 258, 259, 261, 265, 267, 268, 273, 304, 319], "ckpt_dir": [4, 5, 32, 36, 178, 191, 199, 235, 241, 242, 247, 248, 252, 254, 261, 265, 267, 268, 273, 277, 304, 319], "ckpt_file": [235, 241, 242, 247], "ckpt_out": [248, 252, 261, 265, 268, 273], "ckpt_path": [5, 36, 235, 239, 242, 246], "ckpt_root": [235, 239, 242, 246], "claim": [7, 47, 213, 216, 219], "clamp": [235, 241], "clarifi": [158, 170], "class": [2, 3, 4, 5, 6, 9, 10, 25, 28, 32, 35, 36, 39, 41, 59, 62, 70, 142, 145, 178, 181, 189, 191, 195, 207, 211, 212, 229, 233, 235, 236, 237, 238, 241, 242, 245, 248, 251, 256, 259, 261, 263, 264, 267, 268, 269, 270, 271, 277, 295, 299, 303, 305, 306, 307, 310, 311, 322, 324, 325, 329, 330, 335, 336], "class_nam": [2, 25], "classic": [242, 243, 247, 248, 249, 255, 256], "classif": [3, 28, 67, 178, 180, 181, 182, 221, 223, 224, 226, 228, 273, 303, 306, 310, 330], "classifi": [6, 10, 40, 70, 221, 223, 268, 269, 305, 323, 333], "classmat": [213, 216], "claud": [295, 301], "cld": [153, 165, 287, 292], "cld_g54aiirwj1s8t9ktgzikqur41k": [153, 165], "cldrsrc_12345abcdefgh67890ijklmnop": [99, 103, 104, 109, 115, 116, 122, 131, 132], "clean": [0, 10, 69, 84, 89, 91, 98, 122, 134, 188, 213, 220, 236, 238, 239, 250, 261, 265, 267, 269, 270, 273, 276], "cleaner": [178, 180], "cleanli": [178, 188, 248, 250], "cleanup": [3, 4, 5, 6, 8, 9, 10, 28, 32, 37, 42, 55, 66, 71, 80, 82, 135, 155, 167, 178, 180, 203, 235, 241, 242, 247, 248, 254, 255, 260, 268, 277, 303, 304, 306, 310, 311, 320, 332], "clear": [0, 158, 172, 213, 219, 242, 247, 255, 260, 261, 267, 268, 269, 277], "clearli": [142, 143, 213, 220], "cli": [73, 74, 84, 85, 91, 93, 99, 100, 109, 111, 122, 124, 130, 142, 145, 146, 151, 153, 154, 155, 163, 165, 166, 167], "click": [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 16, 17, 25, 26, 29, 32, 33, 36, 38, 41, 49, 56, 61, 67, 73, 78, 84, 85, 99, 100, 109, 111, 119, 122, 124, 136, 142, 144, 145, 149, 150, 151, 152, 153, 154, 156, 157, 162, 163, 164, 165, 166, 168, 169, 205, 207, 213, 215, 221, 223, 229, 231, 278, 279, 280, 287, 288, 293, 295, 296, 304, 305, 319, 325], "client": [153, 165, 231, 278, 279, 284, 285, 287, 291, 292, 295, 298, 299, 300], "cliff": [205, 211], "clipboard": [156, 168], "clitori": [213, 216], "clock": [213, 220], "clog": [213, 220], "clone": [0, 156, 168, 242, 247], "close": [178, 188, 205, 211, 213, 220], "cloud": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 17, 26, 29, 32, 33, 38, 49, 56, 67, 72, 74, 77, 78, 80, 82, 83, 85, 86, 88, 89, 90, 92, 93, 95, 97, 98, 100, 101, 104, 105, 106, 107, 108, 110, 112, 116, 117, 118, 120, 121, 123, 124, 126, 132, 133, 134, 135, 137, 141, 150, 152, 157, 160, 161, 162, 164, 169, 172, 176, 178, 179, 188, 204, 205, 207, 213, 215, 221, 223, 227, 229, 231, 235, 241, 248, 254, 255, 256, 268, 269, 278, 279, 280, 284, 285, 287, 288, 292, 295, 296, 298], "cloud_deployment_id": [99, 104, 109, 116, 122, 132], "cloud_nam": [99, 106, 109, 118], "clouddeploymentid": [99, 101, 104, 109, 112, 116, 122, 132], "cloudflar": [7, 43], "cloudform": [73, 78], "cloudfound": [73, 78, 91, 95], "cloudprovid": [99, 101, 104, 109, 112, 116, 122, 132], "cloudresourcemanag": [91, 94, 122, 125], "cloudwatch": [73, 78], "club": [205, 209, 211, 213, 219], "cluster": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 19, 21, 23, 25, 26, 28, 29, 30, 33, 34, 38, 46, 49, 53, 55, 56, 57, 59, 60, 61, 62, 63, 66, 67, 69, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 86, 88, 89, 90, 91, 97, 98, 101, 105, 106, 107, 108, 111, 112, 113, 117, 118, 119, 120, 121, 128, 133, 134, 135, 137, 139, 140, 142, 143, 148, 150, 151, 152, 154, 155, 156, 157, 159, 160, 162, 163, 164, 166, 167, 168, 169, 173, 175, 176, 179, 180, 181, 184, 186, 188, 189, 190, 191, 192, 195, 196, 197, 200, 204, 207, 209, 211, 213, 215, 223, 227, 231, 235, 236, 237, 241, 242, 243, 244, 247, 248, 249, 250, 252, 254, 255, 256, 257, 260, 261, 262, 268, 269, 274, 277, 278, 279, 280, 284, 285, 287, 288, 292, 295, 296, 303, 304, 305, 306, 307, 308, 310, 311, 318, 324, 326, 328, 329, 330, 334, 335], "cluster_id": [154, 166], "cluster_storag": [3, 4, 5, 8, 9, 10, 28, 31, 36, 37, 53, 62, 70, 142, 144, 153, 165, 178, 180, 181, 186, 188, 190, 195, 203, 235, 237, 239, 241, 242, 246, 247, 248, 250, 252, 254, 255, 257, 258, 260, 261, 262, 263, 268, 269, 270, 271, 273, 274, 277, 303, 304, 306, 307, 310, 311, 318, 319, 320, 330, 332, 335, 336, 338], "clusternam": [99, 102, 109, 114], "clusteronc": [84, 88], "cm": [255, 259], "cm_norm": [255, 259], "cmap": [4, 6, 9, 31, 32, 39, 60, 178, 180, 191, 255, 259, 304, 305, 307, 313, 319, 322, 336], "cnn": [178, 180, 235, 238, 241, 268, 269], "co": [242, 243, 244, 247, 261, 264, 295, 298], "coach": [205, 211], "coars": [10, 69], "cocki": [213, 219], "code": [0, 1, 2, 4, 5, 6, 8, 10, 11, 13, 22, 25, 26, 30, 32, 34, 36, 41, 51, 70, 71, 72, 73, 74, 135, 137, 141, 142, 145, 151, 152, 153, 154, 155, 156, 157, 161, 163, 164, 165, 166, 167, 168, 169, 173, 175, 178, 179, 182, 183, 184, 185, 186, 187, 192, 204, 205, 211, 229, 231, 235, 236, 242, 243, 248, 249, 254, 255, 256, 257, 261, 262, 263, 267, 268, 269, 277, 287, 292, 294, 297, 301, 304, 305, 307, 319, 325, 326, 335], "coder": [295, 301], "cogent": [213, 216], "coher": [213, 219, 220], "coi": [213, 219], "col": [4, 5, 31, 32, 35, 178, 180, 191, 248, 252, 261, 265, 268, 275], "colbert": [205, 211], "cold": [278, 279, 285], "collabor": [158, 171, 172, 248, 249, 251, 254, 339], "collat": [5, 35, 221, 226], "collate_fn": [221, 226], "colleagu": [205, 211], "collect": [1, 2, 8, 9, 13, 16, 18, 51, 59, 64, 73, 75, 136, 148, 149, 178, 180, 187, 189, 191, 205, 211, 242, 246, 247, 255, 258, 259, 268, 273, 307, 334, 339], "collector": [178, 191], "collison": [205, 212], "colour": [213, 220], "column": [3, 5, 8, 9, 28, 35, 51, 54, 60, 151, 163, 178, 190, 195, 205, 210, 213, 217, 220, 235, 237, 239, 241, 242, 246, 248, 250, 252, 254, 255, 257, 258, 261, 263, 265, 267, 268, 270, 271, 275, 277], "column_nam": [255, 258], "column_stack": [255, 258], "columnar": [7, 43, 178, 195, 235, 237, 255, 257, 261, 263, 268, 270], "com": [0, 11, 72, 73, 78, 80, 83, 84, 85, 91, 93, 94, 95, 99, 100, 109, 111, 119, 122, 124, 125, 126, 146, 154, 156, 159, 160, 161, 166, 168, 173, 176, 177, 261, 263, 287, 292, 304, 305, 318, 324, 326], "combin": [2, 7, 22, 43, 178, 188, 213, 219, 220, 235, 236], "combur": [205, 211], "comcast": [205, 211], "come": [2, 21, 80, 82, 152, 154, 161, 164, 166, 205, 211, 213, 216, 219, 220, 255, 260, 261, 267, 268, 277], "comedi": [213, 220], "comfort": [268, 277], "command": [73, 76, 79, 84, 86, 87, 96, 98, 99, 101, 102, 103, 107, 109, 112, 114, 115, 120, 122, 125, 131, 134, 136, 137, 141, 142, 145, 149, 151, 154, 155, 156, 163, 166, 167, 168, 175, 242, 247], "commend": [213, 216], "comment": [156, 160, 168, 177, 229, 233], "commerci": [7, 43, 45], "commiss": [3, 8, 28, 51, 303, 310], "commit": [235, 237, 268, 270], "common": [1, 2, 3, 8, 9, 13, 25, 27, 51, 53, 59, 62, 109, 119, 122, 127, 136, 142, 145, 148, 205, 211, 213, 217, 219, 220, 295, 302], "common_prefix": [91, 95], "commonli": [7, 9, 43, 46, 58, 248, 251], "commun": [2, 7, 8, 9, 25, 47, 54, 64, 73, 78, 261, 263, 278, 279, 282, 287, 293, 294, 295, 302, 306, 331], "compact": [10, 68, 235, 237, 241, 268, 269, 270], "compani": [80, 83, 158, 170], "compar": [6, 7, 41, 46, 137, 138, 152, 164, 235, 241, 242, 247, 261, 265, 268, 275, 277, 278, 279, 281, 287, 289, 305, 306, 325, 328], "comparison": [213, 220], "compat": [142, 143, 178, 194, 268, 269, 278, 279, 284, 287, 290, 295, 298], "compet": [7, 47], "competit": [205, 211], "compil": [11, 72, 206, 214, 222, 230], "complet": [2, 3, 4, 5, 11, 23, 28, 30, 32, 34, 72, 80, 82, 84, 86, 99, 106, 109, 118, 137, 141, 146, 151, 154, 155, 159, 161, 163, 166, 167, 173, 178, 179, 189, 199, 200, 202, 204, 213, 220, 221, 228, 235, 239, 240, 241, 242, 246, 247, 268, 275, 278, 279, 281, 282, 285, 287, 288, 291, 292, 295, 298, 299, 300, 301, 302, 304, 318, 319], "complex": [3, 7, 10, 28, 43, 46, 47, 48, 68, 80, 82, 161, 235, 236, 278, 279, 282, 284, 287, 289, 294, 295, 297, 301, 307, 336], "compli": [5, 36], "complianc": [73, 76, 287, 292, 295, 302], "compliant": [73, 78], "compon": [1, 5, 6, 7, 10, 13, 36, 41, 43, 68, 73, 76, 78, 81, 108, 121, 122, 128, 137, 139, 140, 146, 152, 154, 164, 166, 178, 180, 261, 263, 268, 270, 278, 279, 283, 284, 286, 305, 325], "compos": [4, 5, 6, 9, 10, 29, 31, 32, 35, 38, 39, 41, 56, 61, 68, 69, 178, 180, 186, 196, 235, 237, 268, 270, 271, 277, 304, 305, 306, 313, 316, 322, 326, 330], "composit": [7, 48, 213, 220], "comprehens": [8, 9, 11, 51, 53, 57, 65, 72, 73, 77, 122, 123, 135, 146, 278, 279, 280, 287, 293, 294, 295, 297, 298, 299, 300, 302, 306, 329, 332], "compress": [255, 257, 261, 267], "comput": [1, 2, 3, 4, 5, 8, 9, 10, 13, 15, 16, 17, 18, 22, 25, 28, 30, 31, 32, 34, 36, 43, 45, 47, 54, 55, 57, 61, 63, 64, 66, 68, 73, 76, 78, 80, 82, 83, 84, 90, 94, 95, 98, 99, 101, 106, 109, 112, 118, 122, 125, 126, 127, 134, 135, 150, 151, 154, 155, 156, 158, 159, 160, 161, 162, 163, 166, 167, 168, 171, 172, 173, 177, 178, 179, 181, 182, 193, 205, 207, 213, 217, 221, 223, 225, 226, 227, 229, 231, 237, 248, 249, 252, 254, 255, 258, 259, 260, 273, 277, 278, 279, 281, 282, 284, 285, 303, 306, 307, 308, 309, 310, 311, 313, 315, 329, 330, 331, 334, 336, 339], "computation": [6, 40, 305, 323], "compute_accuraci": [9, 64, 306, 331], "compute_config": [99, 106, 109, 118, 137, 141, 278, 279, 285, 287, 292], "compute_metr": [221, 225], "compute_nodes_service_account_email": [91, 95, 122, 126], "compute_tip_percentag": [8, 52], "computeconfig": [99, 106, 109, 118], "con": [205, 211], "conc": [91, 97], "concat_t": [255, 258], "concept": [7, 9, 45, 56, 80, 82, 135, 142, 143, 178, 181, 280, 321], "conceptu": [4, 31], "concern": [1, 10, 13, 69, 213, 219], "concert": [205, 211], "concis": [295, 298], "conclus": [213, 215, 296], "concomit": [205, 211], "concret": [73, 74], "concurr": [2, 3, 7, 8, 10, 22, 28, 48, 55, 62, 69, 178, 182, 205, 207, 211, 213, 218, 261, 267, 268, 277, 278, 279, 282, 294, 303, 306, 307, 310, 311, 330, 335], "concurrency_limit": [9, 61], "concuss": [205, 209, 211, 212], "conda": [136, 149], "condit": [2, 5, 18, 35, 73, 78, 213, 215, 235, 241, 295, 298], "conductor": [213, 220], "confid": [178, 198, 235, 241, 242, 247, 248, 254, 255, 260], "config": [3, 4, 5, 6, 10, 28, 32, 36, 41, 71, 91, 94, 122, 125, 136, 142, 145, 149, 178, 181, 182, 183, 184, 189, 193, 199, 202, 221, 226, 235, 239, 242, 243, 246, 248, 252, 255, 258, 261, 265, 268, 270, 273, 303, 304, 305, 307, 310, 311, 315, 319, 324, 325, 326, 336], "configur": [6, 9, 10, 17, 35, 41, 57, 61, 69, 71, 73, 75, 76, 77, 78, 80, 81, 83, 84, 85, 86, 90, 93, 95, 99, 100, 101, 106, 109, 111, 112, 118, 124, 126, 127, 136, 137, 141, 149, 150, 154, 155, 156, 159, 162, 166, 167, 168, 173, 175, 176, 179, 180, 181, 183, 198, 202, 204, 221, 223, 224, 226, 227, 228, 229, 233, 235, 239, 242, 246, 248, 249, 252, 253, 256, 261, 262, 265, 268, 269, 273, 277, 284, 286, 288, 294, 297, 302, 305, 307, 326, 334], "configure_optim": [5, 35, 235, 238, 242, 245], "confirm": [156, 168, 178, 180, 189, 202, 235, 237, 239, 248, 250, 252, 255, 257, 260, 261, 263, 268, 270, 295, 298], "conflict": [178, 191], "confluent": [7, 46], "confus": [9, 61, 268, 277], "confusion_matrix": [255, 257, 259], "congratul": [287, 294, 295, 302], "congress": [205, 211], "conjur": [213, 220], "connect": [2, 4, 5, 9, 19, 30, 34, 58, 73, 78, 80, 83, 84, 90, 99, 102, 109, 114, 122, 128, 150, 151, 153, 158, 162, 163, 165, 171, 178, 179, 197, 205, 209, 213, 220, 255, 259], "connector": [8, 9, 55, 58], "consecut": [205, 209, 211], "consid": [1, 2, 4, 5, 16, 18, 20, 21, 22, 24, 32, 36, 67, 142, 143, 213, 216, 218, 219, 235, 237, 287, 294, 295, 301, 304, 314], "consider": [80, 82, 161], "consist": [6, 7, 39, 43, 135, 150, 152, 162, 164, 178, 179, 180, 181, 192, 206, 214, 222, 230, 242, 247, 255, 258, 260, 261, 263, 264, 295, 297, 299, 302, 305, 306, 322, 329], "consol": [84, 87, 88, 91, 96, 97, 99, 103, 106, 109, 115, 118, 119, 122, 131, 133, 136, 142, 143, 146, 148, 151, 155, 156, 163, 167, 168, 175, 248, 252, 295, 302], "conspicu": [205, 211], "constant": [5, 35, 213, 220, 261, 267, 278, 279, 282], "constraint": [5, 35, 205, 212, 213, 220, 278, 279, 283, 295, 301], "construct": [2, 5, 25, 35, 178, 185, 186, 242, 243, 261, 265, 268, 271, 272], "constructor": [2, 9, 10, 25, 62, 70, 205, 207, 306, 307, 330, 335], "consum": [6, 8, 9, 41, 51, 53, 58, 60, 84, 86, 99, 101, 109, 112, 178, 192, 197, 204, 261, 263, 305, 306, 325, 330], "consumptionapi": [9, 60, 64], "contain": [0, 2, 3, 4, 5, 9, 19, 28, 32, 36, 59, 80, 82, 83, 99, 107, 109, 120, 122, 125, 128, 150, 151, 153, 156, 162, 163, 165, 168, 178, 181, 190, 202, 213, 220, 248, 250, 255, 257, 258, 261, 263, 267, 268, 269, 275, 295, 300, 303, 304, 306, 310, 319, 329, 339], "container": [10, 68, 135], "containerfil": [278, 279, 285, 287, 292], "content": [9, 61, 146, 153, 161, 165, 248, 254, 268, 277, 278, 279, 285, 287, 291, 292, 295, 298, 299, 300], "context": [137, 139, 141, 235, 239, 242, 246, 248, 254, 261, 262, 268, 269, 281, 283, 287, 290, 293], "contextu": [136, 137, 140, 141, 148], "contigu": [9, 59, 248, 249, 250], "continu": [4, 7, 32, 48, 137, 141, 150, 153, 162, 165, 178, 199, 200, 201, 202, 205, 211, 213, 220, 235, 237, 242, 247, 248, 253, 268, 270, 273, 275, 276, 284, 286, 295, 301, 304, 317], "contrast": [7, 46, 47, 213, 219, 220], "control": [3, 7, 8, 28, 43, 52, 75, 76, 77, 81, 83, 107, 108, 120, 121, 137, 141, 142, 145, 152, 157, 158, 161, 164, 169, 171, 178, 182, 194, 205, 207, 211, 242, 243, 247, 248, 249, 255, 258, 268, 269, 278, 279, 283, 285, 295, 298, 307, 336], "controversi": [213, 216], "conv1": [4, 6, 31, 40, 41, 178, 181, 304, 305, 313, 319, 323, 326], "conv2": [304, 313, 319], "conv2d": [4, 6, 31, 40, 41, 178, 181, 235, 238, 304, 305, 313, 319, 323, 326], "convei": [213, 220], "conveni": [3, 4, 11, 27, 31, 72, 178, 180], "convent": [178, 179, 213, 220], "converg": [235, 239, 241, 242, 246, 248, 252, 261, 265, 268, 275], "convers": [178, 196, 295, 298, 300, 301], "convert": [2, 5, 8, 25, 35, 52, 178, 186, 191, 194, 195, 196, 205, 209, 212, 215, 221, 226, 229, 233, 235, 237, 239, 248, 249, 250, 255, 256, 258, 261, 263, 268, 270, 271, 277, 278, 279, 281, 304, 317], "convnext": [268, 277], "convolut": [178, 181, 235, 236], "cool": [213, 220], "coordin": [73, 78, 235, 236, 242, 243, 255, 256, 258, 268, 269, 277], "cop": [159, 160, 173, 176], "copi": [2, 4, 7, 18, 32, 43, 142, 144, 151, 152, 155, 156, 159, 163, 164, 167, 168, 175, 178, 179, 186, 205, 211, 248, 250, 252, 255, 258, 261, 265, 267, 268, 275, 277], "cor": [73, 78, 109, 119], "core": [3, 5, 6, 9, 10, 13, 22, 27, 35, 40, 41, 57, 68, 154, 166, 178, 180, 205, 209, 235, 236, 237, 239, 241, 242, 247, 248, 249, 252, 255, 258, 268, 269, 270, 303, 305, 309, 323, 325, 339], "corner": [151, 163, 205, 211], "correct": [4, 7, 32, 48, 137, 141, 178, 181, 182, 185, 186, 188, 193, 199, 200, 205, 211, 248, 253, 255, 256, 259, 260, 261, 263, 268, 269, 272, 273, 304, 315, 316], "correct_squar": [2, 20], "correct_square_mod": [2, 20], "correctli": [11, 72, 109, 119, 158, 170, 178, 180, 189, 191, 202, 235, 237, 248, 250, 252, 255, 257, 261, 263, 268, 270, 271], "correl": [6, 41, 305, 324], "correspond": [1, 7, 15, 48, 178, 190, 248, 251, 304, 313], "corrupt": [235, 236, 237], "cost": [1, 4, 5, 7, 13, 32, 37, 43, 45, 47, 137, 141, 152, 154, 155, 156, 164, 166, 167, 168, 242, 247, 282, 284, 285, 286, 287, 289, 294, 298, 304, 320], "costum": [205, 209, 211], "could": [2, 8, 20, 24, 54, 205, 210, 211, 213, 219, 220, 229, 231, 278, 279, 283], "couldn": [205, 211], "count": [1, 8, 9, 13, 54, 55, 64, 136, 148, 213, 220, 235, 237, 242, 244, 248, 250, 252, 255, 257, 259, 262, 306, 331], "countri": [205, 211, 213, 216, 248, 254, 295, 300], "countrymen": [213, 216], "coup": [295, 299], "coupl": [213, 220], "cours": [73, 76, 135, 136, 142, 143, 148, 149, 213, 219, 220, 295, 302, 339], "cover": [73, 76, 136, 147, 148, 150, 151, 152, 154, 161, 162, 163, 164, 166, 205, 206, 212, 213, 214, 220, 221, 222, 228, 230, 259, 260, 278, 279, 286, 287, 288, 294, 302], "cover_typ": [255, 257], "covtyp": [255, 257, 258, 260], "covtype_xgb_cpu": [255, 258], "coward": [213, 220], "cp": [9, 10, 62, 70, 306, 307, 330, 335], "cpu": [2, 3, 4, 5, 6, 8, 9, 10, 22, 23, 25, 28, 31, 32, 35, 36, 40, 41, 53, 55, 57, 59, 61, 62, 70, 71, 136, 137, 139, 148, 149, 150, 152, 154, 162, 164, 166, 178, 181, 184, 185, 186, 188, 189, 191, 192, 197, 204, 205, 207, 210, 211, 212, 213, 215, 221, 223, 226, 227, 228, 235, 241, 242, 243, 247, 248, 252, 254, 256, 257, 258, 260, 261, 265, 267, 268, 273, 277, 303, 304, 305, 306, 307, 308, 310, 311, 315, 318, 319, 324, 325, 326, 328, 329, 330, 335, 336], "cpus_per_work": [255, 258], "craft": [213, 219, 220], "crappi": [213, 216], "crash": [7, 46, 178, 201, 205, 209, 211, 212, 255, 256, 268, 276], "crazi": [205, 209, 211, 213, 220], "cream": [205, 211], "creat": [2, 3, 6, 7, 8, 9, 12, 16, 18, 25, 27, 28, 32, 41, 44, 45, 50, 52, 58, 75, 76, 77, 79, 80, 82, 83, 87, 88, 90, 92, 93, 96, 97, 98, 102, 103, 104, 106, 108, 113, 114, 115, 116, 118, 121, 124, 128, 129, 131, 132, 133, 134, 136, 137, 141, 149, 150, 151, 153, 154, 156, 157, 158, 160, 162, 163, 165, 166, 168, 169, 171, 176, 180, 188, 191, 194, 195, 202, 204, 206, 207, 209, 210, 214, 215, 222, 229, 230, 231, 232, 233, 235, 237, 249, 255, 260, 261, 262, 268, 269, 271, 272, 277, 278, 279, 282, 285, 287, 290, 291, 292, 294, 295, 298, 299, 300, 302, 303, 304, 305, 306, 307, 310, 311, 315, 317, 318, 324, 325, 329, 335, 338], "create_us": 146, "creation": [2, 18, 135], "cred": [205, 209, 211], "credenti": [84, 85, 91, 94, 99, 100, 109, 111, 122, 125, 128], "credit": [8, 51, 213, 220], "creepi": [213, 220], "creepybr": [213, 220], "creighton": [205, 211], "crime": [213, 220], "criteria": [9, 10, 57, 68, 278, 279, 281], "criterion": [4, 6, 31, 32, 40, 41, 178, 182, 193, 199, 268, 273, 305, 323, 326], "critic": [7, 48, 153, 165, 205, 211], "crop": [235, 237, 268, 269, 270], "cross": [73, 77, 78, 80, 82, 109, 119, 178, 180, 261, 263, 268, 269], "cross_attention_dim": [5, 35], "crossattndownblock2d": [5, 35], "crossattnupblock2d": [5, 35], "crossentropyloss": [4, 6, 29, 31, 32, 38, 40, 41, 178, 180, 182, 193, 199, 268, 273, 304, 305, 313, 315, 319, 323, 326], "crouch": [213, 220], "crow": [213, 219], "crucial": [213, 215, 221, 226, 295, 301], "crush": [205, 211], "cry": [213, 216], "css": 0, "csv": [4, 7, 9, 29, 31, 43, 59, 153, 165, 178, 180, 192, 248, 250, 254, 255, 260, 261, 262, 263, 268, 275, 277, 304, 313], "csv_path": [261, 263], "ctor": [268, 277], "ctrl": [11, 72], "cu128": [137, 141, 278, 279, 285, 287, 292], "cub": [205, 211], "cuda": [4, 6, 9, 31, 32, 40, 62, 178, 182, 184, 185, 188, 189, 191, 193, 205, 210, 211, 221, 226, 235, 241, 242, 247, 261, 267, 268, 277, 278, 279, 284, 304, 305, 306, 307, 313, 316, 323, 326, 330, 335], "cultur": [213, 216], "cumprod": [5, 35], "cumul": [248, 254], "cup": [213, 220], "cure": [213, 220], "curiou": [178, 204, 213, 216], "curl": [11, 72, 146], "current": [2, 4, 7, 8, 22, 31, 47, 54, 99, 102, 109, 114, 122, 125, 142, 144, 156, 160, 168, 177, 178, 187, 188, 194, 200, 221, 228, 235, 239, 242, 243, 246, 248, 250, 295, 300, 303, 304, 305, 307, 310, 311, 318, 324, 326, 336], "current_training_step": [5, 35], "curti": [205, 209, 211, 212], "curv": [178, 180, 190, 238, 261, 265, 269, 270], "custom": [2, 3, 5, 10, 21, 22, 27, 35, 61, 68, 71, 76, 77, 80, 82, 83, 84, 85, 91, 92, 95, 99, 100, 109, 110, 122, 123, 137, 139, 142, 145, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 171, 177, 178, 181, 213, 215, 216, 220, 235, 236, 248, 249, 252, 255, 256, 260, 269, 277, 278, 279, 285, 287, 292, 294, 295, 298, 302, 303, 309], "custom_hid": 0, "custom_light": 0, "custom_nam": [295, 298], "customer_ingress_cidr_rang": [73, 78], "cut": [4, 5, 32, 37, 261, 263, 304, 320], "cv": [295, 298], "cv_job_match": [295, 298], "cybersecur": [295, 298], "d": [8, 9, 51, 52, 54, 59, 60, 61, 137, 141, 142, 144, 146, 205, 209, 211, 212, 213, 219, 220, 235, 237, 239, 242, 244, 246, 247, 248, 249, 254, 255, 258, 259, 261, 263, 267, 268, 270, 277, 306, 307, 329, 330, 335, 336], "d3a9a7d0": [91, 95, 122, 126], "d89d0_00000": [304, 319], "d_": [4, 31, 304, 313], "d_model": [261, 264, 265, 267], "da": [205, 211], "dag": [7, 45, 48], "dai": [205, 211, 213, 220, 295, 300], "daili": [255, 260], "dalla": [205, 211], "damn": [205, 211], "dancer": [213, 220], "daniel": [205, 211, 213, 219], "dark": [146, 213, 219, 220], "darwin": [205, 211], "dash": [213, 220], "dashboard": [4, 5, 7, 11, 30, 34, 44, 46, 72, 80, 83, 136, 137, 139, 140, 141, 143, 145, 148, 149, 151, 156, 160, 163, 168, 176, 178, 179, 204, 242, 247, 287, 293, 294, 307, 336], "dask": [7, 46, 47], "data": [1, 10, 13, 17, 18, 31, 33, 35, 37, 38, 41, 42, 45, 46, 60, 62, 68, 69, 73, 77, 78, 99, 107, 109, 120, 136, 137, 141, 143, 146, 148, 151, 152, 153, 161, 163, 164, 165, 180, 182, 184, 186, 199, 203, 209, 210, 212, 216, 217, 220, 221, 223, 224, 226, 228, 236, 239, 241, 242, 243, 244, 246, 247, 249, 252, 254, 256, 257, 258, 260, 262, 263, 265, 269, 270, 271, 279, 285, 295, 299, 300, 301, 302, 303, 307, 310, 311, 315, 316, 318, 320, 321, 324, 326, 335, 336], "data_dir": [261, 263, 265, 267], "data_load": [4, 5, 6, 31, 32, 35, 39, 40, 41, 178, 182, 193, 194, 199, 304, 305, 313, 315, 319, 322, 323, 326], "data_path": [8, 51, 54], "data_url": [248, 250], "databas": [9, 58, 146, 295, 297, 300], "databaseservic": [142, 145, 146], "databrick": [7, 9, 43, 59], "datadog": [136, 148], "datafram": [3, 7, 8, 28, 46, 52, 179, 180, 195, 215, 235, 237, 239, 248, 250, 252, 254, 255, 259, 260, 261, 263, 268, 277, 303, 310, 311], "dataload": [6, 32, 33, 36, 38, 39, 41, 180, 182, 188, 192, 193, 196, 224, 235, 237, 242, 244, 265, 269, 270, 273, 304, 305, 313, 316, 322, 326], "dataset": [3, 5, 6, 7, 9, 28, 29, 33, 35, 36, 38, 39, 40, 41, 43, 44, 45, 50, 52, 54, 55, 57, 58, 59, 60, 61, 63, 64, 65, 142, 144, 153, 165, 179, 181, 183, 186, 188, 191, 192, 193, 194, 196, 197, 200, 202, 203, 204, 207, 208, 210, 211, 215, 220, 221, 223, 224, 226, 227, 228, 235, 236, 237, 239, 241, 246, 247, 249, 252, 254, 256, 258, 260, 262, 265, 267, 268, 269, 270, 271, 272, 277, 305, 307, 311, 312, 313, 314, 318, 319, 322, 323, 324, 326, 328, 330, 331, 332, 335], "dataset_": [142, 144], "dataset_iter": [178, 194], "dataset_uri": [248, 250], "datasourc": [8, 9, 53, 59, 306, 329], "date": [205, 211, 295, 300, 304, 319], "datetim": [4, 29, 31, 178, 180, 261, 263, 304, 313], "david": [146, 205, 209, 211], "dawson": [213, 219], "day_of_week": [303, 310], "db": 146, "ddim": [235, 241], "ddp": [4, 5, 32, 36, 180, 181, 184, 185, 188, 189, 200, 204, 235, 239, 248, 250, 251, 254, 261, 262, 265, 267, 268, 273, 277], "ddpmschedul": [5, 33, 35], "ddpstrategi": [5, 35], "de": [2, 18, 205, 211, 238, 241, 242, 243, 247, 261, 267], "deactiv": [11, 72], "dead": [213, 220], "deadlock": [2, 23], "deal": [7, 48], "dear": [205, 211], "debat": [205, 211], "debug": [7, 10, 46, 71, 136, 137, 139, 141, 142, 143, 145, 148, 149, 151, 157, 161, 163, 169, 178, 180, 187, 191, 235, 236, 295, 302], "debut": [205, 211], "decemb": [205, 211], "decid": [1, 6, 13, 41, 178, 181, 189, 205, 211, 213, 220, 242, 243, 261, 265, 295, 300, 305, 306, 325, 330], "decis": [80, 82, 255, 256, 257], "declar": [178, 184, 235, 236, 242, 243, 248, 249, 255, 256, 268, 269], "decod": [236, 241, 261, 262, 263, 264, 265, 267, 286], "decode_and_norm": [235, 237], "decoder_input": [261, 264, 265], "decor": [1, 2, 9, 10, 14, 22, 25, 60, 64, 70, 229, 233, 307, 335, 336], "decoupl": [10, 71, 178, 204], "decreas": [178, 190, 261, 265, 268, 275], "dedic": [6, 41, 136, 149, 155, 156, 159, 160, 167, 168, 173, 176, 178, 191, 213, 220, 287, 292, 295, 301, 305, 326], "dedupl": [221, 228, 305, 326], "deep": [8, 51, 178, 180, 205, 207, 213, 220, 221, 223, 224, 227, 228, 248, 250, 254, 268, 270, 287, 293], "deeper": [7, 44, 278, 279, 286, 295, 297, 302, 321], "deepli": [137, 141], "deepseek": [295, 301], "deepspe": [4, 5, 32, 37, 178, 204, 304, 320], "def": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 16, 18, 19, 20, 21, 22, 23, 24, 25, 28, 31, 32, 35, 36, 39, 40, 41, 52, 61, 62, 64, 70, 72, 137, 141, 142, 144, 145, 151, 154, 155, 159, 163, 166, 167, 174, 178, 181, 182, 185, 186, 187, 188, 191, 193, 194, 196, 199, 200, 205, 210, 213, 220, 221, 225, 226, 227, 229, 233, 235, 237, 238, 239, 241, 242, 244, 245, 246, 247, 248, 250, 251, 252, 255, 258, 259, 261, 263, 264, 265, 267, 268, 271, 272, 273, 277, 295, 300, 303, 304, 305, 306, 307, 310, 311, 313, 315, 316, 317, 319, 322, 323, 324, 325, 326, 330, 331, 335, 336], "default": [0, 2, 4, 5, 6, 8, 9, 10, 20, 22, 31, 35, 41, 51, 52, 59, 61, 71, 84, 86, 91, 94, 95, 99, 101, 109, 112, 122, 125, 137, 141, 142, 145, 150, 152, 153, 154, 156, 157, 162, 164, 165, 166, 168, 169, 178, 181, 199, 213, 218, 221, 228, 235, 237, 239, 255, 257, 261, 262, 268, 277, 295, 300, 303, 310, 311, 326], "default_cluster_storag": [142, 144], "default_data_col": [221, 226], "default_root_dir": [5, 35, 235, 239, 242, 246], "default_tracing_servic": 146, "defens": [205, 211, 261, 265], "defin": [1, 2, 3, 4, 6, 9, 10, 16, 20, 22, 25, 28, 31, 32, 41, 61, 70, 71, 75, 99, 106, 109, 118, 122, 125, 137, 139, 150, 152, 155, 156, 159, 160, 162, 164, 167, 168, 175, 177, 179, 184, 185, 186, 188, 191, 192, 197, 205, 207, 211, 212, 213, 220, 221, 223, 226, 227, 229, 233, 235, 237, 239, 250, 256, 259, 260, 261, 262, 263, 267, 268, 271, 272, 273, 277, 278, 279, 282, 287, 290, 295, 299, 301, 303, 305, 310, 311, 324, 326], "definit": [152, 164, 242, 244, 295, 299, 300], "degre": [8, 9, 54, 64, 306, 331], "del": [178, 191], "delai": [213, 219, 307, 336], "deleg": [3, 28], "delet": [4, 31, 73, 78, 84, 86, 89, 91, 98, 99, 101, 107, 109, 112, 120, 122, 134, 153, 165, 178, 180, 203, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277], "delete_object": [153, 165], "deleteobject": [73, 78], "delhi": [205, 211], "deliv": [137, 141, 213, 219], "deloy": [156, 168], "delta": [7, 43, 255, 260, 278, 279, 285, 287, 291, 292, 295, 298], "demand": [11, 72, 80, 81, 83, 150, 152, 156, 160, 162, 164, 168, 176, 235, 241, 263, 307, 334], "demo": [73, 76, 178, 182, 235, 241, 242, 247, 248, 254], "demonstr": [5, 33, 76, 137, 141, 142, 143, 144, 146, 153, 165, 178, 191, 205, 212, 213, 217, 219, 220, 221, 223, 228, 235, 237, 240, 242, 246, 248, 250, 253, 254, 261, 266, 269, 270, 287, 288, 295, 297], "denizen": [213, 216], "denni": [205, 211], "deped": [152, 164], "depend": [0, 7, 9, 17, 23, 45, 46, 47, 61, 73, 78, 84, 86, 99, 101, 109, 112, 129, 146, 150, 151, 152, 161, 162, 163, 164, 178, 192, 206, 214, 221, 222, 223, 230, 235, 237, 242, 244, 248, 250, 255, 257, 261, 262, 263, 268, 270, 278, 279, 281, 283], "depict": [213, 220], "deploi": [3, 10, 28, 67, 71, 73, 75, 76, 77, 78, 86, 95, 101, 112, 135, 142, 145, 146, 152, 156, 164, 168, 207, 231, 234, 242, 247, 255, 260, 261, 267, 278, 279, 280, 281, 283, 284, 285, 286, 289, 291, 294, 296, 297, 299, 300, 302, 303, 307, 310, 333, 334, 339], "deploy": [0, 3, 7, 28, 48, 70, 71, 74, 75, 78, 79, 82, 84, 86, 91, 92, 94, 95, 99, 101, 103, 104, 109, 112, 115, 116, 119, 122, 125, 126, 131, 132, 137, 140, 142, 145, 156, 157, 160, 168, 169, 176, 177, 231, 248, 254, 261, 267, 268, 277, 283, 284, 286, 288, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 310, 311, 335, 337], "deployment_config": [278, 279, 285, 287, 290, 293, 295, 300], "deploymenthandl": [229, 233], "deploymentrespons": [307, 336], "depress": [213, 216], "depth": [304, 317], "deriv": [248, 252], "derp": [84, 86, 99, 101, 109, 112], "desc": [4, 31, 268, 270], "descent": [213, 220], "describ": [84, 86, 99, 101, 109, 112, 113, 213, 220, 242, 243], "descript": [8, 11, 51, 72, 73, 77, 78, 242, 244, 278, 279, 283, 298, 300], "deseri": [1, 7, 13, 46], "design": [1, 7, 8, 9, 11, 13, 43, 46, 47, 50, 53, 60, 72, 135, 213, 220, 221, 223, 227, 228, 229, 231, 255, 258, 261, 262, 268, 273, 278, 279, 282, 295, 298, 302, 307, 334, 336], "desir": [268, 277, 304, 314], "desktop": [151, 163], "despit": [213, 220], "destin": [295, 298], "destroi": [84, 86, 89, 91, 98, 99, 107, 109, 120, 122, 134, 205, 211, 213, 220], "destruct": [213, 220], "detach": [178, 191, 261, 267], "detail": [2, 4, 5, 6, 8, 9, 10, 22, 30, 32, 34, 36, 39, 54, 57, 68, 71, 73, 76, 78, 80, 81, 91, 95, 99, 101, 109, 112, 122, 126, 144, 145, 146, 153, 154, 158, 165, 166, 171, 178, 179, 184, 188, 192, 193, 206, 213, 214, 220, 222, 229, 230, 233, 287, 293, 295, 298, 304, 305, 307, 315, 318, 324, 335, 339], "detailsbr": [213, 220], "detect": [7, 46, 137, 141, 178, 202, 205, 210, 235, 240, 268, 275, 295, 298], "determin": [9, 60, 213, 218, 278, 279, 281], "determinist": [255, 257], "determint": [9, 64], "dev": [84, 86, 99, 101, 109, 112, 242, 243], "deval": [255, 258], "develop": [1, 3, 7, 8, 13, 27, 46, 47, 48, 54, 67, 68, 84, 85, 86, 91, 93, 99, 100, 101, 109, 111, 112, 122, 124, 136, 142, 145, 148, 150, 153, 154, 155, 156, 158, 159, 162, 165, 166, 167, 168, 171, 173, 205, 211, 221, 227, 268, 269, 287, 288, 303, 307, 309, 310, 311, 334, 337], "deviat": [268, 271], "devic": [1, 4, 5, 6, 9, 10, 13, 31, 32, 35, 36, 40, 41, 62, 70, 71, 80, 83, 107, 108, 120, 121, 178, 181, 182, 184, 185, 186, 188, 189, 191, 193, 194, 205, 207, 211, 221, 223, 226, 235, 238, 239, 241, 242, 243, 246, 247, 261, 265, 267, 268, 269, 272, 273, 277, 278, 279, 285, 304, 313, 315, 316, 319], "devop": [135, 161], "df": [3, 5, 8, 28, 35, 51, 52, 53, 54, 178, 195, 235, 239, 242, 246, 248, 250, 252, 254, 255, 257, 259, 260, 261, 263, 265, 268, 271, 275], "di": [2, 20, 213, 219], "diagnos": [136, 148], "diagnost": [261, 265], "diagon": [255, 259], "diagram": [3, 5, 6, 7, 28, 35, 36, 41, 46, 48, 73, 77, 80, 82, 161, 178, 179, 188, 221, 223, 278, 279, 284, 304, 305, 313, 314, 325], "diari": [205, 211], "dict": [2, 3, 4, 5, 6, 9, 10, 19, 28, 31, 32, 35, 36, 41, 61, 62, 64, 70, 137, 141, 178, 182, 188, 193, 194, 196, 197, 199, 200, 204, 205, 208, 210, 221, 224, 226, 235, 237, 242, 244, 261, 267, 278, 279, 285, 287, 290, 293, 295, 298, 300, 303, 304, 305, 306, 307, 310, 311, 313, 315, 317, 324, 330, 331, 335, 336], "dictat": [213, 220], "dictionari": [2, 3, 6, 22, 28, 41, 178, 183, 187, 193, 194, 221, 226, 248, 252, 305, 324, 326], "did": [2, 20, 205, 211, 213, 219, 220, 235, 239, 241, 242, 246, 247], "didn": [205, 211], "diego": [205, 211], "differ": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 16, 28, 30, 34, 41, 45, 54, 61, 64, 69, 72, 73, 74, 76, 78, 80, 82, 122, 127, 135, 136, 137, 140, 142, 143, 145, 149, 151, 153, 163, 165, 178, 179, 182, 184, 192, 193, 194, 205, 207, 211, 213, 216, 221, 226, 227, 235, 241, 242, 247, 255, 258, 261, 265, 268, 269, 277, 278, 279, 281, 282, 283, 289, 295, 297, 298, 303, 305, 306, 310, 311, 324, 325, 331], "differenti": [255, 256], "difficult": [7, 46, 137, 141], "diffus": [4, 32, 33, 36, 37, 239, 244, 246, 304, 320], "diffusionpolici": [246, 247], "digit": [6, 10, 39, 40, 70, 181, 195, 305, 322, 323], "dii": [278, 279, 283], "dilat": [304, 313, 319], "dim": [178, 191, 235, 238, 242, 243, 245, 248, 251, 268, 273, 277], "dimens": [178, 191, 248, 249, 252, 261, 263], "dimension": [255, 256, 257], "dinger": [205, 211], "dir": [152, 153, 154, 164, 165, 166, 178, 180, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 273, 277], "direct": [6, 41, 73, 78, 80, 82, 213, 220, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277, 295, 301, 305, 324], "directli": [2, 7, 18, 19, 43, 73, 78, 80, 82, 83, 151, 154, 158, 163, 166, 171, 178, 192, 235, 236, 239, 242, 243, 247, 248, 249, 250, 252, 255, 257, 258, 261, 262, 268, 269, 275, 295, 299], "director": [213, 219, 220], "directori": [0, 4, 8, 31, 53, 99, 102, 109, 114, 146, 151, 153, 156, 160, 163, 165, 168, 177, 178, 180, 188, 191, 203, 235, 241, 248, 250, 254, 255, 256, 260, 261, 263, 267, 268, 273, 277], "dirpath": [235, 239, 242, 246], "disabl": [5, 36, 178, 191, 221, 228, 304, 305, 307, 318, 324, 326, 335], "disaggreg": [9, 66], "disappoint": [205, 209, 211], "discern": [213, 216], "disconnect": [248, 252], "discontinu": [242, 243], "discount": [248, 254], "discov": [213, 220], "discret": [7, 46], "discuss": [6, 40, 41, 303, 305, 308, 325], "disengag": [8, 51], "disjoint": [178, 182, 213, 219, 220], "disk": [4, 9, 31, 57, 63, 136, 137, 139, 142, 144, 145, 148, 153, 154, 165, 166, 178, 180, 195, 200, 235, 241, 248, 254, 255, 260, 261, 267, 268, 277, 306, 330], "dismiss": [205, 211], "displai": [4, 11, 31, 72, 73, 78, 142, 145, 155, 167, 178, 180, 190, 191, 202, 213, 216, 220, 239, 268, 275, 287, 290, 304, 313], "disrupt": [213, 219], "dist": [178, 197], "dist_val_acc": [268, 273], "distanc": [3, 6, 8, 28, 41, 51, 54, 235, 241, 255, 260, 303, 305, 310, 324], "distil": [261, 267, 278, 279, 282], "distilbert": [229, 233], "distinct": [80, 82, 137, 140, 178, 181, 248, 250, 278, 279, 281], "distract": [213, 220], "distribut": [1, 2, 6, 8, 9, 11, 13, 17, 18, 23, 26, 27, 29, 30, 33, 34, 40, 43, 47, 48, 49, 50, 51, 53, 55, 56, 57, 59, 60, 63, 72, 73, 78, 146, 152, 154, 161, 164, 166, 180, 181, 182, 184, 186, 189, 190, 192, 195, 196, 197, 204, 205, 212, 213, 215, 217, 220, 224, 226, 227, 228, 229, 231, 237, 241, 243, 244, 247, 250, 254, 257, 260, 263, 265, 270, 273, 277, 278, 279, 283, 284, 287, 293, 294, 303, 305, 306, 308, 309, 312, 315, 319, 323, 327, 328, 330], "distributeddataparallel": [4, 5, 32, 36, 178, 179, 182, 185, 188, 189, 248, 251, 304, 316, 317], "distributedsampl": [4, 32, 178, 182, 186, 188, 268, 272, 273, 304, 316], "div_term": [261, 264], "dive": [161, 261, 263, 278, 279, 286, 295, 296, 297, 302, 321], "divers": [7, 43, 46], "divid": [7, 46, 73, 78], "dl_dw": [6, 41, 305, 325], "dmatrix": [3, 28, 255, 257, 258, 259, 303, 310, 311], "dn": [80, 82], "do": [2, 3, 6, 8, 19, 22, 26, 40, 52, 53, 54, 91, 93, 122, 124, 151, 153, 163, 165, 205, 207, 211, 213, 216, 219, 220, 295, 298, 300, 303, 304, 305, 308, 317, 323], "doc": [4, 5, 8, 9, 10, 11, 32, 36, 51, 53, 54, 64, 65, 71, 72, 73, 78, 84, 85, 91, 93, 99, 100, 109, 111, 122, 124, 142, 145, 154, 158, 159, 160, 166, 171, 173, 176, 178, 184, 190, 205, 209, 221, 228, 229, 231, 233, 278, 279, 285, 295, 298, 304, 305, 306, 307, 315, 317, 319, 326, 329, 331, 332, 336], "docker": [80, 82], "dockerfil": [278, 279, 285, 287, 292], "document": [6, 7, 41, 43, 47, 48, 73, 78, 80, 81, 142, 145, 151, 155, 163, 167, 278, 279, 282, 286, 287, 294, 295, 299, 301, 302, 305, 307, 325, 336], "documentari": [213, 216], "doe": [2, 3, 5, 6, 7, 8, 9, 22, 23, 28, 35, 41, 47, 51, 61, 80, 83, 136, 137, 141, 149, 154, 155, 159, 166, 167, 173, 178, 196, 205, 211, 212, 213, 216, 220, 235, 241, 242, 247, 248, 254, 278, 279, 282, 283, 305, 325], "doesn": [2, 4, 8, 9, 19, 31, 54, 64, 80, 82, 153, 165, 178, 180, 205, 211, 213, 216, 219, 220, 261, 263, 278, 279, 283, 306, 331], "doesnt": [213, 220], "dog": [268, 269], "dogma": [213, 220], "dolocationid": [8, 51], "domain": [255, 259, 260], "domin": [7, 46, 255, 259], "don": [1, 2, 4, 8, 9, 11, 16, 21, 32, 52, 61, 72, 161, 178, 179, 180, 184, 205, 211, 213, 216, 220, 235, 237, 241, 248, 254, 268, 269, 272, 306, 330], "donald": [205, 211], "done": [4, 8, 32, 54, 84, 89, 91, 98, 122, 134, 142, 144, 151, 154, 155, 156, 163, 166, 167, 168, 178, 182, 191, 213, 219, 220, 229, 234, 248, 250, 255, 256, 278, 279, 285, 287, 291, 292, 304, 319], "dont": [213, 220], "dool": [205, 211], "dorset": [205, 211], "dot": [2, 22, 205, 209, 211, 235, 236, 242, 243, 248, 249, 251, 254, 255, 256, 268, 269], "dot_product": [248, 251], "doubl": [205, 209, 211, 213, 216], "down": [4, 5, 32, 37, 80, 82, 83, 109, 119, 155, 159, 167, 173, 178, 188, 205, 207, 211, 213, 215, 220, 221, 223, 242, 243, 278, 279, 283, 293, 304, 307, 320, 334, 336], "down_block_typ": [5, 35], "downblock2d": [5, 35], "download": [4, 6, 11, 31, 32, 39, 41, 72, 122, 125, 151, 153, 154, 156, 161, 163, 165, 166, 168, 186, 203, 229, 232, 248, 250, 261, 262, 263, 295, 298, 304, 305, 306, 307, 313, 316, 318, 322, 324, 326, 330, 335], "downsampl": [304, 313, 319], "downsample_pad": [5, 35], "downscal": [229, 234, 307, 336], "downscale_delay_": [307, 336], "downstream": [9, 63, 213, 215, 248, 254, 295, 297, 299, 306, 330], "downtim": [142, 145, 156, 160, 168, 176], "draft": [205, 209, 211, 212], "drag": [205, 211], "dragon": [213, 220], "drama": [213, 216], "draw": [213, 220], "dread": [205, 211], "dream": [213, 219, 220], "dreambr": [213, 220], "dreamnightmar": [213, 220], "dress": [205, 211], "drill": [7, 44], "drive": [213, 219], "driver": [2, 9, 19, 23, 57, 62, 80, 83, 137, 139, 142, 145, 205, 211, 248, 250, 255, 259, 261, 267, 268, 277], "driver_artifact": [221, 228, 303, 304, 310, 311, 318], "drop": [178, 186, 191, 205, 211, 235, 237, 255, 258, 261, 263, 268, 277, 278, 279, 284], "drop_column": [235, 237, 303, 310, 311], "drop_last": [4, 6, 31, 32, 39, 41, 178, 186, 261, 263, 304, 305, 313, 316, 322, 326], "dropdown": [142, 144, 151, 163, 287, 293], "dropna": [248, 252, 261, 265, 268, 275], "dropout": [261, 264], "ds_adjust": [8, 52, 53], "ds_block_based_shuffl": [8, 54], "ds_file_shuffl": [8, 54], "ds_iter": [255, 258], "ds_label": [9, 61], "ds_limit": [8, 53], "ds_meta": [213, 217, 219], "ds_normal": [9, 61, 62, 306, 330], "ds_pred": [9, 62, 63, 64, 65, 306, 330, 331, 332], "ds_randomized_block": [9, 64, 306, 331], "ds_randomized_row": [9, 64, 306, 331], "ds_review": [213, 217, 218], "ds_row_based_shuffl": [8, 54], "ds_tip": [8, 52], "ds_tmp": [268, 277], "dsl": [7, 45], "dtest": [3, 28], "dtrain": [3, 28, 255, 258], "dtype": [5, 35, 137, 141, 178, 196, 205, 211, 212, 235, 237, 242, 244, 247, 248, 252, 261, 263, 264, 303, 307, 310, 311, 336], "due": [2, 4, 5, 7, 9, 10, 20, 30, 34, 46, 57, 61, 68, 137, 141, 178, 179, 205, 209, 213, 219, 220, 229, 234, 278, 279, 282, 287, 293], "dummi": [278, 279, 285, 295, 300], "dummy_data_1000_500": [153, 165], "dummy_data_1000_720": [153, 165], "dummy_data_xxl": [153, 165], "dummy_kei": [279, 285], "dump": [10, 70, 142, 145, 295, 300, 307, 335, 336], "duplic": [178, 179, 188, 200, 261, 263, 268, 269], "durabl": [7, 43], "durat": [142, 145, 146, 154, 166], "dure": [0, 2, 4, 5, 22, 32, 36, 84, 86, 99, 101, 109, 112, 142, 145, 153, 165, 178, 179, 187, 190, 197, 205, 212, 213, 216, 220, 221, 223, 226, 235, 236, 237, 239, 248, 252, 254, 255, 257, 258, 261, 262, 264, 267, 268, 270, 275, 278, 279, 282, 283, 304, 319], "dustin": [205, 211], "dvd": [213, 219], "dynam": [7, 8, 10, 48, 54, 68, 73, 78, 80, 81, 261, 262, 295, 298, 307, 334], "dynamic_lora_loading_path": [295, 298], "e": [0, 1, 2, 4, 6, 7, 8, 9, 10, 13, 19, 20, 22, 32, 41, 43, 46, 52, 53, 57, 59, 61, 68, 69, 71, 73, 78, 80, 82, 83, 84, 85, 86, 91, 92, 99, 100, 101, 105, 106, 107, 109, 110, 112, 117, 118, 120, 122, 123, 136, 137, 139, 142, 144, 148, 151, 153, 154, 156, 157, 160, 163, 165, 166, 168, 169, 176, 177, 178, 182, 183, 188, 189, 190, 192, 194, 198, 201, 204, 205, 207, 235, 236, 242, 243, 248, 249, 252, 255, 258, 261, 267, 268, 270, 278, 279, 282, 287, 293, 303, 305, 306, 307, 310, 311, 325, 329, 330, 336], "each": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 16, 18, 28, 31, 32, 35, 36, 39, 41, 48, 51, 52, 53, 59, 61, 62, 69, 72, 73, 76, 78, 99, 102, 109, 114, 135, 137, 139, 140, 141, 142, 144, 145, 146, 152, 153, 154, 156, 158, 160, 164, 165, 166, 168, 171, 176, 178, 179, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 200, 205, 207, 209, 210, 211, 213, 217, 220, 221, 223, 226, 227, 229, 231, 233, 235, 236, 237, 242, 243, 246, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 259, 261, 262, 263, 265, 267, 268, 269, 270, 271, 273, 275, 277, 278, 279, 281, 282, 287, 290, 293, 295, 297, 298, 302, 303, 304, 305, 306, 307, 308, 315, 322, 324, 325, 326, 329, 330, 334, 336], "earli": [6, 41, 205, 211, 235, 241, 255, 260, 261, 263, 267, 268, 277, 305, 325], "earlier": [151, 152, 154, 163, 164, 166, 213, 219, 235, 237, 268, 276], "early_stopping_round": [255, 260], "earn": [205, 211], "earth": [213, 219], "eas": [7, 45], "easi": [0, 7, 8, 48, 50, 137, 141, 142, 145, 161, 178, 190, 192, 221, 226, 227, 229, 231, 235, 236, 278, 279, 283, 287, 293, 295, 299, 300, 302, 307, 334], "easier": [5, 7, 35, 48, 80, 82, 213, 220, 221, 223, 248, 254], "easili": [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 17, 26, 29, 33, 38, 49, 56, 67, 137, 141, 156, 160, 168, 176, 205, 207, 213, 215, 221, 223, 227, 229, 231, 242, 243, 278, 279, 280, 284, 287, 288, 295, 296], "eastern": [213, 219, 220], "eat": [213, 219], "ec2": [74, 76, 78, 79, 80, 82, 83, 86, 87, 90, 99, 101, 109, 112, 154, 166, 339], "echo": [122, 125, 205, 211], "eclips": [205, 211], "ecolog": [255, 260], "ecosystem": [3, 4, 5, 7, 27, 30, 34, 46, 47, 178, 179, 303, 309, 310, 311], "ed": [205, 211], "eddi": [205, 211, 213, 219, 220], "edg": [205, 211, 261, 267], "edgecolor": [248, 250], "edit": [151, 152, 163, 164, 205, 211], "editor": [91, 93, 122, 124, 151, 152, 155, 163, 164, 167], "educ": [268, 269], "ef": [77, 79, 80, 82, 84, 86, 90, 109, 113, 306, 307, 330, 335], "effect": [2, 3, 19, 28, 73, 78, 153, 165, 178, 183, 213, 219, 220, 248, 251, 252, 261, 267, 287, 289, 295, 298], "effici": [7, 9, 10, 11, 43, 46, 47, 57, 68, 69, 72, 152, 158, 164, 172, 178, 179, 191, 192, 195, 204, 205, 207, 212, 213, 215, 220, 221, 223, 226, 227, 228, 235, 236, 237, 248, 249, 250, 252, 254, 261, 262, 263, 267, 268, 269, 271, 277, 278, 279, 281, 287, 289, 295, 298], "efs_id": [73, 79, 84, 86], "egress": [80, 82], "eid": [205, 211], "eight": [242, 246, 268, 274], "eip": [84, 86, 99, 101, 109, 112], "eipalloc": [84, 86, 99, 101, 109, 112], "either": [8, 9, 52, 58, 142, 145, 155, 159, 167, 175, 178, 191, 205, 211, 213, 220, 248, 250, 278, 279, 283, 306, 330], "eject": [205, 211], "ek": [73, 74, 76, 78, 80, 83, 101, 102, 103, 108, 111, 112, 114, 115, 121, 339], "eks_cluster_nam": [99, 101, 102, 109, 112, 113, 114], "elam": [213, 219], "elaps": [303, 310], "elast": [73, 78, 84, 86, 99, 101, 109, 112], "element": [213, 220, 307, 334], "elev": [255, 256, 259], "elif": [221, 226, 235, 241, 261, 263], "elimin": [7, 43, 255, 257, 278, 279, 282], "ellipsi": [157, 169], "els": [4, 6, 9, 31, 32, 40, 62, 156, 168, 178, 191, 203, 205, 211, 221, 226, 229, 233, 235, 241, 242, 247, 248, 250, 252, 254, 255, 258, 261, 263, 265, 267, 268, 273, 277, 295, 300], "elt": [7, 43], "email": [91, 95, 122, 126, 142, 145, 146, 158, 159, 171, 173], "emb": [205, 210, 211, 248, 249, 254], "embed": [0, 155, 159, 167, 173, 205, 206, 207, 210, 211, 212, 214, 222, 230, 235, 241, 242, 247, 250, 251, 254], "embedd": 0, "embedding_dim": [178, 183, 248, 251, 252, 254], "emit": [235, 239, 278, 279, 282], "emmanuel": [205, 211], "emotion": [213, 219, 220], "emploi": [7, 43], "empti": [84, 86, 89, 91, 98, 99, 101, 107, 109, 120, 122, 134, 156, 160, 168, 177], "emption": [261, 262], "en": [11, 72, 205, 209, 213, 220, 221, 228, 229, 231, 233, 278, 279, 285, 305, 326], "enabl": [1, 2, 3, 7, 8, 9, 10, 13, 17, 22, 27, 43, 46, 48, 53, 55, 57, 69, 73, 77, 78, 80, 81, 83, 93, 124, 134, 136, 137, 141, 142, 145, 146, 149, 152, 153, 154, 156, 160, 161, 164, 165, 166, 168, 176, 192, 198, 200, 201, 204, 205, 207, 212, 213, 215, 217, 220, 221, 227, 235, 236, 237, 241, 242, 243, 247, 248, 249, 252, 255, 256, 261, 262, 263, 267, 268, 269, 274, 277, 294, 295, 297, 298, 300, 302, 303, 309, 310, 311], "enable_access_log": [142, 145], "enable_auto_tool_choic": [295, 300], "enable_checkpoint": [5, 36], "enable_filestor": [91, 95], "enable_lora": [295, 298], "enable_progress_bar": [235, 239, 242, 246], "encapsul": [178, 189, 268, 269], "encod": [142, 145, 205, 210, 211, 213, 215, 241, 242, 243, 249, 254, 261, 263, 264, 278, 279, 281], "encode_batch": [248, 250], "encount": [2, 4, 5, 20, 29, 33, 122, 127, 229, 234], "encourag": [235, 236, 248, 249], "end": [2, 7, 22, 26, 27, 46, 47, 80, 82, 135, 142, 145, 154, 166, 197, 202, 204, 205, 211, 213, 219, 220, 229, 234, 235, 236, 241, 242, 243, 247, 248, 254, 255, 256, 260, 261, 262, 263, 267, 268, 269, 277, 278, 279, 281, 282, 283, 285, 287, 291, 292, 294, 295, 298, 301, 307, 308, 309, 311, 334], "endpoint": [10, 70, 80, 82, 156, 168, 229, 233, 235, 241, 248, 254, 255, 260, 278, 279, 283, 285, 287, 291, 292, 294, 307, 335, 336], "enforc": [0, 2, 7, 9, 22, 43, 61, 152, 164, 295, 299], "engag": [8, 51, 295, 298], "engin": [1, 4, 5, 8, 9, 10, 13, 30, 34, 43, 45, 46, 47, 55, 57, 68, 73, 76, 80, 83, 95, 125, 126, 134, 135, 142, 143, 157, 169, 178, 179, 221, 223, 228, 255, 259, 260, 261, 262, 263, 285, 287, 293, 295, 298, 307, 337], "engine_arg": [278, 279, 285], "engine_kwarg": [278, 279, 285, 287, 290, 293, 295, 298, 299, 300], "english": [229, 233], "enhanc": [7, 43, 47, 136, 137, 140, 141, 148, 295, 297, 300], "enjoi": [205, 211, 213, 220], "enough": [2, 9, 25, 64, 152, 164, 178, 188, 235, 237, 248, 250, 268, 270, 306, 331], "ensembl": [26, 255, 256], "ensu": [213, 220], "ensur": [2, 4, 5, 7, 8, 11, 22, 25, 30, 32, 34, 43, 53, 72, 84, 85, 87, 91, 93, 96, 99, 100, 103, 109, 111, 115, 119, 122, 124, 131, 136, 142, 143, 149, 150, 152, 162, 164, 178, 179, 182, 185, 188, 191, 192, 194, 196, 200, 205, 206, 211, 214, 221, 222, 226, 230, 235, 239, 242, 247, 248, 250, 252, 255, 257, 258, 260, 261, 263, 268, 273, 295, 299, 304, 315, 317], "enter": [146, 151, 155, 156, 159, 163, 167, 168, 175, 213, 216], "enterpris": [156, 160, 168, 176, 278, 279, 284, 287, 292, 294, 295, 302], "entir": [2, 4, 5, 7, 8, 9, 22, 32, 35, 46, 51, 60, 63, 64, 80, 81, 122, 123, 157, 169, 178, 179, 182, 207, 213, 220, 221, 226, 235, 241, 248, 250, 254, 255, 258, 261, 263, 268, 269, 278, 279, 282, 306, 330, 331], "entiti": [157, 169], "entri": [146, 178, 196, 261, 267], "entropi": [268, 269], "entrypoint": [155, 159, 167, 175, 307, 336], "enum": [295, 299, 300], "enumer": [6, 39, 213, 217, 248, 250, 254, 305, 322], "env": [2, 21, 152, 164, 178, 180, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 287, 292], "env_var": [2, 21, 22, 287, 290, 293, 295, 298, 299, 300], "environ": [0, 3, 4, 5, 7, 8, 9, 10, 17, 22, 28, 31, 35, 46, 53, 62, 69, 70, 73, 75, 78, 84, 85, 90, 91, 92, 99, 100, 108, 109, 110, 121, 122, 123, 125, 135, 136, 137, 141, 142, 143, 144, 145, 147, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 165, 166, 167, 168, 169, 171, 173, 205, 207, 221, 223, 224, 227, 229, 231, 235, 236, 237, 239, 244, 246, 247, 248, 250, 255, 256, 257, 261, 263, 268, 269, 270, 278, 279, 281, 287, 290, 293, 295, 298, 300], "environment": [152, 164], "eot": [84, 86, 91, 95, 99, 101, 109, 112, 122, 126], "ep": [304, 313, 319], "ephemer": [178, 180], "epic": [205, 211], "episod": [242, 247], "epoch": [4, 6, 31, 32, 40, 41, 178, 182, 187, 188, 190, 192, 193, 198, 199, 200, 202, 204, 221, 226, 227, 228, 235, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 252, 253, 254, 261, 262, 265, 267, 268, 269, 273, 274, 275, 276, 277, 304, 305, 313, 315, 317, 318, 319, 323, 325, 326], "epoch_loss": [4, 31], "equival": [8, 51, 53], "ergonom": [242, 244], "ernst": [213, 220], "ernsthugo": [213, 220], "erotica": [213, 216], "errno": [8, 53], "error": [2, 4, 5, 7, 8, 9, 20, 29, 33, 46, 53, 57, 61, 99, 107, 109, 120, 122, 127, 136, 137, 141, 142, 145, 148, 152, 164, 207, 229, 234, 235, 236, 248, 249, 254, 255, 256, 295, 299, 304, 305, 318, 324, 326], "erupt": [213, 220], "escap": [213, 220], "especi": [2, 4, 8, 19, 22, 32, 55, 152, 164, 178, 186, 192, 205, 207, 213, 219, 221, 226, 235, 237, 261, 265, 268, 270, 306, 307, 328, 336], "essenti": [7, 46, 73, 77, 178, 198, 221, 226, 278, 279, 286, 287, 294], "establish": [6, 40, 73, 75, 295, 301, 305, 323], "estim": [4, 32, 154, 166, 178, 182], "estimate_pi": [154, 166], "eta": [3, 28, 235, 236, 242, 243, 255, 258, 260], "etc": [1, 2, 3, 4, 5, 8, 9, 13, 25, 28, 30, 34, 51, 53, 59, 73, 74, 75, 78, 79, 80, 83, 142, 144, 145, 178, 179, 183, 202, 255, 258, 287, 290, 303, 310], "ether": [213, 220], "etl": [7, 43, 46, 306, 328], "euler": [235, 241], "europ": [213, 219, 220], "europa": [205, 211, 213, 220], "ev": [205, 211], "eval": [3, 4, 5, 9, 10, 28, 31, 32, 36, 62, 70, 178, 191, 221, 226, 235, 241, 242, 247, 248, 252, 254, 255, 258, 261, 265, 267, 268, 273, 277, 304, 306, 307, 313, 319, 330, 335], "eval_arrow": [255, 258], "eval_dataset": [221, 226], "eval_metr": [3, 28, 255, 258], "eval_pr": [221, 225], "evals_result": [3, 28, 255, 258], "evalu": [2, 6, 22, 40, 73, 76, 135, 178, 191, 221, 223, 224, 225, 226, 235, 236, 241, 242, 243, 247, 248, 249, 254, 256, 257, 260, 268, 269, 273, 277, 295, 301, 305, 307, 323, 336], "evan": [205, 211], "even": [2, 22, 137, 141, 154, 166, 178, 188, 205, 211, 213, 216, 220, 229, 233, 234, 261, 267, 268, 273, 278, 279, 282, 295, 302, 307, 336], "evenli": [178, 182, 183], "event": [4, 5, 7, 30, 34, 46, 136, 137, 139, 140, 148, 154, 166, 178, 179], "eventu": [178, 191], "ever": [3, 27, 213, 216, 219, 303, 309], "everi": [0, 1, 16, 153, 155, 157, 159, 165, 167, 169, 173, 178, 182, 187, 188, 196, 197, 205, 211, 213, 217, 235, 238, 242, 243, 246, 248, 252, 255, 256, 257, 258, 268, 269, 270, 271, 275, 277], "every_n_epoch": [235, 239, 242, 246], "everyon": [213, 220], "everyth": [80, 82, 156, 168, 178, 189, 213, 216, 220, 235, 241, 242, 247, 248, 250, 254, 261, 265, 268, 270], "evil": [213, 219, 220], "evolut": [7, 43], "evolv": [248, 252, 255, 256], "ex": [235, 237], "exact": [2, 9, 11, 22, 61, 72, 150, 162, 287, 292, 295, 299], "exactli": [73, 78, 235, 237, 248, 253, 268, 269, 270, 271, 277], "examin": [99, 106, 109, 118], "exampl": [2, 4, 6, 7, 8, 9, 10, 18, 21, 22, 23, 25, 31, 41, 43, 52, 53, 57, 59, 60, 62, 70, 71, 74, 76, 78, 82, 84, 86, 87, 91, 94, 99, 100, 101, 109, 110, 112, 122, 123, 125, 127, 136, 138, 142, 143, 144, 145, 148, 149, 151, 155, 156, 163, 167, 168, 178, 180, 183, 188, 190, 192, 204, 205, 207, 210, 211, 213, 215, 218, 221, 226, 229, 233, 235, 236, 241, 242, 243, 248, 249, 254, 255, 256, 258, 259, 260, 261, 267, 268, 269, 270, 275, 277, 278, 279, 282, 283, 291, 293, 296, 297, 301, 302, 306, 307, 308, 312, 330, 336], "exce": [229, 233], "excel": [287, 289, 295, 301], "except": [2, 5, 20, 36, 178, 199, 205, 211, 235, 237, 268, 270, 307, 335], "excess": [268, 269], "excit": [205, 211], "exclus": [4, 32, 278, 279, 283], "exdb": [304, 305, 318, 324], "execut": [2, 3, 4, 5, 6, 7, 12, 14, 16, 18, 19, 21, 23, 24, 28, 32, 35, 36, 41, 43, 47, 48, 51, 55, 56, 57, 61, 62, 63, 73, 74, 78, 99, 106, 109, 118, 122, 125, 136, 142, 144, 148, 151, 154, 155, 158, 163, 166, 167, 171, 173, 175, 178, 179, 180, 181, 182, 196, 205, 211, 212, 221, 223, 226, 235, 237, 242, 244, 248, 250, 252, 255, 256, 257, 258, 260, 261, 262, 263, 265, 268, 269, 270, 273, 277, 278, 279, 284, 295, 297, 300, 305, 324], "execute_notebook": 0, "exercis": 161, "exhaust": [178, 198, 255, 258, 278, 279, 283], "exhibit": [213, 220, 261, 263], "exisitng": [80, 83], "exist": [3, 8, 28, 53, 80, 81, 82, 83, 84, 85, 86, 99, 100, 101, 108, 111, 112, 121, 142, 144, 145, 151, 152, 153, 154, 156, 160, 163, 164, 165, 166, 168, 177, 178, 203, 205, 209, 213, 216, 221, 227, 229, 231, 235, 239, 241, 242, 246, 247, 248, 250, 252, 254, 255, 260, 261, 263, 265, 267, 268, 269, 273, 275, 277, 295, 300, 339], "exist_ok": [4, 31, 235, 237, 239, 242, 246, 248, 250, 255, 257, 261, 263, 268, 270, 304, 313], "existing_vpc_id": [73, 78], "exit": [235, 240], "exogen": [261, 267], "exp": [261, 264], "expand": [84, 85, 99, 100, 109, 111, 119, 122, 124, 142, 144, 154, 166, 178, 191, 235, 238], "expect": [4, 6, 32, 41, 151, 163, 178, 181, 188, 235, 239, 248, 250, 255, 257, 261, 267, 268, 270, 278, 279, 283, 305, 307, 324, 336], "expens": [3, 6, 9, 10, 28, 40, 62, 68, 205, 210, 213, 219, 235, 237, 287, 289, 295, 298, 303, 305, 306, 310, 311, 323, 330], "expensive_comput": [2, 24], "expensive_squar": [1, 2, 16, 19, 23, 24], "experi": [1, 4, 6, 13, 32, 41, 135, 136, 137, 141, 142, 143, 148, 150, 151, 152, 156, 161, 162, 163, 164, 168, 178, 180, 182, 204, 213, 216, 229, 234, 242, 247, 255, 260, 268, 269, 276, 277, 287, 294, 295, 298, 302, 324], "experiment": [10, 71, 248, 253], "experiment_nam": [5, 36, 178, 200], "experinc": [160, 176], "expert": [7, 47, 213, 220, 235, 236, 242, 243, 268, 270], "expertli": [213, 220], "explain": [73, 77, 80, 82, 142, 145, 158, 170, 205, 211, 213, 219], "explan": [142, 144, 229, 233, 295, 298], "explicit": [213, 216, 248, 249, 255, 258, 295, 301], "explicitli": [5, 6, 8, 9, 35, 41, 52, 60, 156, 160, 168, 177, 205, 207, 209], "explor": [7, 43, 142, 145, 151, 153, 156, 161, 163, 165, 168, 178, 204, 213, 216, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277, 278, 279, 280, 283, 287, 293, 294, 295, 296, 297, 302], "explos": [137, 141], "export": [122, 125, 136, 146, 149, 242, 247, 287, 290, 291, 295, 298], "expos": [80, 83, 205, 211], "exposehead": [73, 78, 109, 119], "exposur": [73, 78], "expr": [213, 218], "express": [4, 31, 154, 166, 178, 180, 213, 218, 235, 241], "expresswai": [205, 211], "extend": [2, 25, 73, 78, 178, 192, 199, 200, 204, 235, 236, 241, 242, 247, 248, 254, 255, 258, 260, 261, 267, 268, 271, 277], "extens": [9, 59, 151, 153, 163, 165, 295, 300], "extern": [7, 8, 46, 50, 73, 78, 83, 109, 119, 122, 129, 137, 139, 178, 204, 295, 297, 300, 302], "extra": [205, 211, 248, 249, 250, 252, 255, 257, 261, 265, 267, 268, 275, 278, 279, 283], "extra_st": [178, 199, 200], "extract": [5, 7, 9, 35, 43, 61, 109, 113, 248, 250, 254, 268, 275, 304, 305, 318, 326], "extract_dir": [248, 250], "extractal": [248, 250], "extrem": [137, 141, 153, 165, 229, 234, 255, 256, 278, 279, 282, 307, 336], "f": [2, 3, 4, 5, 6, 8, 9, 10, 18, 21, 22, 24, 28, 31, 32, 33, 35, 40, 51, 53, 62, 65, 70, 84, 86, 137, 141, 142, 144, 146, 152, 153, 154, 156, 164, 165, 166, 168, 178, 182, 187, 191, 203, 221, 226, 227, 235, 237, 239, 241, 242, 246, 247, 248, 250, 252, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268, 270, 273, 277, 278, 279, 285, 287, 292, 295, 298, 299, 300, 304, 305, 313, 319, 323], "f1": [205, 211, 268, 277], "f_": [235, 236, 242, 243, 255, 256, 261, 262, 268, 269], "face": [4, 5, 30, 34, 73, 78, 161, 178, 179, 192, 205, 207, 209, 211, 212, 213, 216, 220, 224, 225, 228, 229, 232, 234, 268, 269, 270, 277, 278, 279, 283, 287, 290, 291, 295, 298, 300, 307, 334], "facial": [213, 220], "facilit": [7, 43, 47, 153, 165], "fact": [205, 211, 213, 216, 219, 220], "factor": [252, 254], "fahrenheit": [2, 25, 295, 300], "fail": [2, 4, 5, 9, 20, 30, 34, 57, 61, 137, 141, 178, 179, 198, 200, 201, 248, 252, 255, 258, 261, 266, 304, 305, 318, 324, 326], "failur": [4, 5, 7, 9, 17, 20, 30, 34, 46, 57, 61, 136, 137, 139, 141, 148, 155, 156, 159, 160, 167, 168, 173, 176, 178, 179, 198, 200, 201, 202, 242, 246, 255, 256, 260, 261, 262, 265, 267, 268, 269, 273, 277, 287, 292], "failure_config": [178, 200, 202, 235, 239, 242, 246, 248, 252, 255, 258, 261, 265, 268, 274], "failureconfig": [198, 201, 204, 235, 236, 237, 239, 242, 243, 244, 246, 248, 249, 250, 252, 255, 256, 257, 258, 261, 262, 263, 265, 268, 270, 273, 274, 277], "fair": [205, 211], "fake": [213, 217], "fake_kei": [278, 287, 291, 295, 298, 299, 300], "fall": [248, 250], "fallback": [235, 241, 242, 247], "fals": [3, 4, 5, 6, 10, 28, 31, 32, 35, 36, 40, 41, 70, 142, 145, 178, 181, 184, 235, 238, 239, 241, 242, 245, 246, 247, 248, 250, 252, 255, 257, 258, 261, 263, 265, 267, 268, 273, 275, 277, 303, 304, 305, 307, 310, 311, 313, 319, 323, 326, 335, 336], "famili": [205, 211], "familiar": [135, 142, 143, 178, 194], "fan": [205, 211, 213, 216], "fanatic": [213, 220], "fanchant": [205, 211], "fantasi": [213, 219], "fantast": [213, 220], "far": [213, 216, 219, 255, 258], "fare": [213, 220], "fare_amount": [3, 28, 303, 310], "fashion": [8, 55, 306, 328], "fast": [4, 8, 9, 31, 50, 61, 80, 82, 161, 178, 179, 180, 242, 243, 255, 256, 260, 268, 270, 273, 278, 279, 283, 284, 295, 301], "fastapi": [3, 7, 26, 28, 48, 146, 156, 160, 168, 177, 232, 234], "fastapideploy": [156, 160, 168, 177], "faster": [6, 41, 178, 191, 213, 218, 221, 227, 235, 241, 287, 293, 294], "fastest": [151, 163], "fate": [213, 220], "father": [213, 220], "fault": [4, 5, 7, 8, 9, 30, 34, 46, 50, 55, 57, 73, 77, 78, 160, 176, 179, 199, 200, 202, 235, 236, 237, 239, 241, 242, 243, 246, 247, 248, 249, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262, 266, 267, 269, 273, 274, 277, 287, 292], "fc": [304, 313, 319], "feasibl": [242, 247], "featur": [3, 7, 28, 43, 46, 47, 49, 51, 52, 73, 76, 80, 82, 136, 137, 138, 142, 145, 149, 151, 163, 213, 216, 229, 231, 235, 237, 248, 254, 256, 257, 258, 260, 261, 267, 268, 270, 277, 278, 279, 283, 284, 287, 292, 293, 294, 302, 311, 333], "feature_col": [255, 258, 259], "feature_column": [255, 257, 258, 259, 260], "feature_nam": [255, 258], "feb": [205, 211], "fed": [205, 211], "feder": [73, 78], "fee": [3, 28, 303, 310], "feed": [8, 50, 52, 178, 192, 205, 211, 261, 262, 265, 268, 271], "feedback": [151, 163], "feel": [156, 160, 168, 177, 178, 194, 205, 211, 213, 216, 219, 220, 235, 241, 242, 247, 248, 254, 255, 260, 268, 277], "femal": [213, 216], "fenc": [205, 211], "fend": [213, 219], "ferrari": [205, 211], "ferri": [205, 211], "fetch": [4, 5, 17, 18, 19, 30, 34, 178, 179, 180, 191, 194, 221, 226, 255, 257, 261, 263], "fetch_covtyp": [255, 257], "few": [122, 129, 142, 143, 150, 151, 152, 162, 163, 164, 205, 212, 213, 215, 216, 219, 220, 235, 236, 239, 241, 242, 247, 248, 249, 250, 254, 255, 260, 261, 267, 268, 277, 295, 298], "fewer": [7, 46, 91, 95], "ff": [205, 211], "fiat": [205, 211], "fid": [235, 241], "field": [8, 51, 156, 160, 168, 177, 178, 196, 242, 244, 295, 299], "fifo": [221, 228, 303, 304, 305, 310, 311, 318, 324, 326], "fifoschedul": [6, 41, 305, 325, 326], "fig": [6, 39, 235, 237, 241, 268, 270, 304, 305, 313, 319, 322], "figsiz": [4, 6, 31, 32, 39, 178, 180, 191, 235, 237, 239, 241, 242, 246, 248, 250, 252, 255, 257, 261, 263, 265, 267, 268, 270, 275, 304, 305, 313, 319, 322], "figur": [4, 8, 31, 32, 51, 178, 180, 191, 235, 239, 242, 246, 248, 250, 252, 261, 263, 265, 267, 268, 275, 306, 329], "file": [0, 3, 4, 5, 6, 7, 10, 11, 28, 31, 32, 35, 37, 42, 43, 51, 53, 55, 58, 59, 62, 66, 71, 72, 73, 78, 80, 82, 84, 86, 90, 91, 94, 95, 99, 101, 102, 109, 112, 114, 122, 125, 126, 137, 141, 142, 145, 151, 152, 154, 155, 156, 161, 163, 164, 166, 167, 168, 178, 180, 188, 195, 200, 203, 204, 205, 206, 211, 214, 222, 230, 235, 237, 241, 242, 243, 247, 248, 249, 250, 254, 261, 263, 268, 269, 270, 271, 273, 277, 295, 298, 303, 304, 310, 311, 320, 329, 330], "file_nam": [295, 298], "filenam": [151, 163, 178, 180, 235, 239, 242, 246], "filenotfounderror": [8, 53, 235, 241, 268, 277], "filestor": [80, 82, 91, 92, 95, 122, 126], "filestore_capacity_gb": [91, 95], "filestore_instance_nam": [91, 95, 122, 126], "filestore_loc": [91, 95, 122, 126], "filestore_ti": [91, 95], "filesystem": [5, 35, 142, 144, 178, 180, 221, 228, 261, 262, 304, 305, 319, 324], "fill": [205, 211, 213, 220], "film": [205, 211, 213, 216, 219, 220], "filmbr": [213, 220], "filmmak": [213, 216], "filter": [8, 55, 84, 86, 91, 94, 99, 101, 109, 112, 122, 125, 127, 142, 145, 152, 154, 164, 166, 178, 181, 215, 219, 220, 248, 249, 251, 254], "filterwarn": [235, 239, 242, 246], "final": [1, 2, 6, 9, 16, 23, 41, 65, 122, 125, 154, 161, 166, 178, 181, 186, 190, 191, 197, 201, 202, 203, 204, 213, 219, 220, 221, 228, 235, 238, 242, 247, 248, 250, 254, 255, 258, 260, 261, 265, 267, 268, 273, 274, 276, 277, 295, 300, 305, 306, 326, 332], "find": [2, 3, 4, 5, 9, 21, 28, 30, 34, 61, 73, 78, 84, 86, 91, 98, 99, 101, 109, 112, 113, 119, 122, 125, 134, 142, 144, 153, 154, 155, 156, 157, 161, 165, 166, 167, 168, 169, 178, 179, 204, 213, 216, 219, 220], "fine": [6, 40, 80, 82, 155, 159, 167, 173, 178, 204, 213, 219, 235, 239, 241, 255, 260, 268, 269, 273, 295, 297, 298], "finer": [2, 22], "finest": [205, 209, 211], "finetun": [4, 5, 6, 32, 37, 41, 229, 233, 304, 305, 320, 325], "finish": [2, 24, 178, 190, 242, 247, 255, 258, 268, 276, 278, 279, 282], "fiorentina": [205, 211], "fiorina": [205, 211], "fir": [205, 211, 255, 256], "fire": [205, 211], "firewal": [73, 78, 91, 95, 98, 122, 126], "firewall_policy_nam": [91, 95, 122, 126], "first": [1, 2, 3, 5, 6, 10, 11, 13, 14, 16, 19, 21, 27, 36, 41, 68, 70, 72, 84, 87, 91, 94, 96, 99, 103, 109, 113, 115, 122, 125, 131, 137, 141, 153, 154, 165, 166, 178, 181, 191, 205, 209, 211, 213, 216, 219, 220, 235, 237, 239, 241, 248, 250, 254, 261, 263, 268, 273, 278, 279, 282, 283, 287, 290, 295, 298, 303, 304, 305, 307, 309, 313, 317, 326, 335, 336], "fit": [3, 4, 6, 9, 28, 31, 32, 35, 40, 41, 64, 190, 195, 197, 201, 202, 213, 215, 221, 227, 235, 239, 240, 242, 246, 248, 252, 253, 255, 256, 258, 260, 261, 265, 266, 267, 268, 269, 274, 276, 278, 279, 282, 303, 304, 305, 306, 310, 311, 313, 318, 319, 323, 324, 325, 326, 329, 331], "fit_model": [6, 40], "five": [213, 219, 220, 235, 239, 242, 246, 268, 274], "fix": [178, 182, 221, 226, 235, 236, 248, 250, 261, 263], "flag": [154, 166], "flap": [213, 216], "flashi": [213, 220], "flatten": [235, 237, 268, 270], "flavor": [152, 164], "flawless": [205, 211], "fleet": [213, 216, 220, 261, 262], "flew": [205, 211], "flexibl": [2, 7, 8, 10, 23, 43, 45, 46, 51, 69, 158, 172, 205, 207, 213, 215, 221, 227, 278, 279, 284, 295, 296, 297, 306, 307, 328, 329, 334], "flexibli": [307, 334], "flink": [7, 46], "flip": [268, 269], "flip_sin_to_co": [5, 35], "flippen": [213, 219], "float": [2, 3, 4, 5, 6, 8, 9, 10, 20, 28, 31, 32, 35, 40, 41, 51, 62, 70, 178, 188, 200, 205, 212, 235, 238, 242, 245, 248, 252, 255, 259, 261, 265, 303, 304, 305, 306, 307, 310, 313, 317, 323, 324, 325, 330, 335], "float16": [5, 35], "float32": [10, 70, 205, 211, 212, 235, 237, 242, 244, 247, 248, 252, 261, 263, 264, 267, 303, 310, 311], "floral": [84, 86, 99, 101, 109, 112], "flore": [205, 211], "flow": [46, 73, 78, 136, 148, 158, 172, 242, 243, 295, 300], "flush": [2, 20, 278, 279, 285, 287, 291, 292, 295, 298], "fly": [178, 192], "fmt": [255, 259], "fn_arg": [295, 300], "fn_call": [295, 300], "fn_callabl": [295, 300], "fn_constructor_arg": [255, 259, 260, 261, 267, 268, 277], "fn_constructor_kwarg": [9, 62, 306, 307, 330, 335], "fn_kwarg": [306, 330], "fname": [261, 263], "foam": [205, 211], "focu": [137, 140, 161, 178, 192, 213, 216, 261, 262, 295, 297], "focus": [0, 7, 43, 45, 80, 82, 136, 149, 205, 211, 261, 267, 278, 279, 281], "folder": [3, 4, 8, 9, 10, 28, 31, 53, 62, 70, 146, 151, 153, 155, 163, 165, 167, 206, 214, 222, 230, 255, 260, 261, 267, 268, 277, 295, 298], "follow": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 16, 18, 30, 31, 32, 34, 35, 36, 41, 45, 48, 51, 57, 62, 68, 72, 73, 76, 78, 80, 82, 84, 85, 91, 93, 98, 99, 100, 102, 107, 109, 111, 114, 119, 120, 122, 124, 127, 134, 135, 136, 137, 141, 142, 145, 146, 148, 149, 150, 151, 152, 153, 154, 157, 161, 162, 163, 164, 165, 166, 169, 175, 178, 179, 181, 192, 193, 213, 220, 221, 227, 242, 247, 248, 249, 254, 255, 256, 261, 262, 267, 268, 269, 271, 278, 279, 285, 287, 289, 295, 298, 301, 304, 305, 306, 307, 319, 324, 325, 329, 331, 334], "followup": [213, 220], "fontsiz": [235, 237, 268, 270], "food": [236, 241, 277], "food101": [235, 237, 268, 269, 270, 277], "food101_diffusion_ft": [235, 239], "food101_diffusion_result": [235, 239], "food101_ft_resum": [268, 274, 277], "food101_ft_run": [268, 277], "food101_lit": [235, 237, 268, 270, 271, 273, 274, 277], "food101_single_run": [268, 277], "food101dataset": [269, 272, 277], "footag": [205, 211], "footbal": [205, 209, 211], "footer": 0, "forbidden": [304, 305, 318, 324, 326], "forc": [178, 191, 262, 264, 267, 268, 277], "forcibli": [213, 219], "ford": [213, 216], "forecast": 267, "foreground": [213, 220], "foregroundbr": [213, 220], "forest": [257, 259], "forg": [11, 72], "forget": [205, 211, 213, 220], "forgotten": [213, 220], "forgottenbr": [213, 220], "fork": 0, "form": [2, 18, 213, 220, 248, 250, 303, 309, 310, 311], "format": [6, 9, 39, 59, 61, 146, 161, 178, 192, 195, 221, 226, 235, 237, 248, 250, 261, 263, 268, 269, 270, 295, 297, 298, 299, 300, 302, 305, 322], "fort": [213, 219], "forum": [287, 294, 295, 302], "forward": [5, 35, 178, 179, 182, 193, 199, 205, 211, 221, 226, 238, 239, 242, 243, 245, 246, 248, 249, 251, 255, 257, 261, 264, 267, 304, 313], "found": [6, 41, 73, 78, 178, 184, 199, 202, 203, 213, 220, 235, 241, 242, 247, 248, 249, 255, 258, 268, 277, 305, 324], "foundat": [6, 7, 40, 43, 248, 254, 286, 295, 302, 305, 323], "four": [4, 5, 32, 36, 178, 181, 248, 250, 278, 279, 282], "fourth": [205, 211, 213, 219, 220], "fox": [205, 211], "foxx": [205, 211], "fp16": [278, 279, 283, 287, 289, 293], "fp8": [287, 293], "frac": [268, 271], "fraction": [10, 17, 25, 68, 229, 233, 334], "fragrant": [205, 211], "frame": [255, 257, 258], "framework": [3, 4, 5, 8, 10, 27, 28, 30, 34, 43, 48, 50, 67, 69, 161, 178, 179, 213, 215, 229, 231, 302, 303, 306, 307, 309, 310, 311, 329, 333, 334], "franc": [278, 279, 285, 295, 298], "francisco": [295, 300], "frank": [205, 211], "fraud": [7, 46], "freak": [213, 219], "free": [2, 21, 154, 155, 156, 160, 161, 166, 167, 168, 177, 178, 191, 205, 207, 211, 221, 223, 255, 260, 261, 267, 268, 277, 287, 293], "freed": [178, 191], "freeli": [8, 51, 306, 329], "freq_shift": [5, 35], "frequenc": [5, 35, 248, 250, 255, 257], "frequent": [7, 43, 46], "fresh": [178, 191], "fri": [268, 269], "friction": [1, 13], "fridai": [205, 211], "friend": [205, 211], "friendli": [137, 141, 155, 159, 167, 173, 235, 237, 248, 254, 268, 269, 270, 287, 289], "frighten": [213, 219, 220], "frill": [213, 220], "from": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 18, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 38, 41, 46, 50, 52, 53, 55, 56, 57, 59, 61, 62, 67, 68, 71, 72, 73, 77, 78, 80, 83, 84, 85, 86, 88, 91, 92, 95, 97, 98, 99, 100, 101, 103, 104, 106, 109, 110, 113, 115, 116, 118, 119, 122, 123, 131, 132, 133, 134, 136, 137, 139, 141, 142, 144, 145, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 162, 163, 164, 165, 166, 167, 168, 169, 172, 175, 177, 179, 180, 181, 182, 186, 187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 203, 204, 205, 207, 208, 209, 211, 212, 213, 215, 216, 219, 220, 221, 224, 225, 226, 229, 232, 234, 236, 237, 239, 243, 244, 246, 249, 252, 254, 256, 257, 258, 262, 263, 265, 267, 268, 269, 270, 271, 273, 275, 276, 277, 278, 279, 281, 282, 283, 285, 287, 288, 290, 291, 292, 293, 295, 297, 298, 299, 300, 302, 303, 304, 305, 306, 307, 310, 311, 313, 315, 317, 318, 322, 323, 324, 325, 328, 329, 330, 335, 336, 337], "from_directori": [4, 32, 178, 188, 200, 248, 252, 261, 265, 267, 268, 273, 277, 304, 317], "from_huggingfac": [205, 209], "from_item": [137, 141, 213, 217, 235, 237, 242, 244, 261, 267, 307, 335], "from_numpi": [261, 267, 268, 277], "from_pretrain": [5, 35, 221, 226], "from_pydict": [268, 270], "from_pylist": [261, 263], "from_torch": [9, 59], "fromarrai": [178, 196], "front": [255, 257, 268, 270], "frontal": [213, 216], "fr\u00e9chet": [235, 241], "fsdp": [4, 32, 178, 180, 185, 204], "ft": [205, 209, 211], "fuck": [205, 211], "full": [3, 5, 8, 9, 28, 35, 51, 57, 63, 73, 76, 78, 80, 82, 109, 119, 137, 141, 142, 145, 155, 167, 173, 183, 190, 198, 204, 213, 219, 220, 235, 236, 242, 243, 248, 250, 252, 254, 255, 258, 261, 265, 268, 270, 271, 275, 277, 295, 298, 306, 330], "full_path": [268, 271], "fulli": [2, 4, 5, 18, 30, 34, 73, 74, 150, 158, 161, 162, 171, 178, 179, 181, 192, 197, 235, 236, 239, 242, 243, 248, 249, 250, 252, 255, 256, 258, 268, 269, 273, 307, 336], "fullyshardeddataparallel": [4, 32, 178, 185], "function": [2, 3, 4, 5, 6, 8, 9, 10, 12, 16, 20, 25, 28, 32, 33, 35, 36, 41, 43, 47, 51, 52, 53, 54, 59, 61, 64, 65, 69, 109, 119, 153, 157, 165, 169, 178, 181, 182, 185, 187, 188, 196, 205, 211, 213, 218, 220, 223, 225, 228, 229, 233, 235, 237, 242, 243, 246, 247, 248, 249, 250, 252, 255, 256, 260, 261, 262, 268, 269, 273, 287, 290, 295, 297, 300, 302, 304, 305, 306, 315, 316, 317, 319, 324, 325, 326, 329, 331, 332], "fundament": [2, 10, 17, 69, 80, 82, 135, 142, 143, 278, 279, 280, 281, 285, 307, 334], "further": [9, 59, 62, 152, 164, 178, 190, 213, 220, 229, 233, 268, 271], "fuse": [9, 61], "futur": [1, 8, 15, 54, 142, 145, 213, 216, 219, 220, 242, 243, 261, 262, 263, 264, 265], "future_tru": [261, 267], "g": [0, 1, 2, 6, 7, 8, 9, 10, 13, 19, 20, 22, 41, 43, 46, 53, 57, 59, 61, 68, 69, 73, 78, 80, 82, 83, 84, 85, 91, 92, 98, 99, 100, 109, 110, 122, 123, 134, 136, 137, 139, 142, 144, 148, 151, 153, 154, 156, 157, 160, 163, 165, 166, 168, 169, 176, 177, 178, 182, 183, 188, 189, 190, 192, 194, 198, 201, 204, 205, 207, 209, 211, 248, 252, 255, 258, 261, 267, 287, 293, 303, 305, 306, 310, 311, 325, 329, 330], "g54aiirwj1": [153, 165], "g54aiirwj1s8t9ktgzikqur41k": [153, 165], "gain": [248, 254, 255, 259], "galaxi": [205, 211], "galleri": [205, 211], "gallo": [213, 216], "gambl": [213, 219], "game": [205, 211], "gan": [235, 236], "gandhi": [205, 211], "gang": [213, 219], "gannon": [213, 219], "gap": [7, 43, 261, 263, 295, 298, 307, 337], "gape": [213, 216], "garbag": [1, 13, 178, 180, 191], "garden": [205, 211], "gate": [287, 290, 291, 295, 299, 300], "gatewai": [73, 78, 84, 86, 99, 101, 109, 112], "gather": [3, 28, 154, 166, 268, 270], "gaussian": [235, 236, 241, 242, 244, 261, 267], "gb": [2, 18, 153, 165, 287, 289, 293], "gc": [8, 53, 80, 82, 91, 92, 98, 122, 134, 178, 180, 188, 191, 204, 248, 254, 268, 277, 295, 298], "gca": [255, 259], "gce": [73, 74, 80, 83, 96, 122, 125, 339], "gcloud": [91, 93, 94, 122, 124, 125, 127, 128, 134], "gcp": [73, 74, 78, 80, 82, 83, 93, 94, 95, 97, 98, 122, 124, 125, 126, 130, 135, 157, 158, 169, 171], "gcp_if_": [91, 95], "gcp_project_id": [91, 94, 95, 122, 125, 126, 128, 134], "gcp_region": [91, 94, 95, 122, 125, 126, 127, 128, 132, 134], "gcs_bucket_nam": [91, 95, 98, 122, 126, 134], "gear": [205, 211], "gee": [205, 211], "gener": [1, 8, 9, 10, 11, 13, 35, 52, 54, 60, 62, 64, 70, 72, 91, 95, 137, 141, 142, 144, 145, 146, 153, 154, 155, 159, 165, 166, 167, 173, 178, 180, 191, 204, 205, 206, 207, 210, 213, 214, 216, 222, 230, 239, 243, 247, 248, 249, 254, 261, 262, 267, 282, 286, 295, 297, 298, 299, 300, 301, 302, 304, 306, 307, 313, 319, 329, 330, 331, 336], "generate_synthetic_imag": [137, 141], "generated_bi": [137, 141], "generative_cv": [235, 239, 241], "genit": [213, 216], "geniu": [213, 220], "genr": [248, 254], "geo": [255, 256], "german": [213, 220], "germani": [205, 211, 213, 220], "get": [3, 4, 5, 7, 10, 11, 17, 18, 19, 20, 21, 22, 23, 25, 26, 28, 32, 36, 40, 46, 70, 72, 73, 78, 84, 85, 86, 87, 91, 95, 96, 99, 100, 101, 103, 105, 106, 107, 109, 111, 113, 115, 117, 118, 119, 120, 122, 128, 129, 131, 137, 140, 142, 144, 145, 151, 153, 154, 155, 159, 163, 165, 166, 167, 174, 178, 181, 182, 191, 205, 211, 213, 219, 220, 221, 226, 229, 233, 234, 235, 236, 239, 241, 242, 246, 247, 248, 250, 252, 255, 258, 261, 263, 265, 267, 268, 270, 273, 277, 280, 286, 287, 290, 292, 293, 296, 298, 300, 303, 307, 308, 310, 311, 323, 326, 335, 336], "get_best_result": [3, 6, 28, 41, 303, 305, 310, 311, 324, 326], "get_checkpoint": [178, 198, 199, 204, 235, 237, 239, 242, 244, 246, 248, 250, 252, 255, 257, 258, 260, 261, 263, 265, 268, 269, 270, 273], "get_config_dict": [5, 35], "get_context": [3, 4, 28, 32, 178, 182, 187, 188, 193, 199, 200, 235, 237, 239, 242, 244, 246, 248, 250, 252, 255, 257, 258, 261, 263, 265, 268, 270, 273, 304, 315, 317, 319], "get_current_temperatur": [295, 300], "get_dataset_shard": [5, 36, 178, 194, 235, 239, 242, 246, 248, 250, 252, 255, 257, 258], "get_devic": [5, 33, 35, 221, 226], "get_linear_schedule_with_warmup": [5, 33, 35], "get_model": [255, 258, 259], "get_scor": [255, 259, 260], "get_temperature_d": [295, 300], "get_us": 146, "get_user_profil": 146, "get_world_rank": [3, 4, 28, 32, 178, 187, 188, 200, 235, 239, 242, 246, 248, 252, 255, 258, 261, 265, 268, 273, 304, 317, 319], "get_world_s": [4, 32, 178, 182, 193, 199, 304, 315, 319], "getbucketloc": [73, 78], "getenv": [152, 153, 164, 165], "getlogg": [142, 145], "getobject": [73, 78], "getsizeof": [2, 18], "gettempdir": [235, 239, 242, 246], "gettingstart": 161, "getvalu": [235, 237, 268, 270], "gh": 0, "ghetto": [205, 211], "giant": [213, 220], "gib": [303, 305, 310, 311, 324, 326], "gift": [205, 211], "girl": [205, 211], "git": [0, 156, 157, 160, 161, 168, 169, 177], "github": [0, 11, 72, 73, 78, 99, 102, 104, 109, 114, 116, 122, 129, 132, 156, 161, 168, 261, 263], "githubusercont": [261, 263], "give": [6, 40, 109, 113, 150, 152, 156, 157, 162, 164, 168, 169, 178, 181, 200, 213, 219, 248, 250, 254, 255, 257, 278, 279, 284, 305, 323], "given": [1, 3, 4, 6, 7, 9, 14, 28, 32, 41, 46, 47, 48, 64, 142, 145, 157, 169, 229, 233, 235, 238, 242, 243, 245, 255, 256, 261, 262, 295, 300, 304, 305, 306, 317, 325, 331], "gke": [73, 74, 76, 80, 83, 125, 126, 128, 131, 339], "glanc": [150, 162], "glass": [205, 211], "glob": [235, 241, 242, 244, 247], "global": [84, 86, 91, 95, 99, 101, 109, 112, 122, 125, 126, 178, 182, 235, 238, 248, 250, 268, 271, 273], "global_batch_s": [4, 32, 178, 182, 183, 189, 193, 197, 199, 200, 202, 221, 227, 304, 315, 318, 319], "gloriou": [213, 219], "gloss": [213, 220], "gm": [205, 209, 211], "go": [2, 19, 73, 78, 150, 162, 205, 207, 211, 213, 219, 220, 268, 270, 278, 279, 284, 287, 293, 295, 298], "goal": [3, 28, 160, 161, 176, 205, 211, 221, 223, 242, 243, 268, 269], "goaldotcom": [205, 211], "god": [205, 211], "goe": [154, 157, 166, 169, 205, 209, 211, 213, 216], "gold": [213, 219], "golions2012": [205, 211], "gone": [213, 220], "gonna": [205, 211], "good": [6, 7, 40, 48, 84, 86, 99, 101, 109, 112, 137, 141, 178, 180, 191, 195, 205, 211, 213, 216, 219, 220, 255, 259, 261, 263, 287, 289, 305, 323], "googl": [7, 43, 73, 75, 76, 80, 83, 93, 95, 124, 205, 211], "google_cloud": [122, 125], "google_project_id": [91, 95, 122, 126, 134], "google_region": [91, 95, 122, 126, 134], "googleapi": [91, 94, 122, 125], "gore": [213, 220], "got": [2, 24, 205, 211, 213, 220], "gotrib": [205, 211], "gotten": [213, 220], "gpu": [2, 3, 6, 7, 8, 9, 10, 11, 22, 25, 26, 28, 29, 30, 33, 34, 38, 40, 41, 47, 55, 57, 61, 62, 66, 68, 69, 72, 80, 83, 84, 90, 99, 102, 109, 114, 136, 137, 139, 148, 150, 152, 162, 164, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 191, 192, 197, 204, 205, 207, 211, 212, 221, 223, 226, 227, 228, 229, 233, 235, 236, 239, 241, 242, 243, 246, 247, 248, 249, 252, 254, 255, 258, 262, 268, 269, 272, 274, 277, 278, 279, 280, 281, 282, 283, 284, 287, 288, 289, 290, 291, 293, 294, 295, 296, 301, 302, 303, 305, 306, 308, 310, 311, 318, 323, 324, 326, 328, 330, 334], "grab": [235, 237, 268, 277], "gracefulli": [156, 160, 168, 176, 229, 234], "grad": [178, 182], "grade": [235, 241, 278, 279, 280, 284, 287, 294, 295, 302], "grader": [205, 211], "gradient": [3, 4, 5, 28, 32, 36, 178, 179, 182, 185, 188, 221, 226, 255, 256, 257, 261, 263, 303, 304, 310, 315], "gradual": [137, 141, 160, 176, 235, 236, 255, 256], "grafana": [142, 144, 145, 148, 154, 156, 160, 166, 168, 176, 287, 293], "grai": [4, 6, 9, 31, 32, 39, 60, 178, 180, 191, 304, 305, 307, 313, 319, 322, 336], "grain": [2, 10, 22, 69, 80, 82], "grand": [205, 211], "grant": [73, 78, 80, 82, 213, 216], "granular": [142, 145, 157, 169], "granularli": [307, 336], "graph": [1, 2, 13, 19, 136, 137, 141, 149], "grass": [213, 220], "grayscal": [4, 6, 31, 39, 178, 180, 181, 191, 304, 305, 313, 322], "great": [157, 169, 205, 211, 213, 220, 229, 234, 287, 292], "greater": [153, 154, 165, 166, 303, 310], "greec": [205, 211], "green": [235, 236, 268, 269], "grei": [205, 211], "grep": [84, 86, 99, 101, 105, 106, 107, 109, 112, 117, 118, 120], "grid": [6, 41, 178, 180, 191, 235, 239, 242, 246, 247, 248, 252, 261, 263, 265, 267, 268, 275, 305, 325], "grim": [213, 219, 220], "grimnoir": [213, 220], "ground": [9, 61, 64, 178, 180, 242, 244, 261, 262, 265, 267, 268, 277, 306, 331], "ground_truth_label": [9, 64, 306, 331], "group": [49, 56, 77, 79, 80, 82, 84, 86, 90, 99, 101, 109, 112, 113, 119, 135, 150, 154, 157, 158, 162, 166, 169, 171, 178, 179, 189, 205, 211, 235, 236, 248, 254, 261, 262, 268, 269, 270, 271, 275, 305, 324, 326, 327], "groupbi": [7, 47, 55, 248, 250, 252, 261, 265, 268, 275], "grouplen": [248, 250], "grow": [1, 3, 13, 27, 161, 213, 216, 303, 309], "grpc": [7, 10, 48, 68, 70, 307, 335], "gserviceaccount": [91, 95, 122, 126], "gsutil": [91, 98, 122, 134], "gt": [213, 220], "guarante": [7, 47, 178, 188, 295, 299], "guard": [248, 252, 268, 275], "gucci": [205, 211], "gui": [205, 211], "guid": [4, 5, 9, 11, 32, 36, 59, 72, 80, 82, 84, 85, 99, 100, 109, 110, 122, 123, 135, 136, 142, 143, 145, 147, 149, 156, 161, 168, 177, 178, 188, 205, 211, 221, 228, 229, 233, 287, 294, 295, 297, 298, 299, 300, 302, 304, 305, 317, 319, 326], "gunman": [213, 219], "gym": [242, 243, 244, 247], "gymnasium": [242, 243, 244], "gz": [304, 305, 318, 324, 326], "h": [4, 10, 31, 70, 146, 178, 191, 235, 236, 237, 238, 261, 263, 268, 277, 304, 313], "ha": [2, 4, 5, 7, 11, 18, 30, 31, 34, 46, 47, 48, 72, 73, 76, 91, 98, 109, 119, 122, 134, 137, 141, 142, 145, 150, 152, 153, 154, 157, 158, 160, 162, 164, 165, 166, 169, 171, 176, 178, 179, 181, 195, 196, 205, 209, 211, 212, 213, 216, 219, 220, 221, 226, 229, 234, 248, 250, 287, 289, 295, 298, 302, 307, 336], "had": [8, 54, 205, 211, 213, 216], "hadoop": [7, 46], "hahha": [205, 211], "hail": [213, 220, 261, 262], "half": [205, 211, 213, 219, 220, 261, 262, 263], "halfstarv": [213, 220], "halloween": [205, 209, 211], "halv": [6, 41, 305, 325], "ham": [205, 211], "hamburg": [268, 269], "hand": [11, 72, 135, 161, 205, 211, 213, 219, 248, 250, 268, 269, 278, 279, 286, 295, 297], "handheld": [213, 220], "handl": [2, 3, 7, 9, 10, 20, 22, 25, 28, 43, 46, 47, 48, 57, 68, 69, 73, 78, 80, 81, 82, 122, 129, 136, 137, 141, 146, 148, 150, 155, 160, 161, 162, 167, 173, 176, 178, 179, 180, 181, 182, 184, 186, 191, 192, 193, 197, 198, 200, 203, 205, 212, 213, 215, 221, 223, 226, 229, 231, 235, 236, 239, 241, 242, 243, 244, 248, 249, 250, 252, 254, 255, 257, 258, 261, 262, 268, 269, 270, 272, 273, 275, 278, 279, 281, 284, 287, 290, 292, 295, 299, 303, 307, 310, 311, 336], "handwritten": [6, 10, 39, 40, 70, 178, 180, 305, 322, 323], "hang": [136, 148, 213, 219, 220, 242, 243], "happen": [1, 6, 7, 15, 41, 48, 137, 141, 178, 200, 213, 219, 220, 305, 324], "happi": [205, 211], "happybirthdayremuslupin": [205, 209, 211, 212], "hard": [213, 219, 220], "hardli": [213, 216], "hardwar": [4, 5, 9, 10, 30, 34, 57, 61, 62, 68, 69, 152, 154, 161, 164, 166, 178, 179, 192, 200, 221, 223, 226, 227, 228, 255, 256, 278, 279, 283, 284, 289, 290, 291, 302], "hark": [213, 219], "harm": 12, "harri": [205, 211], "hasattr": [268, 273], "hasek": [205, 211], "hash": [8, 54], "hashicorp": [84, 85, 91, 93, 99, 100, 109, 111, 122, 124], "hashtag": [205, 211], "hat": [248, 249, 268, 269], "hater": [205, 211], "have": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 20, 23, 24, 26, 32, 36, 41, 45, 46, 53, 57, 61, 62, 64, 68, 70, 72, 84, 85, 86, 90, 91, 93, 99, 100, 101, 102, 108, 109, 111, 112, 114, 121, 122, 124, 137, 141, 142, 143, 151, 152, 153, 157, 161, 163, 164, 165, 169, 178, 188, 197, 205, 211, 213, 216, 217, 218, 219, 220, 221, 226, 229, 231, 248, 249, 255, 257, 258, 268, 269, 277, 278, 279, 282, 287, 294, 304, 305, 306, 307, 314, 325, 329, 331, 334], "hdf": [7, 8, 46, 53], "he": [205, 211, 213, 219, 220], "head": [2, 8, 18, 22, 51, 53, 73, 77, 78, 80, 82, 99, 106, 109, 118, 137, 140, 141, 150, 152, 153, 154, 155, 156, 157, 162, 164, 165, 166, 167, 168, 169, 176, 213, 220, 235, 239, 242, 246, 255, 257, 261, 263, 267, 305, 324, 326], "head_nod": [99, 106, 109, 118], "header": [0, 4, 31, 248, 254, 304, 313], "headlei": [205, 209, 211], "headless": [80, 82], "headnodeconfig": [99, 106, 109, 118], "health": [80, 81, 146, 160, 176, 235, 239, 278, 279, 283], "healthi": [152, 156, 164, 168, 261, 265], "heap": [2, 24], "hear": [205, 211], "heard": [213, 216], "heart": [213, 219, 261, 265], "heat": [229, 234], "heatmap": [255, 259], "heaven": [213, 219], "heavi": [4, 8, 9, 32, 54, 64, 150, 162, 178, 192, 306, 331], "heavili": [213, 220], "heavli": [178, 186], "hebdo": [205, 211], "hei": [73, 78, 205, 211], "height": [9, 61, 137, 141, 235, 237, 255, 260, 306, 307, 330, 336], "held": [205, 211], "hell": [205, 211], "hello": [142, 145, 151, 153, 155, 159, 163, 165, 167, 174, 175, 205, 211, 278], "hello_world": [11, 72, 151, 155, 163, 167], "helm": [80, 81, 99, 100, 101, 102, 104, 107, 109, 111, 112, 114, 116, 120, 122, 124, 129, 132, 134], "helm_upgrade_command": [99, 101, 109, 112], "help": [6, 7, 9, 11, 40, 46, 61, 72, 73, 76, 80, 82, 136, 142, 144, 148, 151, 163, 178, 180, 213, 220, 255, 257, 259, 261, 262, 263, 278, 279, 282, 305, 307, 323, 336], "helper": [178, 180, 185, 186, 187, 188, 248, 250, 255, 257, 263, 269, 277, 295, 300], "helper_tool_map": [295, 300], "her": [205, 211, 213, 216, 219, 220], "here": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 16, 17, 25, 26, 28, 29, 31, 32, 33, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 52, 53, 54, 55, 56, 59, 61, 64, 67, 69, 70, 71, 84, 86, 99, 101, 109, 119, 137, 141, 142, 143, 144, 145, 161, 178, 180, 182, 183, 190, 191, 193, 200, 204, 205, 207, 211, 213, 215, 217, 219, 220, 221, 223, 229, 231, 233, 235, 236, 237, 242, 243, 248, 249, 278, 279, 280, 281, 284, 287, 288, 294, 295, 296, 298, 299, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 313, 314, 315, 317, 319, 320, 321, 323, 324, 325, 327, 329, 330, 331, 333, 334, 335, 336], "herm": [295, 300], "hero": [205, 211, 213, 220], "herself": [213, 220], "heteregen": [9, 57], "heterogen": [3, 7, 8, 9, 28, 47, 55, 57, 66, 306, 328, 330], "hf": [287, 290], "hf_d": [235, 237], "hf_dataset": [205, 209], "hf_token": [287, 290, 291, 292, 293, 295, 298, 299, 300], "hi": [205, 209, 211, 213, 216, 219, 220], "hidden": [0, 213, 220, 242, 247], "hide": [213, 219], "hierarch": [142, 145], "hierarchi": [158, 172], "high": [2, 4, 5, 6, 7, 9, 10, 22, 31, 32, 36, 40, 43, 46, 47, 61, 68, 69, 73, 78, 156, 160, 168, 176, 178, 181, 189, 205, 211, 212, 213, 220, 229, 231, 234, 242, 247, 248, 249, 250, 252, 268, 269, 278, 279, 281, 282, 283, 284, 295, 301, 304, 305, 307, 313, 315, 323, 334], "higher": [7, 47, 153, 165, 178, 192, 248, 249, 250, 278, 279, 282, 287, 293, 294, 295, 301, 306, 328], "highest": [213, 219, 235, 239, 248, 254], "highli": [73, 78, 80, 82, 137, 141, 255, 257], "highlight": [2, 5, 7, 18, 36, 46, 178, 188, 213, 220, 248, 250, 255, 259], "hike": [205, 211], "him": [205, 211, 213, 219, 220], "himself": [205, 211, 213, 219], "hindu": [205, 211], "hint": [1, 2, 4, 5, 6, 9, 16, 25, 32, 36, 41, 61, 221, 224, 304, 305, 319, 325], "hire": [213, 219], "hist": [3, 28, 248, 250, 255, 258], "histor": [7, 43, 46, 248, 249, 261, 262], "histori": [178, 190, 248, 252, 261, 263, 265, 267, 268, 275, 277, 295, 300], "hit": [84, 86, 99, 101, 109, 112, 137, 141, 205, 211, 248, 254], "hitchhik": [205, 211], "hiya": [205, 211], "hmu": [205, 211], "hmw": [205, 211], "hoc": [242, 243, 255, 259], "hogan": [205, 211], "hogwart": [205, 209, 211, 212], "hold": [8, 9, 51, 59, 178, 182, 221, 226, 306, 329], "hole": [205, 211], "holidai": [261, 267], "hollywood": [205, 211, 213, 219, 220], "home": [153, 165, 205, 211, 305, 324], "homebrew": [84, 85, 99, 100, 109, 111, 122, 124, 136, 149], "homecom": [205, 211], "homepath": [136, 149], "hong": [213, 219], "hood": [2, 9, 19, 59, 178, 184], "hook": [213, 219, 220, 242, 247], "hop": [205, 211], "hope": [205, 211, 213, 219], "horizon": [261, 263, 264, 265, 267], "horizont": [287, 290, 293], "horribl": [213, 220], "horror": [213, 220], "hospit": [213, 220], "host": [4, 32, 73, 76, 78, 80, 83, 150, 151, 158, 162, 163, 170, 171, 178, 186, 235, 236], "hostnam": [304, 319], "hot": [10, 71, 255, 259, 268, 269], "hotwif": [205, 211], "hour": [137, 141, 205, 211, 235, 237, 261, 262, 263, 267, 268, 270, 278, 279, 283, 303, 310], "hourli": [262, 267], "hous": [213, 219], "housekeep": [178, 180], "hoverboard": [205, 211], "how": [2, 3, 4, 5, 6, 7, 20, 28, 31, 32, 33, 35, 36, 41, 42, 44, 49, 51, 52, 53, 54, 56, 62, 74, 84, 86, 91, 92, 95, 99, 101, 109, 112, 119, 137, 141, 142, 143, 145, 146, 150, 153, 156, 158, 160, 161, 162, 165, 168, 170, 177, 181, 182, 183, 184, 188, 192, 193, 198, 200, 205, 206, 207, 209, 211, 212, 213, 214, 215, 218, 219, 220, 221, 222, 223, 228, 229, 230, 234, 250, 252, 254, 258, 263, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 289, 292, 296, 297, 300, 302, 304, 305, 306, 307, 311, 313, 315, 324, 325, 327, 335, 336], "howev": [2, 7, 8, 9, 22, 46, 53, 54, 64, 80, 82, 178, 188, 213, 219, 306, 331], "html": [0, 11, 72, 84, 85, 99, 100, 109, 111, 159, 160, 173, 176, 205, 209, 221, 228, 229, 231, 233, 278, 279, 285, 305, 326], "http": [0, 3, 7, 10, 11, 28, 48, 70, 72, 73, 78, 80, 83, 84, 85, 91, 93, 99, 100, 102, 104, 109, 111, 114, 116, 119, 122, 124, 129, 132, 136, 142, 145, 146, 149, 154, 156, 159, 160, 161, 166, 168, 173, 176, 177, 205, 209, 221, 228, 229, 231, 233, 234, 248, 250, 261, 263, 278, 279, 285, 287, 291, 292, 295, 298, 299, 300, 303, 304, 305, 307, 310, 311, 318, 324, 326, 335, 336], "huddleston": [205, 211], "hudi": [7, 43], "hug": [205, 207, 209, 212, 213, 216, 224, 225, 228, 229, 232, 234, 268, 269, 270, 277, 287, 290, 291, 295, 298, 300], "huge": [213, 220], "huggingfac": [4, 5, 30, 34, 205, 208, 278, 279, 285, 287, 290, 291, 292, 295, 298, 299], "huggingface_hub": [295, 298], "hugo": [213, 220], "hulk": [205, 211], "human": [8, 51, 178, 188, 205, 211], "humbl": [213, 219], "humor": [213, 219, 220], "hundr": [255, 256], "hunt": [205, 211], "hurt": [205, 211], "hustl": [205, 211], "hvar": [205, 211], "hxwxc": [137, 141], "hybrid": [248, 254, 295, 301], "hydrologi": [255, 256], "hyperband": [6, 41, 305, 325], "hyperparam": [261, 267], "hyperparamet": [4, 5, 7, 26, 32, 36, 38, 40, 42, 46, 178, 182, 183, 189, 197, 200, 202, 235, 241, 242, 247, 248, 254, 255, 258, 260, 261, 267, 268, 277, 303, 304, 310, 311, 315, 321, 323, 324, 325], "hypnot": [213, 220], "i": [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 28, 29, 31, 32, 33, 35, 36, 38, 39, 40, 41, 43, 44, 46, 49, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 78, 80, 81, 83, 84, 88, 89, 91, 95, 97, 99, 100, 101, 105, 106, 107, 109, 110, 112, 117, 118, 119, 120, 122, 123, 133, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 173, 176, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 191, 192, 194, 195, 196, 198, 199, 200, 202, 205, 207, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 223, 226, 227, 228, 233, 234, 235, 236, 237, 239, 241, 242, 243, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 261, 262, 263, 264, 265, 266, 268, 269, 270, 271, 273, 274, 275, 277, 280, 282, 283, 284, 288, 291, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 307, 308, 313, 314, 315, 317, 319, 321, 322, 323, 324, 325, 326, 327, 329, 330, 331, 332, 333, 334, 335, 336], "iam": [76, 77, 79, 80, 82, 83, 84, 85, 86, 90, 91, 92, 94, 95, 99, 100, 101, 111, 112, 119, 122, 125, 126], "ic": [205, 211], "iceberg": [7, 9, 43, 59], "ichiro": [205, 211], "icon": [151, 163, 213, 220, 295, 299], "id": [12, 79, 84, 86, 91, 94, 95, 99, 101, 103, 104, 109, 112, 115, 116, 122, 125, 126, 131, 132, 137, 141, 142, 145, 146, 150, 154, 162, 166, 178, 189, 213, 217, 218, 219, 220, 221, 226, 249, 268, 270, 295, 298, 300], "idea": [84, 86, 99, 101, 109, 112, 213, 220], "ideal": [3, 7, 28, 43, 46, 151, 163, 213, 220, 255, 257, 261, 267, 287, 289, 294], "ident": [73, 78, 80, 83, 91, 95, 99, 101, 109, 112, 122, 126, 157, 169, 178, 179, 188, 193], "identifi": [84, 86, 99, 101, 109, 112, 137, 141, 178, 187, 188, 287, 290, 295, 298], "idiot": [205, 211, 213, 220], "idl": [84, 86, 99, 101, 109, 112, 278, 279, 282, 283], "idx": [5, 35, 178, 191, 248, 254, 261, 263, 268, 271, 277], "idx1": [304, 305, 318, 326], "idx2item": [248, 254], "idx3": [304, 305, 318, 324, 326], "ig": [205, 211], "ignor": [235, 239, 242, 246], "iid": [248, 250, 254], "ill": [205, 211], "illustr": [9, 59, 80, 82, 161, 205, 211, 235, 241], "iloc": [5, 35, 248, 254, 261, 263, 267, 268, 271], "im": [205, 211, 213, 220], "imag": [2, 4, 5, 6, 7, 9, 21, 29, 31, 32, 35, 36, 39, 40, 41, 43, 59, 60, 61, 62, 67, 80, 82, 137, 141, 142, 145, 150, 153, 156, 160, 162, 165, 168, 177, 180, 181, 182, 186, 191, 193, 194, 195, 199, 213, 220, 238, 241, 277, 278, 279, 285, 287, 292, 304, 305, 306, 307, 313, 314, 315, 318, 319, 322, 323, 324, 326, 329, 330, 335, 336], "image_arr": [178, 196], "image_arrai": [137, 141], "image_batch": [307, 336], "image_byt": [235, 237, 268, 270, 271, 277], "image_bytes_raw": [235, 237], "image_classifi": [307, 336], "image_classifier_ingress": [307, 336], "image_height": [137, 141], "image_id": [9, 61, 137, 141], "image_latents_256": [5, 35], "image_uri": [137, 141, 278, 279, 285, 287, 292], "image_width": [137, 141], "imagebatchpredictor": [268, 277], "imagenet": [178, 181, 268, 270, 271], "imagenet_mean": [268, 271, 277], "imagenet_std": [268, 271, 277], "imageri": [213, 220], "imageserviceingress": [307, 336], "imagin": [213, 219, 220], "imbalanc": [255, 257], "imdb": [213, 215, 216, 217, 219, 220], "img": [4, 9, 31, 32, 60, 178, 180, 191, 235, 237, 241, 268, 270, 271, 277], "immatur": [7, 47], "immedi": [1, 2, 7, 9, 15, 22, 46, 60, 156, 160, 168, 177, 178, 202, 235, 240, 268, 271, 278, 279, 282], "immut": [2, 18], "impact": [7, 47, 153, 165, 213, 220, 278, 279, 282, 283, 295, 301], "implement": [1, 4, 5, 6, 7, 8, 9, 16, 32, 35, 36, 40, 43, 47, 48, 54, 61, 62, 67, 68, 73, 77, 142, 145, 146, 156, 160, 168, 177, 178, 181, 182, 192, 205, 207, 210, 268, 269, 305, 306, 323, 330, 333, 334], "implementaiton": [304, 316], "impli": [213, 216], "implicit": [235, 241], "import": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 16, 17, 22, 26, 28, 29, 31, 33, 38, 49, 56, 67, 71, 72, 137, 140, 141, 142, 144, 145, 146, 151, 152, 153, 154, 155, 156, 159, 160, 163, 164, 165, 166, 167, 168, 174, 175, 177, 187, 207, 215, 220, 223, 231, 234, 239, 241, 246, 252, 256, 260, 265, 267, 273, 277, 278, 279, 285, 287, 290, 291, 292, 293, 295, 298, 299, 300, 303, 304, 305, 306, 307, 310, 311, 313, 315, 316, 317, 318, 322, 323, 324, 329, 330, 335, 336], "import_path": [156, 160, 168, 177, 278, 279, 285, 287, 292, 295, 299], "importance_typ": [255, 259], "imposs": [213, 220], "impress": [213, 220], "improv": [1, 8, 9, 10, 16, 54, 55, 57, 66, 68, 136, 137, 140, 148, 178, 192, 235, 241, 255, 256, 260, 261, 265, 267, 295, 298, 299], "imshow": [4, 6, 9, 31, 32, 39, 60, 178, 180, 191, 235, 237, 241, 268, 270, 277, 304, 305, 307, 313, 319, 322, 336], "in_channel": [4, 5, 31, 35, 178, 181, 304, 313], "in_count": [154, 166], "in_featur": [304, 313, 319], "in_proj": [261, 264], "inaccess": [137, 141], "incept": [235, 241], "includ": [0, 2, 3, 4, 7, 8, 9, 10, 21, 22, 28, 31, 32, 43, 46, 47, 51, 64, 69, 73, 76, 80, 82, 84, 90, 99, 108, 109, 121, 136, 142, 144, 148, 149, 151, 152, 153, 154, 155, 156, 159, 163, 164, 165, 166, 167, 168, 173, 178, 190, 200, 203, 221, 223, 226, 227, 229, 231, 248, 250, 252, 261, 265, 268, 275, 278, 279, 283, 295, 298, 303, 304, 308, 310, 318, 319], "include_path": [9, 59, 61, 306, 307, 329, 336], "incom": [268, 277, 307, 334], "incomplet": [178, 186], "incorpor": [7, 43, 268, 269], "incorrect_squar": [2, 20], "increas": [137, 141, 178, 204, 229, 234, 235, 241, 248, 252, 278, 279, 282, 287, 293], "increasingli": [11, 72], "increment": [7, 43, 156, 160, 168, 176, 268, 269], "incur": [7, 47, 142, 145], "independ": [8, 9, 10, 55, 59, 69, 153, 165, 178, 184, 192, 196, 261, 262, 265, 268, 271, 287, 293], "index": [0, 3, 7, 28, 43, 142, 144, 159, 160, 173, 176, 178, 180, 191, 229, 231, 248, 250, 255, 257, 258, 261, 263, 268, 271, 277], "index_col": [255, 258], "indi": [213, 216], "indian": [205, 211], "indic": [2, 11, 22, 72, 178, 191, 248, 249, 250, 254, 255, 259, 268, 271], "individu": [1, 3, 16, 27, 137, 140, 303, 309], "indonesiasayshbdforjustinbieb": [205, 211], "industri": 339, "ineffect": [213, 219], "ineffici": [205, 207, 278, 279, 282], "infer": [2, 7, 8, 9, 10, 22, 25, 26, 46, 47, 48, 51, 55, 57, 62, 66, 70, 152, 155, 159, 160, 164, 167, 173, 176, 179, 180, 188, 190, 204, 206, 211, 214, 222, 229, 230, 233, 235, 236, 241, 242, 243, 247, 250, 256, 258, 262, 263, 269, 283, 288, 294, 295, 298, 301, 303, 306, 307, 310, 311, 330, 332, 335], "inference_mod": [178, 191], "inference_row": [268, 277], "inferf": [248, 249], "infinit": [9, 61], "influenc": [213, 220, 242, 243], "info": [11, 72, 142, 145, 205, 211, 287, 293, 303, 305, 310, 311, 324, 326], "inform": [4, 32, 99, 100, 109, 110, 122, 123, 136, 137, 141, 142, 144, 146, 149, 153, 165, 178, 182, 205, 211, 229, 231, 268, 275, 295, 300, 304, 319], "infrastructur": [4, 5, 11, 30, 34, 72, 74, 75, 81, 82, 91, 92, 135, 137, 140, 141, 151, 156, 158, 160, 161, 163, 168, 170, 171, 176, 178, 179, 201, 242, 243, 248, 254, 268, 269, 283, 287, 292, 294, 295, 298, 302], "ingest": [3, 28, 178, 192, 197, 204, 255, 256, 257, 260, 268, 269, 303, 306, 310, 329], "ingmar": [213, 216], "ingress": [3, 28, 80, 82, 83, 107, 108, 120, 121, 134, 229, 233, 307, 336], "ingress_from_cidr_map": [73, 78], "ingress_with_self": [73, 78], "inher": 161, "inherit": [3, 27, 156, 160, 168, 177, 303, 309], "ini": [136, 149], "init": [11, 72, 84, 86, 91, 95, 99, 101, 109, 112, 122, 126, 154, 166, 178, 180, 205, 209, 213, 216, 221, 227], "initi": [2, 5, 7, 9, 11, 19, 35, 36, 47, 62, 72, 80, 82, 84, 86, 91, 95, 99, 101, 109, 112, 122, 126, 156, 168, 178, 179, 189, 191, 205, 206, 209, 210, 214, 215, 221, 222, 223, 226, 227, 230, 248, 249, 261, 262, 295, 298, 304, 306, 313, 315, 330], "initial_replica": [307, 336], "inject": [142, 145, 178, 183, 186, 235, 236, 242, 243, 244, 245, 268, 272], "inlin": [229, 233], "inner": [213, 219], "inning": [205, 209, 211], "innings": [205, 209, 211], "innoc": [213, 219, 220], "inplac": [255, 257, 304, 313, 319], "input": [5, 8, 9, 10, 35, 51, 53, 54, 57, 61, 62, 63, 64, 65, 70, 142, 144, 153, 165, 178, 180, 181, 182, 192, 204, 205, 210, 213, 218, 221, 226, 229, 233, 238, 255, 256, 261, 262, 263, 264, 265, 267, 278, 279, 281, 303, 306, 307, 310, 311, 329, 331, 332, 336], "input_window": [261, 263, 264, 265, 267], "inputdatabuff": [9, 63], "inscrut": [7, 46], "insert": [278, 279, 282], "insid": [2, 8, 23, 25, 55, 142, 144, 145, 153, 154, 158, 165, 166, 171, 178, 180, 183, 197, 213, 216, 235, 239, 242, 243, 255, 258, 260, 268, 269, 271, 277], "insight": [7, 46, 137, 139, 141], "inspect": [2, 3, 4, 5, 22, 28, 31, 32, 35, 153, 156, 165, 168, 179, 180, 204, 205, 207, 235, 237, 270, 304, 313, 319], "inspir": [205, 211], "instal": [2, 3, 21, 26, 80, 81, 83, 84, 85, 86, 93, 100, 101, 107, 108, 110, 111, 112, 120, 121, 123, 124, 128, 130, 135, 151, 152, 153, 163, 164, 165, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 287, 291], "instanc": [2, 3, 4, 8, 9, 11, 22, 25, 28, 31, 32, 54, 57, 64, 72, 77, 79, 80, 82, 86, 90, 95, 99, 106, 109, 118, 122, 126, 137, 141, 142, 145, 146, 150, 152, 154, 162, 164, 166, 178, 180, 186, 191, 205, 207, 213, 220, 233, 287, 293, 303, 306, 310, 311, 331], "instance_iam_role_arn": [73, 79], "instance_profile_arn": [84, 86], "instance_typ": [99, 106, 109, 118, 137, 141], "instanceid": [84, 86, 99, 101, 109, 112], "instant": [151, 163], "instanti": [2, 5, 25, 35, 268, 269, 274, 307, 335], "instead": [2, 3, 4, 6, 8, 19, 20, 24, 28, 32, 41, 51, 53, 54, 153, 154, 156, 160, 165, 166, 168, 177, 178, 179, 184, 185, 192, 193, 194, 199, 205, 207, 210, 213, 219, 220, 235, 236, 248, 249, 250, 255, 256, 258, 259, 261, 262, 268, 270, 273, 304, 305, 316, 317, 326], "instruct": [80, 81, 152, 161, 164, 278, 279, 285, 287, 289, 290, 293, 295, 298, 299, 301], "instrument": [146, 205, 211], "insubordin": [213, 219], "int": [2, 3, 4, 5, 6, 9, 18, 20, 22, 28, 31, 32, 35, 39, 40, 41, 61, 64, 137, 141, 178, 186, 187, 191, 194, 200, 235, 237, 238, 242, 244, 245, 248, 251, 255, 259, 260, 261, 263, 265, 268, 270, 272, 273, 277, 304, 305, 306, 313, 316, 317, 322, 323, 325, 331], "int32": [255, 259, 261, 263], "int4": [287, 293], "int64": [205, 209, 212, 213, 217, 218, 242, 244, 248, 250], "intact": [178, 182], "integ": [178, 196, 248, 249, 250, 268, 269, 303, 310], "integr": [3, 4, 5, 7, 8, 27, 30, 32, 34, 43, 46, 47, 48, 55, 73, 77, 80, 83, 135, 136, 137, 141, 142, 145, 148, 149, 150, 156, 160, 162, 168, 176, 179, 186, 193, 194, 197, 205, 207, 213, 215, 221, 227, 229, 231, 236, 242, 243, 246, 247, 248, 249, 255, 260, 261, 263, 268, 269, 277, 278, 279, 284, 287, 293, 295, 297, 299, 300, 302, 303, 304, 309, 310, 311, 316], "intellig": [7, 43, 295, 300], "intend": [153, 156, 165, 168, 213, 216, 221, 227], "intens": [2, 3, 6, 7, 11, 22, 28, 39, 47, 72, 178, 192, 213, 220, 278, 279, 283, 287, 289, 305, 322], "intent": [213, 219, 220, 235, 236, 255, 256], "interact": [7, 46, 80, 82, 151, 158, 161, 163, 171, 213, 220, 248, 249, 250, 255, 260, 278, 279, 283, 284, 295, 300, 302, 307, 336], "interconnect": [287, 293], "interest": [205, 211, 213, 216], "interfac": [3, 4, 7, 27, 31, 45, 136, 149, 178, 180, 192, 229, 231, 248, 252], "intermedi": [2, 4, 9, 19, 32, 57, 178, 188, 255, 260, 261, 267, 268, 277, 278, 279, 281, 304, 317], "intern": [6, 42, 73, 78, 137, 139, 178, 193, 248, 252, 254, 268, 272], "internet": [73, 78], "interoper": [7, 43], "interpret": [248, 254, 255, 260, 261, 267], "interrupt": [152, 164, 178, 200, 202, 261, 266, 268, 269], "interv": [5, 7, 35, 48, 261, 262, 263, 307, 336], "intervent": [178, 200, 248, 253, 261, 266, 268, 269], "interview": [8, 55, 205, 211, 306, 332], "intrigu": [213, 219, 220], "intro": [10, 71, 99, 100, 109, 111, 122, 124, 303, 310, 311], "introduc": [1, 7, 10, 13, 46, 67, 136, 137, 140, 147, 151, 163, 307, 333], "introduct": [192, 221, 223, 306, 307, 321, 327, 333], "introductori": [161, 312], "intuit": [7, 48], "invari": [213, 216], "invent": [213, 219, 220, 261, 262], "invert_yaxi": [255, 259], "invest": [205, 209, 211], "invit": [158, 171], "invoc": [1, 14, 16], "invok": [1, 2, 15, 16, 23, 255, 260], "involv": [8, 9, 54, 64, 205, 211, 213, 219, 220, 229, 231, 278, 279, 285, 306, 331], "io": [0, 9, 11, 17, 58, 72, 99, 100, 102, 104, 109, 111, 114, 116, 122, 124, 129, 132, 178, 180, 205, 209, 221, 228, 229, 231, 233, 235, 237, 261, 263, 268, 270, 271, 277, 305, 326], "iot": [7, 46], "ip": [73, 76, 78, 84, 86, 99, 101, 109, 112, 122, 129, 178, 189, 268, 269, 304, 305, 318, 319, 326], "ipykernel": [11, 72], "ipynb": [151, 153, 161, 163, 165], "iran": [205, 211], "iron": [205, 211], "irrupt": [205, 211], "is_avail": [4, 6, 31, 32, 40, 221, 226, 235, 241, 242, 247, 261, 267, 268, 277], "is_big_tip": [303, 310, 311], "isdir": [178, 203, 268, 277], "isfil": [268, 277], "isinst": [2, 19], "islam": [205, 211], "isn": [4, 32, 109, 119, 178, 186, 205, 211, 213, 216, 235, 241, 268, 269, 278, 279, 283, 304, 316], "isol": [7, 10, 43, 69, 73, 75, 78], "issu": [1, 4, 5, 7, 13, 30, 34, 46, 109, 119, 122, 127, 136, 137, 141, 148, 154, 166, 178, 179, 191, 200, 213, 216, 268, 269, 295, 302], "itali": [205, 211], "itbr": [213, 220], "item": [1, 2, 4, 5, 6, 9, 16, 24, 31, 32, 35, 39, 41, 61, 136, 137, 141, 149, 178, 187, 213, 220, 221, 226, 235, 237, 242, 244, 251, 252, 255, 259, 261, 265, 267, 268, 273, 277, 295, 298, 304, 305, 313, 317, 319, 322, 326], "item2idx": [248, 250, 254], "item_col": [248, 250], "item_embed": [248, 251, 254], "item_id": [248, 250, 254], "item_idx": [248, 249, 250, 251, 252], "item_metadata": [248, 254], "item_vec": [248, 251], "item_vector": [248, 254], "iter": [8, 9, 10, 52, 58, 68, 150, 151, 162, 163, 178, 182, 194, 221, 226, 235, 236, 239, 241, 242, 246, 247, 248, 250, 252, 253, 255, 260, 261, 263, 278, 279, 282, 295, 301, 303, 305, 310, 311, 324, 326], "iter_torch_batch": [178, 192, 194, 204, 235, 239, 242, 246, 248, 249, 252, 254], "iterations_since_restor": [304, 319], "iterrow": [248, 254], "its": [1, 2, 4, 6, 7, 8, 9, 10, 13, 18, 22, 31, 41, 43, 53, 55, 59, 69, 109, 119, 137, 140, 141, 142, 144, 145, 153, 156, 158, 160, 165, 168, 171, 176, 178, 179, 180, 182, 189, 190, 191, 205, 207, 211, 213, 220, 229, 231, 234, 235, 236, 237, 248, 249, 252, 255, 256, 257, 258, 259, 261, 262, 268, 271, 273, 278, 279, 282, 284, 287, 293, 305, 306, 326, 329, 332], "itself": [8, 51, 137, 141], "iv": [213, 220], "j": [0, 1, 16, 205, 211, 248, 250, 254], "jack": [213, 219], "jackson": [205, 211], "jadwal": [205, 211], "jai": [205, 209, 211, 213, 219], "jail": [205, 211], "jame": [205, 211, 213, 219], "jami": [205, 211], "jan": [205, 211], "janet": [205, 211], "januari": [205, 209, 211], "java": [7, 46], "jean": [213, 220], "jeanmarc": [213, 220], "jeff": [213, 219], "jgz99": [287, 292], "jit": [9, 10, 62, 70, 306, 307, 330, 335], "job": [3, 5, 6, 28, 30, 34, 36, 41, 73, 78, 80, 82, 83, 84, 88, 90, 91, 97, 99, 106, 109, 118, 119, 122, 133, 137, 139, 141, 142, 144, 152, 153, 154, 157, 158, 164, 165, 166, 169, 171, 178, 179, 181, 182, 184, 188, 189, 191, 197, 198, 200, 201, 204, 205, 209, 211, 212, 213, 219, 220, 235, 241, 242, 247, 248, 254, 255, 256, 260, 261, 267, 268, 269, 274, 277, 295, 298, 305, 312, 314, 319, 324, 339], "job_config": [155, 159, 167, 175], "job_descript": [295, 298], "job_id": [155, 159, 167, 175], "jobconfig": [155, 159, 167, 175], "joe": [205, 211], "john": [205, 209, 211, 213, 216, 219], "johnson": [205, 211, 213, 216], "joi": [205, 211], "join": [4, 5, 7, 8, 31, 32, 36, 47, 55, 73, 78, 137, 141, 146, 153, 165, 178, 188, 191, 199, 200, 205, 211, 215, 217, 220, 235, 239, 241, 242, 246, 247, 250, 252, 255, 257, 258, 261, 263, 265, 267, 268, 270, 273, 277, 295, 298, 302, 304, 313, 317, 319], "join_typ": [213, 219], "joint": [235, 236, 261, 262, 268, 270], "joke": [287, 291], "journei": [146, 213, 220], "jpeg": [7, 43, 235, 236, 237, 241, 268, 270], "jq": [84, 86, 99, 101, 109, 112], "json": [3, 7, 9, 10, 28, 43, 59, 67, 70, 142, 145, 146, 229, 234, 235, 237, 242, 244, 248, 250, 255, 257, 268, 270, 273, 279, 285, 296, 297, 298, 300, 302, 303, 307, 310, 311, 335, 336], "json_method1": [295, 299], "json_request": [10, 70, 142, 145, 307, 335, 336], "json_schema": [295, 299], "juda": [205, 211], "judg": [213, 219, 255, 257], "juggl": [10, 68], "jule": [205, 211], "jump": [10, 70, 213, 220, 242, 243, 307, 335], "jun": [205, 211], "june": [3, 28, 122, 125, 161, 303, 310], "jupyt": [152, 164], "jupyter_execute_notebook": 0, "jupyterlab": [151, 163], "just": [6, 8, 40, 41, 51, 53, 84, 88, 91, 97, 99, 106, 109, 118, 122, 133, 151, 157, 163, 169, 178, 192, 200, 205, 211, 213, 216, 217, 219, 220, 235, 236, 237, 248, 249, 268, 270, 277, 278, 279, 283, 295, 300, 305, 306, 325, 329], "justifi": [205, 211, 213, 219], "justin": [205, 211], "j\u00e4reg\u00e5rd": [213, 220], "k": [205, 209, 211, 221, 226, 242, 243, 247, 248, 254, 255, 256, 261, 267, 268, 277, 278, 279, 282], "k8": [73, 74, 76, 83, 99, 101, 102, 106, 109, 112, 114, 118, 122, 125, 133, 135, 136, 149, 339], "kafka": [7, 46], "katharina": [213, 220], "ke": [205, 211], "keep": [5, 35, 152, 157, 164, 169, 178, 179, 181, 182, 188, 191, 192, 193, 200, 203, 205, 211, 212, 213, 218, 235, 237, 239, 242, 243, 246, 247, 248, 249, 250, 252, 255, 257, 258, 261, 265, 267, 268, 270, 273, 274, 275, 277, 278, 279, 285], "keeper": [213, 219], "kei": [6, 7, 8, 9, 10, 31, 41, 43, 54, 56, 64, 69, 76, 78, 80, 82, 84, 86, 99, 101, 109, 112, 142, 144, 153, 161, 165, 178, 180, 181, 182, 188, 193, 199, 221, 223, 248, 254, 255, 258, 259, 268, 273, 280, 281, 285, 293, 305, 306, 325, 331], "kenni": [205, 211], "kept": [205, 211, 248, 250], "kernel": [11, 72, 80, 83, 178, 181, 278, 279, 284], "kernel_s": [4, 6, 31, 40, 41, 178, 181, 304, 305, 313, 319, 323, 326], "kerri": [205, 209, 211], "kessler": [213, 220], "keylogg": [295, 298], "kick": [248, 252, 268, 274], "kiddo": [205, 211], "kill": [6, 41, 178, 191, 213, 216, 219, 220, 235, 237, 305, 325], "kim": [205, 211], "kind": [11, 72, 255, 257], "kingdom": [213, 220], "kitchen": [205, 211], "klaviyo": [307, 337], "klondik": [213, 219], "km": [73, 78], "know": [2, 7, 24, 47, 137, 141, 178, 198, 200, 213, 219, 220, 261, 262, 295, 298], "knowledg": [135, 142, 143, 255, 259, 260, 287, 294], "known": [235, 236], "kong": [213, 219], "kpop": [205, 211], "kri": [205, 211], "kube": [99, 102, 107, 109, 114, 119, 120], "kubeconfig": [99, 102, 109, 114], "kubectl": [99, 100, 102, 105, 106, 107, 109, 111, 114, 117, 118, 119, 120, 124, 129, 134], "kuberai": [136, 149], "kubernet": [10, 68, 73, 75, 76, 100, 101, 103, 108, 111, 112, 115, 121, 124, 125, 129, 131, 134, 135], "kucinich": [205, 211], "kueue": [80, 82], "kurt": [205, 211], "kv": [255, 259, 281, 283, 286, 287, 293], "kv_cache_util": [287, 293], "kvedzwag2qa8i5bj": [287, 292], "l": [3, 4, 8, 9, 28, 31, 32, 51, 53, 59, 109, 119, 235, 236, 242, 243, 248, 249, 268, 269, 304, 306, 313, 329], "l4": [278, 279, 285, 295, 298, 299], "l40": [287, 290, 291, 293, 295, 300], "l6": [205, 210], "lab": [6, 41, 305, 325], "label": [3, 4, 6, 9, 10, 28, 31, 32, 39, 40, 41, 61, 62, 64, 70, 142, 143, 178, 180, 182, 191, 193, 194, 195, 196, 199, 205, 209, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 225, 226, 229, 234, 235, 237, 239, 241, 242, 246, 248, 252, 255, 257, 258, 259, 260, 261, 265, 267, 270, 271, 275, 277, 303, 304, 305, 306, 310, 311, 313, 315, 318, 319, 322, 323, 326, 330, 331], "label_col": [255, 258], "label_column": [3, 28, 255, 258, 303, 310, 311], "label_nam": [235, 237, 268, 270, 277], "labeled_batch": [9, 61], "labia": [213, 216], "labour": [205, 211], "lack": [7, 43, 47], "lag": [261, 267], "lai": [205, 211], "lakehous": [9, 59, 178, 192, 204], "lambda": [5, 35, 137, 141, 153, 165, 213, 218, 255, 259, 260], "land": [0, 255, 256], "landscap": [46, 229, 234, 339], "languag": [4, 5, 7, 32, 37, 43, 45, 46, 248, 254, 281, 304, 320], "laptop": [11, 72, 213, 215], "lar": [213, 220], "larami": [213, 219], "larg": [0, 3, 4, 5, 7, 8, 9, 17, 18, 19, 25, 28, 32, 35, 36, 43, 46, 55, 57, 137, 141, 153, 165, 178, 198, 205, 207, 210, 212, 213, 215, 218, 220, 221, 223, 226, 227, 228, 229, 231, 235, 236, 237, 241, 248, 250, 254, 255, 256, 257, 260, 261, 262, 267, 268, 270, 281, 282, 283, 287, 289, 294, 295, 301, 304, 306, 314, 328], "large_mat_from_object_stor": [2, 18], "large_matrix": [2, 18], "larger": [7, 47, 153, 165, 178, 204, 213, 218, 235, 241, 242, 247, 248, 249, 268, 277, 307, 336], "largest": [295, 298], "last": [2, 4, 5, 7, 24, 30, 34, 46, 47, 109, 113, 156, 160, 168, 177, 179, 186, 190, 198, 199, 200, 205, 209, 211, 212, 221, 228, 248, 252, 253, 255, 258, 260, 261, 263, 265, 268, 271, 275, 278, 279, 282, 295, 300, 306, 328], "last_login": 146, "lastmodifi": [153, 165], "latenc": [7, 10, 46, 48, 68, 142, 145, 153, 165, 242, 247, 268, 277, 282, 286, 287, 293, 295, 301], "latent": [5, 35, 248, 249], "later": [6, 40, 146, 151, 163, 178, 180, 184, 188, 190, 195, 196, 213, 220, 235, 237, 238, 242, 245, 248, 249, 250, 252, 255, 257, 268, 271, 278, 279, 284, 304, 305, 307, 313, 323, 336], "latest": [4, 11, 32, 72, 84, 85, 99, 100, 109, 111, 122, 130, 151, 163, 178, 190, 198, 199, 200, 201, 202, 205, 209, 213, 219, 229, 231, 233, 239, 242, 243, 246, 247, 248, 249, 252, 253, 254, 261, 262, 265, 266, 267, 268, 269, 276, 304, 319], "latin": [248, 254], "laugh": [205, 211], "laughter": [205, 211], "launch": [1, 2, 3, 5, 6, 8, 9, 10, 12, 17, 24, 26, 29, 33, 38, 49, 56, 67, 73, 75, 143, 152, 156, 157, 158, 160, 164, 168, 169, 171, 176, 182, 184, 191, 205, 207, 213, 215, 218, 221, 223, 229, 231, 236, 243, 255, 256, 258, 260, 262, 267, 269, 278, 279, 280, 288, 295, 296, 312, 315], "layer": [3, 27, 73, 75, 178, 181, 235, 241, 248, 250, 261, 262, 287, 290, 293], "layer1": [304, 313, 319], "layer2": [304, 313, 319], "layer3": [304, 313, 319], "layer4": [304, 313, 319], "layers_per_block": [5, 35], "layout": [248, 250], "lazi": [8, 51, 52, 56, 205, 211, 212, 248, 250, 255, 257, 306, 330], "lbc": [99, 102, 109, 114], "lbl": [235, 237], "le": [205, 211], "lead": [2, 7, 22, 24, 47, 205, 211, 213, 220], "leader": [205, 211], "leagu": [205, 211], "leakag": [261, 263], "lean": [268, 269], "learn": [2, 3, 4, 5, 6, 8, 9, 20, 22, 27, 32, 35, 41, 43, 46, 55, 66, 84, 90, 137, 138, 139, 180, 182, 190, 205, 206, 207, 210, 212, 213, 214, 215, 216, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 241, 247, 250, 251, 252, 254, 257, 263, 265, 267, 270, 277, 278, 279, 280, 287, 294, 302, 304, 305, 306, 316, 319, 325, 332], "learning_r": [178, 183], "least": [2, 20, 22, 73, 78, 80, 82, 83, 278, 279, 285], "leav": [4, 31, 84, 86, 99, 101, 150, 152, 156, 162, 164, 168, 213, 219, 235, 239, 255, 257, 260, 261, 267, 268, 277], "lecun": [304, 305, 318, 324], "left": [1, 7, 16, 48, 151, 152, 163, 164, 213, 220, 248, 253, 254, 255, 260, 261, 266, 278, 279, 282], "leftarrow": [235, 236, 242, 243], "leftov": [178, 203], "legend": [235, 239, 242, 246, 248, 252, 261, 265, 267, 268, 275], "leigh": [205, 211], "lemieux": [205, 211], "len": [2, 4, 5, 24, 31, 32, 35, 154, 166, 178, 180, 191, 213, 217, 229, 233, 248, 250, 252, 254, 255, 257, 258, 259, 260, 261, 263, 265, 268, 270, 271], "lena": [213, 216], "length": [9, 57, 84, 86, 99, 101, 109, 112, 229, 233, 261, 263, 278, 279, 281, 282, 283, 287, 290, 295, 301], "leo": [213, 220], "leopold": [213, 220], "lesnar": [205, 211], "less": [213, 216, 235, 241, 295, 298], "lesson": [303, 308], "let": [1, 2, 3, 4, 5, 6, 8, 9, 10, 16, 18, 19, 20, 22, 24, 25, 26, 28, 31, 32, 35, 36, 39, 40, 41, 51, 52, 53, 54, 59, 60, 62, 64, 65, 70, 91, 95, 99, 101, 102, 109, 114, 142, 144, 151, 152, 153, 156, 157, 163, 164, 165, 168, 169, 178, 180, 181, 182, 186, 188, 235, 236, 237, 238, 239, 242, 243, 248, 249, 255, 256, 257, 261, 265, 268, 270, 272, 277, 278, 279, 283, 285, 286, 287, 289, 290, 291, 292, 293, 294, 295, 297, 298, 299, 300, 302, 303, 304, 305, 306, 307, 308, 313, 314, 315, 318, 322, 323, 324, 329, 331, 332, 335, 336, 338], "level": [0, 4, 5, 6, 7, 10, 17, 20, 31, 32, 36, 40, 46, 69, 73, 78, 80, 83, 136, 137, 140, 142, 145, 148, 154, 157, 166, 169, 178, 181, 189, 213, 216, 248, 250, 252, 261, 262, 267, 268, 269, 278, 279, 282, 284, 304, 305, 307, 313, 315, 323, 334], "leverag": [7, 9, 10, 43, 57, 66, 68, 80, 81, 142, 145, 205, 207, 212, 213, 215, 221, 223, 227, 235, 241, 242, 247, 255, 260], "lexu": [205, 211, 295, 299], "lgbm": [4, 5, 30, 34, 178, 179], "lh": [8, 53], "li": [213, 220], "liar": [205, 211], "lib": [153, 165, 205, 211, 339], "libomp": [3, 26], "librari": [1, 4, 6, 7, 8, 9, 10, 13, 32, 40, 41, 46, 47, 50, 51, 59, 61, 62, 64, 69, 70, 137, 141, 150, 153, 154, 162, 165, 166, 178, 180, 206, 207, 214, 215, 222, 223, 225, 227, 230, 231, 234, 235, 237, 242, 244, 248, 250, 261, 263, 268, 270, 305, 306, 307, 323, 324, 328, 329, 330, 331, 335, 336], "licens": [11, 72], "lie": [205, 211, 242, 244], "life": [205, 211, 213, 216, 220], "lifecycl": [7, 10, 48, 69, 80, 81, 82, 153, 155, 159, 161, 165, 167, 173, 175, 179, 304, 317], "lift": [150, 162], "light": [0, 9, 61, 205, 211], "lightli": 161, "lightn": [4, 30, 34, 236, 237, 238, 241, 242, 243, 244, 246, 247, 339], "lightning_training_loop": [5, 35], "lightningmodul": [5, 35, 236, 243], "lightweight": [10, 68, 154, 166, 205, 210, 248, 249, 250, 254, 255, 257, 261, 263, 267, 268, 269], "lik": [213, 220], "like": [2, 3, 4, 5, 6, 7, 8, 9, 19, 20, 21, 22, 28, 31, 32, 36, 41, 43, 46, 47, 48, 51, 53, 55, 62, 73, 79, 80, 82, 83, 84, 86, 87, 91, 94, 95, 96, 97, 99, 101, 103, 109, 112, 115, 118, 122, 131, 151, 152, 163, 164, 178, 179, 182, 183, 187, 189, 194, 205, 211, 213, 215, 216, 219, 220, 221, 227, 229, 231, 234, 235, 241, 242, 247, 248, 249, 250, 254, 255, 256, 258, 278, 279, 284, 287, 290, 291, 295, 299, 304, 305, 306, 307, 313, 314, 315, 325, 329, 330, 336], "likeeeeeeeee": [205, 211], "likelihood": [268, 269], "limit": [6, 7, 8, 41, 43, 53, 62, 80, 83, 84, 86, 99, 101, 109, 112, 152, 153, 164, 165, 278, 279, 281, 295, 298], "limousin": [3, 8, 28, 51, 303, 310], "line": [136, 149, 151, 156, 160, 163, 168, 177, 213, 220, 235, 236, 261, 262, 268, 269], "linear": [2, 6, 25, 41, 242, 245, 261, 264, 304, 305, 313, 319, 325], "linearmodel": [2, 25], "lineup": [205, 209, 211, 212], "link": [0, 80, 82, 156, 168, 178, 184, 295, 297, 302], "linux": [136, 149], "list": [2, 3, 8, 9, 19, 28, 51, 53, 59, 65, 84, 86, 88, 89, 91, 94, 97, 99, 101, 105, 106, 107, 109, 112, 113, 117, 118, 120, 122, 125, 127, 133, 134, 137, 141, 151, 153, 155, 158, 163, 165, 167, 171, 178, 190, 191, 195, 213, 219, 235, 237, 242, 244, 248, 250, 254, 255, 257, 258, 261, 267, 278, 279, 283, 295, 298, 306, 329, 332], "list_": [261, 263], "list_objects_v2": [153, 165, 295, 298], "listbucket": [73, 78], "listbucketmultipartupload": [73, 78], "listdir": [268, 277], "listfil": [9, 63], "listmultipartuploadpart": [73, 78], "lit": [205, 209, 211], "lite": [235, 236, 237, 270], "liter": [205, 211], "littl": [205, 211, 213, 219, 220], "live": [205, 211, 213, 219, 229, 234, 248, 254, 261, 262], "ll": [4, 5, 6, 11, 32, 35, 40, 72, 73, 74, 91, 93, 99, 103, 106, 109, 115, 118, 122, 124, 126, 131, 137, 138, 150, 151, 154, 161, 162, 163, 166, 180, 181, 183, 184, 189, 204, 205, 211, 213, 219, 261, 262, 278, 279, 280, 286, 287, 288, 289, 290, 292, 296, 298, 305, 323], "llama": [278, 279, 285, 290, 291, 292, 293, 295, 298, 301], "llamafactoryai": [295, 298], "llm": [9, 57, 282, 286, 289, 291, 292, 294, 297, 299, 300, 302], "llm_config": [278, 279, 285, 287, 290, 293, 295, 298, 299, 300], "llmconfig": [278, 279, 285, 287, 290, 293, 295, 298, 300], "lo": [11, 72], "load": [2, 3, 7, 10, 22, 25, 28, 32, 35, 38, 43, 48, 49, 55, 56, 57, 62, 68, 70, 73, 78, 80, 83, 84, 90, 107, 108, 120, 121, 151, 152, 156, 160, 163, 164, 168, 176, 177, 179, 180, 186, 188, 192, 198, 200, 202, 204, 207, 210, 212, 215, 220, 221, 223, 224, 225, 226, 227, 228, 229, 231, 233, 236, 240, 241, 242, 247, 249, 252, 254, 256, 259, 260, 262, 265, 267, 269, 271, 273, 277, 278, 279, 284, 285, 287, 290, 292, 295, 298, 300, 303, 307, 310, 311, 319, 321, 327, 328, 330, 335, 336], "load_data": [3, 28], "load_dataset": [205, 208, 209, 213, 216, 221, 224, 226, 235, 237, 261, 263, 268, 270, 277], "load_from_checkpoint": [5, 36], "load_model": [3, 28, 255, 258, 303, 310, 311], "load_model_ray_train": [4, 32, 178, 182, 185, 193, 199, 304, 315, 316, 319], "load_model_torch": [4, 31, 304, 313], "load_state_dict": [4, 31, 32, 178, 191, 199, 235, 241, 242, 247, 248, 252, 254, 261, 265, 267, 268, 273, 277, 304, 313, 319], "loadbalanc": [80, 83], "loaded_df": [5, 35], "loaded_model": [4, 31, 304, 313], "loaded_model_ray_train": [4, 5, 32, 36, 304, 319], "loader": [4, 32, 178, 193, 194, 196, 199, 221, 223, 235, 239, 248, 249, 261, 263, 268, 269, 271, 272, 277, 315, 316], "loan": [205, 211], "loc": [303, 305, 310, 311, 324, 326], "local": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 17, 23, 26, 28, 29, 31, 32, 33, 38, 46, 49, 53, 56, 62, 65, 67, 68, 70, 71, 73, 78, 84, 87, 91, 96, 99, 103, 107, 109, 115, 120, 122, 131, 142, 143, 144, 145, 147, 148, 156, 160, 161, 168, 177, 178, 179, 180, 188, 191, 192, 205, 207, 209, 211, 213, 215, 221, 223, 227, 228, 229, 231, 235, 236, 237, 239, 242, 243, 246, 248, 249, 255, 256, 258, 261, 262, 268, 273, 277, 278, 279, 280, 285, 288, 294, 295, 296, 305, 306, 307, 317, 319, 324, 330, 334], "local_fil": [153, 165], "local_file_path": [295, 298], "local_idx": [268, 271], "local_path": [4, 9, 10, 31, 32, 62, 70, 142, 145, 295, 298, 304, 306, 307, 313, 330, 335, 336], "local_pred_fold": [9, 65], "local_storag": [5, 35, 304, 313], "local_zip": [248, 250], "localhost": [0, 3, 10, 28, 70, 142, 145, 229, 234, 278, 279, 285, 287, 291, 295, 298, 299, 300, 303, 307, 310, 311, 335, 336], "locat": [4, 11, 31, 32, 72, 91, 95, 122, 126, 142, 145, 153, 157, 165, 169, 178, 180, 188, 190, 255, 256, 295, 300], "lock": [11, 72, 154, 166, 206, 213, 214, 219, 222, 230], "lodg": [255, 256], "lofton": [205, 211], "log": [4, 5, 7, 9, 30, 32, 34, 35, 43, 46, 63, 73, 77, 78, 84, 87, 88, 91, 92, 96, 97, 99, 103, 106, 109, 115, 118, 119, 122, 131, 133, 136, 137, 139, 140, 141, 143, 146, 148, 151, 155, 156, 161, 163, 167, 168, 173, 178, 179, 180, 182, 187, 188, 189, 190, 193, 197, 202, 204, 221, 228, 235, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 252, 255, 256, 258, 260, 261, 264, 265, 267, 268, 269, 273, 278, 279, 284, 287, 293, 303, 304, 305, 310, 311, 317, 326, 339], "log_engine_metr": [287, 290, 293], "log_every_n_step": [5, 35], "log_level": [142, 145], "log_result": [154, 166], "logdir": [221, 228, 303, 304, 310, 311, 318], "logger": [142, 145], "logging_config": [142, 145], "logic": [2, 4, 5, 7, 9, 10, 22, 32, 36, 47, 48, 61, 69, 73, 75, 78, 178, 181, 193, 199, 221, 228, 229, 233, 234, 235, 236, 239, 241, 242, 243, 248, 249, 252, 261, 263, 268, 269, 273, 295, 301, 303, 304, 305, 307, 310, 311, 318, 324, 326, 336], "login": [84, 87, 91, 94, 96, 99, 103, 109, 115, 122, 125, 131, 151, 163], "logist": [303, 310, 311], "logit": [9, 10, 62, 70, 178, 181, 191, 221, 225, 226, 268, 273, 277, 306, 307, 330, 335], "logloss": [303, 310, 311], "loguniform": [6, 41, 305, 325, 326], "loki": [205, 211], "lol": [205, 211], "london": [205, 211], "long": [2, 3, 4, 5, 6, 22, 28, 30, 32, 34, 36, 40, 73, 77, 158, 171, 178, 179, 198, 205, 211, 213, 220, 248, 250, 252, 261, 262, 278, 279, 282, 283, 295, 301, 303, 304, 305, 310, 311, 314, 323], "longer": [4, 7, 32, 48, 178, 182, 191, 193, 203, 204, 235, 241], "longrightarrow": [242, 243, 261, 262], "look": [2, 3, 4, 6, 7, 8, 9, 25, 28, 31, 32, 41, 48, 54, 60, 64, 84, 86, 87, 91, 96, 99, 101, 103, 109, 112, 113, 115, 122, 131, 142, 144, 178, 180, 182, 194, 205, 209, 211, 213, 219, 220, 235, 241, 255, 257, 287, 294, 295, 298, 302, 304, 306, 307, 313, 315, 331, 335, 336], "look_back_period_": [307, 336], "lookup": [248, 249], "loop": [12, 30, 34, 36, 179, 180, 181, 183, 184, 188, 189, 190, 192, 196, 197, 198, 200, 201, 202, 204, 221, 223, 226, 235, 236, 239, 241, 247, 249, 256, 260, 262, 268, 269, 273, 277, 295, 300, 312, 317], "lora": [278, 279, 282, 286, 296, 297, 302], "lora_checkpoint": [295, 298], "lora_config": [295, 298], "lose": [178, 198, 201], "loss": [4, 5, 6, 31, 32, 35, 40, 41, 178, 180, 181, 182, 187, 188, 190, 193, 199, 213, 220, 221, 226, 227, 228, 236, 238, 243, 245, 247, 255, 256, 258, 260, 267, 269, 270, 273, 274, 303, 304, 305, 310, 311, 313, 315, 317, 318, 319, 323, 325, 326], "loss_fn": [5, 35, 235, 238, 242, 245, 261, 265], "loss_funct": [304, 313, 315, 319], "loss_ms": [5, 35], "lot": [4, 5, 30, 34, 178, 179], "louboutin": [205, 209, 211], "loung": [205, 211], "love": [205, 211, 213, 220, 229, 234], "low": [7, 46, 48, 242, 245, 247, 261, 267, 268, 269, 277, 278, 279, 282, 284, 295, 298, 301], "lower": [153, 154, 165, 166, 205, 211, 229, 233, 287, 289, 294, 295, 301], "lowercas": [229, 233], "lowest": [268, 274], "lr": [4, 5, 6, 31, 32, 35, 36, 40, 41, 178, 182, 193, 199, 221, 226, 227, 235, 238, 242, 245, 248, 252, 261, 265, 267, 268, 273, 274, 304, 305, 313, 315, 319, 323, 325, 326], "lr_schedul": [5, 35], "lssf": [11, 72], "lstm": [261, 267], "lstrip": [153, 165], "lt": [213, 220], "luck": [205, 211, 213, 219], "lucki": [205, 211], "lupin": [205, 209, 211, 212], "lustr": [7, 43], "ly": [213, 220], "m": [0, 2, 4, 11, 24, 31, 72, 205, 211, 213, 219, 220, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 304, 313], "m1": [11, 72], "m2": [11, 72], "m3": [11, 72], "m5": [137, 141, 154, 166], "mac": [205, 211, 221, 226], "machin": [2, 3, 8, 11, 20, 27, 43, 46, 55, 72, 73, 76, 83, 136, 137, 139, 147, 150, 151, 152, 155, 159, 162, 163, 164, 167, 175, 205, 206, 207, 210, 211, 212, 213, 214, 215, 220, 221, 222, 223, 227, 228, 229, 230, 231, 255, 256, 307, 334], "maco": [3, 11, 26, 72, 136, 149], "macosx": [11, 72], "maddon": [205, 211], "made": [5, 10, 36, 69, 213, 216, 219, 220, 248, 252], "madison": [205, 211], "magnitud": [261, 263], "mai": [2, 4, 5, 22, 24, 25, 29, 33, 84, 86, 91, 95, 98, 99, 101, 109, 112, 119, 122, 126, 127, 129, 134, 136, 149, 152, 153, 154, 157, 159, 160, 164, 165, 166, 169, 173, 176, 178, 192, 205, 211, 213, 219, 220, 229, 231, 255, 259, 268, 277], "maiden": [205, 211], "main": [0, 2, 4, 5, 8, 10, 23, 32, 35, 54, 71, 84, 86, 91, 95, 99, 101, 109, 112, 137, 141, 142, 145, 146, 155, 156, 167, 168, 175, 176, 178, 182, 205, 211, 213, 220, 223, 248, 250, 261, 263, 278, 279, 285, 287, 290, 306, 328], "maintain": [2, 7, 10, 25, 43, 48, 69, 73, 75, 153, 165, 213, 220, 242, 243, 278, 279, 282], "mainten": [80, 81], "major": [213, 216], "make": [0, 2, 3, 4, 5, 6, 7, 8, 10, 11, 19, 23, 28, 32, 35, 36, 41, 42, 48, 51, 54, 55, 71, 72, 122, 134, 137, 141, 151, 153, 156, 157, 160, 161, 163, 165, 168, 169, 177, 178, 180, 186, 190, 192, 195, 196, 197, 198, 200, 201, 205, 207, 211, 213, 215, 216, 219, 220, 221, 223, 227, 229, 231, 233, 235, 236, 237, 242, 244, 248, 254, 255, 256, 257, 261, 262, 263, 266, 267, 268, 269, 271, 278, 279, 281, 295, 297, 298, 303, 305, 310, 311, 325], "make_pendulum_dataset": [242, 244], "makedir": [235, 237, 239, 242, 246, 248, 250, 255, 257, 261, 263, 268, 270], "malaga": [205, 211], "male": [213, 216, 220], "mall": [205, 211], "malloc": [136, 149], "mamba": [206, 214, 222, 230], "man": [205, 211, 213, 216, 219, 220], "manag": [1, 4, 5, 7, 13, 17, 30, 34, 43, 73, 75, 76, 77, 78, 81, 83, 91, 93, 95, 99, 103, 107, 109, 115, 120, 122, 124, 131, 135, 136, 137, 140, 142, 145, 149, 150, 151, 152, 153, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 175, 177, 178, 179, 193, 205, 211, 221, 227, 229, 231, 235, 236, 239, 241, 242, 243, 246, 249, 254, 255, 256, 261, 262, 268, 269, 272, 273, 277, 284, 286, 287, 292, 303, 308], "mani": [1, 3, 8, 9, 10, 13, 17, 21, 22, 28, 51, 61, 68, 84, 86, 99, 101, 109, 112, 154, 156, 160, 166, 168, 177, 178, 181, 183, 184, 186, 205, 209, 210, 211, 213, 216, 220, 229, 234, 248, 249, 250, 255, 258, 261, 267, 268, 269, 278, 279, 282, 287, 294, 295, 299, 301], "manipul": [213, 219], "mann": [213, 219, 220], "manner": [3, 8, 9, 28, 49, 51, 53, 56, 213, 215, 221, 227, 268, 269, 306, 327], "mansbridg": [205, 211], "manual": [4, 32, 91, 98, 122, 134, 179, 180, 182, 184, 185, 188, 198, 200, 204, 205, 211, 221, 226, 235, 236, 241, 242, 243, 248, 249, 253, 255, 256, 258, 260, 261, 262, 266, 268, 269, 272, 273, 275], "manual_se": [261, 265], "map": [5, 35, 137, 141, 178, 196, 221, 226, 242, 243, 248, 250, 254, 255, 256, 257, 261, 267, 268, 269, 270, 271, 273, 277, 295, 298, 300, 303, 307, 310, 311, 335], "map_batch": [3, 8, 9, 28, 52, 61, 62, 64, 142, 144, 205, 207, 210, 211, 212, 235, 237, 242, 244, 248, 250, 255, 259, 260, 261, 267, 268, 277, 303, 306, 307, 310, 311, 330, 331, 335], "map_group": [8, 9, 54, 55, 64, 306, 331], "map_loc": [4, 5, 32, 36, 178, 191, 235, 241, 242, 247, 248, 252, 254, 261, 265, 267, 268, 273, 277, 304, 319], "mapbatch": [9, 63, 205, 212], "mapreduc": [7, 46], "mar": [229, 234], "marathon": [205, 211], "marc": [213, 220], "march": [205, 211], "mario": [205, 211], "mark": [136, 149, 213, 220], "markdown": 0, "marker": [235, 239, 242, 246, 248, 252, 261, 265, 267, 268, 275], "market_typ": [137, 141], "marlei": [205, 211], "marri": [213, 216], "martial": [213, 219], "martin": [205, 211], "mask": [221, 226], "mass": [205, 211], "massiv": [9, 57], "master": [11, 72, 213, 219, 221, 228, 261, 263, 295, 297, 305, 326], "mat1_ref": [2, 18], "mat2_ref": [2, 18], "match": [2, 9, 22, 62, 99, 107, 109, 120, 154, 156, 160, 166, 168, 177, 178, 202, 205, 211, 255, 257, 287, 290, 295, 298, 299, 301], "matching_analysi": [295, 298], "materi": [3, 8, 10, 28, 51, 52, 54, 56, 57, 59, 60, 61, 62, 64, 70, 137, 141, 142, 144, 205, 207, 212, 248, 250, 255, 257, 258, 260, 268, 277, 303, 307, 310, 311, 329, 331, 335, 336], "materialized_d": [205, 212], "materializeddataset": [205, 212, 213, 217], "math": [1, 16, 242, 244, 261, 263, 264], "mathbb": [235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269], "mathcal": [235, 236, 242, 243, 248, 249, 268, 269], "mathemat": [295, 301], "matmul": [2, 18, 248, 254], "matplotlib": [4, 6, 9, 29, 38, 56, 178, 180, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 304, 305, 307, 313, 322, 336], "matric": [268, 277], "matrix": [252, 254], "matrixfactorizationmodel": [248, 251, 252, 254], "matt": [205, 211], "matter": [205, 211, 213, 216, 268, 271, 278, 279, 283], "matur": [7, 47, 156, 160, 168, 176], "max": [8, 9, 54, 61, 64, 153, 165, 213, 220, 235, 239, 242, 246, 248, 252, 261, 263, 265, 268, 273, 287, 290, 306, 307, 330, 331, 336], "max_": [306, 330], "max_depth": [3, 28, 255, 258, 260, 303, 310, 311], "max_epoch": [5, 35, 36, 235, 239, 241, 242, 246], "max_failur": [178, 200, 204, 235, 239, 242, 246, 248, 252, 255, 258, 261, 265, 268, 269, 274], "max_len": [261, 264], "max_length": [221, 226], "max_lora": [295, 298], "max_lora_rank": [295, 298], "max_model_len": [278, 279, 285, 290, 295, 298, 299, 300], "max_nod": [99, 106, 109, 118, 137, 141], "max_num_adapters_per_replica": [295, 298], "max_ongoing_request": [7, 48], "max_replica": [278, 279, 285, 287, 290, 293, 295, 300, 307, 336], "max_retri": [2, 20], "max_siz": [9, 62], "max_step": [5, 36], "max_t": [235, 238, 242, 245], "maxim": [7, 8, 43, 48, 54, 205, 207, 278, 279, 281, 283, 284], "maximum": [99, 106, 109, 118, 150, 152, 162, 164, 178, 202, 278, 279, 281, 282, 287, 289, 293, 294, 295, 298, 301, 307, 336], "maxpool": [304, 313, 319], "maxpool2d": [304, 313, 319], "maxpumperla": [0, 221, 228], "mayb": [213, 219], "mb": [261, 263], "mcintir": [213, 219], "md": [0, 142, 145, 206, 214, 222, 230], "mdmad": [205, 211], "me": [73, 78, 205, 211, 213, 216, 219, 220, 287, 291, 292], "mean": [2, 6, 8, 9, 11, 18, 22, 41, 51, 52, 54, 60, 64, 72, 122, 125, 178, 185, 191, 235, 236, 242, 243, 248, 249, 254, 261, 263, 267, 268, 271, 272, 277, 278, 279, 282, 305, 306, 324, 325, 331], "meant": [7, 43, 45, 248, 254], "meantim": [213, 219], "measur": [248, 254], "meat": [213, 216], "mechan": [7, 9, 43, 57, 136, 148, 268, 269, 273], "medium": [278, 279, 283, 286, 291, 294, 295, 301], "meet": [9, 10, 11, 57, 68, 72, 205, 211, 278, 279, 282, 307, 334], "melodrama": [213, 220], "member": [157, 169], "memori": [1, 2, 4, 5, 8, 9, 13, 18, 22, 24, 29, 31, 32, 33, 35, 46, 47, 50, 51, 57, 59, 60, 61, 63, 64, 136, 137, 139, 141, 148, 154, 166, 178, 180, 186, 191, 196, 207, 211, 213, 215, 220, 235, 237, 241, 248, 250, 257, 261, 267, 268, 270, 271, 277, 281, 282, 284, 286, 287, 289, 291, 293, 295, 297, 298, 301, 303, 305, 306, 310, 311, 324, 326, 330, 331], "memory_usag": [8, 51], "memorydb": 77, "men": [213, 216, 219], "mental": [213, 216], "mention": [213, 220, 261, 267], "merg": [8, 55, 248, 254], "merlin": [205, 211], "messag": [2, 7, 11, 25, 46, 72, 137, 139, 146, 235, 239, 242, 246, 278, 279, 285, 287, 291, 292, 295, 298, 299, 300, 304, 305, 307, 318, 324, 335], "messages_cv": [295, 298], "messages_nemoguard": [295, 298], "messages_yara": [295, 298], "messi": [205, 209, 211], "meta": [248, 252, 261, 265, 268, 273, 278, 279, 285, 287, 289, 290, 293, 295, 298], "meta_path": [268, 273], "metadata": [7, 8, 48, 55, 137, 141, 154, 166, 178, 200, 205, 209, 212, 213, 215, 217, 219, 242, 246, 248, 252, 254, 255, 260, 268, 271], "method": [2, 4, 5, 6, 8, 9, 10, 18, 25, 30, 34, 35, 41, 52, 60, 62, 63, 64, 70, 84, 85, 91, 92, 99, 100, 109, 110, 122, 123, 136, 149, 178, 179, 205, 207, 210, 213, 220, 221, 226, 235, 241, 248, 249, 254, 305, 306, 307, 326, 330, 335], "method_nam": [2, 25], "metric": [3, 5, 6, 28, 30, 34, 36, 40, 41, 73, 78, 136, 137, 139, 140, 141, 143, 148, 149, 151, 156, 159, 163, 168, 173, 179, 180, 181, 182, 189, 193, 197, 199, 200, 202, 204, 223, 227, 228, 235, 239, 240, 241, 242, 243, 246, 247, 249, 250, 254, 255, 257, 258, 260, 261, 262, 265, 266, 267, 268, 269, 273, 274, 275, 276, 277, 287, 290, 293, 295, 302, 303, 305, 307, 310, 311, 312, 315, 319, 323, 324, 325, 326, 336, 339], "metrics_datafram": [4, 5, 32, 36, 178, 190, 235, 239, 242, 246, 248, 252, 261, 265, 268, 275, 304, 319], "metrics_interval_": [307, 336], "mf_ray_train": [248, 252], "miami": [205, 211], "mic": [205, 209, 211], "michael": [205, 211, 213, 220], "micro": [268, 273], "microservic": [146, 158, 171, 278, 279, 283], "mid": [178, 202, 255, 256, 268, 276], "mid_block_scale_factor": [5, 35], "middl": [205, 211], "midwai": [205, 211], "might": [3, 4, 5, 6, 9, 26, 28, 30, 34, 41, 61, 80, 82, 84, 86, 99, 101, 109, 112, 122, 125, 178, 179, 213, 216, 235, 241, 255, 258, 260, 268, 277, 278, 279, 282, 303, 310, 311], "migrat": [10, 35, 70, 178, 181, 242, 243, 307, 312, 335], "milan": [205, 211], "mile": [3, 7, 8, 28, 46, 47, 51, 303, 306, 310, 328], "million": [3, 28, 205, 209, 211, 213, 220, 303, 310], "min": [3, 6, 8, 9, 28, 41, 54, 60, 61, 64, 255, 258, 265, 267, 268, 274, 287, 290, 303, 305, 306, 307, 310, 311, 324, 325, 326, 330, 331, 336], "min_": [306, 330], "min_nod": [99, 106, 109, 118], "min_replica": [278, 279, 285, 287, 290, 293, 295, 300, 307, 336], "min_siz": [9, 62], "mind": [205, 211, 213, 216, 220], "mine": [205, 211], "minecraft": [205, 209, 211, 212], "miner": [213, 219], "mini": [4, 5, 31, 32, 36, 178, 179, 235, 236, 242, 243], "miniconda": [11, 72], "miniforge3": [11, 72], "minilm": [205, 210], "minim": [4, 5, 6, 7, 30, 34, 35, 41, 43, 46, 73, 76, 78, 80, 82, 178, 179, 221, 227, 235, 238, 242, 243, 247, 248, 249, 255, 256, 268, 269, 277, 303, 305, 310, 311, 324, 325], "minimalist": 0, "minimum": [73, 78, 99, 106, 109, 118, 150, 152, 162, 164, 307, 336], "minu": [303, 310, 311], "minut": [4, 8, 31, 51, 91, 95, 99, 101, 109, 112, 122, 126, 129, 150, 152, 162, 164, 205, 211, 261, 262, 263], "mirror": [261, 262, 268, 269], "mise": [213, 220], "miseenscen": [213, 220], "misfortun": [213, 219], "mismatch": [261, 265], "miss": [205, 211, 213, 219, 220, 235, 241, 242, 246], "missouri": [205, 211], "mistak": [213, 219], "mistral": [295, 301], "mitch": [205, 211], "mix": [5, 35, 36, 235, 241, 242, 247, 268, 277], "mkdir": [4, 31, 304, 313], "ml": [3, 7, 8, 10, 11, 27, 28, 45, 47, 48, 55, 67, 68, 69, 70, 72, 152, 153, 155, 157, 159, 160, 161, 164, 165, 167, 169, 173, 176, 205, 207, 212, 229, 231, 232, 233, 248, 250, 254, 268, 269, 303, 306, 307, 309, 310, 311, 328, 332, 333, 334, 335], "mlbcentral": [205, 211], "mlflow": [178, 204, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277], "mlogloss": [255, 258], "mlop": [3, 27, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277, 295, 298, 303, 309, 310, 311], "mlp": [242, 245, 247, 248, 254], "mm": [2, 22], "mmlu": [295, 301], "mnist": [4, 6, 9, 10, 29, 31, 32, 38, 39, 40, 41, 59, 61, 62, 64, 70, 142, 145, 179, 182, 185, 186, 188, 190, 191, 195, 203, 204, 305, 306, 307, 316, 318, 319, 322, 323, 324, 326, 329, 330, 331, 335, 336], "mnist_app": [10, 70, 71, 142, 145, 307, 335, 336], "mnist_app_handl": [10, 70], "mnist_classifi": [10, 70, 307, 335, 336], "mnist_classifier_arg": [9, 62], "mnist_deploy": [307, 335], "mnist_deployment_handl": [307, 335, 336], "mnist_pr": [9, 65, 66, 306, 332], "mnist_preprocessor": [307, 336], "mnistclassifi": [9, 10, 62, 63, 70, 306, 307, 330, 335, 336], "mnt": [3, 4, 5, 8, 9, 10, 28, 31, 35, 36, 37, 53, 62, 70, 142, 144, 153, 165, 178, 180, 181, 186, 188, 190, 195, 203, 235, 237, 239, 241, 242, 246, 247, 248, 250, 252, 254, 255, 257, 258, 260, 261, 262, 263, 268, 269, 270, 271, 273, 274, 277, 303, 304, 306, 307, 310, 311, 313, 318, 319, 320, 330, 332, 335, 336, 338], "mock": [213, 219], "modal": [8, 9, 55, 66, 306, 332], "modano": [205, 211], "mode": [0, 3, 6, 10, 28, 41, 56, 71, 178, 182, 191, 221, 226, 295, 299, 303, 305, 310, 311, 324, 325, 326], "model": [2, 7, 8, 9, 10, 22, 25, 29, 33, 37, 38, 40, 43, 44, 47, 48, 52, 55, 57, 62, 64, 66, 68, 69, 70, 71, 152, 153, 155, 159, 164, 165, 167, 173, 179, 180, 182, 188, 189, 190, 191, 193, 198, 199, 200, 202, 204, 206, 207, 210, 212, 213, 214, 220, 221, 222, 223, 224, 225, 226, 227, 228, 230, 232, 234, 235, 236, 238, 239, 241, 242, 243, 244, 246, 247, 250, 252, 253, 254, 256, 257, 259, 260, 262, 263, 265, 267, 270, 273, 277, 281, 283, 284, 285, 288, 291, 292, 294, 296, 297, 298, 299, 300, 302, 303, 306, 307, 310, 311, 312, 315, 317, 319, 320, 321, 325, 330, 331, 334, 335, 336, 337, 338], "model1": [3, 28], "model1_predict": [3, 28], "model2": [3, 28], "model2_predict": [3, 28], "model_config": [5, 35], "model_dump": [3, 28, 295, 300], "model_id": [278, 279, 285, 287, 290, 293, 295, 298, 299, 300], "model_json_schema": [295, 299], "model_kwarg": [261, 267], "model_loading_config": [278, 279, 285, 287, 290, 293, 295, 298, 299, 300], "model_nam": [5, 35, 36, 152, 164, 205, 210], "model_path": [3, 4, 10, 28, 32, 70, 178, 191, 255, 258, 304, 319], "model_predict": [3, 28], "model_select": [3, 26, 255, 257], "model_sourc": [278, 279, 285, 287, 290, 293, 295, 298, 299, 300], "model_state_dict": [178, 199], "modelcheckpoint": [235, 239, 241, 242, 246], "modelwork": [178, 191], "moder": [278, 279, 282, 295, 298, 301], "modern": [7, 43, 46, 205, 207, 212, 213, 215, 221, 223, 228], "modif": [156, 160, 168, 177], "modifi": [2, 3, 4, 8, 9, 10, 24, 28, 31, 53, 62, 70, 84, 86, 91, 95, 99, 101, 109, 112, 122, 127, 142, 144, 145, 153, 154, 156, 160, 165, 166, 168, 177, 198, 235, 236, 268, 269], "modul": [4, 31, 32, 73, 77, 78, 91, 95, 109, 113, 142, 145, 185, 188, 192, 198, 199, 200, 248, 251, 254, 261, 264, 267, 268, 269, 277, 278, 279, 280, 285, 286, 287, 292, 294, 295, 297, 302, 304, 313, 316, 317], "modular": [73, 77, 78, 248, 249], "mofo": [205, 211], "mom": [205, 211], "momentum": [221, 226, 304, 313, 319], "mondai": [205, 211], "monei": [213, 216, 219], "mongodb": [7, 43], "monitor": [4, 5, 7, 30, 32, 34, 46, 80, 81, 136, 137, 139, 140, 141, 142, 143, 144, 148, 149, 151, 155, 156, 161, 163, 167, 168, 178, 179, 187, 188, 255, 260, 261, 267, 278, 288, 290, 292, 294, 295, 301, 302, 304, 317], "monro": [205, 211], "month": [3, 8, 28, 51, 205, 211, 295, 300], "moon": [205, 211], "more": [1, 2, 6, 7, 8, 9, 10, 16, 19, 20, 22, 23, 24, 30, 31, 34, 37, 40, 41, 45, 47, 48, 54, 55, 57, 61, 64, 66, 68, 69, 71, 73, 78, 82, 84, 86, 99, 100, 101, 109, 110, 112, 122, 123, 136, 137, 139, 141, 142, 144, 149, 154, 158, 166, 171, 178, 179, 180, 184, 188, 190, 204, 206, 213, 214, 219, 220, 221, 222, 226, 228, 229, 230, 231, 233, 234, 235, 241, 248, 254, 255, 259, 260, 261, 265, 268, 277, 278, 279, 282, 284, 285, 289, 294, 297, 304, 305, 307, 315, 316, 319, 320, 323, 325, 326, 328, 331, 332, 334, 336], "morn": [205, 211], "moron": [213, 219], "morri": [213, 219], "morti": [205, 211], "mosh": [205, 211], "most": [2, 7, 8, 9, 19, 25, 45, 52, 58, 60, 80, 82, 150, 152, 153, 158, 162, 164, 165, 171, 178, 190, 202, 213, 219, 220, 229, 234, 235, 239, 242, 246, 248, 250, 253, 255, 259, 268, 270, 273, 275, 295, 298, 299, 306, 330], "most_rec": [153, 165], "mostli": [278, 279, 283], "motion": [242, 243], "motiv": [213, 216], "mount": [73, 78, 153, 165], "mountaincar": [242, 247], "mouth": [213, 219], "move": [4, 5, 6, 7, 31, 32, 35, 40, 41, 43, 161, 178, 181, 182, 185, 186, 188, 189, 191, 193, 194, 205, 207, 213, 220, 221, 226, 235, 241, 242, 243, 247, 255, 257, 261, 263, 268, 269, 278, 279, 286, 304, 305, 313, 316, 323, 326], "movement": [5, 35], "movi": [205, 211, 213, 216, 219, 220, 249, 250], "movielen": [249, 254], "mp": [205, 210, 211, 212, 221, 223, 226, 227], "mse": [235, 236, 239, 242, 246, 248, 249, 252], "mse_loss": [5, 35, 248, 252], "mseloss": [235, 238, 242, 245], "mta": [205, 211], "mtv": [213, 220], "mtvstar": [205, 211], "mtvstarsof2015": [205, 211], "much": [7, 8, 47, 51, 53, 137, 141, 205, 211, 213, 216, 219, 220, 235, 241, 248, 249, 255, 257, 278, 279, 283, 295, 298], "muck": [213, 220], "muddi": [213, 219], "multi": [8, 9, 10, 53, 55, 66, 68, 73, 75, 152, 164, 178, 179, 180, 189, 192, 197, 204, 235, 236, 239, 241, 242, 243, 248, 249, 250, 254, 255, 256, 258, 268, 269, 277, 278, 279, 282, 283, 284, 287, 289, 293, 294, 295, 298, 301, 302, 306, 307, 332, 334], "multi_actor_tracing_ray_serve_exampl": 146, "multiclass": [304, 313, 315, 319], "multiclassaccuraci": [268, 273], "multimod": [235, 236], "multipl": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 16, 18, 22, 23, 27, 30, 32, 34, 36, 41, 43, 46, 53, 55, 61, 62, 68, 69, 73, 78, 80, 82, 135, 137, 139, 142, 145, 146, 152, 158, 164, 171, 178, 179, 186, 188, 192, 195, 205, 207, 210, 213, 215, 218, 221, 223, 226, 227, 228, 229, 231, 235, 236, 237, 241, 242, 243, 247, 248, 250, 252, 254, 255, 256, 260, 261, 265, 267, 268, 270, 275, 277, 278, 279, 282, 283, 284, 287, 288, 289, 290, 293, 294, 295, 297, 298, 301, 302, 303, 305, 306, 307, 309, 310, 311, 324, 329, 330, 334, 339], "multiplex": [9, 10, 57, 68, 307, 334], "multipli": [2, 25, 261, 267], "multiprocess": [2, 22, 268, 271], "multithread": [2, 9, 22, 61], "multivari": [261, 267], "mum": [205, 209, 211], "muslim": [205, 211], "must": [178, 202, 235, 236, 242, 243, 295, 298], "mutat": [2, 25, 205, 212], "mutual": [278, 279, 283], "my": [0, 2, 21, 73, 78, 91, 94, 95, 146, 151, 163, 205, 209, 211, 213, 216, 220, 278, 279, 285, 287, 290, 291, 292, 293, 295, 298, 299, 300], "my_custom_env": [2, 21], "my_simple_model": [6, 41, 305, 324], "my_xgboost_func": [3, 28], "myself": [205, 211, 213, 216, 220], "mysentimentmodel": [229, 233], "mysql": [7, 43], "n": [2, 4, 11, 22, 24, 32, 72, 80, 82, 109, 119, 153, 165, 178, 182, 205, 206, 209, 211, 212, 213, 214, 220, 222, 230, 235, 236, 238, 239, 242, 243, 247, 249, 255, 259, 260, 268, 277, 287, 292, 295, 298, 300], "n_step": [242, 244, 247], "nab": [261, 263], "naiv": [9, 10, 57, 68, 235, 241], "naiveti": [213, 219, 220], "nake": [213, 219], "nam": [109, 113], "name": [4, 5, 6, 7, 10, 11, 31, 32, 36, 41, 48, 70, 71, 72, 73, 77, 78, 79, 84, 86, 87, 88, 91, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 106, 109, 112, 113, 114, 115, 118, 122, 125, 126, 131, 133, 134, 137, 141, 142, 144, 145, 146, 150, 151, 152, 153, 154, 155, 156, 157, 162, 163, 164, 165, 166, 167, 168, 169, 175, 178, 182, 188, 189, 191, 197, 200, 202, 213, 216, 219, 235, 237, 239, 241, 242, 246, 247, 248, 250, 252, 254, 255, 258, 261, 265, 268, 270, 274, 277, 278, 279, 285, 287, 292, 295, 298, 299, 300, 303, 304, 305, 307, 310, 311, 313, 318, 324, 326, 335, 336], "namespac": [99, 101, 102, 104, 105, 106, 107, 109, 112, 114, 116, 117, 118, 120, 122, 129, 132, 134, 178, 188], "nandito": [205, 211], "narrat": [213, 220], "naruto": [205, 211], "nash": [205, 211], "nashnewvideo": [205, 211], "nat": [73, 78, 80, 82, 84, 86, 99, 101, 109, 112], "natgatewai": [84, 86, 99, 101, 109, 112], "nation": [205, 211, 213, 219, 229, 234], "nativ": [1, 3, 7, 15, 27, 43, 45, 46, 47, 80, 81, 83, 137, 140, 141, 161, 213, 219, 220, 235, 239, 241, 242, 243, 246, 247, 261, 262, 268, 269, 278, 279, 283, 284], "nativesbr": [213, 220], "natur": [229, 234, 295, 300], "navig": [136, 142, 144, 146, 149, 151, 155, 156, 157, 160, 163, 167, 168, 169, 177], "nbsp": [150, 153, 162, 165], "nc": [213, 216], "nccl": [221, 227, 268, 269], "ndarrai": [6, 9, 10, 41, 61, 62, 64, 70, 205, 210, 212, 261, 267, 305, 306, 307, 324, 330, 331, 335], "ndcg": [248, 254], "ndim": [178, 191], "necessari": [4, 8, 32, 52, 84, 85, 86, 90, 91, 94, 95, 99, 100, 101, 108, 109, 110, 112, 119, 121, 122, 123, 125, 178, 186, 205, 209, 221, 223, 224, 229, 233, 255, 257, 268, 269, 304, 307, 316, 336], "need": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 16, 18, 19, 21, 30, 32, 34, 36, 41, 43, 46, 51, 53, 55, 57, 62, 65, 68, 70, 72, 73, 74, 75, 83, 84, 85, 86, 87, 91, 92, 93, 95, 96, 98, 99, 100, 101, 103, 109, 111, 112, 113, 115, 119, 122, 124, 126, 127, 131, 134, 135, 136, 137, 141, 149, 150, 151, 152, 156, 162, 163, 164, 168, 178, 179, 180, 181, 184, 186, 188, 192, 193, 196, 203, 204, 205, 211, 213, 219, 220, 221, 227, 229, 231, 235, 237, 241, 242, 243, 248, 250, 252, 254, 255, 257, 258, 259, 268, 269, 270, 271, 272, 273, 275, 278, 279, 283, 287, 288, 289, 290, 292, 294, 295, 296, 298, 299, 300, 301, 303, 304, 305, 306, 307, 308, 310, 311, 317, 324, 328, 335], "neg": [7, 47, 213, 216, 229, 234], "nemoguard": [295, 298], "nephew": [205, 211], "nest": [17, 19], "net": [5, 35, 235, 238, 242, 245, 268, 273], "netflix": [8, 9, 55, 66, 306, 332], "network": [4, 5, 6, 8, 9, 30, 34, 40, 54, 57, 64, 73, 76, 78, 80, 82, 83, 109, 119, 122, 134, 135, 136, 137, 139, 141, 148, 153, 165, 178, 179, 235, 236, 238, 268, 269, 278, 279, 283, 305, 306, 323, 331], "networkinterfaceid": [84, 86, 99, 101, 109, 112], "neural": [6, 40, 248, 254, 261, 264, 268, 269, 305, 323], "never": [0, 205, 211, 213, 219, 220], "new": [2, 3, 7, 8, 24, 25, 28, 44, 51, 80, 83, 84, 86, 102, 106, 109, 114, 118, 150, 151, 152, 153, 154, 155, 157, 160, 162, 163, 164, 165, 166, 167, 169, 175, 176, 178, 202, 205, 206, 211, 213, 214, 216, 219, 220, 222, 230, 239, 242, 247, 255, 256, 260, 261, 262, 268, 276, 278, 279, 282, 287, 292, 295, 300, 304, 307, 315, 336, 339], "newaxi": [255, 259], "newli": [122, 128], "newsha": [205, 211], "next": [2, 6, 8, 24, 41, 53, 73, 76, 91, 93, 99, 103, 109, 115, 122, 124, 131, 142, 144, 148, 150, 152, 155, 156, 160, 162, 164, 167, 168, 177, 180, 185, 205, 211, 237, 250, 258, 262, 263, 270, 273, 281, 296, 298, 301, 304, 305, 307, 318, 324, 325, 326, 336], "nf": [153, 165], "nfl": [205, 209, 211], "nginx": [80, 83, 107, 108, 120, 121, 134], "nhead": [261, 264, 265, 267], "nhl": [205, 209, 211, 212], "nia": [205, 211], "niall": [205, 211], "nice": [178, 204], "nicer": [7, 47], "nick": [205, 211], "nicki": [205, 211], "nigga": [205, 211], "night": [205, 209, 211, 212, 213, 219, 220], "nightli": [242, 247, 261, 267], "nightmar": [213, 219, 220], "nightmarish": [213, 220], "nine": [235, 237, 268, 270], "nirvana": [205, 211], "nlb": [80, 83], "nlp": [178, 204], "nn": [4, 5, 6, 29, 31, 32, 33, 38, 40, 41, 178, 180, 181, 185, 188, 200, 221, 224, 235, 237, 238, 242, 244, 245, 248, 250, 251, 261, 263, 264, 265, 268, 270, 273, 304, 305, 313, 316, 317, 323, 326], "no_grad": [4, 9, 10, 31, 32, 62, 70, 235, 241, 242, 247, 248, 252, 254, 261, 265, 268, 273, 304, 306, 307, 313, 319, 330, 335], "no_restart": [178, 191], "node": [1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 18, 20, 22, 30, 31, 32, 34, 40, 51, 53, 61, 68, 72, 73, 76, 77, 78, 80, 82, 83, 84, 86, 91, 92, 95, 99, 101, 102, 106, 112, 114, 118, 119, 122, 126, 128, 137, 139, 140, 141, 148, 150, 153, 154, 155, 156, 159, 162, 165, 166, 167, 168, 173, 176, 178, 179, 180, 182, 188, 189, 192, 196, 200, 201, 204, 205, 211, 213, 215, 217, 235, 236, 241, 242, 243, 248, 249, 254, 255, 256, 257, 258, 263, 266, 267, 268, 269, 273, 277, 278, 279, 282, 283, 287, 289, 290, 292, 293, 294, 295, 301, 303, 305, 307, 308, 323, 324, 326, 334], "node_ip": [304, 319], "nodegroup": [109, 113], "noderol": [109, 113], "nofril": [213, 220], "noir": [213, 219, 220], "noirlik": [213, 220], "nois": [5, 35, 238, 241, 242, 243, 244, 245, 247, 261, 263], "noise_schedul": [5, 35], "noised_lat": [5, 35], "noiser": [235, 241], "noisi": [235, 238, 242, 243, 245], "noisy_act": [242, 244, 245], "noisy_img": [235, 238], "non": [4, 10, 31, 71, 142, 145, 178, 179, 180, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 273, 278, 279, 283, 284, 285, 287, 291, 293, 295, 298, 299, 300], "non_block": [178, 191], "none": [4, 5, 6, 9, 31, 32, 35, 41, 62, 178, 187, 188, 191, 200, 221, 228, 235, 239, 241, 242, 246, 247, 248, 252, 254, 255, 258, 261, 264, 265, 268, 271, 277, 304, 305, 313, 317, 324, 325], "norm": [178, 191, 261, 263, 267], "norm_ep": [5, 35], "norm_num_group": [5, 35], "normal": [4, 6, 9, 29, 31, 32, 38, 39, 41, 56, 61, 63, 178, 180, 186, 191, 192, 196, 235, 236, 241, 243, 245, 247, 248, 249, 254, 255, 257, 259, 262, 267, 268, 269, 271, 277, 304, 305, 306, 307, 313, 316, 319, 322, 326, 330, 336], "normalci": [213, 220], "normalis": [261, 263, 268, 271], "normalize_cpu": [178, 191], "normalized_batch": [9, 61, 306, 307, 330, 336], "normalized_img": [4, 31, 32], "north": [205, 211, 213, 219], "not_ready_ref": [2, 24], "note": [6, 8, 10, 11, 12, 17, 19, 20, 31, 35, 41, 52, 53, 54, 60, 61, 64, 70, 71, 72, 73, 76, 80, 83, 84, 86, 91, 95, 99, 101, 103, 109, 112, 115, 122, 126, 131, 136, 142, 145, 148, 149, 180, 192, 194, 196, 205, 211, 212, 213, 218, 219, 220, 229, 234, 248, 254, 255, 257, 268, 269, 273, 278, 279, 284, 304, 305, 306, 307, 316, 317, 324, 330, 331, 335], "notebook": [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 17, 19, 26, 29, 31, 33, 38, 49, 56, 67, 84, 85, 91, 92, 99, 100, 109, 110, 136, 137, 138, 142, 143, 147, 150, 152, 153, 158, 161, 162, 164, 165, 170, 178, 179, 180, 205, 206, 207, 212, 214, 220, 221, 222, 223, 228, 229, 230, 231, 234, 235, 236, 242, 243, 247, 248, 249, 250, 255, 260, 261, 262, 268, 269, 270, 277, 278, 279, 280, 287, 288, 295, 296, 304, 305, 306, 307, 312, 321, 327, 333, 339], "noth": [178, 191, 205, 211, 213, 220], "notic": [4, 8, 32, 53, 178, 182, 205, 211, 213, 220], "notif": [142, 145, 146], "notificationservic": 146, "nov": [205, 211, 304, 313], "novelti": [248, 254], "now": [2, 3, 4, 5, 9, 10, 11, 19, 28, 31, 32, 36, 62, 70, 72, 73, 78, 84, 86, 90, 91, 95, 99, 101, 107, 108, 109, 119, 120, 121, 122, 126, 142, 144, 145, 151, 154, 155, 157, 163, 166, 167, 169, 178, 181, 186, 188, 189, 191, 193, 194, 195, 196, 197, 199, 200, 205, 211, 213, 219, 220, 235, 236, 237, 239, 241, 242, 247, 248, 250, 252, 254, 255, 257, 260, 268, 277, 278, 279, 285, 287, 291, 292, 293, 294, 295, 297, 298, 299, 300, 302, 304, 306, 307, 313, 314, 315, 330, 335, 336], "nowher": [213, 216], "np": [1, 2, 4, 5, 6, 9, 10, 12, 17, 18, 22, 24, 29, 31, 32, 33, 35, 38, 41, 56, 61, 62, 64, 67, 70, 137, 141, 142, 145, 178, 180, 191, 196, 205, 208, 210, 211, 221, 224, 225, 235, 237, 242, 244, 247, 248, 250, 255, 257, 258, 259, 261, 263, 267, 268, 270, 277, 305, 306, 307, 324, 325, 330, 331, 335, 336], "nthread": [255, 258], "ntop": [248, 254], "nude": [213, 216], "nuditi": [213, 216], "nuge": [205, 211], "nugent": [205, 211], "null": [84, 86, 99, 101, 109, 112], "num": [136, 149, 304, 315], "num_actor": [268, 277], "num_block": [205, 212, 213, 217], "num_boost_round": [3, 28, 255, 258], "num_class": [4, 31, 178, 181, 255, 258, 268, 273, 277, 304, 313, 315, 319], "num_cpu": [2, 8, 9, 22, 54, 61, 235, 237, 255, 259, 260], "num_decoder_lay": [261, 264], "num_encoder_lay": [261, 264], "num_epoch": [4, 6, 31, 32, 40, 41, 178, 182, 183, 189, 193, 197, 199, 200, 202, 304, 305, 313, 315, 318, 319, 323, 326], "num_gpu": [2, 6, 9, 22, 40, 61, 62, 178, 191, 205, 211, 261, 267, 268, 277, 306, 307, 330, 335, 336], "num_imag": [137, 141], "num_item": [248, 250, 251, 252, 254], "num_label": [221, 226], "num_lay": [261, 264, 265, 267], "num_parquet_shard": [248, 250], "num_partit": [213, 219], "num_replica": [229, 233, 234, 307, 335, 336], "num_return": [2, 24], "num_row": [205, 209, 212, 213, 216, 217, 218, 268, 271], "num_row_group": [268, 271], "num_sampl": [3, 6, 28, 41, 303, 305, 310, 311, 324, 325, 326], "num_to_keep": [235, 239, 242, 246, 248, 252, 255, 258, 261, 265, 268, 274], "num_training_step": [5, 35], "num_us": [248, 250, 251, 252, 254], "num_warmup_step": [5, 35, 36], "num_work": [3, 4, 5, 28, 32, 35, 36, 178, 179, 184, 204, 221, 227, 228, 235, 239, 242, 246, 248, 252, 255, 258, 261, 263, 265, 268, 269, 271, 272, 274, 277, 303, 304, 310, 311, 315], "number": [2, 3, 4, 5, 6, 8, 9, 22, 24, 28, 32, 36, 39, 41, 51, 53, 59, 61, 62, 80, 83, 137, 139, 142, 144, 145, 150, 152, 154, 162, 164, 166, 178, 181, 182, 183, 189, 197, 202, 205, 207, 210, 213, 216, 218, 221, 223, 226, 227, 228, 229, 233, 234, 242, 244, 248, 250, 252, 268, 277, 287, 290, 293, 295, 299, 303, 304, 305, 306, 307, 310, 311, 315, 318, 322, 324, 325, 329, 330, 336], "numenta": [261, 263], "numer": [8, 51, 178, 180, 235, 241, 255, 256, 268, 270, 295, 298], "numpi": [1, 2, 4, 5, 6, 8, 9, 10, 12, 17, 29, 33, 38, 52, 56, 59, 62, 67, 70, 137, 141, 142, 145, 178, 180, 191, 196, 205, 208, 212, 221, 224, 235, 237, 241, 242, 244, 248, 250, 255, 257, 258, 261, 263, 267, 268, 270, 277, 305, 306, 307, 324, 330, 335, 336], "nuremburg": [205, 211], "nutshel": [303, 309], "nvdp": [99, 102, 107, 109, 114, 120], "nvidia": [80, 83, 107, 108, 120, 121, 205, 211, 295, 298], "nvlink": [287, 293], "nvme": [4, 31, 178, 179, 180], "nyc": [3, 8, 28, 51, 54, 142, 144, 267], "nyc_taxi": [261, 263], "nyc_taxi_2021": [303, 310, 311], "nyc_taxi_t": [261, 263], "nyc_taxi_transform": [261, 265], "o": [1, 2, 4, 5, 10, 12, 17, 21, 22, 29, 31, 32, 33, 35, 36, 71, 136, 137, 141, 148, 152, 153, 164, 165, 178, 180, 188, 191, 199, 200, 203, 205, 206, 209, 211, 214, 221, 222, 224, 230, 235, 237, 239, 241, 242, 244, 246, 247, 248, 250, 252, 254, 255, 257, 258, 260, 261, 263, 265, 267, 268, 270, 273, 275, 277, 278, 287, 290, 293, 295, 298, 300, 304, 313, 317, 319], "ob": [242, 244, 245, 247], "obj": [153, 165, 295, 298], "obj_ref": [2, 18], "object": [1, 3, 4, 6, 7, 8, 9, 10, 15, 16, 19, 25, 28, 32, 40, 41, 47, 48, 51, 63, 64, 68, 70, 73, 78, 91, 98, 122, 134, 154, 156, 160, 166, 168, 177, 178, 188, 189, 190, 196, 202, 204, 205, 207, 211, 212, 250, 254, 255, 258, 260, 268, 270, 275, 278, 279, 282, 287, 290, 295, 298, 300, 303, 304, 305, 306, 307, 310, 311, 315, 318, 319, 323, 324, 325, 329, 330, 331, 335], "object_ref": [2, 24], "objectref": [1, 2, 15, 18, 19, 24], "oblig": [213, 220], "oblivi": [213, 220], "obs_dim": [242, 245, 247], "obs_sampl": [242, 247], "observ": [4, 5, 30, 32, 34, 141, 156, 160, 168, 176, 178, 179, 221, 228, 242, 243, 244, 245, 255, 257, 261, 264, 268, 271, 278, 295, 302, 305, 326], "observed_data": [142, 144], "obtain": [153, 165, 221, 226], "obtus": [213, 216], "obviou": [213, 216], "occupi": [178, 203], "occur": [7, 46, 137, 141, 178, 196, 201, 248, 250], "ocean": [205, 211], "oct": [205, 209, 211], "octob": [205, 211], "off": [0, 4, 6, 9, 31, 32, 39, 60, 178, 180, 191, 205, 209, 211, 213, 220, 235, 237, 241, 248, 252, 253, 255, 259, 260, 261, 266, 268, 270, 271, 274, 277, 287, 293, 295, 298, 301, 304, 305, 313, 319, 322], "offenc": [205, 211], "offend": [205, 211], "offer": [3, 7, 10, 27, 43, 45, 46, 47, 68, 69, 80, 81, 82, 155, 158, 159, 167, 172, 173, 213, 216, 220, 235, 237, 287, 289, 293], "offici": [73, 78, 91, 95, 136, 142, 145, 149, 151, 152, 163, 164], "offlin": [154, 166, 242, 243, 244, 248, 250, 255, 256, 303, 310, 311], "offlinemnistclassifi": [307, 335], "offlinepredictor": [3, 28, 303, 310, 311], "offload": [153, 165, 268, 269], "often": [7, 43, 46, 47, 161, 178, 191, 248, 250, 255, 259], "oh": [205, 211], "olap": [7, 43], "old": [160, 176, 205, 211, 213, 216, 220, 255, 260, 261, 267, 268, 277], "older": [142, 145, 235, 241], "oltp": [7, 43], "olympics2012": [205, 211], "omp_num_thread": [2, 22], "on_demand": [137, 141], "on_epoch": [235, 238, 242, 245], "on_fit_start": [5, 35], "on_step": [5, 35], "onc": [1, 9, 16, 17, 19, 23, 25, 61, 62, 84, 88, 91, 97, 99, 106, 109, 118, 122, 133, 151, 152, 153, 155, 163, 164, 165, 167, 178, 179, 180, 188, 191, 205, 207, 210, 221, 226, 235, 239, 248, 249, 255, 259, 261, 262, 267, 268, 269, 271, 277, 278, 279, 282, 285, 287, 291, 292, 295, 298, 303, 306, 310, 311, 330], "one": [1, 2, 3, 4, 5, 7, 8, 9, 10, 16, 20, 22, 23, 24, 28, 30, 34, 44, 48, 53, 55, 57, 68, 69, 73, 78, 80, 82, 83, 84, 86, 99, 101, 109, 112, 136, 142, 144, 145, 149, 152, 153, 154, 158, 160, 164, 165, 166, 171, 176, 178, 179, 181, 184, 186, 190, 192, 204, 205, 207, 211, 213, 216, 219, 220, 235, 239, 255, 257, 258, 262, 265, 266, 268, 269, 271, 273, 274, 278, 279, 281, 282, 283, 284, 295, 297, 298, 301, 304, 307, 313, 334], "onehellofanighttour": [205, 211], "ones": [2, 7, 24, 45, 84, 86, 99, 101, 109, 112, 151, 163, 205, 211, 229, 231, 278, 279, 282, 287, 289], "ongo": [80, 81, 135, 307, 336], "onli": [1, 2, 4, 8, 9, 16, 25, 32, 51, 53, 60, 61, 63, 64, 73, 76, 80, 82, 84, 87, 91, 96, 99, 103, 109, 115, 122, 131, 136, 137, 140, 141, 144, 149, 152, 153, 154, 157, 158, 161, 164, 165, 166, 169, 171, 179, 181, 182, 187, 200, 203, 204, 205, 211, 213, 215, 216, 218, 219, 220, 221, 226, 227, 235, 241, 242, 243, 247, 248, 250, 252, 254, 255, 256, 258, 261, 263, 265, 267, 268, 269, 271, 273, 275, 278, 279, 282, 284, 295, 298, 304, 306, 307, 317, 330, 331, 336], "onlin": [7, 10, 43, 70, 154, 160, 166, 176, 178, 192, 205, 206, 211, 214, 222, 230, 233, 261, 267, 268, 277, 303, 307, 310, 311, 335], "onlinemnistclassifi": [10, 70, 307, 335, 336], "onlinemnistpreprocessor": [307, 336], "onlinepredictor": [303, 310, 311], "onto": [2, 6, 9, 22, 40, 60, 178, 191, 305, 323], "onu": [7, 46], "oom": [4, 5, 9, 29, 33, 61, 137, 141], "op": [268, 270], "open": [0, 1, 4, 7, 11, 13, 31, 43, 72, 91, 94, 136, 137, 140, 142, 144, 146, 149, 151, 152, 153, 154, 155, 161, 163, 164, 165, 166, 167, 205, 211, 213, 219, 235, 237, 268, 270, 271, 277, 295, 300, 301, 304, 313], "openai": [278, 279, 284, 285, 287, 290, 291, 292, 295, 298, 299, 300], "openapi": [307, 336], "opentelemetri": [136, 146, 148], "oper": [7, 43, 44, 47, 49, 55, 56, 57, 60, 61, 73, 75, 82, 91, 95, 100, 101, 102, 105, 107, 108, 110, 112, 114, 117, 119, 120, 121, 123, 126, 134, 135, 136, 142, 143, 144, 146, 148, 149, 158, 171, 205, 212, 213, 215, 217, 218, 219, 221, 224, 235, 236, 237, 268, 269, 278, 279, 281, 283, 303, 310, 311, 327], "opinion": [73, 77, 213, 216], "oppos": [213, 220], "opt": [136, 149], "opt_path": [268, 273], "opt_state_path": [261, 265], "optim": [4, 5, 6, 7, 8, 9, 10, 29, 31, 32, 35, 38, 40, 41, 43, 47, 52, 54, 55, 61, 68, 73, 78, 80, 82, 178, 180, 182, 193, 198, 199, 200, 202, 204, 205, 207, 210, 221, 226, 235, 238, 242, 245, 248, 252, 254, 261, 263, 265, 268, 269, 270, 273, 277, 280, 284, 286, 288, 294, 295, 301, 302, 303, 304, 305, 307, 310, 311, 313, 315, 319, 323, 324, 325, 326, 334], "optimizerlrschedul": [5, 33, 35], "option": [2, 3, 4, 8, 9, 10, 20, 22, 26, 27, 31, 54, 59, 62, 64, 69, 71, 76, 77, 82, 84, 86, 90, 91, 92, 95, 98, 101, 107, 108, 112, 120, 121, 134, 137, 141, 142, 145, 152, 164, 178, 180, 191, 194, 205, 207, 211, 221, 228, 229, 233, 234, 235, 237, 239, 242, 246, 248, 252, 258, 260, 261, 264, 265, 267, 278, 279, 282, 285, 295, 298, 299, 303, 305, 306, 307, 309, 310, 311, 326, 331, 335, 336, 339], "optuna": [6, 38, 41, 305, 324, 325], "optunasearch": [6, 41, 305, 325], "orang": [2, 18], "orc": [7, 43], "orchestr": [43, 80, 81, 82, 146, 178, 179, 180, 184, 189, 235, 236, 241, 242, 243, 248, 249, 252, 254, 255, 260, 261, 262, 263, 265, 267, 268, 269, 270, 273, 283, 287, 290], "order": [1, 6, 13, 41, 137, 139, 150, 162, 205, 211, 213, 219, 255, 257, 258, 305, 325], "ordinari": [213, 216], "oregon": [205, 211], "org": [158, 171, 172, 248, 250, 339], "org_967t9ah1lbk1yqf1zau6a1v247": [153, 165], "org_xxxxxxx": [91, 95], "organ": [7, 8, 43, 51, 73, 75, 78, 91, 95, 135, 154, 157, 161, 166, 169, 172, 178, 188], "organiz": [80, 82, 161], "orient": [303, 310, 311], "origin": [4, 32, 109, 119, 178, 181, 205, 209, 211, 212, 213, 220, 235, 237, 248, 253, 254, 261, 263, 268, 269, 304, 315], "original_user_id": [248, 254], "oscar": [205, 211], "oss": [137, 140, 141], "ossci": [304, 305, 318, 324, 326], "other": [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 23, 25, 27, 28, 30, 31, 32, 34, 36, 41, 43, 45, 47, 51, 69, 72, 73, 78, 80, 81, 122, 134, 136, 148, 151, 153, 154, 163, 165, 166, 178, 179, 180, 183, 191, 192, 205, 211, 212, 213, 219, 220, 221, 223, 229, 231, 235, 239, 241, 242, 247, 248, 250, 252, 261, 265, 268, 269, 295, 297, 303, 305, 306, 309, 310, 311, 325, 328, 329], "otherwis": [4, 7, 31, 48, 248, 250, 295, 298], "otlp": 146, "our": [2, 4, 5, 6, 8, 9, 10, 23, 32, 35, 36, 39, 40, 41, 51, 59, 65, 70, 142, 144, 146, 154, 155, 166, 167, 178, 188, 205, 211, 213, 220, 221, 225, 290, 291, 292, 295, 298, 299, 300, 305, 306, 307, 315, 323, 324, 329, 332, 336], "out": [2, 3, 4, 5, 7, 8, 9, 20, 24, 28, 29, 32, 33, 38, 41, 47, 55, 58, 136, 137, 141, 148, 152, 153, 154, 155, 156, 160, 164, 165, 166, 167, 168, 176, 177, 178, 179, 207, 209, 211, 213, 216, 219, 220, 235, 236, 237, 242, 244, 248, 250, 254, 255, 260, 261, 267, 268, 271, 277, 307, 336], "out_channel": [4, 5, 31, 35, 178, 181, 304, 313], "out_featur": [304, 313, 319], "out_img_byt": [235, 237], "out_label": [235, 237], "out_proj": [261, 264], "out_ref": [2, 19], "outbound": [73, 78], "outbr": [213, 220], "outdoor": [205, 211], "outhous": [205, 211], "outlier": [10, 68], "outlook": 288, "output": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 16, 20, 28, 31, 32, 35, 40, 41, 50, 51, 53, 59, 61, 63, 65, 70, 72, 84, 86, 87, 91, 95, 96, 98, 99, 101, 103, 109, 112, 113, 115, 122, 125, 126, 129, 131, 134, 142, 144, 145, 154, 156, 159, 166, 168, 173, 178, 179, 180, 181, 182, 188, 190, 193, 199, 203, 205, 207, 221, 226, 235, 238, 241, 248, 250, 254, 261, 262, 264, 267, 278, 279, 281, 283, 286, 287, 290, 296, 297, 298, 300, 302, 304, 305, 306, 313, 315, 319, 323, 326, 328, 329, 330, 332], "output_column": [213, 220], "output_csv": [248, 250], "output_dir": [235, 237, 268, 270], "output_path": [137, 141], "output_s": [304, 313, 319], "outsid": [4, 5, 32, 36, 80, 83, 304, 315], "outstand": [7, 48], "over": [2, 3, 6, 8, 9, 25, 28, 41, 52, 55, 57, 58, 80, 82, 150, 152, 153, 154, 155, 156, 162, 164, 165, 166, 167, 168, 178, 182, 193, 198, 205, 211, 213, 219, 220, 221, 226, 235, 241, 248, 252, 254, 255, 256, 260, 265, 267, 268, 269, 277, 303, 305, 310, 311, 325], "overal": [221, 223, 255, 258, 306, 328], "overcom": [205, 211], "overfit": [268, 275], "overhead": [1, 4, 5, 7, 13, 30, 34, 46, 151, 161, 163, 178, 179, 242, 247, 278, 279, 282, 287, 293], "overlap": [137, 140, 178, 179, 261, 262], "overload": [137, 141], "overr": [213, 220], "overrid": [84, 86, 91, 95, 99, 101, 109, 112, 156, 160, 168, 177, 178, 181], "overriden": [155, 159, 167, 173], "overse": [304, 315], "oversubscrib": [2, 22], "overview": [6, 7, 8, 9, 12, 26, 36, 40, 43, 49, 56, 67, 137, 138, 142, 144, 147, 149, 153, 165, 178, 179, 288, 294, 296, 305, 306, 308, 312, 323, 327, 333, 339], "overwhelm": [137, 141], "overwrit": [151, 163], "own": [2, 4, 8, 10, 18, 31, 53, 69, 80, 82, 91, 95, 150, 153, 156, 158, 160, 162, 165, 168, 171, 176, 178, 180, 205, 209, 211, 213, 220, 229, 231, 234, 235, 236, 237, 239, 242, 243, 246, 248, 249, 252, 255, 256, 257, 258, 261, 262, 268, 271, 273, 277, 287, 292, 295, 302, 303, 308], "owner": [91, 93, 122, 124, 158, 171], "ownership": [213, 219], "ox": [205, 211], "p": [213, 220, 235, 236, 261, 267], "p50": [142, 145], "p90": [142, 145], "p99": [142, 145], "pa": [235, 237, 255, 257, 258, 261, 263, 268, 270], "pack": [205, 211, 235, 238], "packag": [11, 72, 152, 153, 164, 165, 178, 200, 206, 214, 222, 230, 235, 241, 242, 244, 248, 254, 255, 260], "pad": [4, 6, 31, 40, 41, 178, 181, 221, 226, 235, 238, 304, 305, 313, 319, 323, 326], "page": [0, 8, 9, 54, 64, 142, 144, 278, 279, 286, 287, 293, 306, 331], "pagedattent": [278, 279, 284], "pagerduti": [142, 145], "pai": [213, 220, 278, 279, 284], "paid": [3, 8, 28, 51], "pain": [7, 46], "pair": [4, 31, 248, 249, 268, 271], "pal": [205, 211], "pale": [213, 220], "pan": [213, 219], "pancak": [268, 269], "panda": [3, 4, 5, 8, 26, 29, 33, 49, 51, 52, 53, 54, 178, 180, 190, 195, 215, 216, 235, 237, 242, 244, 248, 250, 255, 257, 258, 259, 260, 261, 263, 267, 268, 270, 277, 303, 304, 310, 311, 313], "panel": [1, 2, 16, 24], "pant": [205, 211], "paper": [205, 211], "par": [213, 220], "parallel": [6, 7, 8, 9, 11, 12, 35, 41, 46, 50, 52, 54, 57, 59, 72, 182, 184, 192, 195, 196, 197, 205, 207, 209, 211, 212, 213, 215, 217, 218, 221, 226, 229, 231, 235, 236, 237, 241, 242, 243, 244, 246, 248, 249, 250, 254, 255, 256, 257, 259, 261, 262, 267, 268, 269, 270, 277, 281, 283, 290, 294, 305, 306, 315, 324, 329, 330], "parallel_strategi": [4, 32, 178, 185, 204], "parallel_strategy_kwarg": [4, 32, 178, 185], "param": [3, 28, 229, 234, 248, 252, 255, 258, 303, 310, 311], "param_nam": [178, 183], "param_spac": [3, 6, 28, 41, 303, 305, 310, 311, 324, 325, 326], "paramet": [1, 3, 4, 5, 6, 8, 9, 16, 28, 31, 32, 35, 40, 41, 52, 54, 61, 62, 64, 84, 87, 91, 96, 99, 103, 109, 115, 122, 131, 152, 156, 160, 164, 168, 177, 178, 182, 183, 185, 192, 193, 197, 199, 204, 205, 211, 213, 218, 221, 223, 226, 229, 233, 235, 238, 242, 245, 248, 252, 255, 258, 261, 265, 268, 269, 273, 278, 279, 283, 287, 289, 294, 295, 298, 300, 302, 304, 305, 306, 313, 315, 319, 323, 325, 326, 331], "parameter": [10, 71, 178, 199], "paramor": [205, 211], "parcel": [213, 220], "parent": [4, 31, 304, 313], "parish": [205, 211], "pariti": [80, 82], "park": [205, 211, 229, 234], "parquet": [3, 5, 7, 8, 9, 28, 35, 43, 51, 53, 54, 55, 59, 65, 137, 141, 142, 144, 153, 165, 178, 192, 195, 203, 204, 236, 241, 242, 247, 249, 256, 258, 260, 262, 265, 269, 273, 277, 303, 306, 310, 311, 332], "parquet_256": [235, 237, 268, 270, 271], "parquet_dir": [248, 250, 255, 257, 261, 263, 265], "parquet_fil": [268, 271], "parquet_path": [235, 237, 261, 263, 268, 271, 272, 277], "parquetdataset": [5, 35], "parquetfil": [268, 271], "pars": [7, 10, 48, 70, 154, 166, 261, 263, 279, 285, 287, 291, 295, 299, 307, 335], "parseabl": [295, 297, 299], "part": [0, 6, 8, 38, 49, 51, 109, 113, 151, 163, 213, 220, 221, 223, 227, 248, 250, 287, 290, 304, 305, 306, 307, 312, 321, 327, 333], "parti": [136, 148, 205, 211], "particular": [9, 61, 213, 216], "particularli": [2, 23, 205, 211], "partit": [9, 57, 205, 207, 209, 235, 236, 242, 243, 255, 257, 268, 271], "partner": [205, 211, 213, 219], "pass": [3, 4, 5, 6, 7, 8, 9, 10, 17, 22, 25, 28, 32, 35, 36, 41, 47, 51, 53, 57, 62, 70, 156, 168, 178, 179, 182, 183, 184, 185, 189, 192, 193, 195, 196, 197, 199, 200, 202, 204, 205, 211, 221, 226, 248, 249, 255, 258, 268, 277, 303, 304, 305, 306, 307, 310, 311, 313, 315, 324, 326, 329, 330, 335, 336], "passeng": [3, 8, 28, 51, 54, 213, 220, 267, 303, 310], "passenger_count": [3, 8, 28, 51, 303, 310], "passrol": [73, 78], "past": [151, 152, 154, 155, 163, 164, 166, 167, 175, 205, 211, 261, 262, 263, 264, 265, 267], "past_list": [261, 267], "past_norm": [261, 267], "patch": [7, 47, 80, 82], "path": [3, 4, 5, 7, 8, 9, 10, 28, 29, 31, 32, 35, 36, 48, 53, 60, 61, 62, 64, 65, 70, 71, 80, 83, 122, 125, 137, 141, 142, 144, 145, 153, 165, 178, 180, 188, 189, 190, 191, 197, 199, 200, 202, 203, 221, 228, 235, 237, 239, 240, 241, 242, 246, 247, 248, 250, 252, 254, 255, 257, 258, 260, 261, 263, 265, 267, 268, 270, 273, 277, 287, 290, 295, 298, 303, 304, 305, 306, 310, 311, 313, 317, 318, 319, 324, 331], "pathlib": [3, 4, 28, 29, 31, 178, 180, 261, 263, 304, 313], "paths_to_delet": [178, 203, 268, 277], "patient": [205, 211], "pattern": [7, 10, 12, 17, 19, 22, 43, 71, 178, 188, 193, 204, 241, 250, 254, 260, 277], "payload": [3, 28, 303, 310, 311], "payment_typ": [8, 51, 54], "pb": [137, 141], "pc": [205, 209, 211, 212], "pd": [3, 4, 5, 8, 26, 28, 29, 31, 33, 35, 49, 51, 52, 178, 180, 195, 213, 216, 235, 237, 242, 244, 248, 250, 254, 255, 257, 259, 260, 261, 263, 267, 268, 270, 303, 304, 310, 311, 313], "pdf": [248, 250], "pe": [261, 264], "peac": [205, 211], "peak": [278, 279, 283], "peer": [80, 82], "penalti": [142, 145], "pend": [221, 228, 303, 304, 310, 311, 318], "pendulum": [246, 247], "pendulum_diffus": [242, 246, 247], "pendulum_diffusion_ft": [242, 246], "pendulum_diffusion_result": [242, 246], "peopl": [205, 211, 213, 216, 220], "per": [8, 9, 51, 54, 59, 60, 136, 142, 144, 145, 148, 153, 160, 165, 176, 181, 183, 184, 188, 189, 197, 223, 227, 235, 238, 239, 242, 245, 246, 250, 252, 255, 256, 258, 259, 260, 261, 262, 265, 267, 268, 269, 273, 275, 277, 278, 279, 283, 284, 287, 290, 293, 295, 298, 306, 307, 329, 336], "per_worker_batch": [178, 182], "percentag": [8, 52], "percentil": [261, 267], "perceptu": [235, 241], "perfect": [205, 211, 213, 220, 235, 237, 287, 289], "perform": [1, 2, 3, 5, 6, 7, 8, 9, 10, 16, 22, 26, 27, 28, 35, 36, 38, 41, 43, 46, 47, 48, 49, 52, 54, 55, 56, 57, 61, 62, 64, 67, 68, 70, 136, 137, 140, 142, 145, 148, 152, 153, 154, 156, 160, 164, 165, 166, 168, 176, 178, 188, 190, 192, 194, 198, 204, 205, 210, 211, 212, 213, 218, 219, 220, 221, 223, 228, 235, 237, 241, 242, 243, 247, 248, 254, 255, 257, 259, 260, 261, 262, 265, 267, 268, 269, 273, 277, 278, 279, 280, 281, 287, 289, 293, 294, 295, 299, 300, 301, 302, 303, 305, 306, 307, 309, 310, 325, 328, 330, 331, 335, 336], "performantli": [11, 72], "perhap": [6, 7, 41, 47, 213, 219, 220, 305, 325], "period": [150, 152, 162, 164, 261, 267, 278, 279, 283, 307, 336], "permiss": [73, 78, 80, 82, 84, 85, 91, 92, 99, 100, 109, 111, 119, 153, 157, 165, 169], "permut": [235, 241], "persi": [205, 211], "persis": [178, 180], "persist": [4, 5, 31, 32, 36, 56, 57, 73, 77, 78, 80, 82, 136, 137, 140, 148, 153, 154, 159, 165, 166, 173, 179, 180, 181, 182, 186, 195, 198, 200, 203, 242, 246, 248, 250, 253, 255, 256, 257, 269, 273, 317, 327], "person": [213, 220, 248, 249, 254], "perspect": [213, 216, 220], "pertain": [154, 166], "phase": [221, 226, 286], "philip": [205, 211], "philosop": [213, 220], "philosophi": [213, 219], "photo": [205, 211, 268, 269], "photograph": [213, 220, 235, 236, 268, 270], "physic": [2, 9, 22, 61, 137, 139], "pi": [154, 166, 242, 243, 244, 247], "pi4_sampl": [154, 166], "pi_": [242, 243], "pic": [205, 211], "pick": [109, 113, 178, 202, 204, 205, 211, 213, 220, 248, 253, 261, 266, 268, 269, 276, 277, 278, 279, 285], "pickup": [261, 262], "pid": [221, 228, 304, 305, 318, 319, 324, 326], "piec": [142, 144, 205, 211], "pil": [4, 29, 178, 180, 186, 191, 196, 235, 237, 268, 270], "pile": [213, 216], "pin": [178, 191, 235, 237, 268, 269, 272], "pine": [255, 256], "pinecon": [7, 43], "pink": [213, 216], "pinterest": [9, 66], "pioneer": [7, 46], "pip": [0, 11, 17, 72, 122, 130, 136, 146, 149, 151, 152, 153, 163, 164, 165, 206, 214, 222, 230, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 287, 291], "pipelin": [3, 5, 7, 8, 9, 17, 28, 35, 43, 46, 53, 55, 57, 61, 62, 66, 137, 141, 143, 152, 164, 178, 180, 192, 194, 195, 197, 204, 213, 215, 220, 229, 232, 233, 235, 236, 237, 242, 243, 247, 248, 249, 255, 256, 260, 261, 262, 267, 268, 269, 271, 277, 278, 279, 282, 283, 294, 295, 302, 306, 328, 330, 332], "pipeline_parallel_s": [287, 293], "pitch": [205, 211], "pivot": [242, 243], "pixel": [6, 9, 39, 61, 178, 180, 186, 195, 196, 236, 237, 239, 242, 244, 268, 270, 305, 322], "pixeldiffus": [235, 238, 239, 241], "pizza": [268, 269], "pl": [5, 33, 35, 36, 235, 237, 238, 239, 242, 244, 245, 246], "pl_ckpt": [235, 241], "place": [0, 7, 48, 178, 180, 199, 201, 205, 211, 213, 220, 229, 234, 278, 279, 282], "placehold": [84, 85, 91, 92, 99, 100, 107, 109, 110, 120, 122, 123, 151, 163, 278, 279, 285, 287, 291], "placement": [178, 182, 185, 186, 189, 242, 243, 261, 267, 268, 269, 272, 273, 277], "plai": [151, 163, 205, 211, 213, 219, 268, 277], "plain": [235, 237, 268, 269], "plan": [9, 60, 61, 63, 73, 78, 80, 82, 83, 84, 86, 91, 95, 99, 101, 107, 109, 112, 120, 122, 126, 135, 142, 145, 205, 209, 211, 295, 298], "plane": [75, 77], "planner": [261, 262], "plate": [205, 211], "plateau": [261, 267], "platform": [7, 43, 45, 46, 73, 76, 80, 82, 135, 136, 137, 140, 142, 143, 144, 145, 148, 149, 154, 159, 160, 161, 166, 173, 176, 178, 180, 205, 209, 221, 223, 227, 278, 279, 283, 307, 337], "plausibl": [242, 247], "pleas": [73, 78, 84, 87, 91, 96, 99, 100, 103, 107, 109, 110, 115, 120, 122, 123, 131, 134, 142, 144, 145, 161, 206, 214, 222, 230, 248, 254, 268, 269, 295, 298], "plot": [178, 180, 190, 191, 213, 216, 219, 220, 237, 238, 244, 245, 250, 255, 257, 260, 263, 267, 270, 277, 307, 336], "plotlin": [213, 220], "plt": [4, 6, 9, 29, 31, 32, 38, 39, 56, 60, 178, 180, 191, 235, 237, 239, 241, 242, 244, 246, 248, 250, 252, 255, 257, 259, 261, 263, 265, 267, 268, 270, 275, 277, 304, 305, 307, 313, 319, 322, 336], "plu": [73, 78, 178, 180, 242, 244, 268, 270], "plugin": [5, 36, 80, 83, 107, 108, 120, 121, 122, 128, 235, 239, 242, 246], "pm": [242, 243], "pndm": [235, 241], "png": [9, 61], "poc": [80, 82], "pod": [80, 82, 83, 99, 105, 106, 107, 109, 117, 118, 120, 122, 129, 278, 279, 283], "point": [4, 9, 10, 31, 61, 71, 73, 76, 80, 82, 84, 85, 91, 92, 99, 100, 109, 110, 122, 123, 146, 153, 165, 178, 182, 188, 202, 205, 207, 211, 213, 219, 220, 255, 257, 278, 279, 285, 287, 292], "pointless": [213, 216], "pole": [255, 256], "polici": [73, 77, 78, 80, 82, 83, 91, 95, 98, 119, 122, 126, 238, 241, 244], "polish": [213, 220], "polit": [213, 216, 220, 295, 298], "politician": [213, 216], "poll": [205, 211], "pont": [205, 211], "pool": [9, 62], "poor": [10, 68], "poorli": [6, 41, 305, 325], "popul": [8, 51, 84, 86, 99, 101], "popular": [3, 7, 27, 28, 45, 46, 205, 210, 248, 250, 295, 298], "porn": [213, 216], "porno": [213, 216], "pornograph": [213, 216], "port": [136, 149, 255, 256], "portion": [221, 226], "pos_enc": [261, 264], "posit": [7, 47, 205, 209, 211, 213, 215, 216, 218, 220, 229, 234, 261, 264], "posix": [73, 78], "possibl": [213, 215, 287, 294, 295, 300, 302], "possibli": [7, 48], "post": [3, 4, 5, 6, 10, 28, 32, 37, 42, 70, 73, 78, 142, 145, 146, 229, 233, 235, 236, 248, 249, 254, 303, 304, 307, 310, 311, 320, 335, 336], "poster": [213, 220], "postgresql": [7, 43], "postwar": [213, 219, 220], "potato": [213, 216], "potemkin": [213, 220], "potenti": [2, 20, 137, 139], "potter": [205, 211], "power": [2, 23, 205, 207, 213, 215, 221, 223, 227, 255, 260, 295, 296, 297, 300], "powershel": [295, 298], "pq": [235, 237, 261, 263, 268, 270, 271], "practic": [4, 11, 32, 72, 73, 76, 91, 95, 135, 136, 148, 158, 170, 178, 179, 188, 191, 204, 205, 212, 213, 217, 221, 223, 261, 263, 268, 269, 278, 296, 297, 298, 302], "practition": [3, 27, 303, 309], "prayer": [205, 211], "pre": [9, 62, 91, 95, 151, 152, 153, 163, 164, 165, 221, 226, 229, 232, 233, 235, 241, 248, 254, 261, 262, 268, 269, 306, 328, 329, 330], "preced": [213, 220], "precis": [5, 35, 36, 235, 241, 242, 247, 268, 277, 278, 279, 283, 287, 289], "precomput": [268, 271, 278, 279, 281], "preconfigur": [178, 181], "pred": [4, 31, 32, 178, 191, 242, 245, 248, 252, 255, 259, 260, 261, 264, 265, 267, 268, 273, 277], "pred_d": [255, 259, 260, 261, 267, 268, 277], "pred_label": [255, 258, 259], "pred_nois": [235, 238, 241, 242, 247], "pred_norm": [261, 267], "pred_prob": [255, 258], "pred_row": [261, 267, 268, 277], "predefin": [152, 164], "predic": [8, 55], "predict": [3, 6, 7, 9, 10, 28, 41, 44, 62, 65, 70, 205, 207, 221, 225, 226, 229, 231, 233, 234, 235, 236, 238, 241, 242, 243, 245, 248, 249, 251, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 267, 268, 270, 277, 295, 301, 304, 305, 306, 307, 311, 313, 319, 324, 330, 332, 335, 336], "predicted_label": [9, 10, 62, 64, 70, 142, 145, 268, 277, 306, 307, 330, 331, 335, 336], "predicted_prob": [303, 310, 311], "prediction_pipelin": [3, 28], "predictor": [3, 28, 268, 277, 278, 279, 281, 303, 310, 311], "preemption": [9, 57, 154, 166, 178, 201], "prefer": [80, 82, 84, 87, 91, 96, 99, 103, 109, 115, 122, 129, 131, 146, 151, 163, 213, 220, 235, 241, 295, 301], "prefer_spot": [137, 141], "prefetch": [268, 277], "prefetch_batch": [5, 36, 178, 194], "prefil": 286, "prefix": [4, 8, 32, 53, 91, 95, 153, 165, 248, 254, 261, 267, 268, 277], "prefix_for_the_resources_ad": [91, 95], "preinstal": [150, 162], "prem": [150, 162], "premier": [205, 211], "premis": [73, 76, 157, 169], "prepar": [2, 5, 21, 36, 135, 156, 160, 168, 177, 182, 185, 189, 192, 204, 213, 215, 219, 220, 221, 223, 226, 242, 246, 265, 267, 269, 277, 295, 298], "prepare_data_load": [4, 32, 179, 181, 182, 188, 204, 261, 263, 268, 269, 270, 272, 277, 304, 316], "prepare_model": [4, 32, 179, 181, 182, 188, 204, 248, 249, 250, 252, 261, 263, 265, 268, 269, 270, 273, 277, 304, 316], "prepare_train": [235, 239, 242, 246], "preprocess": [4, 5, 7, 8, 9, 32, 35, 43, 51, 55, 61, 152, 164, 178, 180, 186, 191, 192, 196, 197, 204, 206, 214, 215, 221, 222, 223, 230, 235, 236, 237, 242, 243, 247, 248, 250, 254, 261, 262, 268, 269, 270, 271, 303, 306, 307, 310, 311, 328, 336], "preprocess_imag": [235, 237], "preprocessed_df": [213, 220], "preprocessor": [8, 55, 213, 216, 307, 336], "preprocessor_app": [307, 336], "preprocessor_handl": [307, 336], "prerequisit": [92, 110, 123], "presenc": [255, 260], "present": [4, 7, 31, 46, 142, 143, 178, 180, 213, 219, 220, 248, 250, 255, 256, 258, 261, 263, 267, 268, 277, 278, 279, 283, 295, 299], "preserv": [178, 179, 181, 255, 257, 261, 262], "press": [11, 72], "pressur": [137, 141, 229, 234, 278, 279, 282], "pretend": [213, 217], "pretenti": [213, 216], "pretrain": [5, 35, 36, 37, 205, 210], "pretrainedconfig": [5, 35], "pretti": [213, 219, 268, 277], "prevent": [9, 61, 261, 263], "preview": [0, 84, 86, 91, 95, 99, 101, 109, 112, 296], "previou": [99, 104, 109, 116, 122, 132, 142, 143, 151, 163, 178, 192, 193, 199, 202, 229, 234, 248, 254, 255, 256, 268, 269, 278, 279, 281, 282, 295, 300], "previous": [4, 31, 178, 199], "price": [3, 28, 205, 209, 211, 261, 262, 303, 310], "priest": [205, 211], "primari": [7, 46, 136, 149], "primarili": [5, 9, 35, 63, 80, 82, 261, 265, 306, 307, 330, 336], "prime": [205, 211], "primit": [268, 270], "princip": [73, 78], "print": [2, 3, 4, 5, 6, 9, 11, 18, 20, 22, 24, 28, 31, 32, 35, 40, 41, 61, 72, 142, 144, 151, 152, 153, 154, 155, 159, 163, 164, 165, 166, 167, 174, 178, 182, 187, 203, 205, 209, 212, 213, 216, 217, 218, 219, 220, 221, 226, 227, 229, 234, 235, 237, 239, 240, 241, 242, 244, 246, 247, 248, 250, 252, 254, 255, 257, 258, 259, 260, 261, 263, 265, 266, 267, 268, 270, 271, 273, 274, 276, 277, 278, 279, 285, 287, 291, 292, 295, 298, 299, 300, 304, 305, 313, 317, 319, 323, 325, 326], "print_metrics_ray_train": [4, 32, 178, 182, 187, 193, 199, 304, 315, 317, 319], "printout": [268, 277], "prior": [7, 43, 151, 163, 255, 258], "priorit": [6, 41, 305, 325], "privat": [73, 76, 78, 80, 82, 84, 86, 87, 91, 96, 99, 101, 103, 109, 112, 115, 122, 125, 131, 153, 157, 165, 169], "private_subnet": [73, 78], "privileg": [73, 78, 80, 82], "pro": [295, 301], "prob": [2, 20, 255, 259], "probabilist": [261, 267], "probabl": [205, 211, 268, 269], "problem": [8, 51, 136, 148, 154, 166, 278, 279, 284, 295, 301], "proce": [5, 35, 304, 307, 313, 318, 319, 336], "process": [1, 3, 4, 5, 6, 8, 9, 10, 14, 15, 16, 17, 19, 21, 23, 28, 31, 32, 35, 36, 40, 41, 43, 48, 50, 53, 55, 57, 58, 59, 61, 62, 69, 73, 76, 78, 122, 123, 137, 139, 142, 144, 152, 154, 156, 160, 164, 166, 168, 176, 178, 179, 181, 182, 184, 186, 188, 189, 191, 192, 195, 205, 207, 209, 210, 211, 212, 220, 221, 223, 226, 227, 228, 229, 231, 237, 242, 243, 247, 248, 249, 250, 255, 256, 259, 261, 262, 263, 267, 268, 271, 282, 284, 285, 286, 287, 288, 299, 300, 304, 305, 306, 307, 313, 314, 315, 323, 325, 328, 330, 334], "processed_d": [235, 237], "prod": [2, 21], "produc": [3, 4, 5, 7, 8, 9, 28, 31, 32, 36, 44, 53, 59, 178, 188, 213, 219, 235, 236, 248, 254, 261, 267, 304, 306, 313, 319, 329], "product": [2, 7, 11, 22, 29, 33, 38, 47, 49, 56, 72, 73, 76, 135, 142, 145, 152, 153, 164, 165, 178, 188, 204, 205, 207, 213, 220, 229, 231, 235, 241, 248, 249, 250, 251, 254, 255, 256, 258, 268, 269, 278, 279, 280, 281, 283, 284, 287, 288, 289, 292, 293, 294, 295, 296, 297, 299, 301, 302, 333], "production": [155, 159, 160, 161, 167, 173, 176, 178, 204], "profession": 135, "profil": [80, 82, 84, 86, 90, 136, 148, 242, 247, 268, 277], "profile_data": 146, "prog_bar": [5, 35, 235, 238, 242, 245], "program": [2, 24, 142, 145, 295, 301], "programm": [307, 334], "programmat": [142, 145, 155, 159, 167, 173, 175, 307, 336], "progress": [4, 32, 154, 166, 178, 179, 187, 190, 198, 200, 201, 248, 250, 252, 268, 269, 270, 304, 317], "project": [0, 9, 10, 57, 68, 73, 75, 78, 91, 93, 94, 95, 122, 124, 125, 126, 128, 134, 146, 153, 165, 170, 172, 205, 211, 213, 220], "project_numb": [91, 95], "prometheu": 148, "promot": [261, 267], "promote_opt": [255, 258], "prompt": [11, 72, 150, 151, 162, 163, 278, 279, 281, 282, 295, 298], "promptli": [213, 219], "proof": [80, 82, 205, 211], "proper": [84, 85, 91, 92, 99, 100, 109, 111, 178, 182], "properli": [7, 11, 46, 72, 136, 149, 206, 214, 221, 222, 223, 229, 230, 234, 268, 270, 295, 300], "properti": [7, 43, 295, 300], "proport": [8, 9, 53, 59, 62, 248, 250, 306, 329], "proprietari": [7, 43], "prosper": [205, 211], "protect": [255, 256], "protocol": [7, 10, 43, 48, 68, 146], "prototyp": [151, 163, 287, 289, 294], "prove": [235, 240, 268, 276, 307, 336], "provid": [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 17, 27, 30, 31, 34, 43, 46, 48, 49, 50, 56, 64, 68, 69, 71, 72, 75, 76, 77, 78, 80, 83, 84, 86, 91, 95, 97, 101, 106, 109, 112, 122, 126, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 165, 166, 167, 168, 169, 172, 175, 176, 178, 179, 180, 183, 192, 197, 202, 213, 215, 221, 223, 227, 229, 231, 235, 239, 248, 250, 252, 255, 259, 261, 263, 265, 268, 269, 278, 279, 280, 282, 284, 287, 289, 290, 292, 293, 294, 295, 297, 298, 299, 300, 303, 304, 305, 306, 307, 309, 310, 311, 318, 324, 326, 327, 328, 331, 334, 336], "provis": [4, 5, 9, 30, 34, 57, 80, 81, 82, 83, 135, 158, 171, 235, 236, 255, 256, 268, 269, 303, 310, 311], "proxi": [7, 48, 142, 145, 146], "proxim": [255, 256], "proxy_http_request": [142, 145, 146], "proxy_route_to_replica": [142, 145, 146], "prune": [235, 241, 242, 247, 255, 260], "pseudo": [235, 241], "pt": [4, 9, 10, 31, 32, 62, 66, 70, 71, 178, 188, 191, 199, 200, 235, 241, 248, 252, 254, 261, 265, 267, 268, 273, 277, 304, 306, 307, 313, 317, 319, 330, 335, 336, 338], "public": [3, 8, 9, 10, 28, 51, 54, 59, 61, 62, 64, 70, 73, 76, 78, 142, 144, 205, 207, 211, 212, 213, 215, 216, 219, 220, 287, 290, 306, 307, 329, 330, 331, 335, 336], "public_subnet": [73, 78], "publicli": [157, 169, 295, 298], "publish": [156, 168], "pull": [151, 163, 205, 211, 235, 237, 248, 252, 255, 258, 261, 265, 268, 269, 270, 271, 275], "pulocationid": [8, 51], "pumpkin": [205, 211], "pun": [213, 216], "punchestown": [205, 211], "punctuat": [213, 220], "pure": [235, 236, 248, 249], "purpl": [205, 211], "purpos": [1, 11, 13, 43, 72, 78, 213, 216, 217, 268, 269, 306, 330], "push": [151, 163, 205, 211, 213, 220, 268, 269], "pushdown": [8, 55], "put": [2, 12, 18, 73, 78, 205, 211, 213, 219, 220], "putobject": [73, 78], "pwd": [4, 31], "py": [0, 9, 10, 11, 60, 64, 71, 72, 84, 88, 91, 97, 99, 106, 109, 118, 122, 133, 136, 137, 141, 142, 144, 149, 151, 153, 154, 155, 156, 163, 165, 166, 167, 168, 175, 176, 278, 279, 285, 287, 290, 293, 295, 298, 299, 300], "py311": [278, 279, 285, 287, 292], "py312": [137, 141], "pyarrow": [7, 9, 43, 59, 142, 144, 235, 237, 242, 244, 248, 250, 255, 257, 258, 261, 263, 268, 269, 270], "pydant": [3, 26, 295, 299, 307, 336], "pydata": [261, 263], "pyflink": [7, 46], "pypi": [153, 165], "pyplot": [4, 6, 9, 29, 38, 56, 178, 180, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 304, 305, 307, 313, 322, 336], "pyproj": [152, 164], "pyspark": [7, 46], "python": [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 19, 20, 25, 27, 32, 36, 41, 43, 45, 46, 47, 50, 68, 72, 84, 88, 91, 97, 99, 106, 109, 118, 122, 133, 136, 142, 144, 145, 146, 149, 151, 152, 153, 154, 155, 156, 161, 163, 164, 165, 166, 167, 168, 173, 178, 181, 191, 206, 214, 221, 222, 223, 228, 229, 230, 231, 235, 237, 242, 244, 248, 249, 250, 255, 257, 261, 263, 268, 270, 278, 279, 283, 285, 287, 290, 292, 295, 298, 300, 303, 305, 307, 309, 324, 334], "python3": [0, 153, 165], "pythonmalloc": [136, 149], "pytorch": [9, 30, 34, 38, 59, 178, 179, 180, 181, 182, 186, 188, 192, 193, 194, 196, 204, 224, 227, 228, 229, 231, 235, 236, 237, 239, 241, 242, 243, 244, 246, 248, 249, 250, 251, 252, 254, 262, 264, 268, 269, 270, 271, 277, 312, 315, 316, 317, 321, 339], "pyyaml": 0, "q": [248, 250, 278, 279, 282, 295, 301], "q2": [205, 211], "q_q": [205, 211], "qp": [142, 145], "qt": [205, 209, 211, 212], "qtr": [205, 211], "quad": [235, 236, 242, 243, 248, 249], "qualif": [295, 298], "qualit": [268, 277], "qualiti": [7, 43, 235, 241, 248, 254, 287, 289], "quantiz": [278, 279, 282, 294], "queri": [7, 43, 84, 86, 99, 101, 109, 112, 113, 137, 140, 141, 142, 145, 146, 154, 156, 166, 168, 229, 234, 287, 291, 295, 298], "question": [205, 211], "queu": [80, 82], "queue": [1, 7, 10, 13, 48, 69, 80, 82, 142, 145, 307, 334], "quick": [1, 4, 12, 26, 31, 32, 73, 76, 80, 82, 154, 166, 178, 180, 188, 189, 235, 237, 239, 248, 250, 255, 257, 258, 268, 270, 277, 295, 298, 303, 308], "quickli": [8, 11, 51, 72, 151, 157, 163, 169, 213, 220, 235, 237, 255, 257, 261, 265, 268, 270, 275, 278, 279, 283], "quickstart": [84, 85, 91, 93, 99, 100, 109, 111, 122, 124], "quit": [205, 211, 213, 220], "quot": [205, 211], "quota": [73, 75, 278, 279, 283], "qwen": [295, 299, 300, 301], "qwen2": [295, 299], "qwen3": [295, 300], "r": [0, 7, 11, 43, 72, 91, 98, 122, 134, 146, 205, 206, 211, 213, 214, 216, 222, 230, 242, 243, 248, 249, 250, 255, 256, 258, 261, 262, 268, 269, 270, 304, 313], "r1": [295, 301], "r2": [7, 43], "race": [205, 211, 213, 216, 219], "radio": [205, 211], "rafe": [205, 211], "ragnarok": [205, 211], "rahul": [205, 211], "rai": [13, 14, 15, 18, 19, 20, 21, 22, 23, 35, 40, 46, 51, 52, 53, 54, 59, 60, 61, 63, 64, 65, 70, 71, 73, 75, 76, 78, 81, 83, 90, 98, 105, 106, 107, 108, 117, 118, 120, 121, 135, 140, 141, 147, 148, 150, 151, 152, 153, 155, 156, 158, 160, 162, 163, 164, 165, 167, 168, 171, 172, 173, 176, 177, 180, 181, 183, 184, 185, 186, 187, 189, 190, 199, 200, 201, 202, 203, 208, 209, 210, 211, 224, 226, 227, 232, 233, 238, 241, 244, 247, 251, 253, 254, 260, 266, 270, 273, 274, 275, 276, 283, 286, 292, 293, 294, 297, 299, 300, 302, 313, 317, 318, 319, 323, 329, 330, 331, 335], "railwai": [213, 220], "rais": [2, 20, 235, 239, 241, 242, 246, 268, 277], "ram": [9, 61], "ramen": [268, 269], "rammstein": [205, 211], "rand": [2, 10, 18, 22, 70, 142, 145, 307, 335, 336], "randint": [2, 4, 5, 6, 24, 31, 32, 35, 41, 137, 141, 178, 180, 191, 235, 238, 242, 244, 295, 300, 303, 305, 310, 311, 324, 325], "randn": [235, 241, 242, 244, 247], "randn_lik": [5, 35, 235, 238], "random": [1, 2, 4, 6, 8, 9, 10, 12, 17, 18, 20, 22, 24, 31, 32, 41, 54, 64, 70, 91, 97, 109, 118, 137, 141, 142, 145, 154, 166, 178, 180, 191, 235, 237, 241, 242, 243, 244, 247, 248, 250, 254, 255, 257, 268, 269, 270, 295, 300, 303, 305, 306, 307, 310, 311, 325, 331, 335, 336], "random_shuffl": [8, 9, 54, 64, 235, 237, 242, 244, 255, 257, 306, 331], "random_st": [3, 28, 255, 257, 268, 271], "randomize_block_ord": [8, 9, 54, 64, 248, 250, 306, 331], "randomli": [4, 8, 9, 31, 32, 54, 64, 178, 180, 248, 254, 306, 331], "rang": [1, 2, 4, 6, 9, 16, 18, 20, 24, 31, 32, 40, 41, 61, 73, 78, 137, 141, 154, 166, 178, 180, 182, 191, 193, 196, 199, 213, 217, 221, 226, 229, 231, 235, 241, 242, 244, 247, 248, 252, 255, 259, 261, 262, 263, 265, 268, 271, 273, 303, 304, 305, 307, 310, 313, 315, 319, 323, 325, 326, 335, 336], "rank": [3, 4, 5, 28, 32, 36, 179, 181, 182, 187, 189, 200, 204, 235, 239, 252, 254, 255, 258, 259, 260, 261, 265, 268, 272, 273, 295, 298, 304, 317], "rap": [205, 211], "rapid": [151, 163], "rapidli": [150, 162], "rate": [5, 6, 35, 41, 80, 83, 142, 145, 178, 182, 205, 211, 213, 216, 221, 226, 227, 235, 241, 242, 247, 251, 252, 254, 261, 267, 268, 277, 278, 279, 282, 305, 325], "rather": [9, 60, 213, 216, 220, 248, 254, 255, 257, 261, 262, 278, 279, 281], "ratings_d": [248, 250], "ratings_parquet": [248, 250], "ratings_parquet_uri": [248, 250], "ratio": [146, 255, 257, 259], "rattl": [213, 220], "rattler": [205, 211], "ravenstein": [213, 220], "raw": [7, 43, 109, 113, 137, 140, 141, 178, 195, 196, 205, 211, 221, 226, 235, 237, 241, 248, 250, 254, 255, 256, 257, 259, 261, 263, 268, 269, 270, 277, 303, 304, 305, 310, 318, 324, 326], "raw_path": [248, 250], "ray_actor_opt": [307, 335, 336], "ray_data_synthet": [137, 141], "ray_dedup_log": [221, 228, 305, 326], "ray_enable_windows_or_osx_clust": [136, 149], "ray_pl_ckpt": [235, 239, 242, 246], "ray_result": [221, 228, 305, 324], "ray_scheduler_ev": [304, 305, 307, 318, 324, 335], "ray_train_v2_en": [235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270], "rayddp": [235, 239], "rayddpstrategi": [5, 33, 36, 235, 239, 242, 246], "raylightningenviron": [5, 33, 36, 235, 237, 239, 242, 244, 246], "rayproject": [152, 164], "rayserv": [229, 233], "raytaskerror": [2, 20], "raytrainreportcallback": [5, 33, 36, 235, 236, 239, 242, 243, 246, 247, 255, 256, 257, 258, 259, 260], "raytrainwork": [221, 228, 304, 318], "raytrainxgboosttrain": [3, 26, 28], "rayturbo": [9, 10, 57, 68], "rbac": [158, 171, 172], "rbi": [205, 209, 211], "rd": [73, 78, 205, 211, 255, 257], "rdata": [261, 263, 267, 268, 277], "re": [2, 6, 9, 10, 11, 20, 40, 64, 71, 72, 84, 87, 89, 91, 96, 98, 99, 102, 103, 109, 114, 115, 122, 131, 134, 135, 151, 154, 155, 156, 163, 166, 167, 168, 178, 185, 186, 191, 203, 204, 205, 211, 213, 216, 235, 236, 237, 241, 242, 243, 247, 248, 249, 254, 255, 256, 257, 260, 261, 267, 268, 269, 270, 287, 291, 292, 295, 302, 306, 331], "reach": [80, 82, 178, 202, 213, 219, 278, 279, 281], "read": [1, 2, 4, 5, 7, 10, 16, 19, 23, 24, 31, 32, 37, 43, 51, 53, 56, 59, 71, 73, 78, 158, 171, 178, 180, 191, 192, 195, 213, 219, 220, 235, 237, 248, 250, 255, 257, 261, 263, 268, 269, 270, 271, 277, 303, 304, 310, 311, 320, 329], "read_csv": [4, 8, 31, 51, 248, 250, 254, 261, 263, 304, 313], "read_databricks_t": [9, 59], "read_imag": [9, 59, 61, 64, 306, 307, 329, 331, 336], "read_json": [8, 51], "read_parquet": [3, 5, 8, 9, 28, 35, 51, 54, 59, 142, 144, 178, 195, 235, 237, 248, 250, 255, 257, 268, 277, 303, 310, 311], "read_row_group": [268, 271], "read_tabl": [261, 263, 268, 271], "readabl": [8, 51, 178, 188], "readfil": [9, 63], "readi": [2, 11, 24, 72, 84, 87, 91, 96, 99, 102, 103, 109, 114, 115, 122, 131, 135, 150, 151, 156, 160, 162, 163, 168, 176, 178, 186, 196, 204, 205, 211, 221, 226, 242, 244, 247, 248, 249, 250, 261, 263, 268, 269, 271, 278, 279, 286, 287, 289, 294, 295, 298, 299, 302], "readm": [142, 145, 206, 214, 222, 230, 339], "ready_ref": [2, 24], "real": [7, 46, 205, 211, 213, 217, 221, 226, 243, 247, 248, 249, 254, 255, 260, 261, 267, 268, 269, 278, 279, 281, 295, 300, 302], "realist": [213, 220, 235, 236, 248, 250, 268, 277], "realiti": [213, 216], "realknowncaus": [261, 263], "realli": [205, 211, 213, 216, 219, 220], "reason": [137, 139, 153, 165, 242, 247, 278, 279, 282, 284, 287, 289, 295, 301], "reasoning_pars": [295, 300], "reassur": [268, 271], "rebuild": [0, 178, 191, 248, 254], "rec": [235, 237, 261, 263, 268, 270], "rec_sys_tutori": [248, 250, 252, 254], "recal": [307, 335], "recalcul": [278, 279, 282], "recap": [6, 41, 305, 324], "receiv": [4, 5, 7, 32, 36, 48, 80, 82, 178, 179, 229, 231, 248, 249, 250, 252, 261, 263, 295, 298, 300], "recent": [153, 165, 178, 190, 202, 213, 220, 235, 239, 242, 246, 248, 253, 261, 267, 268, 273, 275], "recent_kei": [153, 165], "recent_nam": [153, 165], "recip": [4, 31, 205, 211], "recipi": 146, "reclaim": [235, 241, 248, 254], "recommend": [0, 3, 4, 5, 6, 7, 8, 9, 10, 26, 29, 32, 33, 35, 38, 46, 49, 52, 56, 66, 67, 71, 73, 78, 80, 82, 142, 143, 152, 164, 178, 180, 188, 204, 251, 278, 279, 280, 287, 288, 296, 298, 299], "recomput": [248, 254], "record": [3, 8, 28, 51, 136, 148, 235, 237, 261, 263, 268, 270, 275, 303, 310, 311], "recov": [9, 57, 178, 198, 235, 236, 255, 260, 261, 262, 267], "recoveri": [178, 188, 198, 200, 204, 235, 236, 242, 247, 248, 249, 252, 261, 265, 268, 269, 277], "recreat": [261, 267, 307, 336], "recurr": [261, 262], "recurs": [84, 89, 99, 107, 109, 120, 178, 203], "red": [205, 211, 213, 220, 235, 236, 268, 269, 278, 279, 282], "redefin": [3, 28], "redeploi": [242, 243], "redi": [73, 77], "redshift": [7, 43], "reduc": [3, 6, 7, 8, 9, 10, 26, 38, 43, 49, 56, 60, 61, 67, 152, 164, 205, 212, 221, 227, 261, 267, 278, 279, 280, 295, 297, 299], "reduct": [287, 293], "redund": [178, 188, 268, 277, 278, 279, 282], "ref": [1, 2, 15, 16, 18, 19, 22, 24, 25], "refer": [1, 2, 5, 8, 9, 13, 15, 18, 19, 20, 22, 35, 51, 61, 65, 73, 78, 84, 85, 91, 93, 99, 100, 109, 110, 111, 122, 123, 124, 136, 142, 145, 148, 149, 151, 153, 158, 163, 165, 171, 178, 190, 191, 221, 225, 278, 279, 281, 304, 306, 316, 329, 332], "reflect": [7, 43, 268, 275], "refresh": [213, 219], "reg": [3, 28], "regard": [205, 211], "regardless": [4, 5, 32, 35, 261, 265], "region": [73, 79, 84, 86, 91, 94, 95, 99, 101, 102, 104, 109, 112, 114, 116, 122, 125, 126, 128, 132, 153, 165, 295, 298, 303, 304, 305, 310, 311, 318, 324, 326], "regist": [76, 78, 80, 82, 85, 86, 89, 90, 92, 95, 97, 100, 101, 105, 108, 110, 112, 117, 121, 123, 126, 136, 142, 143, 148, 152, 158, 161, 164, 171, 229, 233, 235, 241, 248, 254, 268, 277], "register_buff": [261, 264], "register_us": 146, "registr": [91, 95, 98, 122, 134, 135], "registration_complet": 146, "registri": [268, 277], "regress": [6, 41, 255, 260, 305, 325], "regular": [1, 14, 154, 166, 178, 181, 248, 254, 268, 271, 277], "reimplement": [178, 193], "reinforc": [7, 46], "rel": [0, 4, 32, 261, 267], "rel_path": [295, 298], "relat": [99, 107, 109, 113, 120, 146, 154, 166], "relationship": [6, 41, 73, 75, 78, 305, 325], "releas": [11, 72, 84, 86, 99, 101, 107, 109, 112, 120, 178, 191, 205, 211, 213, 216, 242, 247], "relev": [4, 8, 9, 32, 54, 55, 64, 248, 254, 261, 265, 268, 271, 295, 298, 306, 331], "reli": [7, 10, 43, 46, 48, 68, 178, 192, 255, 257, 268, 270], "reliabl": [4, 5, 7, 9, 30, 34, 46, 57, 155, 156, 159, 160, 167, 168, 173, 176, 178, 179, 198, 204, 268, 273, 295, 297, 299, 302], "religi": [205, 211], "religion": [205, 211], "reload": [10, 71, 178, 191, 200, 201, 248, 254, 295, 298], "relpath": [295, 298], "relu": [235, 238, 242, 245, 304, 313, 319], "remain": [137, 141, 178, 181, 188, 205, 209, 211, 212, 235, 237, 248, 250, 261, 265, 287, 289], "remaind": [248, 250], "remark": [205, 211], "remast": [205, 211], "remateri": [255, 257], "rememb": [154, 155, 156, 166, 167, 168, 178, 188, 213, 220, 235, 241, 287, 293], "remind": [205, 211, 213, 220], "remot": [2, 3, 6, 9, 10, 11, 12, 16, 18, 19, 20, 21, 22, 23, 24, 25, 28, 40, 59, 70, 72, 146, 151, 154, 155, 159, 163, 166, 167, 174, 178, 191, 255, 256, 259, 260, 268, 277, 306, 307, 329, 335, 336], "remote_add": [1, 2, 14, 15, 19, 22, 23], "remote_funct": [1, 15], "remote_path": [9, 10, 62, 70], "remov": [2, 5, 8, 11, 25, 35, 53, 72, 99, 107, 109, 120, 178, 180, 188, 203, 248, 254, 268, 277, 278, 279, 284], "remove_code_output": 0, "remu": [205, 209, 211, 212], "renam": [151, 163, 255, 257, 261, 263], "renew": [80, 83], "rent": [213, 216], "repackag": [242, 243], "repartit": [8, 54, 137, 141, 205, 207, 209, 212], "repeat": [221, 228, 255, 259, 261, 264, 304, 305, 318, 326], "repeatedli": [205, 210], "replac": [84, 85, 86, 87, 91, 92, 94, 96, 97, 99, 100, 101, 103, 104, 109, 110, 112, 113, 115, 116, 118, 122, 123, 125, 128, 131, 132, 137, 141, 151, 153, 156, 163, 165, 168, 178, 192, 193, 196, 205, 211, 213, 220, 235, 241, 242, 243, 247, 248, 254, 261, 267, 268, 277, 278, 279, 282, 284, 295, 298], "replic": [2, 4, 5, 18, 32, 36, 178, 179, 188], "replica": [7, 48, 68, 142, 145, 146, 156, 168, 178, 179, 229, 233, 234, 268, 277, 278, 279, 284, 285, 290, 295, 298, 307, 334, 336], "replica_handle_request": [142, 145, 146], "repo": [0, 99, 102, 104, 109, 114, 116, 122, 129, 132, 278, 279, 285, 295, 298], "repo_id": [295, 298], "report": [5, 6, 7, 31, 36, 40, 41, 44, 179, 180, 181, 182, 190, 193, 197, 199, 200, 204, 205, 211, 235, 236, 238, 239, 241, 242, 243, 244, 248, 249, 250, 252, 255, 258, 261, 262, 265, 267, 268, 269, 273, 275, 277, 303, 305, 307, 310, 311, 312, 315, 319, 323, 324, 325, 326, 336], "report_metrics_torch": [4, 31, 304, 313], "reportedli": [205, 211], "repositori": [0, 122, 129, 156, 168], "repres": [3, 6, 8, 28, 39, 51, 213, 220, 248, 249, 251, 278, 279, 283, 303, 310], "represent": [278, 279, 281, 283], "reproduc": [11, 72, 206, 214, 222, 230, 235, 237, 248, 250, 255, 256, 257, 268, 271], "republican": [205, 211], "req": [151, 163], "request": [1, 3, 7, 9, 10, 16, 17, 23, 26, 28, 48, 62, 67, 68, 69, 70, 80, 82, 83, 142, 145, 178, 191, 231, 233, 261, 263, 267, 268, 277, 278, 279, 281, 282, 284, 290, 293, 295, 298, 299, 300, 303, 307, 310, 311, 334, 335, 336], "request_data": [303, 310, 311], "requir": [0, 2, 4, 5, 7, 8, 9, 10, 22, 23, 29, 30, 33, 34, 43, 46, 54, 63, 64, 68, 69, 73, 75, 76, 77, 78, 80, 82, 83, 93, 95, 102, 108, 114, 121, 124, 130, 134, 135, 146, 152, 153, 155, 158, 159, 161, 164, 165, 167, 171, 173, 178, 179, 185, 192, 200, 205, 206, 211, 214, 221, 222, 226, 227, 230, 235, 236, 242, 243, 248, 250, 261, 263, 268, 269, 273, 275, 282, 284, 287, 289, 290, 291, 294, 299, 300, 306, 330, 331], "rerun": [255, 260], "res18": [178, 197], "resampl": 262, "rescal": [235, 241], "research": [205, 207, 221, 227, 287, 289, 294, 295, 299], "reserv": [1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 17, 22, 26, 29, 33, 38, 43, 49, 56, 84, 85, 91, 92, 99, 100, 109, 110, 122, 123, 136, 137, 138, 142, 143, 147, 205, 207, 213, 215, 221, 223, 229, 231, 278, 279, 280, 287, 288, 295, 296], "reset": [178, 182, 242, 244, 304, 313, 315, 319], "reshap": [5, 9, 35, 57], "resid": [261, 267], "residu": [261, 262], "resili": [156, 160, 168, 176, 178, 198, 200, 204, 242, 243, 255, 256, 261, 262, 287, 292], "resiz": 269, "resnet": [179, 180, 185, 191, 235, 241, 268, 269, 304, 313, 319], "resnet18": [4, 6, 29, 31, 32, 38, 40, 41, 178, 180, 181, 188, 190, 268, 270, 273, 277, 304, 305, 313, 318, 319, 323, 326], "resolut": [5, 35, 36], "resolv": [4, 32, 109, 119], "resourc": [4, 5, 6, 7, 10, 17, 23, 25, 32, 36, 41, 47, 48, 57, 68, 69, 74, 75, 78, 79, 80, 81, 82, 83, 89, 90, 92, 93, 98, 106, 107, 108, 113, 118, 119, 120, 121, 124, 134, 135, 136, 142, 144, 148, 150, 152, 154, 155, 156, 158, 160, 162, 164, 166, 167, 168, 171, 172, 177, 178, 181, 184, 189, 191, 197, 200, 202, 205, 207, 213, 218, 221, 223, 227, 228, 242, 243, 247, 261, 262, 268, 269, 281, 282, 289, 301, 303, 304, 305, 307, 310, 311, 318, 324, 325, 326, 334, 336, 338], "resources_per_work": [221, 227, 255, 258], "resp": [142, 145], "respect": [213, 219, 220], "respond": [295, 298], "respons": [10, 70, 80, 83, 142, 145, 146, 152, 153, 164, 165, 229, 231, 234, 278, 279, 281, 283, 285, 287, 291, 292, 295, 297, 298, 299, 300, 307, 334, 335, 336], "response_format": [295, 299], "rest": [99, 107, 109, 120, 178, 181, 193, 199, 213, 220, 229, 231, 248, 250, 268, 270], "restart": [11, 72, 152, 153, 164, 165, 178, 180, 198, 200, 201, 229, 234, 242, 246, 247, 248, 252, 255, 256, 295, 298], "restor": [190, 198, 199, 200, 204, 213, 220, 235, 236, 241, 248, 253, 254], "restored_train": [178, 202], "restrict": [157, 169], "result": [3, 6, 9, 11, 12, 16, 17, 18, 19, 25, 28, 41, 57, 72, 84, 88, 91, 97, 99, 106, 109, 118, 122, 133, 151, 154, 155, 159, 163, 166, 167, 174, 179, 180, 181, 188, 189, 191, 202, 204, 205, 207, 213, 219, 220, 221, 226, 227, 228, 229, 233, 234, 235, 239, 240, 242, 243, 246, 248, 250, 252, 253, 254, 255, 258, 260, 265, 266, 268, 270, 274, 275, 276, 277, 295, 300, 303, 305, 310, 311, 312, 318, 324, 325, 326], "resum": [4, 5, 30, 34, 179, 188, 198, 199, 200, 201, 204, 236, 239, 241, 242, 243, 246, 249, 252, 254, 255, 256, 258, 260, 265, 267, 268, 269, 273, 276], "resume_from_checkpoint": [248, 253], "retain": [137, 140, 154, 166, 255, 258, 261, 265], "retent": [213, 220, 268, 269], "rethink": [213, 220], "retrain": [242, 247, 248, 249, 255, 260, 261, 267, 268, 277], "retri": [4, 5, 9, 17, 30, 34, 61, 159, 173, 179, 198, 201, 202, 204, 235, 239, 242, 243, 248, 249, 252, 255, 258, 268, 269, 273, 274], "retriev": [1, 2, 15, 25, 178, 191, 194, 248, 252, 255, 258], "retry_except": [2, 20], "return": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 28, 31, 32, 35, 39, 41, 52, 61, 62, 64, 70, 71, 72, 137, 141, 142, 144, 151, 154, 155, 159, 163, 166, 167, 174, 178, 181, 182, 185, 186, 187, 190, 191, 194, 196, 202, 205, 210, 211, 213, 220, 221, 225, 226, 229, 233, 235, 237, 238, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 257, 258, 259, 260, 261, 263, 264, 267, 268, 271, 272, 277, 295, 299, 300, 303, 304, 305, 306, 307, 310, 311, 313, 316, 317, 319, 322, 324, 330, 331, 335, 336], "reus": [2, 9, 18, 62, 73, 78, 178, 180, 191, 205, 210, 255, 257, 259, 260, 261, 267, 278, 279, 282, 306, 330], "reusabl": [261, 262], "reveal": [248, 250], "reveng": [213, 219], "revers": [255, 259, 261, 263, 268, 277], "review": [9, 59, 84, 86, 91, 95, 99, 101, 109, 112, 122, 126, 153, 165, 213, 215, 218, 219, 220, 221, 223, 228], "rewrit": [4, 5, 30, 34, 178, 179, 248, 254, 268, 269], "rf": [3, 4, 5, 6, 8, 9, 28, 32, 37, 42, 53, 55, 66, 303, 304, 306, 307, 310, 311, 320, 332, 338], "rg_idx": [268, 271], "rg_meta": [268, 271], "rgb": [178, 181, 191, 235, 236, 237, 268, 269, 271, 277], "rice": [268, 269], "rich": [7, 46, 151, 163], "richer": [242, 247], "rick": [205, 211], "ricki": [205, 211], "ride": [3, 28, 213, 219, 261, 262, 267, 303, 310], "ridicul": [213, 220], "ridlei": [213, 220], "rifl": [213, 219], "riget": [213, 220], "right": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 16, 17, 26, 29, 32, 33, 38, 43, 49, 56, 60, 70, 84, 85, 91, 92, 99, 100, 109, 110, 122, 123, 136, 137, 138, 142, 143, 147, 151, 163, 178, 181, 188, 194, 205, 207, 211, 213, 215, 219, 220, 221, 223, 229, 231, 242, 243, 278, 279, 280, 282, 287, 288, 295, 296, 301, 302, 304, 307, 316, 335], "rightarrow": [255, 256], "rigid": [7, 45], "rip": [213, 219], "rise": [268, 275], "risibl": [213, 216], "risk": [213, 220], "riskbr": [213, 220], "river": [205, 211, 213, 220], "riverboat": [213, 219], "rkn": [137, 141], "rllib": [7, 46], "rm": [3, 4, 5, 6, 8, 9, 10, 28, 32, 37, 42, 53, 55, 66, 71, 84, 89, 91, 98, 99, 107, 109, 120, 122, 134, 303, 304, 306, 307, 310, 311, 320, 332, 338], "rmse": [3, 6, 28, 41, 248, 254, 305, 324, 325], "rmtree": [178, 203, 235, 241, 242, 247, 248, 250, 254, 255, 260, 261, 267, 268, 277], "road": [255, 256], "roadmap": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 17, 26, 29, 32, 33, 38, 43, 49, 56, 67, 142, 143, 178, 181, 278, 279, 280, 287, 288, 295, 296, 303, 304, 305, 306, 307, 308, 312, 321, 327, 333], "roar": [213, 219], "robert": [213, 219], "robin": [80, 83, 205, 211], "robot": [242, 247], "robust": [0, 7, 43, 178, 201, 204, 242, 243, 261, 267, 268, 269, 295, 299], "rock": [205, 209, 211], "role": [77, 79, 80, 82, 83, 84, 86, 90, 91, 93, 99, 101, 112, 119, 122, 124, 137, 140, 153, 165, 213, 220, 278, 279, 285, 287, 291, 292, 295, 298, 299, 300], "roll": [142, 145, 156, 160, 168, 176, 205, 211, 242, 244, 287, 292], "rollin": [205, 211], "rollout": [10, 68, 142, 145, 242, 243], "roma": [205, 211], "roman": [213, 219], "ronda": [213, 219], "roof": [205, 211], "root": [1, 4, 5, 6, 16, 30, 31, 32, 34, 39, 41, 136, 142, 145, 146, 149, 153, 165, 178, 179, 180, 186, 248, 254, 295, 298, 304, 305, 313, 316, 322, 326], "roughli": [235, 237, 242, 244, 268, 270], "round": [80, 83, 205, 211, 213, 220, 248, 250, 255, 256, 258, 260], "rout": [2, 10, 25, 68, 73, 78, 80, 82, 83, 146, 229, 233, 278, 279, 283, 307, 336], "route_prefix": [3, 28, 156, 160, 168, 177, 295, 299, 307, 336], "row": [4, 5, 31, 32, 35, 51, 59, 61, 142, 144, 178, 180, 190, 191, 195, 196, 205, 207, 209, 211, 212, 213, 215, 216, 217, 218, 219, 220, 235, 237, 239, 248, 249, 250, 252, 254, 255, 256, 257, 259, 261, 263, 267, 268, 269, 271, 275, 277, 329], "row_group": [268, 271], "row_group_idx": [268, 271], "row_group_map": [268, 271], "royal": [205, 211], "rpc": [2, 7, 25, 43], "rstrip": [153, 165], "rubbish": [213, 220], "rubbl": [213, 219, 220], "rube": [213, 219], "ruin": [213, 220], "rule": [7, 48, 73, 78, 152, 164, 205, 211, 295, 298], "rumor": [205, 211], "run": [0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 17, 19, 21, 25, 26, 28, 29, 33, 35, 37, 38, 40, 41, 42, 47, 49, 50, 51, 53, 55, 56, 57, 60, 62, 66, 67, 68, 69, 70, 71, 72, 73, 75, 77, 81, 83, 84, 86, 87, 88, 89, 90, 96, 97, 98, 99, 101, 102, 103, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 117, 118, 120, 121, 122, 125, 131, 133, 134, 136, 137, 139, 140, 141, 143, 145, 146, 148, 149, 150, 151, 152, 153, 154, 156, 158, 161, 162, 163, 164, 165, 166, 168, 171, 175, 176, 179, 180, 181, 182, 183, 184, 186, 188, 189, 190, 192, 196, 197, 198, 201, 202, 203, 204, 207, 210, 211, 213, 215, 219, 220, 221, 223, 226, 227, 228, 229, 231, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 246, 247, 248, 249, 252, 253, 254, 255, 256, 258, 260, 262, 265, 266, 269, 271, 273, 274, 275, 276, 278, 279, 280, 283, 285, 288, 289, 291, 293, 294, 295, 296, 297, 298, 299, 300, 304, 305, 306, 307, 314, 318, 319, 320, 323, 324, 325, 326, 328, 335, 336], "run_command": [151, 163], "run_config": [3, 4, 5, 28, 32, 36, 178, 188, 189, 197, 200, 202, 235, 239, 242, 246, 248, 252, 255, 258, 261, 265, 268, 274, 303, 304, 310, 311, 318, 319], "runawai": [205, 211], "runconfig": [3, 4, 5, 26, 28, 29, 32, 36, 179, 180, 197, 200, 202, 204, 235, 237, 239, 242, 244, 246, 248, 250, 252, 255, 256, 257, 258, 261, 262, 263, 265, 268, 269, 270, 273, 274, 275, 277, 303, 304, 310, 311, 318], "runnabl": [142, 143], "runnng": [154, 166], "runtim": [10, 17, 69, 142, 145, 150, 152, 162, 164, 221, 224, 229, 233, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270, 295, 297, 298, 339], "runtime_env": [2, 21, 22, 221, 224, 287, 290, 293, 295, 298, 299, 300], "runtimeenv": [221, 224], "runwai": [8, 55, 306, 332], "ruse": [213, 219], "rust": [7, 43], "ruth": [213, 219], "rw": [304, 313], "ryan": [205, 211], "s3": [3, 7, 8, 9, 10, 28, 43, 51, 53, 54, 59, 61, 62, 64, 70, 77, 79, 80, 82, 84, 86, 89, 90, 99, 101, 107, 109, 112, 113, 119, 120, 142, 144, 153, 165, 178, 188, 204, 235, 241, 248, 254, 268, 277, 295, 298, 303, 304, 305, 306, 307, 310, 311, 318, 324, 326, 329, 330, 331, 335, 336], "s3_bucket_id": [73, 79, 84, 86], "s3_f": [142, 144], "s3_kei": [153, 165, 295, 298], "s3_path": [5, 35], "s3f": [5, 33, 35], "s3filesystem": [5, 35, 142, 144], "s5": [278, 279, 282], "s6": [205, 211, 278, 279, 282], "s7": [278, 279, 282], "s_": [242, 243], "s_k": [242, 243], "saatchi": [205, 211], "sacrif": [242, 243], "safe": [84, 86, 99, 101, 109, 112, 160, 176, 178, 180, 198, 205, 211, 235, 237, 261, 265, 268, 273], "safetensor": [295, 298], "safeti": [295, 299], "sai": [2, 19, 73, 78, 155, 159, 167, 175, 205, 211, 213, 219, 220], "said": [205, 211, 213, 216], "sake": [8, 52], "salad": [268, 269], "sam": [205, 211], "same": [2, 3, 4, 5, 6, 8, 10, 11, 21, 28, 32, 35, 36, 41, 52, 54, 69, 70, 72, 80, 82, 84, 86, 99, 101, 109, 112, 153, 155, 165, 167, 173, 178, 181, 184, 185, 186, 188, 193, 196, 197, 199, 202, 204, 205, 211, 212, 213, 216, 220, 235, 236, 237, 239, 242, 247, 248, 254, 255, 258, 260, 261, 267, 268, 271, 275, 277, 278, 279, 285, 287, 292, 294, 295, 302, 303, 304, 305, 307, 310, 311, 317, 326, 335], "sampl": [3, 4, 5, 6, 28, 32, 35, 41, 73, 78, 84, 86, 90, 91, 95, 99, 101, 102, 109, 112, 114, 122, 126, 129, 136, 148, 153, 165, 179, 182, 183, 191, 213, 216, 220, 229, 233, 237, 244, 248, 254, 255, 257, 261, 263, 268, 269, 270, 271, 277, 303, 305, 306, 307, 310, 324, 325, 330, 336], "sample_act": [242, 247], "sample_batch": [255, 259, 303, 310, 311], "sample_count": [154, 166], "sample_idx": [4, 31, 32, 178, 180], "sample_imag": [235, 241], "sample_s": [5, 35], "sampler": [4, 5, 32, 36, 178, 179, 182, 193, 268, 273, 304, 315, 319], "samsara": [307, 337], "samsung": [205, 211], "san": [205, 211, 295, 300], "sander": [205, 211], "saniti": [178, 180, 189, 239, 242, 246, 248, 250, 255, 257], "sat": [205, 211], "satisfi": [2, 18], "satur": [137, 141], "saturdai": [205, 211], "save": [2, 4, 5, 22, 31, 32, 36, 109, 119, 152, 153, 154, 164, 165, 166, 179, 181, 182, 190, 193, 197, 198, 199, 202, 204, 213, 219, 235, 236, 237, 238, 241, 242, 243, 246, 247, 248, 252, 253, 254, 255, 256, 258, 260, 261, 262, 265, 268, 269, 270, 273, 277, 278, 279, 283, 295, 298, 315, 317, 318, 319], "save_checkpoint_and_metrics_ray_train": [4, 32, 178, 182, 188, 193, 304, 315, 317, 319], "save_checkpoint_and_metrics_ray_train_with_extra_st": [178, 199, 200], "save_checkpoint_and_metrics_torch": [4, 31, 304, 313], "save_hyperparamet": [5, 35], "save_last": [235, 239, 242, 246], "save_model": [3, 28], "save_top_k": [235, 239, 242, 246], "saw": [205, 211], "sayhellodebuglog": [142, 145], "sayhellodefaultlog": [142, 145], "scaffold": [10, 71], "scala": [7, 46], "scalabl": [3, 7, 8, 10, 27, 43, 46, 47, 50, 67, 69, 73, 78, 153, 155, 156, 158, 159, 160, 161, 165, 167, 168, 172, 173, 176, 178, 192, 204, 205, 207, 212, 213, 215, 220, 221, 227, 228, 229, 231, 235, 236, 242, 247, 248, 249, 250, 255, 256, 259, 260, 261, 262, 267, 268, 269, 271, 281, 286, 287, 294, 295, 302, 303, 307, 309, 333], "scalar": [2, 25, 235, 238, 242, 243, 245], "scale": [1, 2, 6, 7, 8, 9, 10, 11, 13, 22, 30, 34, 35, 41, 43, 46, 48, 51, 55, 61, 62, 69, 72, 73, 78, 80, 81, 82, 83, 99, 106, 109, 118, 119, 135, 137, 141, 152, 154, 156, 158, 160, 161, 164, 166, 168, 172, 176, 179, 180, 181, 186, 189, 192, 196, 198, 200, 202, 204, 207, 212, 213, 215, 221, 223, 227, 231, 234, 235, 236, 237, 241, 244, 248, 249, 250, 251, 254, 255, 256, 257, 260, 261, 262, 263, 267, 268, 269, 270, 271, 277, 278, 279, 280, 283, 284, 289, 290, 292, 295, 301, 303, 305, 306, 307, 310, 314, 324, 332, 334, 336], "scaling_config": [3, 4, 5, 28, 32, 36, 178, 184, 189, 197, 200, 202, 221, 227, 235, 239, 242, 246, 248, 252, 255, 258, 261, 265, 268, 274, 303, 304, 310, 311, 315, 318, 319], "scalingconfig": [3, 4, 5, 28, 29, 32, 36, 179, 180, 204, 221, 224, 227, 235, 236, 237, 239, 242, 243, 244, 246, 248, 249, 250, 252, 255, 256, 257, 258, 261, 262, 263, 265, 268, 270, 274, 303, 304, 310, 311, 315], "scan": [213, 219], "scari": [213, 220], "scenario": [2, 7, 22, 24, 43, 73, 78, 84, 86, 99, 101, 109, 112, 221, 226, 268, 269, 287, 289, 294, 307, 334], "scene": [213, 216, 220, 248, 249], "schedul": [1, 2, 3, 4, 5, 6, 8, 9, 13, 15, 16, 22, 23, 25, 28, 31, 35, 41, 53, 61, 62, 80, 82, 154, 166, 205, 211, 221, 228, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 269, 277, 278, 279, 284, 303, 305, 310, 311, 318, 324, 325, 326], "schema": [7, 8, 43, 51, 205, 209, 212, 213, 217, 218, 235, 237, 248, 250, 261, 263, 295, 299, 300], "schemat": [4, 5, 32, 36, 178, 179], "schlong": [213, 216], "school": [205, 211], "schumer": [205, 211], "scienc": [7, 46], "scientif": [7, 46, 242, 244, 255, 257, 295, 301], "scikit": [229, 231, 255, 257], "scipt": [142, 145], "scope": [73, 75, 137, 140, 153, 158, 165, 171], "score": [205, 211, 229, 234, 235, 239, 241, 248, 249, 250, 254, 255, 258, 260, 261, 263, 265, 295, 298], "scoreless": [205, 209, 211], "scott": [213, 220], "scotu": [205, 211], "scratch": [6, 41, 84, 86, 99, 101, 109, 112, 235, 239, 242, 246, 268, 269, 277, 305, 325], "screen": [150, 151, 157, 162, 163, 169, 213, 220], "script": [0, 137, 141, 142, 145, 151, 154, 156, 158, 163, 166, 168, 171, 235, 241, 242, 247, 255, 256, 268, 269, 295, 298], "scroll": [109, 119], "scrumptiou": [84, 86, 99, 101, 109, 112], "sdk": [91, 93, 122, 124, 125, 155, 167], "sea": [205, 211], "seaborn": [255, 257], "seal": [213, 220], "seamless": [7, 43, 46, 47, 153, 165, 213, 215, 242, 243, 247, 261, 266, 267, 278, 279, 284, 295, 300], "seamlessli": [7, 8, 11, 47, 55, 72, 161, 178, 193, 200, 205, 207, 235, 236, 242, 243, 248, 254, 255, 256, 260, 261, 262, 263, 268, 269], "search": [3, 6, 28, 38, 41, 73, 78, 154, 166, 235, 241, 242, 247, 255, 260, 261, 267, 268, 277, 303, 305, 310, 311, 324, 325, 326], "search_alg": [6, 41, 305, 325, 326], "season": [261, 263], "seattl": [213, 219], "second": [0, 1, 2, 6, 16, 19, 41, 109, 113, 122, 125, 142, 144, 145, 205, 211, 215, 255, 260, 268, 269, 276, 287, 290, 303, 305, 310, 326], "secondarili": [9, 63, 306, 330], "secret": [73, 78, 205, 211, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270], "section": [0, 6, 40, 73, 76, 77, 80, 82, 136, 142, 144, 145, 148, 149, 151, 152, 154, 163, 164, 166, 178, 180, 305, 323], "secur": [75, 77, 79, 80, 82, 84, 86, 90, 91, 95, 109, 119, 135, 158, 172, 278, 279, 283, 284, 287, 292, 295, 302], "security_group_descript": [73, 78], "security_group_id": [73, 79, 84, 86], "security_group_nam": [73, 78], "securitygroup": [73, 77, 78], "sedan": [295, 299], "see": [1, 2, 4, 5, 6, 8, 9, 10, 11, 16, 19, 25, 32, 35, 36, 40, 42, 53, 54, 55, 64, 66, 71, 72, 73, 78, 80, 81, 84, 86, 88, 91, 95, 97, 99, 101, 106, 109, 112, 118, 119, 122, 125, 133, 136, 137, 141, 142, 144, 145, 149, 153, 156, 157, 158, 165, 168, 169, 171, 178, 182, 184, 189, 190, 198, 205, 206, 209, 211, 213, 214, 216, 219, 220, 221, 222, 228, 229, 230, 233, 234, 235, 236, 241, 242, 243, 261, 265, 267, 268, 269, 278, 279, 283, 285, 287, 290, 292, 295, 298, 299, 300, 304, 305, 306, 307, 315, 316, 317, 319, 323, 326, 331, 332, 336, 337], "seed": [235, 241, 242, 244, 248, 250, 268, 271], "seek": [235, 237], "seem": [213, 220], "seen": [205, 211, 213, 216, 220, 287, 294], "segment": [73, 78, 261, 267, 306, 330], "seiz": [213, 216], "select": [4, 11, 31, 32, 72, 84, 86, 99, 101, 109, 112, 142, 144, 150, 151, 156, 157, 162, 163, 168, 169, 178, 180, 221, 223, 226, 248, 254, 255, 258, 261, 267, 268, 275, 287, 293, 300, 302], "select_column": [3, 28], "selector": [157, 169], "self": [2, 3, 5, 9, 10, 25, 28, 35, 62, 70, 142, 145, 178, 191, 205, 210, 229, 233, 235, 238, 239, 242, 245, 246, 248, 251, 255, 259, 261, 262, 263, 264, 267, 268, 269, 271, 277, 303, 306, 307, 310, 311, 330, 335, 336], "sell": [213, 219], "semant": [2, 23, 25], "semi": [7, 43], "send": [2, 25, 137, 141, 142, 145, 146, 231, 233, 295, 299, 300, 303, 307, 310, 311, 334, 336], "send_welcome_email": 146, "sens": [3, 6, 26, 41, 213, 219, 220, 303, 305, 308, 325], "sent": [7, 48, 142, 145, 146, 178, 191, 213, 219], "sentenc": [178, 184, 205, 207, 208, 210, 211, 295, 298, 301], "sentence_transform": [205, 208], "sentencetransform": [205, 207, 208, 210, 211, 212], "sentiment": [205, 209, 229, 231, 233, 234], "sep": [248, 250, 254], "separ": [1, 2, 4, 6, 7, 10, 15, 23, 32, 41, 43, 69, 136, 149, 153, 158, 165, 172, 178, 186, 248, 249, 255, 257, 305, 325], "sept": [205, 211], "sequenc": [8, 51, 213, 220, 221, 223, 224, 226, 228, 255, 256, 263, 264, 278, 279, 282], "sequenti": [6, 40, 235, 238, 242, 245, 261, 262, 278, 279, 281, 304, 305, 313, 319, 323], "sequoia": [136, 149], "seri": [135, 263, 267], "serial": [1, 7, 13, 43, 46, 255, 258, 268, 269], "serializ": [235, 237], "series_id": [261, 263], "serious": [213, 216, 220], "serv": [46, 47, 70, 71, 73, 75, 76, 84, 85, 99, 100, 109, 110, 136, 137, 140, 143, 149, 156, 157, 160, 168, 169, 176, 177, 178, 191, 204, 206, 214, 222, 230, 232, 233, 235, 241, 242, 247, 248, 250, 251, 254, 255, 260, 261, 267, 268, 277, 282, 286, 289, 292, 293, 294, 297, 299, 300, 302, 303, 310, 311, 335, 338], "serve_llama": [278, 279, 285], "serve_llama_3_1_70b": [287, 290, 291, 292, 293], "serve_my_lora_app": [295, 298], "serve_my_qwen": [295, 299], "serve_my_qwen3": [295, 300], "server": [0, 136, 149, 278], "serverless": [73, 76], "servic": [6, 7, 42, 48, 67, 68, 73, 77, 78, 80, 82, 83, 84, 86, 91, 92, 94, 95, 99, 101, 109, 112, 119, 122, 125, 126, 129, 134, 142, 145, 146, 152, 153, 154, 157, 158, 164, 165, 166, 169, 171, 206, 214, 222, 229, 230, 231, 278, 279, 282, 285, 288, 291, 293, 294, 333, 336, 339], "session": [146, 153, 158, 165, 171], "session_2024": [303, 304, 310, 311, 318], "session_2025": [221, 228], "session_latest": [136, 142, 145, 146, 149], "set": [0, 1, 2, 4, 5, 6, 9, 10, 16, 21, 22, 30, 32, 34, 35, 36, 39, 41, 61, 70, 71, 73, 74, 77, 80, 83, 84, 86, 90, 91, 94, 95, 99, 101, 102, 104, 108, 109, 112, 114, 116, 121, 122, 125, 126, 132, 142, 145, 147, 150, 152, 156, 158, 160, 162, 164, 168, 170, 177, 178, 179, 180, 181, 184, 186, 189, 191, 198, 205, 211, 213, 218, 219, 220, 221, 223, 226, 227, 228, 229, 233, 235, 237, 248, 250, 252, 255, 258, 261, 262, 265, 268, 269, 271, 288, 293, 294, 296, 298, 299, 303, 304, 307, 310, 311, 318, 321, 322, 324, 326, 334, 335, 336], "set_epoch": [4, 32, 178, 182, 193, 268, 273, 304, 315, 319], "set_float32_matmul_precis": [242, 247], "set_grad_en": [221, 226, 261, 267, 268, 277], "set_index": [261, 263], "set_titl": [6, 39, 235, 237, 268, 270, 304, 305, 313, 319, 322], "seth": [205, 211], "setup": [5, 7, 35, 36, 46, 73, 74, 76, 80, 81, 82, 99, 107, 109, 120, 135, 142, 143, 145, 148, 152, 157, 161, 164, 169, 171, 172, 178, 181, 189, 197, 199, 200, 202, 206, 214, 222, 223, 228, 230, 239, 243, 250, 252, 257, 260, 263, 270, 287, 289, 293, 294, 295, 300, 303, 310, 311, 339], "seven": [278, 279, 282], "sever": [7, 46, 73, 77, 154, 166, 213, 219, 229, 231, 278, 279, 282, 283, 284], "sevigni": [213, 216], "sex": [205, 211, 213, 216], "sexist": [205, 211], "sg": [73, 78, 84, 86], "sgd": [221, 226], "sh": [11, 72, 99, 100, 109, 111, 122, 124, 235, 237], "shallow": [255, 256], "shame": [213, 220], "shape": [2, 5, 8, 9, 22, 35, 53, 61, 80, 82, 137, 141, 152, 153, 164, 165, 178, 191, 205, 211, 212, 235, 238, 242, 244, 247, 255, 257, 261, 263, 264, 267, 268, 271, 306, 307, 330, 336], "shard": [3, 4, 5, 28, 32, 36, 178, 179, 181, 182, 184, 186, 188, 192, 193, 194, 195, 197, 200, 235, 236, 237, 239, 241, 242, 243, 244, 246, 247, 248, 249, 250, 252, 254, 255, 256, 258, 260, 261, 262, 263, 268, 277, 278, 279, 283, 304, 315], "shard_0": [268, 270, 271], "share": [0, 2, 4, 7, 8, 10, 18, 31, 43, 53, 68, 73, 77, 78, 80, 82, 84, 90, 91, 92, 109, 119, 136, 149, 157, 161, 169, 178, 180, 188, 203, 205, 211, 238, 242, 245, 255, 256, 257, 261, 262, 263, 267, 268, 269, 295, 298, 302], "shared_path": [153, 165], "shared_storag": [153, 165], "sharetea": [205, 211], "she": [205, 211, 213, 216, 219, 220], "sheeran": [205, 211], "shell": [9, 10, 62, 70, 122, 125], "sheriff": [213, 219], "shift": [151, 155, 156, 159, 160, 163, 167, 168, 175, 176, 255, 257, 261, 262, 265], "shine": [205, 211, 213, 219, 220], "shippuden": [205, 211], "shit": [205, 211], "shock": [213, 216], "shoe": [213, 220], "shoot": [205, 211, 213, 219], "shootout": [213, 219], "short": [213, 216, 255, 258, 261, 262], "shorter": [287, 293], "shot": [213, 216, 220], "should": [2, 4, 5, 8, 9, 10, 11, 24, 25, 32, 36, 50, 54, 63, 70, 72, 80, 82, 109, 119, 142, 144, 156, 160, 168, 177, 178, 181, 184, 205, 211, 213, 216, 219, 229, 234, 235, 241, 242, 247, 248, 254, 255, 260, 268, 277, 295, 298, 304, 315], "should_checkpoint": [304, 319], "show": [2, 3, 5, 6, 9, 22, 28, 35, 36, 41, 60, 62, 63, 84, 89, 91, 92, 99, 107, 109, 120, 142, 143, 146, 154, 166, 178, 179, 180, 188, 189, 190, 191, 193, 205, 206, 207, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 222, 230, 235, 237, 239, 241, 242, 243, 246, 248, 250, 252, 255, 257, 259, 261, 263, 265, 267, 268, 269, 270, 275, 277, 278, 279, 282, 284, 295, 297, 305, 307, 325, 336], "showcas": [205, 206, 211, 213, 214, 216, 220, 222, 229, 230, 231, 295, 296], "shown": [142, 145, 178, 180, 213, 216], "shuffl": [4, 5, 6, 31, 32, 35, 36, 39, 41, 49, 56, 178, 182, 186, 192, 193, 194, 204, 221, 224, 226, 242, 244, 248, 250, 255, 260, 261, 263, 265, 268, 271, 272, 273, 304, 305, 313, 316, 322, 326, 327], "shut": [178, 188, 205, 207, 213, 215, 221, 223, 293], "shutdown": [223, 231, 287, 291, 293, 295, 298, 299, 300, 303, 307, 310, 311, 336, 338], "shutil": [178, 180, 203, 235, 237, 241, 242, 244, 247, 248, 250, 254, 255, 257, 260, 261, 263, 267, 268, 270, 277], "sick": [205, 211], "sid": [73, 78], "side": [7, 47, 213, 219, 220, 235, 241, 248, 254, 268, 277], "sidebar": 0, "sidecar": [278, 279, 283], "sidestep": [235, 236], "sidewalk": [213, 219], "sight": [213, 216], "sign": [9, 10, 62, 70, 136, 142, 143, 148, 205, 211], "signal": [137, 139, 261, 263], "signatur": [5, 6, 36, 41, 305, 324, 325], "signifi": [8, 51], "signific": [4, 5, 7, 30, 34, 47, 178, 179, 205, 207, 278, 279, 283], "significantli": [137, 140, 152, 164], "signup": 161, "silicon": [11, 72, 205, 207, 211, 221, 223, 226], "silu": [5, 35], "sim": [235, 236, 242, 243], "similar": [3, 28, 80, 83, 136, 142, 144, 149, 235, 241, 287, 290, 291, 303, 310, 311], "similarli": [2, 7, 25, 44, 229, 233, 248, 250], "simpl": [1, 3, 4, 5, 6, 7, 8, 10, 13, 14, 28, 31, 35, 40, 41, 48, 50, 52, 70, 143, 151, 153, 156, 157, 163, 165, 168, 169, 178, 180, 183, 189, 191, 206, 213, 214, 219, 220, 222, 229, 230, 231, 233, 235, 236, 241, 248, 249, 250, 251, 254, 268, 269, 277, 278, 279, 282, 285, 286, 287, 289, 295, 301, 303, 305, 307, 310, 323, 325, 335, 336], "simple_pipelin": [142, 144], "simpler": [287, 294, 295, 298], "simpli": [4, 31, 229, 231, 255, 260, 268, 276], "simplifi": [91, 95, 150, 161, 162, 268, 269, 306, 330], "simul": [2, 20, 146, 231, 242, 243, 247, 248, 250, 268, 269], "simultan": [137, 141, 278, 279, 281], "sin": [242, 243, 244, 247, 261, 264], "sinc": [2, 8, 22, 51, 137, 141, 142, 145, 153, 165, 178, 181, 188, 205, 211, 213, 220, 235, 237, 248, 253, 278, 279, 282], "sing": [205, 211], "singl": [2, 7, 8, 9, 11, 22, 29, 32, 33, 36, 46, 51, 60, 72, 142, 145, 178, 179, 180, 182, 191, 203, 205, 211, 213, 215, 220, 235, 236, 237, 242, 246, 248, 249, 255, 256, 257, 260, 261, 262, 268, 269, 270, 271, 278, 279, 281, 282, 283, 287, 289, 294, 295, 297, 298, 302, 303, 307, 310, 311, 314, 336], "single_gpu_mnist": [304, 313, 320], "sink": [3, 9, 28, 58], "sinusoid": [261, 264], "sisterlif": [205, 211], "sit": [205, 209, 211, 213, 216], "site": [0, 73, 78, 153, 165, 213, 216], "situat": [213, 220], "six": [278, 279, 282], "size": [4, 6, 8, 9, 32, 39, 52, 57, 61, 137, 141, 142, 145, 150, 152, 154, 162, 164, 166, 178, 180, 181, 182, 183, 205, 209, 211, 213, 219, 221, 226, 227, 235, 238, 239, 241, 242, 244, 247, 248, 254, 258, 261, 263, 264, 267, 268, 270, 278, 279, 282, 283, 286, 291, 293, 295, 298, 304, 305, 315, 322], "size_in_byt": [2, 18], "sj": [205, 209, 211, 212], "skagwai": [213, 219], "skew": [9, 57, 248, 250, 255, 257], "skill": [295, 298], "skip": [235, 237, 255, 256, 257, 268, 277], "sklearn": [3, 26, 255, 257], "skylynn": [205, 211], "slack": [142, 145], "sleazi": [213, 219], "sleep": [1, 2, 16, 19, 24, 142, 144, 213, 220], "slice": [3, 28, 235, 236, 248, 250, 255, 256, 258, 268, 269, 271], "slick": [213, 220], "slide": [205, 211, 262], "slim": [152, 164], "slip": [213, 219], "slo": [278, 279, 282], "slope": [255, 256], "slot": [178, 180, 261, 262], "slow": [10, 68, 205, 207, 287, 289], "slow_adjust_total_amount": [142, 144], "slower": [213, 218], "slowest": [8, 9, 54, 64, 306, 331], "slowli": [213, 220], "slowyourrol": [205, 211], "sm": [205, 211], "small": [7, 8, 9, 43, 51, 52, 60, 64, 154, 166, 235, 236, 242, 247, 248, 250, 254, 261, 267, 268, 269, 278, 279, 283, 287, 289, 294, 295, 298, 301, 306, 307, 330, 331, 336], "small_siz": [221, 226], "small_unet_model_config": [5, 35], "smaller": [0, 2, 22, 73, 78, 137, 141, 295, 298, 301], "smallest": [150, 162], "smart": [213, 219, 278, 279, 284], "smith": [205, 209, 211, 212], "smoke": [178, 204], "smoothl1": [261, 265], "smoothl1loss": [261, 265], "smoothli": [213, 215], "sn": [255, 257, 259], "snake": [205, 211], "snap": [205, 209, 211], "snapshot": [153, 165, 235, 240, 255, 260], "snapshot_download": [295, 298], "snicker": [213, 220], "snippet": [4, 5, 32, 36, 151, 153, 163, 165, 304, 319], "snowflak": [7, 9, 43, 59], "so": [0, 5, 6, 8, 35, 41, 52, 53, 136, 137, 141, 148, 150, 151, 152, 154, 156, 160, 162, 163, 164, 166, 168, 177, 178, 179, 180, 181, 186, 187, 188, 189, 191, 192, 193, 195, 196, 198, 199, 200, 202, 205, 209, 211, 213, 216, 219, 220, 229, 231, 235, 238, 242, 245, 248, 250, 255, 257, 258, 261, 262, 263, 265, 268, 269, 271, 277, 295, 301, 304, 305, 317, 324, 326], "socket": [7, 46], "softbal": [205, 211], "softmax": [255, 256], "softprob": [255, 258], "softwar": [136, 148, 287, 291, 295, 298], "soil": [255, 256, 259], "sole": [73, 78, 213, 220], "solid": [248, 254], "solut": [1, 2, 4, 5, 6, 7, 9, 10, 16, 25, 30, 32, 34, 36, 41, 47, 57, 61, 68, 73, 77, 178, 179, 192, 221, 227, 261, 267, 278, 279, 280, 284, 286, 295, 302, 304, 305, 306, 319, 325, 328], "solv": [278, 279, 283, 295, 301], "some": [3, 4, 5, 6, 7, 8, 9, 28, 32, 37, 42, 45, 46, 54, 55, 59, 60, 64, 109, 119, 137, 140, 141, 142, 143, 153, 154, 165, 166, 205, 211, 213, 216, 217, 218, 219, 220, 278, 279, 282, 287, 291, 295, 297, 299, 304, 306, 320, 329, 331], "someth": [2, 19, 99, 103, 109, 115, 122, 131, 205, 211, 213, 219, 268, 269], "sometim": [2, 22, 84, 86, 99, 101, 109, 112, 213, 220], "somewher": [2, 25], "song": [205, 211], "sonnet": [295, 301], "soon": [2, 24, 205, 211], "sophist": [6, 7, 41, 47, 295, 297, 300, 302, 305, 325], "sorri": [205, 209, 211, 212], "sort": [7, 8, 43, 54, 153, 165, 213, 216, 248, 249, 250, 254, 255, 259, 268, 277], "sort_index": [248, 252, 261, 265, 268, 275], "soul": [205, 209, 211], "sound": [205, 211, 213, 220], "soup": [205, 211], "sourc": [0, 1, 9, 11, 13, 59, 72, 73, 78, 122, 125, 136, 137, 140, 149, 161, 178, 192, 204, 213, 217, 219, 220], "south": [205, 211], "sox": [205, 211], "space": [2, 3, 6, 7, 24, 28, 41, 44, 73, 78, 178, 203, 213, 219, 235, 241, 242, 247, 248, 249, 254, 255, 260, 261, 267, 268, 277, 305, 324, 325], "span": [146, 205, 211, 261, 263], "spark": [8, 46, 55], "spatial": [255, 256], "spawn": [178, 189, 191], "speak": [213, 220], "speci": [255, 260], "special": [7, 8, 43, 53, 205, 209, 211, 213, 219, 287, 294, 295, 297, 298, 301, 302], "specif": [2, 5, 8, 12, 13, 22, 25, 35, 36, 54, 64, 80, 83, 91, 92, 95, 122, 126, 136, 137, 139, 142, 145, 148, 149, 152, 153, 154, 157, 164, 165, 166, 169, 178, 189, 191, 235, 236, 241, 255, 259, 260, 268, 271, 278, 279, 284, 287, 290, 295, 301, 306, 331], "specifi": [2, 4, 5, 6, 8, 9, 10, 11, 20, 22, 32, 36, 41, 51, 61, 62, 69, 71, 72, 80, 82, 146, 150, 151, 152, 156, 157, 160, 162, 163, 164, 168, 169, 177, 178, 185, 197, 202, 221, 223, 226, 227, 228, 229, 233, 248, 252, 295, 298, 299, 304, 305, 307, 315, 318, 324, 325, 336], "specific": [178, 181], "speed": [3, 4, 5, 6, 7, 10, 28, 30, 34, 40, 43, 68, 153, 165, 178, 179, 213, 215, 220, 235, 237, 268, 270, 277, 295, 301, 305, 307, 323, 336], "speedup": [205, 207], "speific": [154, 166], "spend": [213, 216], "spike": [278, 279, 283, 284, 287, 292], "spiki": [150, 152, 162, 164], "spill": [9, 63, 306, 330], "spillov": [154, 166], "spin": [9, 62, 154, 156, 166, 168, 176, 205, 207, 210, 255, 256, 306, 330], "split": [3, 9, 28, 61, 64, 153, 165, 178, 179, 180, 182, 183, 193, 205, 209, 211, 213, 216, 220, 236, 258, 259, 261, 263, 270, 277, 278, 279, 283, 287, 290, 303, 306, 310, 311, 331], "split_at_indic": [235, 237, 242, 244], "split_idx": [242, 244], "split_notebook": 0, "split_proportion": [248, 250], "spoil": [213, 219, 220], "spoiler": [213, 220], "spoken": [213, 219], "spot": [9, 57, 152, 154, 164, 166, 205, 211, 255, 259], "spotifi": [6, 8, 42, 55], "sprai": [213, 220], "sprang": [213, 220], "spruce": [255, 256], "spur": [213, 219], "spy": [136, 149], "sql": [7, 43, 47], "sqrt": [1, 6, 16, 41, 261, 264, 305, 324, 325], "sqrt_add": [1, 16], "squad": [205, 211], "squar": [1, 2, 16, 19, 178, 180, 205, 211, 235, 236, 248, 249, 254], "square_ref": [2, 19], "square_ref_1": [2, 23], "square_ref_2": [2, 23], "square_valu": [2, 19], "squarederror": [3, 28], "squeez": [6, 39, 235, 241, 242, 247, 248, 254, 261, 264, 265, 305, 322], "src": [261, 264], "ssh": [73, 76, 78, 268, 269], "ssl": [80, 83], "sso": [278, 279, 284], "sst": [229, 233], "st": [73, 78, 205, 211], "stabil": [235, 241, 248, 252, 261, 263], "stabilityai": [5, 35, 36], "stabl": [4, 32, 33, 36, 37, 278, 279, 285, 304, 320], "stablediffus": [5, 35, 36], "stack": [5, 35, 84, 88, 91, 95, 97, 99, 101, 106, 109, 112, 118, 122, 126, 133, 137, 140, 141, 161, 242, 244, 255, 257, 261, 262, 267, 268, 277], "stadium": [205, 211], "stage": [2, 4, 5, 7, 9, 22, 31, 35, 45, 57, 61, 62, 142, 144, 178, 180, 188, 205, 211], "stagnant": [261, 265], "stai": [178, 179, 193, 205, 211, 235, 236], "stakehold": [261, 267], "standalon": [178, 181], "standard": [5, 7, 35, 36, 43, 91, 95, 142, 145, 152, 155, 159, 164, 167, 173, 178, 180, 182, 186, 188, 196, 213, 216, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 264, 268, 269, 270, 271], "stander": [213, 220], "stapl": [213, 216], "star": [205, 211, 213, 220, 242, 243, 248, 250], "stare": [213, 216], "starlett": [10, 67, 303, 307, 310, 311, 335], "start": [2, 3, 4, 5, 8, 9, 10, 11, 17, 21, 22, 26, 28, 29, 31, 32, 33, 35, 36, 38, 49, 56, 59, 61, 62, 67, 72, 73, 76, 84, 85, 88, 91, 92, 97, 99, 100, 106, 109, 110, 111, 118, 122, 123, 125, 133, 137, 141, 142, 143, 144, 145, 148, 151, 152, 153, 154, 155, 159, 163, 164, 165, 166, 167, 173, 178, 179, 180, 181, 184, 185, 189, 197, 198, 199, 201, 205, 207, 213, 215, 216, 219, 223, 226, 227, 229, 231, 234, 235, 236, 241, 242, 243, 247, 248, 250, 261, 263, 265, 268, 269, 270, 276, 280, 282, 286, 287, 288, 291, 295, 296, 298, 301, 303, 304, 307, 308, 310, 313, 318, 323, 336], "start_epoch": [178, 199, 248, 252, 261, 265, 268, 273], "start_token": [261, 265], "starter": [156, 168], "startswith": [248, 254, 268, 277], "startup": [2, 21, 80, 82, 136, 149, 287, 294], "starv": [213, 220], "state": [2, 3, 7, 10, 25, 28, 46, 47, 56, 68, 69, 73, 78, 84, 86, 99, 101, 109, 112, 137, 139, 142, 144, 151, 163, 198, 199, 202, 204, 205, 210, 211, 213, 216, 220, 235, 241, 244, 245, 247, 248, 253, 255, 259, 261, 262, 267, 268, 277, 278, 279, 284, 295, 300, 303, 310, 311], "state_dict": [4, 31, 32, 178, 188, 191, 200, 235, 241, 242, 247, 248, 252, 254, 261, 265, 267, 268, 273, 277, 304, 313, 317, 319], "stateless": [1, 9, 14, 62, 235, 237], "statement": [4, 31, 73, 78], "static": [7, 9, 48, 57, 278, 279, 282], "station": [213, 220], "statist": [8, 55, 137, 139], "stats_d": [255, 259, 260], "statu": [84, 88, 91, 97, 99, 106, 109, 118, 122, 133, 137, 141, 142, 144, 146, 155, 156, 167, 168, 175, 205, 211, 221, 228, 303, 304, 305, 307, 310, 311, 318, 324, 326, 335], "std": [8, 9, 54, 64, 178, 191, 261, 263, 267, 268, 271, 277, 306, 331], "stderr": [142, 145], "steadi": [235, 239], "steadili": [178, 190, 261, 265], "steak": [268, 269], "steal": [213, 219], "steam": [213, 216], "step": [1, 2, 4, 5, 6, 7, 9, 12, 14, 16, 17, 31, 32, 35, 40, 41, 48, 58, 61, 84, 85, 86, 91, 93, 95, 98, 99, 100, 101, 103, 104, 109, 110, 111, 112, 115, 116, 122, 123, 124, 127, 131, 132, 134, 137, 141, 142, 143, 156, 160, 161, 168, 177, 179, 181, 182, 183, 186, 190, 191, 193, 195, 196, 199, 205, 211, 213, 215, 221, 226, 236, 243, 244, 249, 250, 252, 256, 257, 262, 263, 265, 269, 273, 282, 296, 301, 304, 305, 313, 315, 316, 319, 323, 324, 325, 326], "step_size_hour": [261, 267], "sterl": [205, 211], "steven": [205, 211], "stewart": [213, 219], "still": [2, 4, 7, 19, 32, 47, 151, 163, 178, 188, 193, 194, 200, 202, 205, 211, 213, 219, 229, 234, 235, 237, 261, 265, 268, 270, 304, 317], "stillkidrauhl": [205, 211], "stockholm": [213, 216], "stop": [6, 11, 41, 72, 178, 191, 213, 220, 229, 234, 235, 241, 255, 260, 261, 267, 268, 277, 278, 279, 281, 305, 325], "storag": [5, 7, 8, 9, 31, 35, 36, 43, 46, 50, 52, 53, 59, 65, 73, 77, 78, 84, 90, 91, 92, 94, 95, 122, 125, 126, 137, 140, 141, 150, 161, 162, 179, 180, 181, 186, 189, 190, 197, 198, 200, 202, 204, 242, 247, 250, 261, 262, 263, 268, 269, 270, 277, 295, 298, 306, 307, 317, 328, 329, 330, 332, 335, 339], "storage_fold": [3, 4, 8, 9, 10, 28, 31, 32, 53, 55, 62, 65, 66, 70, 71], "storage_path": [3, 4, 5, 28, 32, 35, 36, 178, 179, 188, 197, 200, 202, 235, 239, 242, 246, 248, 252, 253, 255, 258, 261, 265, 268, 273, 274, 303, 304, 310, 311, 318], "store": [5, 7, 9, 10, 24, 25, 35, 36, 43, 47, 48, 63, 64, 68, 73, 77, 136, 142, 144, 149, 154, 166, 178, 180, 181, 188, 189, 190, 195, 196, 197, 200, 204, 205, 207, 211, 212, 213, 216, 220, 235, 236, 239, 241, 242, 247, 248, 250, 252, 254, 255, 256, 257, 260, 261, 263, 265, 268, 269, 270, 273, 275, 303, 306, 310, 311, 330, 331], "stori": [213, 216, 220], "storylin": [213, 219, 220], "str": [3, 4, 5, 6, 9, 10, 28, 31, 32, 35, 41, 61, 62, 64, 70, 137, 141, 178, 188, 200, 205, 210, 229, 233, 255, 258, 261, 267, 268, 271, 272, 277, 295, 299, 300, 304, 305, 306, 307, 313, 317, 324, 330, 331, 335, 336], "strang": [213, 220], "stranger": [213, 219], "strategi": [5, 36, 156, 160, 168, 176, 235, 237, 239, 242, 246, 294], "stratifi": [255, 257], "streak": [205, 209, 211], "stream": [3, 8, 9, 10, 28, 48, 55, 57, 68, 142, 145, 178, 192, 193, 194, 196, 197, 204, 205, 209, 211, 212, 235, 236, 242, 243, 247, 248, 249, 250, 252, 254, 255, 256, 257, 258, 261, 263, 267, 278, 279, 283, 285, 287, 291, 292, 295, 298, 306, 307, 328, 330, 334], "streaming_split": [9, 60], "streamlin": [73, 77, 80, 81, 155, 159, 167, 175], "street": [213, 219], "strength": [295, 298], "stretch": [205, 211], "strftime": [4, 31, 304, 313], "strict": [235, 241, 242, 247], "stride": [4, 6, 31, 40, 41, 178, 181, 261, 263, 304, 305, 313, 319, 323, 326], "strike": [213, 219, 220], "string": [91, 95, 99, 101, 104, 109, 112, 116, 122, 132, 154, 166, 205, 209, 212, 213, 217, 218, 220, 268, 270, 295, 299, 300], "strip": [248, 254, 261, 267, 268, 277], "strong": [137, 141, 248, 251, 261, 263, 278, 279, 283, 287, 289], "stronger": [287, 289], "structur": [9, 47, 55, 59, 142, 145, 170, 172, 213, 216, 235, 236, 242, 243, 247, 248, 249, 261, 264, 265, 279, 286, 296, 297, 298, 300, 302, 303, 310, 311, 339], "stuck": [10, 68, 137, 141], "student": [213, 216], "studi": [4, 5, 32, 37, 213, 216, 304, 320], "studio": [151, 163, 205, 211, 213, 219], "stuff": [205, 211], "stun": [213, 219, 220], "stupid": [213, 220], "style": [0, 5, 35, 178, 194, 204, 213, 220, 235, 241, 248, 250, 252, 261, 262, 268, 269], "sub": [2, 23], "subdirectori": 0, "subfold": [5, 35], "subject": [295, 301], "submiss": [6, 41, 155, 159, 167, 173, 305, 325], "submit": [6, 40, 84, 88, 91, 97, 99, 106, 109, 118, 122, 133, 137, 141, 151, 154, 163, 166, 205, 211, 305, 323], "subnet": [76, 79, 84, 86, 90, 91, 92, 95, 122, 126, 135], "subnet_id": [73, 79, 84, 86], "suboptim": [9, 57], "subplot": [6, 39, 178, 180, 191, 235, 237, 241, 248, 250, 268, 270, 304, 305, 313, 319, 322], "subprocess": [9, 10, 56, 62, 67, 70, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270], "subraman": [205, 211], "subsampl": [255, 260], "subsequ": [136, 142, 145, 148, 149, 213, 220, 278, 279, 283], "subset": [4, 6, 8, 9, 32, 41, 52, 59, 60, 157, 169, 221, 226, 235, 236, 237, 248, 250, 255, 257, 261, 263, 268, 269, 270, 306, 330], "substanc": [213, 220], "substanti": [261, 265], "subtract": [235, 241], "subword": [278, 279, 281], "success": [6, 41, 146, 305, 325], "successfulli": [84, 87, 90, 91, 96, 99, 103, 108, 109, 115, 121, 122, 131, 152, 155, 164, 167, 242, 247, 287, 294, 295, 298], "suck": [205, 211], "sudo": [136, 149], "suffer": [10, 68], "suffici": [235, 237, 287, 291], "suffix": [1, 15], "suggest": [9, 62, 255, 259, 295, 298], "suit": [7, 43, 178, 179, 248, 251], "suitabl": [0, 213, 220, 221, 223, 235, 241, 248, 250], "sum": [2, 8, 9, 18, 51, 54, 60, 64, 154, 166, 213, 216, 248, 251, 252, 255, 259, 260, 306, 331], "sum_ref": [2, 19], "sum_valu": [2, 19], "summar": [205, 212, 278, 279, 282, 287, 294, 295, 298, 301, 302], "summari": [207, 223, 288, 295, 298, 302], "summer": [205, 209, 211, 213, 220], "summerslam": [205, 209, 211], "summit": [8, 9, 55, 66, 304, 306, 318, 319, 332], "sun": [205, 211, 213, 219, 220], "sunbeam": [84, 86], "sunda": [205, 211], "sundai": [205, 209, 211], "super": [5, 35, 205, 206, 211, 214, 222, 230, 235, 238, 242, 245, 248, 251, 261, 264], "superior": [213, 220], "supervis": [242, 244, 261, 263, 268, 269], "suppli": [213, 219, 242, 247], "support": [2, 4, 5, 7, 8, 9, 10, 11, 22, 25, 30, 31, 34, 43, 46, 47, 48, 55, 57, 59, 64, 65, 68, 71, 72, 78, 80, 82, 83, 136, 137, 140, 142, 143, 145, 148, 161, 178, 179, 180, 194, 199, 200, 202, 213, 215, 221, 227, 229, 231, 242, 243, 248, 253, 254, 255, 257, 260, 268, 273, 278, 287, 293, 295, 298, 299, 302, 306, 331, 332], "suppos": [205, 211], "suptitl": [235, 237, 241, 268, 270], "sur": [205, 211], "sure": [0, 4, 32, 122, 134, 151, 163, 178, 203, 205, 209, 211, 212, 235, 241, 248, 254, 268, 271, 295, 298], "surfac": [142, 145], "surg": [261, 267], "surpris": [213, 220], "surprisingli": [2, 25, 205, 211], "surround": [213, 216], "surviv": [205, 209, 211, 212, 242, 247], "sushi": [268, 269], "suspens": [213, 220], "suv": [295, 299], "swai": [213, 220], "swap": [152, 164, 235, 241, 242, 247, 268, 277], "swede": [213, 216], "swedish": [213, 216], "sweep": [235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277], "swing": [205, 211, 213, 216, 242, 243], "switch": [178, 204, 248, 254, 295, 297, 298], "switcher": 0, "sy": [1, 2, 12, 17, 18, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270], "sydow": [213, 220], "symbol": [11, 72, 242, 243], "sync": [151, 163, 178, 179, 205, 211, 268, 269, 273], "sync_dist": [5, 35, 235, 238, 242, 245], "sync_on_comput": [268, 273], "synchron": [2, 4, 18, 32, 178, 179, 184, 185, 188, 200, 235, 239, 268, 273, 304, 317], "synthet": [137, 141, 242, 247], "synthetic_image_output": [137, 141], "system": [1, 2, 5, 7, 8, 9, 13, 20, 35, 43, 46, 53, 61, 66, 73, 78, 80, 82, 99, 102, 107, 109, 114, 119, 120, 122, 125, 136, 137, 139, 140, 142, 145, 148, 149, 153, 156, 160, 161, 165, 168, 176, 178, 204, 254, 261, 266, 268, 269, 295, 297, 298, 299, 300, 303, 305, 310, 311, 324, 326], "t": [1, 2, 4, 7, 8, 9, 11, 16, 19, 20, 21, 31, 32, 47, 52, 54, 61, 64, 72, 80, 82, 109, 119, 142, 144, 153, 161, 165, 178, 179, 180, 184, 186, 191, 205, 209, 211, 213, 216, 219, 220, 235, 236, 237, 238, 241, 242, 243, 245, 247, 248, 250, 254, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 277, 278, 279, 283, 304, 306, 316, 330, 331], "t10k": [304, 305, 318, 326], "t4": [9, 62, 122, 127, 152, 164, 303, 304, 305, 310, 311, 318, 324, 326], "t_": [242, 243], "t_futur": [261, 267], "t_img": [235, 238], "t_past": [261, 267], "t_scale": [235, 238], "tab": [142, 144, 150, 151, 152, 153, 155, 156, 162, 163, 164, 165, 167, 168], "tabl": [6, 7, 9, 41, 43, 59, 84, 86, 99, 101, 109, 112, 255, 258, 261, 263, 268, 270, 271], "tabular": [8, 51, 178, 195, 248, 249, 254, 257, 260, 268, 270, 306, 329], "tackl": [261, 262], "tag": [7, 47, 84, 86, 99, 101, 109, 112, 248, 254, 255, 260], "tail": [248, 250], "tailor": [178, 181], "take": [2, 3, 4, 5, 6, 8, 9, 10, 25, 28, 30, 31, 32, 34, 36, 40, 41, 51, 52, 60, 61, 70, 73, 79, 84, 86, 91, 95, 99, 101, 109, 112, 122, 126, 129, 142, 144, 180, 181, 188, 205, 211, 213, 216, 219, 220, 221, 225, 237, 251, 257, 263, 270, 295, 297, 303, 304, 305, 307, 310, 311, 314, 319, 323, 335], "take_al": [248, 250], "take_batch": [8, 9, 52, 60, 61, 62, 205, 211, 255, 257, 303, 306, 307, 310, 311, 330, 335, 336], "takeawai": 280, "taken": [213, 216, 268, 277], "talent": [205, 211, 213, 220], "talk": [8, 9, 55, 66, 205, 211, 213, 220, 295, 298, 306, 332], "taman": [205, 211], "tank": [205, 211], "target": [3, 4, 5, 9, 28, 31, 35, 61, 73, 78, 178, 188, 195, 206, 214, 222, 230, 242, 244, 248, 249, 250, 255, 257, 260, 261, 263, 265, 295, 298, 307, 336], "target_num_rows_per_block": [137, 141], "target_ongoing_request": [307, 336], "target_path": [235, 241, 242, 247, 248, 254], "task": [1, 3, 6, 7, 8, 9, 10, 13, 14, 15, 16, 18, 24, 25, 28, 40, 45, 46, 47, 51, 53, 57, 59, 61, 62, 68, 99, 100, 109, 111, 122, 124, 142, 144, 151, 152, 154, 163, 164, 166, 213, 215, 220, 221, 224, 227, 235, 237, 241, 242, 247, 248, 251, 254, 255, 259, 260, 268, 269, 278, 279, 283, 287, 289, 298, 304, 305, 306, 313, 315, 319, 323, 329, 330], "task_id": [154, 166], "taskpoolmapoper": [9, 63], "tatum": [213, 219], "tavakolian": [205, 211], "tax": [3, 28, 205, 211, 303, 310], "taxi": [3, 8, 28, 51, 54, 142, 144, 267], "taximet": [8, 51], "taxiwindowdataset": [261, 263], "tb": [9, 57], "tbh": [205, 211], "tbl": [255, 258], "tc": [295, 300], "tcm": [213, 219], "tcp": [73, 78], "td3": [242, 247], "tea": [205, 211], "teach": [235, 236, 242, 243], "teacher": [213, 216, 262, 264], "team": [7, 46, 157, 158, 169, 170, 172, 205, 211, 268, 269], "tear": [155, 159, 167, 173], "teardown": [80, 82], "tech": [205, 211], "technic": [1, 13, 213, 220], "techniqu": [6, 40, 213, 220, 305, 323], "technologi": [7, 43], "ted": [205, 211], "teen": [205, 211], "telemetri": [136, 137, 141, 148], "tell": [3, 5, 28, 36, 178, 184, 188, 205, 211, 213, 216, 219, 268, 274, 287, 291, 292], "temp": [84, 86, 99, 101, 109, 112, 178, 180, 188, 261, 265, 268, 270, 273], "temp_checkpoint_dir": [4, 32, 178, 188, 200, 304, 317], "tempdir": [268, 273], "temperatur": [2, 25, 295, 300], "tempfil": [4, 29, 32, 178, 180, 188, 200, 235, 237, 239, 242, 246, 248, 250, 252, 255, 257, 261, 265, 268, 270, 273, 304, 317], "templat": [80, 83, 156, 168, 178, 204, 278, 279, 286, 289], "tempor": [242, 247], "temporari": [178, 188, 242, 247, 248, 252, 255, 260, 261, 267, 268, 273, 277, 278, 279, 283], "temporarydirectori": [4, 32, 178, 188, 200, 248, 252, 261, 265, 268, 273, 304, 317], "ten": [268, 269], "tenant": [248, 254, 295, 302], "tenni": [205, 211], "tensor": [4, 5, 9, 10, 31, 32, 35, 62, 70, 178, 186, 187, 191, 194, 196, 221, 224, 237, 238, 241, 242, 247, 248, 254, 261, 263, 278, 279, 283, 287, 290, 294, 304, 306, 307, 313, 317, 330, 335], "tensor_parallel_s": [287, 290, 293, 295, 300], "tensorflow": [229, 231], "term": [2, 7, 19, 48, 73, 77, 213, 216, 261, 262], "termin": [4, 11, 31, 72, 80, 83, 84, 87, 88, 89, 91, 96, 98, 99, 103, 106, 107, 109, 115, 118, 120, 122, 125, 131, 134, 142, 144, 146, 151, 153, 154, 155, 156, 163, 165, 166, 167, 168, 176, 178, 180, 202, 221, 228, 242, 244, 279, 285, 287, 290, 292, 303, 304, 310, 311, 318], "terminologi": [158, 170], "terraform": [73, 77, 78, 79, 85, 87, 89, 90, 93, 96, 98, 100, 103, 107, 108, 111, 113, 115, 120, 121, 124, 127, 131, 134, 135], "terrain": [255, 260], "terribl": [213, 220], "test": [3, 6, 10, 28, 39, 68, 70, 71, 86, 95, 101, 112, 126, 135, 136, 149, 151, 156, 163, 168, 178, 204, 205, 211, 221, 226, 227, 231, 268, 271, 287, 291, 294, 295, 299, 300, 301, 302, 303, 305, 307, 310, 311, 322, 335, 336], "test_job": [84, 88, 91, 97, 99, 106, 109, 118, 122, 133], "test_siz": [3, 28, 255, 257, 303, 310, 311], "texan": [205, 211], "text": [5, 7, 35, 43, 109, 113, 137, 140, 153, 165, 205, 207, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 223, 226, 229, 233, 234, 242, 243, 248, 249, 261, 262, 282, 286, 295, 300], "text_token": [213, 220], "textembedd": [205, 210, 211, 212], "tf": [84, 86, 91, 95, 99, 101, 109, 112], "tfenv": [84, 85, 99, 100, 109, 111, 122, 124], "tfi": [205, 211], "tfvar": [84, 86, 99, 101, 109, 112], "tgt": [261, 264], "than": [4, 7, 9, 32, 45, 47, 48, 60, 61, 137, 141, 151, 163, 213, 216, 218, 219, 220, 248, 254, 255, 257, 259, 261, 262, 265, 278, 279, 281, 285, 287, 289, 295, 298, 303, 310], "thank": [146, 205, 211, 261, 265, 295, 302], "thats": [213, 220], "theater": [213, 216, 220], "thei": [2, 7, 8, 9, 10, 11, 21, 22, 24, 46, 47, 52, 60, 66, 69, 72, 84, 86, 99, 101, 109, 112, 137, 141, 150, 152, 162, 164, 178, 183, 192, 198, 205, 211, 213, 216, 219, 220, 229, 231, 255, 257, 278, 279, 281, 283, 306, 330], "them": [0, 1, 2, 4, 5, 7, 9, 13, 15, 21, 23, 24, 25, 31, 35, 48, 62, 73, 76, 79, 80, 83, 109, 119, 137, 141, 151, 153, 154, 157, 163, 165, 166, 169, 178, 180, 191, 205, 211, 213, 220, 235, 237, 238, 241, 242, 243, 246, 247, 248, 249, 252, 254, 268, 269, 270, 271, 275, 287, 292, 295, 298], "theme": [0, 146], "themselv": [213, 219], "theoret": [213, 220], "therefor": [155, 159, 167, 173, 213, 216], "theta": [235, 236, 242, 243, 247, 255, 256, 261, 262, 268, 269], "theta_": [242, 243], "theta_dot": [242, 247], "thfc": [205, 211], "thi": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 28, 29, 31, 32, 33, 35, 36, 37, 38, 40, 41, 42, 43, 49, 50, 51, 52, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 73, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 90, 91, 92, 93, 94, 95, 99, 100, 101, 103, 106, 108, 109, 110, 112, 113, 115, 118, 119, 121, 122, 123, 124, 125, 126, 131, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 157, 158, 159, 161, 162, 163, 164, 165, 166, 169, 170, 171, 173, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 203, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 225, 226, 227, 228, 229, 230, 231, 233, 234, 237, 239, 246, 250, 251, 252, 253, 257, 258, 263, 264, 265, 266, 270, 271, 272, 273, 274, 275, 278, 279, 280, 281, 282, 284, 286, 287, 288, 289, 293, 295, 296, 297, 298, 300, 304, 305, 306, 307, 311, 312, 314, 315, 316, 317, 319, 320, 321, 323, 324, 325, 327, 329, 330, 331, 332, 333, 335, 336, 337, 339], "thine": [205, 211], "thing": [2, 21, 205, 211, 213, 219, 220, 295, 300], "think": [1, 2, 12, 17, 205, 207, 211, 213, 215, 219, 220, 221, 223, 229, 231, 295, 301], "third": [6, 41, 109, 113, 136, 148, 205, 211, 213, 219, 220, 305, 326], "tho": [205, 211], "thoma": [205, 211], "thor": [205, 211], "those": [7, 47, 99, 106, 109, 118, 154, 166, 213, 216, 220, 255, 259], "though": [152, 164, 205, 211, 278, 279, 282], "thought": [213, 216], "three": [1, 5, 8, 9, 16, 35, 51, 58, 59, 137, 140, 178, 204, 213, 220, 235, 241, 268, 274, 278, 279, 284, 285, 286, 306, 329], "thriller": [213, 220], "throb": [213, 216], "through": [2, 3, 4, 5, 6, 7, 19, 28, 29, 35, 38, 44, 47, 73, 78, 79, 80, 83, 84, 85, 99, 100, 109, 110, 122, 123, 137, 139, 140, 142, 145, 146, 150, 151, 161, 162, 163, 175, 178, 180, 183, 204, 205, 207, 211, 212, 213, 219, 220, 235, 236, 242, 247, 255, 256, 258, 261, 262, 268, 269, 278, 279, 281, 284, 286, 287, 288, 295, 297, 300, 304, 305, 312, 321], "throughout": [11, 72, 268, 270], "throughput": [7, 9, 48, 66, 142, 144, 145, 178, 192, 204, 205, 207, 212, 255, 259, 268, 277, 278, 279, 282], "throw": [99, 107, 109, 120, 205, 212], "thru": [213, 216], "thu": [4, 31, 80, 83], "thumb": [7, 48], "thursdai": [205, 209, 211, 212], "ti": [4, 5, 32, 36, 178, 181], "ticket": [205, 209, 211], "tidi": [255, 260, 261, 263, 267, 268, 277], "tie": [205, 211], "tiger": [205, 211, 213, 220], "tight": [205, 211], "tight_layout": [178, 191, 235, 237, 239, 241, 242, 246, 248, 250, 252, 261, 263, 265, 267, 268, 270, 275], "tightli": [73, 78], "tild": [242, 243], "tilt": [7, 45], "timber": [205, 211], "time": [1, 2, 3, 4, 5, 6, 7, 9, 10, 12, 16, 17, 18, 19, 20, 21, 22, 24, 25, 28, 30, 32, 34, 36, 40, 46, 48, 61, 68, 80, 82, 137, 141, 142, 144, 145, 152, 154, 158, 164, 166, 171, 178, 179, 180, 192, 193, 200, 205, 207, 211, 213, 216, 219, 220, 221, 228, 235, 236, 242, 243, 246, 247, 248, 249, 252, 254, 255, 258, 260, 263, 265, 267, 268, 269, 276, 278, 279, 281, 282, 283, 287, 290, 294, 295, 300, 303, 304, 305, 310, 311, 314, 317, 318, 323, 324, 326], "time_since_restor": [304, 319], "time_this_iter_": [304, 319], "time_total_": [304, 319], "timedelta": [261, 263], "timelin": [2, 24], "timeseri": [142, 144], "timeseriesbatchpredictor": [261, 267], "timeseriestransform": [261, 264, 265, 267], "timestamp": [4, 31, 146, 178, 180, 248, 250, 254, 261, 263, 304, 313, 319], "timestep": [5, 35, 235, 236, 238, 241, 242, 243, 244, 245, 247], "tini": [235, 238, 241, 242, 245, 255, 257, 261, 267, 268, 271], "tint": [213, 220], "tip": [3, 8, 28, 51, 52, 304, 305, 307, 318, 324, 335], "tip_amount": [3, 8, 28, 51, 52, 142, 144], "tip_percentag": [8, 52], "titan": [205, 211], "titl": [4, 9, 31, 32, 60, 122, 125, 178, 180, 191, 205, 209, 211, 235, 239, 242, 246, 250, 252, 255, 257, 259, 261, 263, 265, 267, 268, 275, 277], "tl": [80, 83], "tlc": [8, 51], "tloss": [221, 226], "tmp": [136, 142, 145, 146, 149, 154, 166, 221, 228, 303, 304, 310, 311, 318], "tmp_checkpoint": [268, 277], "tmpdir": [248, 252, 261, 265, 268, 273], "to_arrow_ref": [255, 258], "to_csv": [8, 53, 248, 250, 261, 263], "to_datetim": [261, 263], "to_json": [303, 310, 311], "to_numpi": [255, 258, 259, 261, 263, 267], "to_panda": [8, 9, 54, 64, 213, 220, 255, 258, 268, 271, 306, 331], "to_parquet": [8, 53, 178, 195, 248, 250, 255, 257, 268, 271], "to_pylist": [261, 263], "to_tensor": [178, 191], "todai": [11, 72, 205, 211], "todo": [142, 145], "togeth": [3, 4, 5, 9, 10, 12, 27, 32, 36, 61, 69, 178, 181, 188, 189, 205, 211, 307, 334, 336], "toke": [287, 290], "token": [146, 156, 168, 215, 216, 223, 224, 228, 278, 279, 281, 282, 283, 284, 285, 287, 290, 291, 292, 293, 295, 298, 299, 300, 301], "tokenization_fn": [213, 220], "tokenize_funct": [221, 226], "toler": [4, 5, 7, 8, 9, 30, 34, 46, 50, 55, 57, 73, 77, 78, 156, 160, 168, 176, 179, 199, 200, 202, 235, 236, 237, 239, 241, 242, 243, 246, 247, 248, 249, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262, 266, 267, 269, 273, 274, 277, 287, 292], "tolist": [10, 70, 142, 145, 178, 191, 195, 248, 254, 261, 263, 307, 335, 336], "toll": [3, 8, 28, 51], "tolls_amount": [3, 8, 28, 51], "tomorrow": [205, 209, 211, 212, 295, 300], "ton": [213, 220], "tone": [295, 298], "tonight": [205, 209, 211, 212], "tonit": [205, 211], "too": [3, 8, 9, 17, 28, 53, 61, 84, 86, 99, 101, 109, 112, 205, 212, 213, 215, 219, 220, 261, 263, 287, 289], "took": [2, 22], "tool": [3, 7, 27, 43, 47, 48, 80, 81, 84, 85, 91, 93, 99, 100, 109, 111, 122, 124, 136, 137, 139, 140, 141, 142, 144, 148, 149, 151, 154, 161, 163, 166, 205, 207, 213, 215, 221, 223, 227, 228, 242, 243, 248, 250, 255, 257, 261, 263, 279, 286, 296, 297, 302, 303, 309, 310, 311], "tool_cal": [295, 300], "tool_call_cli": [295, 300], "tool_call_id": [295, 300], "tool_call_pars": [295, 300], "tool_choic": [295, 300], "top": [1, 3, 8, 13, 17, 21, 24, 27, 50, 150, 157, 162, 169, 205, 211, 229, 231, 242, 247, 249, 255, 259, 261, 265, 268, 277, 295, 298, 303, 309], "top20": [205, 211], "top_item_id": [248, 254], "top_items_df": [248, 254], "top_scor": [248, 254], "topic": [205, 211, 288, 294, 298], "topic_safety_output_restrict": [295, 298], "topk": [248, 254], "torch": [4, 6, 9, 10, 29, 30, 31, 32, 33, 34, 36, 38, 39, 40, 41, 56, 62, 67, 70, 178, 180, 181, 182, 185, 186, 187, 188, 191, 199, 200, 205, 208, 221, 224, 226, 235, 237, 238, 239, 241, 242, 244, 245, 246, 247, 248, 249, 250, 252, 254, 261, 263, 264, 265, 267, 268, 269, 270, 273, 277, 304, 305, 306, 307, 313, 316, 317, 318, 319, 322, 323, 326, 330, 335], "torch_": [4, 31, 304, 313], "torch_config": [221, 227], "torch_d": [9, 59], "torchconfig": [221, 224, 227], "torchmetr": [268, 273, 304, 313, 315, 319], "torchrec": [248, 249], "torchscript": [242, 247], "torchtrain": [4, 29, 32, 33, 180, 182, 183, 184, 192, 195, 200, 202, 204, 221, 223, 224, 227, 236, 237, 241, 243, 244, 248, 249, 250, 252, 253, 254, 261, 262, 263, 265, 267, 269, 270, 277, 319], "torchtrainer_2025": [221, 228], "torchtrainer_4dd7a_00000": [221, 228], "torchtrainer_4dd7a_00000_0_2025": [221, 228], "torchtrainer_d89d0_00000_0_2024": [304, 319], "torchvis": [4, 6, 9, 29, 38, 56, 178, 180, 181, 196, 235, 237, 268, 269, 270, 304, 305, 306, 307, 313, 322, 323, 330, 336], "torqu": [242, 243, 247], "torranc": [205, 211], "total": [2, 3, 4, 8, 25, 28, 32, 51, 178, 182, 183, 221, 227, 235, 237, 242, 244, 278, 279, 283, 287, 293, 303, 304, 305, 310, 311, 313, 324, 326], "total_amount": [8, 51, 52, 142, 144], "total_amt": [6, 41, 305, 324], "totensor": [4, 6, 9, 29, 31, 32, 38, 39, 41, 56, 61, 178, 180, 186, 191, 196, 268, 271, 277, 304, 305, 306, 307, 313, 316, 319, 322, 326, 330, 336], "touch": [213, 216, 248, 254, 255, 257], "tough": [229, 234], "tougher": [229, 234], "tour": [1, 12, 205, 211], "tourism": [205, 211], "tourist": [295, 298], "toward": [7, 45, 206, 214, 222, 229, 230, 234, 242, 243, 248, 250], "tower": [248, 254], "town": [205, 211, 213, 219], "tpot": [278, 279, 283], "tpu": [2, 4, 5, 22, 32, 36, 178, 181], "tqdm": [4, 31, 221, 224, 235, 237, 248, 250, 268, 270], "tr_model": [261, 264], "trace": [4, 5, 7, 30, 34, 46, 136, 143, 148, 178, 179, 213, 216], "traceback": [7, 46], "track": [4, 11, 32, 72, 137, 140, 154, 155, 166, 167, 178, 179, 190, 204, 206, 213, 214, 220, 222, 230, 235, 239, 242, 243, 247, 248, 249, 252, 254, 255, 260], "track_running_stat": [304, 313, 319], "tractabl": [235, 236], "trade": [287, 293, 295, 301], "tradit": [248, 249, 255, 256, 278, 279, 283], "traffic": [73, 78, 109, 119, 122, 129, 142, 145, 156, 160, 168, 176, 229, 231, 234, 261, 262, 278, 279, 283, 284, 287, 292], "trail": [205, 211, 213, 219], "train": [6, 7, 8, 9, 39, 40, 41, 42, 44, 46, 47, 50, 52, 55, 58, 60, 62, 66, 152, 153, 155, 159, 160, 164, 165, 167, 173, 176, 180, 181, 183, 184, 185, 186, 194, 195, 196, 200, 203, 205, 209, 211, 213, 216, 220, 224, 229, 232, 233, 238, 240, 241, 244, 245, 251, 254, 259, 263, 264, 270, 273, 277, 278, 279, 281, 303, 305, 306, 310, 311, 317, 322, 323, 324, 325, 326, 330], "train_arrow": [255, 258], "train_batch": [268, 273], "train_bert": [221, 223, 227, 228], "train_config": [221, 227, 248, 252, 254], "train_count": [235, 237], "train_ctx": [3, 28], "train_d": [178, 195, 196, 197, 235, 237, 239, 242, 244, 246, 248, 250, 252, 255, 257, 258], "train_data": [4, 6, 32, 39, 41, 178, 186, 304, 305, 313, 316, 319, 322, 326], "train_dataload": [5, 35, 36, 235, 239, 242, 246], "train_dataset": [221, 226, 303, 310, 311], "train_df": [255, 257], "train_frac": [248, 250], "train_func": [255, 256, 258, 260], "train_func_per_work": [221, 226, 227], "train_label": [304, 313], "train_linear_model": [6, 41, 305, 325], "train_load": [4, 5, 31, 32, 35, 178, 186, 235, 239, 242, 246, 248, 252, 261, 265, 268, 273, 304, 313, 316], "train_loop": [236, 241, 242, 243, 246], "train_loop_config": [3, 4, 5, 28, 32, 36, 189, 197, 200, 202, 221, 227, 248, 252, 255, 258, 261, 265, 268, 274, 304, 318, 319], "train_loop_per_work": [5, 36, 178, 200, 202, 221, 227, 235, 239, 248, 249, 252, 261, 262, 265, 269, 274], "train_loop_ray_train": [4, 5, 32, 36, 178, 181, 182, 183, 184, 189, 304, 315, 318, 319], "train_loop_ray_train_ray_data": [178, 193, 197], "train_loop_ray_train_with_checkpoint_load": [178, 199, 200, 202], "train_loop_torch": [4, 6, 31, 40, 304, 305, 313, 323], "train_loss": [235, 238, 239, 242, 245, 246, 248, 249, 252, 261, 265, 268, 273, 275], "train_loss_sum": [261, 265], "train_loss_tot": [268, 273], "train_my_simple_model": [6, 41, 305, 324, 325], "train_my_simple_model_2024": [305, 324], "train_my_simple_model_3207e_00000_0_a": [305, 324], "train_my_simple_model_3207e_00000terminated10": [305, 324], "train_my_simple_model_3207e_00001terminated10": [305, 324], "train_my_simple_model_3207e_00002terminated10": [305, 324], "train_my_simple_model_3207e_00003terminated10": [305, 324], "train_my_simple_model_3207e_00004terminated10": [305, 324], "train_parquet": [255, 257], "train_pytorch": [6, 41, 305, 324, 326], "train_pytorch_7cf0c_00000terminated10": [305, 326], "train_pytorch_7cf0c_00001terminated10": [305, 326], "train_record": [261, 263], "train_test_split": [3, 26, 28, 248, 250, 255, 257, 303, 310, 311], "trainabl": [3, 6, 28, 41, 305, 324, 325, 326], "trainbr": [213, 220], "traincontext": [4, 32, 178, 182], "trainer": [3, 4, 5, 28, 32, 35, 36, 180, 181, 190, 197, 200, 201, 202, 204, 221, 224, 227, 235, 239, 240, 242, 246, 248, 249, 252, 253, 260, 261, 265, 266, 268, 269, 274, 276, 277, 303, 304, 310, 311, 315, 318, 319], "training_iter": [304, 319], "training_step": [5, 35, 235, 238, 242, 245], "trainingargu": [221, 224], "trajectori": [242, 247], "transact": [7, 43], "transfer": [1, 2, 7, 8, 9, 13, 19, 43, 54, 61, 64, 137, 141, 268, 277, 306, 331], "transform": [4, 5, 6, 7, 10, 29, 31, 32, 33, 38, 39, 41, 43, 44, 46, 49, 50, 51, 54, 55, 56, 58, 60, 64, 69, 70, 80, 83, 180, 186, 192, 197, 205, 208, 210, 212, 213, 215, 216, 220, 221, 223, 224, 226, 228, 229, 232, 233, 235, 236, 237, 242, 244, 247, 248, 250, 255, 257, 267, 269, 270, 272, 277, 303, 304, 305, 307, 310, 313, 316, 322, 326, 327, 329, 331, 335, 336], "transform_imag": [178, 196], "transient": [178, 200, 201, 268, 273], "transit": [153, 165, 235, 236, 242, 243, 248, 249, 255, 256, 261, 262], "transpar": [235, 239], "transpos": [235, 237], "travel": [205, 211, 295, 298], "treat": [213, 216, 219, 268, 269], "tree": [3, 4, 5, 28, 30, 31, 34, 35, 178, 179, 205, 209, 211, 213, 216, 255, 256, 257, 303, 310], "tree_method": [3, 28, 255, 258], "tremend": [213, 220], "trend": [205, 211], "tri": [213, 216, 255, 256], "trial": [3, 6, 28, 41, 213, 220, 221, 228, 303, 304, 305, 310, 311, 318, 324, 325, 326], "trial_id": [304, 319], "tribul": [213, 220], "trier": [213, 220], "trigger": [9, 60, 63, 205, 212, 242, 244, 255, 257, 260], "trim": [235, 237, 268, 270], "trip": [3, 8, 28, 51, 54, 213, 219, 261, 263, 267, 303, 310], "trip_amount": [3, 28], "trip_dist": [3, 8, 28, 51, 54, 303, 310], "trip_dur": [303, 310], "trivial": [278, 279, 284], "trndnl": [205, 211], "troubleshoot": [136, 149, 151, 154, 163, 166, 295, 302], "truck": [295, 299], "true": [0, 2, 4, 5, 6, 8, 9, 10, 20, 31, 32, 35, 36, 39, 41, 51, 59, 61, 62, 70, 91, 95, 142, 144, 145, 146, 178, 179, 180, 184, 186, 191, 213, 216, 221, 226, 235, 237, 238, 239, 242, 245, 246, 248, 250, 252, 253, 255, 257, 259, 261, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 285, 287, 290, 291, 292, 293, 295, 298, 300, 303, 304, 305, 306, 307, 310, 313, 315, 316, 319, 322, 326, 329, 336], "truli": [205, 211, 213, 220], "trump": [205, 211], "truncat": [221, 226, 229, 233, 234, 235, 237, 242, 244], "trust": [73, 75, 78, 205, 211, 261, 267], "truth": [9, 61, 64, 178, 180, 242, 244, 261, 262, 265, 267, 268, 277, 306, 331], "try": [1, 2, 6, 7, 16, 20, 41, 47, 84, 85, 99, 100, 107, 109, 111, 120, 122, 124, 178, 204, 213, 219, 220, 235, 237, 241, 255, 260, 268, 270, 287, 294, 295, 302, 304, 305, 318, 324, 325, 326], "tryna": [205, 211], "tsui": [213, 219], "ttft": [278, 279, 283], "ttm": [213, 219, 220], "tuesdai": [205, 211], "tune": [7, 9, 40, 46, 47, 61, 155, 159, 167, 173, 178, 194, 204, 213, 220, 235, 241, 242, 247, 248, 254, 255, 259, 260, 261, 267, 268, 269, 277, 278, 279, 284, 287, 294, 295, 297, 298, 301, 302, 303, 310, 311, 323, 339], "tune_config": [3, 6, 28, 41, 303, 305, 310, 311, 324, 325, 326], "tuneconfig": [3, 6, 28, 41, 303, 305, 310, 311, 324, 325, 326], "tuner": [3, 6, 28, 41, 303, 305, 310, 311, 324, 325, 326], "tupl": [2, 5, 9, 19, 35, 62, 178, 192, 193], "turn": [1, 6, 8, 14, 41, 53, 205, 211, 213, 220, 255, 256], "tutori": [73, 76, 151, 153, 163, 165, 180, 203, 235, 236, 241, 242, 243, 248, 249, 250, 254, 255, 256, 261, 262, 263, 265, 267, 268, 269, 270, 287, 289], "tv": [205, 211, 213, 219, 220], "tweet": [205, 211], "tweet_ev": [205, 209], "twilight": [205, 211], "twitter": [205, 211], "two": [1, 2, 16, 18, 19, 20, 73, 78, 80, 82, 136, 142, 144, 146, 149, 152, 155, 159, 164, 167, 175, 178, 181, 190, 195, 205, 211, 217, 220, 248, 250, 252, 254, 255, 257, 261, 263, 268, 269, 275, 282], "txt": [0, 11, 72, 146, 153, 165, 206, 214, 222, 230], "type": [2, 4, 5, 6, 7, 8, 9, 10, 20, 32, 33, 35, 38, 43, 46, 55, 59, 61, 62, 67, 74, 78, 80, 82, 84, 88, 91, 97, 99, 106, 109, 118, 122, 127, 133, 137, 141, 142, 145, 146, 150, 152, 158, 162, 164, 171, 205, 208, 213, 216, 221, 224, 256, 259, 260, 278, 279, 283, 287, 290, 300, 301, 305, 307, 324, 335], "typic": [8, 50, 80, 83, 161, 205, 207, 213, 220, 235, 237, 248, 250, 268, 269, 287, 289, 295, 298], "u": [5, 6, 35, 36, 40, 84, 86, 91, 94, 95, 99, 101, 109, 112, 122, 125, 126, 127, 136, 142, 144, 149, 153, 165, 178, 181, 205, 211, 213, 216, 220, 242, 243, 248, 249, 250, 254, 295, 298, 302, 303, 304, 305, 306, 307, 310, 311, 318, 323, 324, 326, 330, 336], "u002c": [205, 211], "u002c000": [205, 211], "u2019": [205, 211], "u2019ll": [205, 211], "u2019m": [205, 211], "u2019r": [205, 211], "u2019t": [205, 211], "u2019v": [205, 211], "u_": [242, 243, 248, 249], "u_k": [242, 243], "uber": [6, 42], "ubj": [3, 28, 303, 310, 311], "ubyt": [304, 305, 318, 324, 326], "udf": [7, 46], "ui": [0, 73, 78, 153, 155, 165, 167, 307, 336], "uid": [248, 250, 254], "uint8": [137, 141, 178, 196, 307, 336], "un": [213, 220], "unabl": [213, 220], "unassoci": [84, 86, 99, 101, 109, 112], "unattach": [84, 86, 99, 101, 109, 112], "unavail": [2, 22], "unavoid": [213, 216], "unbound": [9, 61], "uncas": [229, 233], "uncertainti": [261, 267], "unchang": [178, 196], "uncl": [213, 220], "uncom": [91, 95, 99, 102, 109, 114, 221, 227, 295, 299, 300], "uncondit": [235, 241], "unconnect": [213, 220], "unconnectedbr": [213, 220], "under": [2, 9, 19, 59, 122, 125, 142, 144, 152, 156, 160, 164, 168, 177, 178, 180, 181, 184, 188, 195, 213, 219, 235, 237, 239, 241, 242, 246, 247, 248, 250, 254, 261, 262, 263, 265, 268, 270, 273], "underbrac": [261, 262], "underli": [2, 3, 4, 5, 9, 18, 28, 30, 34, 60, 157, 169, 178, 179, 287, 290], "undersid": [213, 220], "understand": [6, 9, 40, 61, 73, 74, 76, 135, 136, 137, 141, 142, 145, 148, 154, 156, 166, 168, 213, 219, 220, 221, 226, 248, 254, 255, 256, 268, 269, 278, 279, 280, 281, 283, 285, 286, 287, 289, 290, 305, 323], "understat": [213, 220], "understood": [287, 294], "underutil": [278, 279, 281, 282], "uneasy": [205, 211], "unet": [5, 35], "unet2dconditionmodel": [5, 33, 35], "unexpect": [4, 5, 30, 34, 136, 148, 178, 179, 205, 211, 261, 263], "ungat": [287, 290, 295, 298], "unifi": [3, 7, 11, 27, 43, 46, 72, 73, 79, 161, 278, 279, 284], "uniform": [2, 3, 23, 28, 221, 226, 242, 243], "uniformli": [9, 57], "uniniti": [9, 62, 306, 330], "uninstal": [99, 107, 109, 120, 122, 134], "uniqu": [8, 51, 153, 165, 178, 186, 188, 213, 217, 220, 248, 254, 268, 270, 278, 279, 283, 287, 290, 306, 329], "unique_item": [248, 254], "unique_us": [248, 254], "unit": [9, 10, 64, 69, 71, 213, 216, 268, 270, 278, 279, 283, 295, 300, 306, 331], "univari": [261, 264], "univers": [213, 219], "unless": [8, 52, 155, 157, 159, 167, 169, 173, 178, 191, 199, 213, 216], "unlik": [155, 159, 167, 173, 178, 180, 196, 205, 211], "unnecessari": [9, 10, 61, 68, 154, 155, 156, 166, 167, 168, 178, 188, 191, 205, 211, 242, 247, 255, 258], "unnot": [213, 220], "unpredict": [278, 279, 283], "unread": [235, 237], "unregist": [84, 89, 99, 107, 109, 120], "unreleas": [205, 211], "unrelentingli": [213, 219, 220], "unrival": [213, 220], "uns4": [205, 211], "unshuffl": [255, 257], "unsloth": [278, 279, 285, 287, 290, 295, 298], "unsqueez": [4, 31, 32, 178, 191, 242, 247, 261, 263, 264, 265, 267, 304, 313, 319], "unstabl": [235, 236], "unstructur": [7, 43, 57, 339], "until": [2, 4, 7, 8, 18, 32, 48, 51, 52, 137, 141, 178, 189, 205, 211, 213, 220, 255, 258, 278, 279, 282, 283, 304, 306, 318, 330], "untitl": [151, 163], "unus": [84, 86, 99, 101, 109, 112], "unveil": [205, 211], "unwrap": [4, 32, 178, 188, 200, 304, 317], "up": [0, 2, 4, 5, 6, 7, 8, 9, 10, 20, 21, 30, 31, 34, 35, 40, 41, 48, 51, 60, 62, 69, 70, 73, 74, 77, 80, 83, 84, 86, 89, 90, 91, 94, 95, 98, 101, 102, 106, 108, 112, 114, 118, 121, 122, 125, 126, 134, 142, 143, 147, 148, 152, 158, 164, 170, 176, 179, 189, 200, 202, 205, 207, 210, 211, 213, 215, 216, 219, 220, 221, 223, 227, 237, 239, 243, 246, 252, 253, 256, 257, 258, 266, 269, 270, 274, 276, 278, 279, 283, 285, 288, 293, 294, 296, 298, 301, 306, 321, 324, 330, 334, 335, 336], "up_block_typ": [5, 35], "upblock2d": [5, 35], "upcom": 49, "updat": [0, 3, 4, 5, 6, 7, 10, 11, 28, 32, 36, 41, 43, 71, 72, 99, 102, 107, 109, 114, 120, 122, 129, 152, 154, 156, 164, 166, 168, 177, 178, 179, 182, 188, 199, 206, 214, 221, 222, 226, 229, 230, 231, 235, 241, 261, 267, 268, 273, 287, 292, 305, 307, 313, 325, 336], "upgrad": [10, 69, 99, 101, 102, 104, 109, 112, 114, 116, 129, 132, 151, 156, 160, 163, 168, 176, 235, 241, 242, 243, 247], "upload": [4, 32, 153, 165, 255, 260, 295, 298, 304, 317], "upload_fil": [153, 165, 295, 298], "upon": [154, 166], "upper": [151, 163], "upright": [242, 243], "upscal": [307, 336], "upscale_delay_": [307, 336], "upset": [205, 211], "ur": [205, 211], "uri": [153, 165], "url": [7, 48, 155, 167, 261, 263], "urljoin": [279, 285, 287, 291], "urllib": [279, 285, 287, 291], "urmitz": [213, 220], "us": [1, 2, 10, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 33, 35, 36, 37, 40, 42, 43, 44, 45, 46, 48, 49, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 68, 70, 71, 76, 77, 79, 81, 83, 84, 85, 87, 90, 91, 92, 95, 96, 99, 100, 103, 108, 109, 110, 111, 115, 119, 121, 122, 123, 124, 125, 126, 127, 131, 136, 137, 140, 141, 143, 144, 146, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158, 161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 180, 181, 182, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 197, 198, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 217, 218, 219, 220, 221, 222, 223, 225, 226, 227, 228, 230, 233, 234, 237, 239, 241, 244, 246, 247, 251, 252, 254, 258, 259, 260, 263, 265, 267, 270, 273, 275, 277, 278, 279, 281, 282, 283, 284, 285, 288, 289, 291, 292, 294, 297, 302, 304, 309, 311, 315, 316, 317, 318, 319, 320, 321, 323, 324, 325, 327, 329, 330, 332, 335, 337], "usabl": [178, 196, 295, 299], "usag": [10, 68, 136, 137, 139, 141, 148, 154, 166, 221, 228, 248, 252, 278, 279, 282, 284, 295, 297, 298, 301, 303, 304, 305, 307, 310, 311, 318, 324, 326, 334], "use_gpu": [3, 4, 5, 28, 32, 36, 178, 179, 184, 235, 239, 242, 246, 248, 252, 255, 258, 261, 265, 268, 269, 274, 303, 304, 310, 311, 315], "use_gpu_actor": [268, 277], "usecol": [248, 254], "user": [2, 4, 7, 9, 11, 18, 22, 32, 46, 47, 61, 72, 73, 75, 80, 82, 122, 125, 137, 139, 140, 141, 142, 145, 150, 153, 154, 155, 157, 159, 160, 162, 165, 166, 167, 169, 170, 173, 176, 178, 182, 185, 186, 205, 209, 211, 212, 221, 228, 229, 231, 251, 252, 278, 279, 281, 282, 283, 285, 287, 291, 292, 295, 298, 299, 300, 301, 304, 305, 313, 326], "user2idx": [248, 250, 254], "user_col": [248, 250], "user_embed": [248, 251, 254], "user_id": [146, 248, 250, 254], "user_idx": [248, 249, 250, 251, 252, 254], "user_nam": [122, 125], "user_storag": [153, 165], "user_vec": [248, 251], "user_vector": [248, 254], "userguid": [84, 85, 99, 100, 109, 111], "userservic": [142, 145, 146], "usual": [7, 8, 45, 52, 178, 191, 213, 219], "utc": [304, 313], "util": [2, 4, 5, 6, 9, 10, 22, 31, 32, 33, 35, 38, 39, 41, 57, 61, 66, 68, 69, 142, 145, 154, 156, 160, 166, 168, 176, 178, 179, 180, 186, 188, 192, 204, 221, 223, 224, 226, 235, 237, 242, 244, 247, 248, 250, 261, 263, 268, 269, 270, 278, 279, 281, 282, 283, 284, 304, 305, 307, 313, 316, 317, 322, 336], "uuid": [242, 244, 248, 250, 255, 257, 261, 263, 268, 270], "uv": [206, 214, 222, 230], "ux": [7, 46], "v": [45, 46, 73, 76, 83, 99, 105, 106, 109, 117, 118, 151, 155, 163, 167, 178, 179, 191, 203, 205, 211, 221, 226, 248, 249, 254, 261, 265, 267, 268, 277, 278, 279, 282, 295, 301, 339], "v1": [80, 83, 242, 243, 244, 247, 279, 285, 287, 291, 292, 295, 298, 299, 300], "v2": [178, 179, 205, 210, 235, 236, 239, 242, 243, 248, 249, 255, 256, 261, 262], "v_": [248, 249], "val": [239, 244, 248, 250, 252, 255, 257, 258, 261, 263, 265, 267, 268, 271, 273, 275, 277], "val_batch": [268, 273], "val_d": [235, 237, 239, 242, 244, 246, 248, 250, 252, 255, 257, 258, 259, 260], "val_dataload": [235, 239, 242, 246], "val_df": [255, 257], "val_load": [235, 239, 242, 246, 248, 252, 261, 265, 268, 273], "val_loss": [235, 238, 239, 242, 245, 246, 248, 249, 252, 261, 265, 268, 273, 274, 275], "val_loss_sum": [261, 265], "val_loss_tot": [268, 273], "val_parquet": [255, 257], "val_pd": [255, 258, 259], "val_record": [261, 263], "val_xb": [268, 273], "val_yb": [268, 273], "valid": [3, 28, 135, 156, 168, 221, 226, 235, 238, 239, 242, 243, 244, 245, 246, 249, 258, 259, 260, 269, 273, 274, 277, 295, 299, 303, 307, 310, 311, 336], "valid_dataset": [303, 310, 311], "valid_dataset_featur": [303, 310, 311], "validation_step": [235, 238, 242, 245], "valu": [2, 4, 6, 9, 18, 19, 23, 31, 39, 60, 61, 84, 85, 86, 91, 92, 95, 99, 100, 101, 102, 107, 109, 110, 112, 114, 120, 122, 123, 128, 129, 142, 144, 145, 156, 160, 168, 177, 178, 183, 186, 187, 190, 196, 213, 218, 235, 236, 239, 242, 243, 244, 248, 250, 254, 261, 262, 263, 264, 268, 273, 275, 281, 304, 305, 313, 322], "valuabl": [213, 219], "value_count": [248, 250, 255, 257], "valueerror": [2, 20, 235, 239, 242, 246], "values_nginx": [99, 102, 109, 114], "values_nginx_gke_priv": [122, 129], "values_nginx_gke_publ": [122, 129], "values_nvdp": [99, 102, 109, 114], "vamp": [205, 211], "vampett": [205, 211], "vampir": [205, 211], "van": [205, 211], "vanilla": [5, 26, 36, 38, 261, 262, 263, 278, 279, 282], "var": [73, 78, 122, 134, 178, 180, 235, 237, 242, 244, 248, 250, 255, 257, 261, 263, 268, 270], "varepsilon": [235, 236], "varepsilon_": [242, 243], "varepsilon_k": [242, 243], "vari": [7, 8, 9, 43, 54, 57, 62, 64, 278, 279, 283, 306, 331], "variabl": [2, 21, 22, 84, 86, 91, 95, 99, 101, 109, 112, 122, 125, 152, 153, 154, 164, 165, 166], "variat": [6, 41, 305, 325], "varieti": [9, 59, 64, 213, 215, 306, 331], "variou": [7, 44, 152, 153, 164, 165, 178, 192, 221, 227, 287, 294], "vast": [7, 43], "ve": [84, 90, 205, 211, 213, 216, 220, 268, 277, 278, 279, 286, 287, 294, 295, 297, 302], "vector": [7, 9, 43, 61, 205, 207, 242, 244, 247, 248, 249, 251, 255, 260, 278, 279, 281], "veget": [255, 256], "veloc": [242, 243], "venu": [229, 234], "venv": 0, "verbos": [142, 145], "veri": [4, 5, 6, 7, 8, 31, 32, 35, 36, 41, 47, 52, 178, 180, 205, 211, 213, 220, 261, 267, 287, 289, 304, 305, 306, 314, 325, 330], "verif": 135, "verifi": [5, 9, 35, 61, 84, 89, 91, 94, 107, 119, 120, 122, 125, 128, 129, 130, 152, 154, 155, 164, 166, 167, 257, 261, 263], "vermaelen": [205, 211], "version": [7, 11, 43, 72, 73, 78, 84, 85, 91, 93, 99, 100, 102, 109, 111, 114, 122, 124, 129, 130, 136, 137, 140, 142, 145, 149, 150, 160, 162, 176, 178, 193, 205, 206, 211, 214, 222, 229, 230, 231, 235, 237, 239, 261, 263, 295, 298, 306, 331], "versu": [235, 241, 255, 260, 268, 277], "via": [0, 1, 2, 4, 5, 7, 9, 13, 22, 30, 34, 46, 47, 57, 73, 76, 80, 82, 83, 84, 86, 91, 95, 99, 101, 109, 112, 136, 142, 143, 145, 146, 148, 151, 157, 158, 159, 163, 169, 171, 173, 178, 179, 182, 183, 184, 191, 196, 204, 205, 211, 213, 219, 235, 238, 242, 246, 248, 250, 252, 268, 272, 273], "vicki": [205, 209, 211], "vid": [205, 211], "video": [7, 9, 43, 57, 205, 211, 213, 216, 220], "vietnam": [213, 216], "view": [6, 8, 9, 10, 11, 41, 51, 52, 61, 71, 72, 73, 78, 84, 86, 88, 91, 95, 97, 99, 101, 106, 109, 112, 118, 119, 122, 133, 136, 137, 141, 142, 144, 145, 148, 152, 154, 164, 166, 213, 216, 220, 235, 238, 242, 245, 287, 293, 304, 305, 307, 318, 324, 325, 335], "viewer": [213, 216], "vincent": [213, 216], "violat": [295, 298], "viridi": [255, 259], "virtual": [0, 73, 76, 78, 83, 136, 149, 152, 164, 278, 279, 282], "virtuou": [213, 216], "visibl": [178, 187, 213, 216, 261, 267, 287, 293], "vision": [178, 204, 241, 277], "visit": [6, 41, 136, 149, 305, 307, 325, 336], "visual": [6, 7, 8, 9, 39, 44, 51, 60, 136, 137, 141, 148, 149, 151, 154, 163, 166, 213, 219, 220, 239, 241, 242, 243, 246, 252, 256, 265, 275, 287, 293, 306, 314, 321, 329], "visualis": [261, 263], "vit": [178, 200, 202], "vit_b_16": [268, 277], "vit_l_32": [268, 277], "vllm": [285, 286, 287, 293, 294, 295, 299], "vm": [73, 74, 76, 83, 84, 88, 91, 95, 97, 122, 126, 135, 339], "vocal": [205, 211], "voic": [205, 211, 213, 220], "volatil": [4, 31, 178, 180], "volleybal": [205, 211], "volum": [4, 7, 31, 43, 46, 137, 141, 153, 165, 178, 180, 255, 257, 268, 269, 277], "von": [213, 220], "vpc": [76, 77, 79, 80, 82, 84, 86, 90, 91, 92, 95, 99, 100, 101, 109, 111, 112, 122, 126, 134, 135, 278, 279, 284], "vpc_cidr": [73, 78], "vpc_id": [73, 78, 79, 84, 86], "vpcid": [84, 86, 99, 101, 109, 112], "vram": [205, 211, 287, 290], "vscode": [156, 168], "vtripl": [295, 298], "vulva": [213, 216], "w": [6, 10, 41, 70, 153, 165, 178, 191, 205, 209, 211, 213, 220, 235, 236, 237, 238, 268, 277, 305, 325], "w0": [2, 25], "w1": [2, 25], "wa": [5, 8, 35, 51, 137, 141, 152, 155, 164, 167, 178, 202, 205, 211, 213, 216, 219, 220, 248, 253, 303, 310], "wai": [2, 7, 8, 9, 11, 19, 44, 45, 50, 60, 61, 72, 73, 76, 151, 155, 158, 159, 163, 167, 172, 175, 178, 188, 205, 211, 213, 215, 219, 220, 229, 231, 234, 242, 247, 268, 269, 271, 277, 287, 293, 307, 336], "wait": [1, 15, 16, 17, 23, 122, 129, 152, 164, 205, 211, 278, 279, 282], "wake": [205, 211, 213, 220], "walk": [4, 6, 29, 38, 84, 85, 99, 100, 109, 110, 122, 123, 151, 153, 163, 165, 205, 211, 212, 235, 236, 255, 256, 261, 262, 287, 288, 295, 298, 304, 305, 312, 321], "walter": [213, 219], "wander": [213, 216], "wanna": [205, 211], "want": [0, 1, 2, 4, 5, 7, 8, 9, 10, 15, 18, 19, 24, 25, 32, 35, 44, 51, 53, 54, 61, 62, 64, 70, 71, 73, 78, 91, 95, 98, 109, 112, 122, 125, 134, 151, 154, 163, 166, 178, 180, 186, 187, 191, 205, 209, 210, 211, 213, 216, 219, 220, 221, 228, 261, 262, 265, 278, 279, 283, 303, 306, 307, 310, 330, 331, 335], "war": [205, 211, 213, 216], "warehous": [9, 59], "warm": [5, 35, 205, 211], "warmth": [205, 211], "warn": [205, 211, 213, 219, 220, 235, 239, 242, 246], "warner": [205, 209, 211], "wasn": [213, 219, 261, 263], "wast": [213, 220], "watch": [122, 129, 205, 211, 213, 220], "water": [213, 220], "wave": [213, 220], "wc": [8, 51], "we": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 16, 19, 20, 21, 23, 25, 28, 31, 32, 35, 36, 40, 41, 44, 45, 51, 52, 53, 54, 59, 61, 62, 63, 64, 70, 84, 85, 91, 93, 99, 100, 102, 106, 109, 111, 114, 118, 122, 124, 126, 142, 144, 150, 151, 152, 153, 154, 155, 156, 161, 162, 163, 164, 165, 166, 167, 168, 178, 180, 181, 182, 183, 184, 185, 188, 189, 191, 193, 194, 195, 196, 197, 199, 200, 205, 207, 211, 213, 216, 217, 219, 220, 221, 225, 227, 228, 229, 231, 232, 234, 248, 249, 252, 268, 275, 278, 279, 280, 282, 285, 286, 288, 289, 290, 292, 296, 298, 299, 304, 305, 306, 307, 313, 314, 315, 317, 319, 323, 324, 325, 326, 329, 330, 331, 335, 336, 338], "wealth": [213, 220], "weather": [261, 267], "weaviat": [7, 43], "web": [136, 143, 148, 151, 154, 163, 166, 229, 231, 242, 247], "webservic": 232, "websit": [0, 136, 149], "webster": [213, 219], "wed": [205, 211], "wednesdai": [205, 209, 211], "week": [205, 211, 261, 262, 263], "weight": [2, 25, 178, 179, 182, 188, 191, 200, 204, 235, 241, 242, 247, 248, 252, 254, 255, 257, 260, 261, 267, 268, 277, 278, 279, 283, 287, 290, 304, 313], "weight_decai": [5, 35, 36], "weights_onli": [4, 5, 32, 36, 178, 191, 304, 319], "welbeck": [205, 211], "welcom": [146, 303, 308], "well": [2, 7, 25, 47, 73, 78, 80, 82, 152, 158, 164, 172, 205, 211, 213, 219, 220, 229, 231, 248, 250, 251], "wellcraft": [213, 220], "welllll": [205, 211], "went": [213, 220], "were": [5, 35, 178, 202, 205, 211, 213, 219, 220, 255, 257], "werewolf": [213, 220], "west": [84, 86, 99, 101, 109, 112, 153, 165, 295, 298, 303, 304, 305, 310, 311, 318, 324, 326], "west2": [91, 94, 95, 122, 125, 126, 127], "western": [213, 219, 220], "wget": [248, 250], "what": [1, 3, 5, 6, 9, 15, 26, 28, 35, 41, 49, 61, 74, 91, 92, 122, 126, 137, 138, 140, 141, 147, 150, 162, 184, 204, 205, 211, 213, 216, 220, 250, 280, 285, 298, 300, 303, 305, 308, 310, 311, 324], "whatev": [255, 260, 261, 267, 268, 277], "when": [1, 2, 6, 11, 15, 16, 18, 20, 21, 22, 24, 25, 29, 33, 41, 48, 49, 52, 53, 56, 58, 59, 60, 61, 62, 63, 64, 67, 72, 84, 86, 89, 91, 95, 98, 99, 101, 102, 106, 109, 112, 114, 118, 122, 126, 134, 137, 140, 141, 142, 143, 145, 151, 152, 153, 154, 155, 156, 160, 163, 164, 165, 166, 167, 168, 177, 186, 189, 190, 192, 200, 203, 205, 207, 210, 211, 213, 216, 220, 229, 234, 235, 237, 241, 242, 247, 248, 252, 254, 255, 257, 261, 265, 267, 268, 270, 273, 275, 278, 279, 281, 282, 287, 291, 292, 293, 295, 300, 305, 325, 327, 329, 330, 331, 336], "where": [1, 4, 5, 6, 7, 8, 13, 32, 36, 39, 47, 48, 51, 109, 112, 122, 127, 136, 137, 140, 149, 153, 157, 158, 161, 165, 169, 171, 188, 189, 190, 197, 202, 205, 211, 213, 216, 219, 220, 221, 226, 229, 231, 243, 249, 250, 253, 256, 262, 266, 269, 271, 278, 279, 281, 287, 289, 303, 304, 305, 310, 311, 314, 318, 322], "wherea": [7, 45, 48], "wherev": [205, 211], "whether": [4, 5, 6, 11, 32, 36, 41, 72, 135, 153, 157, 165, 169, 178, 181, 184, 185, 186, 221, 223, 248, 252, 255, 257, 261, 265, 303, 304, 305, 310, 315, 325], "which": [1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 17, 18, 19, 23, 24, 25, 28, 32, 35, 41, 44, 47, 51, 52, 53, 54, 60, 61, 62, 63, 73, 76, 83, 84, 86, 99, 101, 109, 112, 113, 119, 122, 125, 136, 137, 141, 142, 145, 149, 152, 156, 164, 168, 178, 180, 182, 186, 187, 188, 190, 193, 205, 207, 213, 216, 219, 220, 221, 223, 226, 227, 229, 231, 235, 236, 237, 255, 256, 259, 261, 265, 267, 268, 269, 270, 273, 277, 287, 289, 293, 295, 298, 300, 304, 305, 306, 307, 319, 324, 325, 330, 336], "while": [2, 3, 7, 10, 23, 24, 27, 43, 47, 69, 137, 140, 141, 142, 145, 151, 152, 154, 161, 163, 164, 166, 178, 179, 181, 182, 192, 193, 194, 213, 216, 219, 220, 235, 237, 239, 248, 249, 250, 255, 257, 261, 262, 263, 268, 269, 270, 275, 287, 289, 295, 298], "whilst": [213, 220], "white": [6, 39, 205, 211, 213, 220, 305, 322], "who": [73, 78, 135, 157, 169, 205, 211, 213, 216, 219, 220, 221, 228, 278, 279, 283], "whole": [3, 28, 255, 258, 303, 310], "whose": [213, 219, 220], "why": [9, 56, 178, 179, 198, 205, 211, 213, 219, 288, 294], "whyyyyyyi": [205, 211], "wichita": [205, 211], "wide": [7, 46, 122, 129, 205, 211, 213, 220, 229, 231], "widescreen": [213, 219], "width": [9, 61, 137, 141, 235, 237, 306, 307, 330, 336], "wife": [213, 220], "wildlif": [229, 234], "wilki": [213, 219], "willam": [205, 209, 211], "william": [205, 211, 213, 219], "wilmer": [205, 211], "win": [7, 47, 205, 209, 211], "wind": [213, 219], "window": [11, 72, 91, 94, 136, 149, 151, 163, 178, 204, 213, 216, 262, 264, 267, 287, 293], "wire": [178, 204, 235, 239], "wise": [178, 191], "wish": [205, 211], "with_resourc": [6, 41, 305, 325, 326], "within": [2, 7, 22, 43, 73, 75, 78, 80, 82, 136, 149, 151, 153, 155, 157, 158, 159, 163, 165, 167, 169, 171, 175, 242, 247, 248, 250], "without": [2, 7, 10, 18, 19, 22, 24, 43, 69, 73, 78, 136, 149, 151, 152, 161, 163, 164, 178, 179, 198, 200, 201, 213, 216, 220, 235, 236, 241, 242, 243, 248, 249, 250, 252, 253, 254, 255, 258, 259, 260, 261, 262, 263, 267, 268, 269, 276, 278, 279, 282, 295, 298, 299], "woman": [213, 219, 220], "women": [213, 216], "won": [2, 7, 20, 47, 213, 216], "wonder": [6, 41, 80, 82, 205, 209, 211], "wood": [205, 211], "wooden": [213, 219], "word": [213, 220, 278, 279, 281], "work": [1, 2, 4, 6, 8, 10, 11, 16, 18, 31, 32, 40, 53, 54, 56, 69, 72, 80, 82, 83, 151, 153, 154, 156, 157, 161, 163, 165, 166, 168, 169, 185, 195, 204, 205, 207, 210, 211, 213, 219, 220, 221, 227, 240, 242, 243, 248, 249, 250, 252, 255, 256, 257, 268, 269, 271, 276, 277, 278, 279, 281, 287, 290, 294, 295, 297, 304, 315], "worker": [2, 3, 8, 9, 10, 11, 18, 20, 21, 23, 25, 28, 50, 54, 62, 64, 69, 72, 73, 78, 80, 82, 99, 106, 109, 118, 137, 139, 141, 150, 156, 162, 168, 179, 181, 183, 184, 185, 186, 187, 188, 189, 191, 194, 195, 196, 197, 198, 200, 201, 205, 211, 213, 218, 223, 227, 228, 235, 236, 237, 238, 239, 241, 242, 243, 246, 247, 248, 249, 250, 252, 256, 257, 260, 261, 262, 263, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 304, 306, 315, 317, 331], "worker_devic": [205, 211], "worker_nod": [99, 106, 109, 118, 137, 141], "worker_rank": [3, 28], "workernodegroupconfig": [99, 106, 109, 118], "workflow": [7, 11, 43, 45, 46, 47, 67, 72, 80, 82, 150, 151, 154, 155, 161, 162, 163, 166, 167, 178, 179, 188, 191, 204, 205, 206, 207, 212, 213, 214, 215, 220, 221, 222, 227, 228, 229, 230, 231, 242, 243, 255, 256, 257, 261, 262, 267, 268, 269, 295, 300], "working_dir": [155, 159, 167, 175, 278, 279, 285, 287, 292], "workload": [2, 3, 4, 5, 7, 8, 9, 11, 22, 27, 28, 32, 35, 43, 46, 47, 55, 57, 72, 73, 78, 81, 83, 84, 90, 99, 108, 109, 121, 136, 137, 140, 141, 143, 149, 150, 152, 153, 154, 155, 159, 160, 161, 162, 164, 165, 166, 167, 172, 173, 176, 178, 186, 192, 204, 229, 231, 241, 247, 254, 267, 277, 278, 279, 283, 287, 289, 294, 303, 306, 309, 328, 332, 339], "workload_identity_pool_provid": [91, 95, 122, 126], "workloadidentitypool": [91, 95, 122, 126], "workloadserviceaccountnam": [99, 101, 104, 109, 112, 116, 122, 132], "workshop": [150, 159, 160, 162, 173, 176], "workspac": [73, 77, 78, 80, 82, 151, 152, 153, 154, 155, 156, 157, 158, 163, 164, 165, 166, 167, 168, 169, 171, 173, 242, 243, 255, 260, 261, 267, 268, 269, 277, 287, 289, 293, 294, 339], "workspace_v2": [151, 163], "world": [4, 32, 142, 145, 151, 155, 159, 163, 167, 174, 175, 178, 182, 205, 211, 213, 220, 221, 226, 235, 239, 248, 249, 254, 268, 269, 295, 302], "world_rank": [4, 32, 178, 187, 189], "world_siz": [4, 32, 178, 182, 193], "worri": [268, 269], "worth": [205, 211], "would": [4, 5, 8, 32, 35, 36, 52, 53, 54, 137, 141, 178, 184, 205, 211, 213, 215, 217, 219, 220, 221, 226, 287, 289, 294, 304, 314], "wound": [213, 219], "wrangl": [178, 180], "wrap": [2, 4, 19, 32, 156, 160, 161, 168, 177, 179, 180, 181, 182, 186, 188, 189, 197, 236, 243, 244, 250, 258, 262, 263, 269, 272, 304, 315, 316], "wright": [205, 211], "write": [1, 2, 3, 4, 5, 6, 7, 9, 13, 16, 25, 28, 32, 36, 41, 43, 49, 50, 52, 56, 58, 60, 61, 65, 73, 78, 137, 141, 146, 151, 153, 163, 165, 178, 188, 195, 200, 235, 237, 248, 249, 261, 263, 268, 269, 270, 271, 273, 304, 305, 306, 319, 325, 330, 332], "write_csv": [8, 53], "write_parquet": [3, 8, 9, 28, 53, 60, 65, 137, 141, 142, 144, 235, 237, 261, 263, 306, 332], "write_t": [261, 263, 268, 270], "writefil": [142, 144], "writer": [4, 31, 304, 313], "writerow": [4, 31, 304, 313], "written": [7, 11, 45, 72, 136, 142, 145, 149, 178, 180, 188, 213, 219, 235, 237, 255, 257, 261, 263, 268, 273], "wrong": [205, 211], "wrote": [8, 53, 205, 211, 235, 237, 248, 250, 255, 257, 268, 270], "wt": [205, 209, 211], "ww2": [213, 220], "wwe": [205, 209, 211], "wyom": [213, 219], "x": [1, 2, 5, 6, 7, 16, 18, 19, 20, 24, 35, 41, 44, 137, 141, 146, 154, 166, 178, 191, 205, 211, 235, 236, 238, 242, 245, 247, 255, 258, 261, 264, 267, 268, 269, 277, 305, 325], "x_": [235, 236, 242, 243], "x_0": [235, 236, 238, 242, 243], "x_t": [235, 236, 238, 242, 243], "x_test": [3, 28], "x_train": [3, 28], "xb": [261, 263, 268, 273], "xgb": [4, 5, 30, 34, 178, 179, 255, 256, 257, 258, 259, 260], "xgb_model": [255, 258], "xgb_param": [255, 258], "xgboost": [6, 41, 257, 259, 260, 303, 310, 311], "xgboost_predict": [3, 28], "xgboosterror": [3, 26], "xgboosttrain": [3, 26, 28, 255, 257, 258, 303, 310, 311], "xgboosttrainer_2024": [303, 310, 311], "xgboosttrainer_81312_00000terminated10": [303, 310, 311], "xgboosttrainer_81312_00001terminated10": [303, 310, 311], "xgboosttrainer_81312_00002terminated10": [303, 310, 311], "xgbpredictor": [255, 259, 260], "xing": [91, 95, 122, 126], "xlabel": [235, 239, 242, 246, 248, 250, 252, 255, 259, 261, 265, 267, 268, 275], "xxx": [91, 95, 109, 112, 122, 125, 126], "xxxx": [91, 95, 122, 126], "xxxxx": [84, 86, 91, 95, 96, 99, 103, 109, 115, 122, 126, 131], "xxxxxx": [84, 86, 99, 101, 109, 112], "xxxxxxx": [99, 101, 109, 112], "xxxxxxxx": [84, 86, 99, 101, 109, 112], "xxxxxxxxx": [84, 86], "xxxxxxxxxx": [84, 86], "xxxxxxxxxxxx": [84, 86, 99, 101, 109, 112], "y": [2, 4, 6, 11, 18, 31, 41, 72, 154, 166, 205, 211, 255, 258, 268, 269, 279, 285, 287, 291, 293, 295, 298, 299, 300, 304, 305, 313, 325], "y_test": [3, 28], "y_train": [3, 28], "ya": [205, 211], "yaml": [10, 71, 99, 102, 109, 114, 122, 129, 137, 141, 146, 156, 168, 278, 279, 285, 287, 292, 295, 299], "yanke": [205, 209, 211], "yann": [304, 305, 318, 324], "yara": [295, 298], "yard": [205, 211], "yb": [261, 263, 268, 273], "ye": [73, 78, 205, 211], "year": [3, 28, 205, 211, 213, 216, 220, 295, 298, 300], "yellow": [3, 28, 213, 216, 278, 279, 282, 303, 310], "yellow_tripdata_": [3, 28], "yellow_tripdata_2011": [8, 51, 54, 142, 144], "yellow_tripdata_2021": [3, 28], "yelp": [221, 223, 228], "yelp_review_ful": [221, 226], "yepo": [205, 211], "yesterdai": [205, 211], "yet": [213, 220, 248, 249, 250, 268, 269], "yield": [2, 23], "ylabel": [235, 239, 242, 246, 248, 250, 252, 255, 257, 259, 261, 263, 265, 267, 268, 275], "yml": 0, "york": [3, 8, 28, 51, 261, 262], "you": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 17, 19, 20, 21, 22, 23, 24, 26, 28, 29, 30, 31, 32, 33, 34, 36, 38, 41, 49, 50, 51, 52, 53, 54, 55, 56, 60, 61, 62, 63, 64, 65, 67, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 106, 107, 108, 109, 111, 112, 113, 114, 115, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 131, 133, 134, 136, 137, 138, 139, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 156, 157, 161, 162, 163, 164, 165, 166, 168, 169, 175, 180, 183, 184, 185, 186, 187, 188, 189, 190, 191, 197, 202, 203, 204, 205, 207, 209, 210, 211, 213, 215, 216, 217, 218, 219, 220, 221, 223, 226, 227, 228, 229, 231, 234, 237, 239, 244, 245, 246, 250, 252, 257, 258, 263, 264, 265, 270, 271, 272, 275, 278, 279, 280, 282, 283, 284, 285, 287, 288, 291, 292, 294, 295, 296, 297, 298, 299, 300, 302, 303, 304, 305, 306, 307, 308, 312, 316, 319, 321, 325, 330, 331, 332, 333, 334], "young": [213, 216, 220], "your": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 14, 16, 21, 23, 25, 28, 31, 32, 36, 41, 53, 57, 58, 61, 62, 64, 65, 68, 70, 74, 75, 77, 78, 81, 82, 84, 85, 86, 87, 88, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 110, 112, 114, 115, 116, 118, 119, 120, 122, 123, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 142, 144, 145, 146, 150, 151, 152, 153, 157, 158, 162, 163, 164, 165, 169, 171, 176, 178, 179, 180, 181, 182, 183, 186, 188, 189, 192, 198, 201, 203, 205, 207, 210, 211, 213, 215, 219, 220, 221, 223, 227, 228, 235, 236, 242, 243, 248, 249, 250, 252, 254, 255, 256, 258, 260, 261, 262, 263, 265, 267, 268, 269, 274, 275, 277, 278, 279, 282, 283, 284, 285, 287, 290, 291, 292, 293, 295, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 316, 319, 325, 330, 331, 334], "your_anyscale_org_id": [91, 95], "your_gcp_project_nam": [122, 134], "your_project_id": [91, 94], "yourself": [2, 21, 235, 236], "yr": [205, 211], "ytick": [255, 259], "yunikorn": [80, 82], "z": [205, 211, 261, 263], "zentropa": [213, 219, 220], "zero": [7, 43, 142, 145, 156, 160, 168, 176, 221, 226, 229, 234, 255, 257, 261, 264, 265, 267, 268, 273, 277, 278, 279, 283, 284, 287, 294], "zero_copy_onli": [255, 258], "zero_grad": [4, 6, 31, 32, 40, 41, 178, 182, 193, 199, 221, 226, 248, 252, 261, 265, 268, 273, 304, 305, 313, 315, 319, 323, 326], "zeros_lik": [261, 265], "zilliz": [7, 43], "zip": [6, 39, 235, 237, 241, 248, 250, 254, 255, 259, 268, 270, 305, 322], "zip_ref": [248, 250], "zipfil": [248, 250], "zone": [8, 51, 73, 78, 99, 101, 109, 112, 122, 127], "zprofil": [11, 72], "zsh": [122, 125], "zshrc": [122, 125], "zuoma": [205, 211], "\u03b8": [242, 244, 247], "\u03c0": [154, 166, 242, 244], "\u03f5": [235, 238, 242, 245]}, "titles": ["Ray Enablement Content: Jupyter Book Publishing", "Introduction to Ray Core: Getting Started", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "Introduction to Ray Train + PyTorch", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "Introduction to Ray Tune", "Introduction to Ray Data: Industry Landscape", "Introduction to Ray Data: Ray Data + Structured Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "Introduction to Ray Serve with PyTorch", "Introduction to Ray: Developer", "Introduction to Ray Core: Getting Started", "0. Overview", "1. Creating Remote Functions", "2. Executing Remote Functions", "4. Putting It All Together", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "1. Object store", "2. Chaining Tasks and Passing Data", "3. Task retries", "4. Task Runtime Environments", "5. Resource allocation and management", "6. Nested Tasks", "7. Pattern: Pipeline data processing and waiting for results", "8. Ray Actors", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "1. Overview of the Ray AI Libraries", "2. Quick end-to-end example", "Introduction to Ray Train + PyTorch", "1. When to use Ray Train", "2. Single GPU Training with PyTorch", "3. Distributed Data Parallel Training with Ray Train and PyTorch", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "1. When to use Ray Train", "2. Single GPU Training with PyTorch Lightning", "3. Distributed Training with Ray Train and PyTorch Lightning", "4. Ray Train in Production", "Introduction to Ray Tune", "1. Loading the data", "2. Starting out with vanilla PyTorch", "3. Hyperparameter tuning with Ray Tune", "4. Ray Tune in Production", "Introduction to Ray Data: Industry Landscape", "The Compute Layer", "The Orchestration Layer", "Distributed Computing Frameworks", "Data Processing with Ray Data", "Ray Serve", "Introduction to Ray Data: Ray Data + Structured Data", "0. What is Ray Data?", "2. Loading Data", "3. Transforming Data", "4. Writing Data", "5. Data Operations: Shuffling, Grouping and Aggregation", "6. When to use Ray Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "1. When to Consider Ray Data", "2. How to work with Ray Data", "3. Loading data", "3. Lazy execution mode", "4. Transforming data", "5. Stateful transformations with Ray Actors", "6. Materializing data", "7. Data Operations: grouping, aggregation, and shuffling", "8. Persisting data", "9. Ray Data in production", "Introduction to Ray Serve with PyTorch", "1. When to Consider Ray Serve", "2. Overview of Ray Serve", "3. Implement an image classification service", "4. Development workflow", "Introduction to Ray: Developer", "Anyscale Administrator Overview", "Anyscale Administrator Overview", "1. What is an Anyscale Cloud?", "2. Cloud Deployment Types", "3. A Demonstrative Example of Resource Creation with AWS EC2", "3.1 IAM Role Definition", "4. Register Anyscale Cloud to Your Cloud Provider", "Deployment Options: Virtual Machines vs. Kubernetes", "Deployment Options: Virtual Machines vs. Kubernetes", "2. Virtual Machines (VM) vs. Kubernetes (K8s)", "3. (Optional) More Kubernetes Deployments Components", "Introduction: Deploy Anyscale Ray on AWS EC2 Instances", "Introduction: Deploy Anyscale Ray on AWS EC2 Instances", "1. Create Anyscale Resources with Terraform", "2. Register the Anyscale Cloud", "3. Test", "4. Cleanup", "5. Conclusion", "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)", "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)", "Prerequisites", "1. Installation", "2. Create Anyscale Resources with Terraform", "3. Register the Anyscale Cloud", "4. Test", "5. Cleanup", "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster", "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster", "1. Create Anyscale Resources with Terraform", "2. Install Kubernetes Components", "3. Register the Anyscale Cloud", "4. Install the Anyscale Operator", "5. Verify the Installation", "6. Test", "7. Clean up", "8. Conclusion", "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster", "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster", "Prerequisites", "1. Create Anyscale Resources with Terraform", "2. Attach Required IAM Policies to Your existing EKS\u2019s Node Role", "3. Install Kubernetes Components", "4. Register the Anyscale Cloud", "5. Install the Anyscale Operator", "6. Verify the Installation", "7. Test", "8. Troubleshooting", "9. Clean up", "10. Conclusion", "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster", "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster", "Prerequisites", "1. Installation", "2. Create Anyscale Resources with Terraform", "3. Troubleshooting GPU Availability", "4. kubectl Configuration", "5. Install NGINX Ingress Controller", "6. (Optional) Upgrade Anyscale Dependencies", "7. Register the Anyscale Cloud", "8. Install the Anyscale Operator", "8. Test", "9. Cleanup", "Welcome to Anyscale Administration", "Observability Introduction", "Ray and Anyscale Observability Introduction", "Ray and Anyscale Observability Introduction", "Ray Observability", "Anyscale Observability", "Example", "Ray and Anyscale Observability in Detail", "Ray and Anyscale Observability in Detail", "Data Pipeline Observability (Ray Data)", "Web Application Observability (Ray Serve)", "Multi-Actor Ray Serve Tracing Example", "Observability Introduction", "Observability Overview", "Setting Up Local Ray Observability", "101 \u2014 Introduction to Anyscale Workspaces", "101 \u2013 Developing Application with Anyscale", "101 \u2013 Compute Configs and Execution Environments in Anyscale", "101 \u2013 Storage Options in the Anyscale Platform", "101 \u2013 Debug and Monitor Your Anyscale Application", "101 \u2013 Introduction to Anyscale Jobs", "101 \u2013  Introduction to Anyscale Services", "101 \u2013 Collaboration on Anyscale", "101 - Anyscale Organization and Cloud Setup", "Content Used", "Sources", "Last Updated 6/19", "101 \u2014 Introduction to Anyscale Workspaces", "101 \u2013 Developing Application with Anyscale", "101 \u2013 Compute Configs and Execution Environments in Anyscale", "101 \u2013 Storage Options in the Anyscale Platform", "101 \u2013 Debug and Monitor Your Anyscale Application", "101 \u2013 Introduction to Anyscale Jobs", "101 \u2013  Introduction to Anyscale Services", "101 \u2013 Collaboration on Anyscale", "101 - Anyscale Organization and Cloud Setup", "\ud83d\udccc Overview of Structure", "\ud83e\udde0 Summary", "Content Used", "Part 1. Creating and Submitting your first job", "Part 2. Automation and Scheduling", "Sources", "Part 1: Starting your first Anyscale Service", "\ud83d\udcda 01 \u00b7 Introduction to Ray Train", "\ud83d\udcda 01 \u00b7 Introduction to Ray Train", "01 \u00b7 Imports", "04 \u00b7 Define ResNet-18 Model for MNIST", "05 \u00b7 Define the Ray Train Loop (DDP per-worker)", "06 \u00b7 Define <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop_config</span></code>", "07 \u00b7 Configure Scaling with <code class=\"docutils literal notranslate\"><span class=\"pre\">ScalingConfig</span></code>", "08 \u00b7 Wrap the Model with <code class=\"docutils literal notranslate\"><span class=\"pre\">prepare_model()</span></code>", "09 \u00b7 Build the DataLoader with <code class=\"docutils literal notranslate\"><span class=\"pre\">prepare_data_loader()</span></code>", "10 \u00b7 Report Training Metrics", "11 \u00b7 Save Checkpoints and Report Metrics", "14 \u00b7 Create the <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code>", "16 \u00b7 Inspect the Training Results", "18 \u00b7 Load a Checkpoint for Inference", "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data", "01 \u00b7 Define Training Loop with Ray Data", "02 \u00b7 Build DataLoader from Ray Data", "03 \u00b7 Prepare Dataset for Ray Data", "05 \u00b7 Define Image Transformation", "07 \u00b7 Configure <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code> with Ray Data", "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train", "01 \u00b7 Modify Training Loop to Enable Checkpoint Loading", "02 \u00b7 Save Full Checkpoint with Extra State", "04 \u00b7 Launch Fault-Tolerant Training", "05 \u00b7 Manual Restoration from Checkpoints", "07 \u00b7 Clean Up Cluster Storage", "\ud83c\udf89 Wrapping Up &amp; Next Steps", "Batch Inference with Ray Data", "Data Processing and ML examples with Ray", "Batch Inference with Ray Data", "Architecture", "Load a dataset", "Batch Inference Class", "Create a batch data and call the model", "Run inference on the entire dataset", "Data Processing with Ray Data", "Data Processing and ML examples with Ray", "Data Processing with Ray Data", "Library Imports", "Convert to Ray Dataset", "Filter Ray Dataset", "Join Two Ray Datasets", "Preprocessing with a Tokenizer", "Distributed training with Ray Train, PyTorch and Hugging Face", "Data Processing and ML examples with Ray", "Distributed training with Ray Train, PyTorch and Hugging Face", "1. Architecture", "3. Metrics Setup", "4. Training function per worker", "5. Main Training Function", "6. Start Training", "Online Model Serving with Ray Serve", "Data Processing and ML examples with Ray", "Online Model Serving with Ray Serve", "Architecture", "FastAPI webservice and deploy a model", "Simulate Client: Send test requests", "04-d1 Generative computer-vision pattern with Ray Train", "04-d1 Generative computer-vision pattern with Ray Train", "1. Imports and setup", "8. Pixel diffusion LightningModule", "9. Ray Train <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop</span></code> (Lightning + Ray integration)", "12. Resume from latest checkpoint", "13. Reverse diffusion sampler", "04-d2 Diffusion-Policy Pattern with Ray Train", "04-d2 Diffusion-Policy Pattern with Ray Train", "1. Imports and setup", "4. DiffusionPolicy LightningModule", "5. Distributed Train loop with checkpointing", "8. Reverse diffusion helper", "04e Recommendation system pattern with Ray Train", "04e Recommendation system pattern with Ray Train", "1. Imports", "7. Define matrix factorization model", "8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)", "11. Resume training from checkpoint", "12. Inference: recommend top-N items for a user", "04b Tabular workload pattern with Ray Train", "04b Tabular workload pattern with Ray Train", "1. Imports", "8. Define the Ray Train worker loop (Arrow-based, memory-efficient)", "12. Confusion matrix visualization", "15. Continue training from the latest checkpoint", "04c Time-Series workload pattern with Ray Train", "04c Time-Series workload pattern with Ray Train", "1. Imports", "9. PositionalEncoding and Transformer model", "10. Ray Train training loop (with teacher forcing)", "13. Resume training from checkpoint", "14. Inference helper \u2014 Ray Data batch predictor on GPU", "04a Computer-vision pattern with Ray Train", "04a Computer-vision pattern with Ray Train", "1. Imports", "6. Custom <code class=\"docutils literal notranslate\"><span class=\"pre\">Food101Dataset</span></code> for Parquet", "10. Helper: Ray-prepared DataLoaders", "11. <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop_per_worker</span></code>", "12. Launch distributed training with <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code>", "13. Plot training and validation loss curves", "14. Demonstrate fault-tolerant resumption", "15. Batch inference with Ray Data", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "What is LLM Serving?", "Key Concepts and Optimizations", "Challenges in LLM Serving", "Ray Serve LLM + Anyscale Architecture", "Getting Started with Ray Serve LLM", "Key Takeaways", "Deploy a Medium-Sized LLM with Ray Serve LLM", "Deploy a Medium-Sized LLM with Ray Serve LLM", "Overview: Why Medium-Sized Models?", "Setting up Ray Serve LLM", "Local Deployment &amp; Inference", "Deploying to Anyscale Services", "Advanced Topics: Monitoring &amp; Optimization", "Summary &amp; Outlook", "Advanced LLM Features with Ray Serve LLM", "Advanced LLM Features with Ray Serve LLM", "Overview: Advanced Features Preview", "Example: Deploying LoRA Adapters", "Example: Getting Structured JSON Output", "Example: Setting up Tool Calling", "How to Choose an LLM?", "Conclusion: Next Steps", "Introduction to the Ray AI Libraries", "Introduction to Ray Train", "Intro to Ray Tune", "Intro to Ray Data", "Intro to Ray Serve", "Introduction to the Ray AI Libraries", "1. Overview of the Ray AI Libraries", "2. End-to-end example: predicting taxi tips in New York", "3. Running an experiment with Ray AI libraries", "Introduction to Ray Train", "1. PyTorch introductory example (single GPU)", "2. Distributed Data Parallel Training with Ray Train and PyTorch (multiple GPUs)", "3. Overview of the training loop in Ray Train", "4. Migrating the model and dataset to Ray Train", "5. Reporting checkpoints and metrics", "6. Launching the distributed training job", "7. Accessing the training results", "8. Ray Train in Production", "Intro to Ray Tune", "1. Loading and visualizing data", "2. Setting up a PyTorch model", "3. Introduction to Ray Tune", "4. Diving deeper into Ray Tune concepts", "5. Hyperparameter tuning the PyTorch model using Ray Tune", "Intro to Ray Data", "1. When to use Ray Data", "2. Loading Data", "3. Transforming Data", "4. Data Operations: Grouping, Aggregation, and Shuffling", "5. Persisting Data", "Intro to Ray Serve", "1. Overview of Ray Serve", "2. Implement an Classifier service", "3. Advanced features of Ray Serve", "4. Ray Serve in Production", "Clean up", "Ray Enablement Content"], "titleterms": {"": [109, 113, 136, 149, 152, 164, 242, 243, 255, 256, 261, 262], "0": [1, 8, 13, 50, 73, 74, 178, 188], "01": [178, 179, 180, 191, 193, 199, 204], "02": [178, 180, 192, 194, 200, 204, 339], "02_service_hello_world": [160, 177], "03": [178, 180, 195, 198, 200, 204, 339], "04": [178, 181, 195, 201, 235, 236, 242, 243], "04a": [268, 269], "04b": [255, 256], "04c": [261, 262], "04e": [248, 249], "05": [178, 182, 196, 202], "06": [178, 183, 196, 202], "07": [178, 184, 197, 203], "08": [178, 185, 197], "09": [178, 186], "1": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 16, 18, 21, 22, 24, 27, 28, 30, 31, 32, 34, 35, 36, 39, 50, 57, 61, 62, 64, 68, 72, 73, 75, 78, 80, 81, 82, 84, 86, 91, 94, 95, 99, 101, 102, 109, 112, 114, 122, 125, 126, 136, 146, 149, 153, 155, 156, 159, 160, 161, 165, 167, 168, 174, 175, 177, 221, 224, 235, 237, 242, 243, 244, 248, 250, 255, 257, 261, 263, 268, 270, 278, 279, 282, 283, 284, 285, 287, 289, 293, 295, 301, 303, 304, 305, 306, 307, 309, 313, 322, 328, 334], "10": [4, 32, 109, 121, 178, 187, 235, 237, 239, 242, 247, 248, 252, 255, 258, 261, 265, 268, 270, 272], "100k": [248, 250], "101": [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 176, 235, 237, 268, 269, 270, 339], "11": [178, 188, 235, 239, 248, 253, 255, 258, 261, 265, 268, 273], "12": [178, 188, 235, 240, 248, 254, 255, 259, 261, 265, 268, 274], "13": [178, 188, 235, 241, 248, 254, 255, 259, 261, 266, 268, 275], "14": [178, 189, 235, 241, 248, 254, 255, 259, 261, 267, 268, 276], "15": [178, 189, 235, 241, 255, 260, 261, 267, 268, 277], "16": [178, 190, 255, 260, 261, 267, 268, 277], "17": [178, 190, 255, 260, 268, 277], "18": [178, 181, 191], "19": [161, 178, 191], "2": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 15, 16, 19, 22, 24, 28, 31, 32, 35, 36, 40, 51, 58, 59, 61, 62, 64, 69, 72, 73, 76, 78, 80, 82, 84, 87, 91, 94, 95, 99, 102, 109, 113, 114, 122, 125, 126, 136, 146, 149, 153, 155, 159, 160, 161, 165, 167, 174, 175, 177, 221, 224, 235, 237, 242, 243, 244, 248, 250, 255, 257, 261, 263, 268, 270, 278, 279, 282, 283, 284, 285, 287, 293, 295, 301, 303, 304, 305, 306, 307, 310, 314, 323, 329, 335], "20": [178, 191], "3": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 15, 20, 22, 28, 31, 32, 35, 36, 41, 52, 59, 60, 64, 70, 72, 73, 77, 78, 80, 82, 83, 84, 88, 91, 96, 99, 102, 103, 109, 114, 122, 127, 136, 149, 153, 159, 160, 161, 165, 174, 175, 177, 221, 225, 235, 237, 242, 243, 244, 248, 250, 255, 257, 261, 263, 268, 270, 278, 279, 282, 283, 284, 285, 287, 289, 293, 295, 301, 303, 304, 305, 306, 307, 310, 311, 315, 324, 330, 336], "30": [261, 263], "4": [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 16, 21, 28, 31, 32, 35, 36, 37, 42, 53, 61, 71, 72, 73, 78, 79, 80, 82, 83, 84, 89, 91, 97, 99, 102, 104, 109, 114, 115, 122, 128, 136, 149, 153, 159, 160, 165, 174, 177, 221, 226, 235, 237, 242, 245, 248, 250, 255, 257, 261, 263, 268, 270, 278, 279, 282, 283, 285, 287, 293, 295, 301, 304, 305, 306, 307, 316, 325, 331, 337], "5": [2, 3, 4, 5, 8, 9, 11, 22, 28, 31, 32, 36, 54, 62, 72, 73, 78, 84, 90, 91, 98, 99, 105, 109, 116, 122, 129, 159, 160, 174, 177, 221, 227, 235, 237, 242, 246, 248, 250, 255, 257, 261, 263, 268, 270, 287, 293, 304, 305, 306, 317, 326, 332], "6": [2, 3, 4, 5, 8, 9, 11, 23, 28, 31, 32, 36, 55, 63, 72, 73, 78, 99, 106, 109, 117, 122, 130, 159, 161, 174, 221, 228, 235, 237, 242, 246, 248, 250, 255, 257, 261, 263, 268, 271, 304, 318], "7": [2, 4, 5, 8, 9, 11, 24, 32, 36, 55, 64, 72, 73, 78, 99, 107, 109, 118, 122, 131, 159, 174, 221, 228, 235, 237, 242, 246, 248, 251, 255, 257, 261, 263, 268, 271, 304, 319], "70b": [287, 289], "8": [2, 4, 8, 9, 11, 25, 32, 55, 65, 72, 73, 78, 99, 108, 109, 119, 122, 132, 133, 221, 228, 235, 238, 242, 247, 248, 252, 255, 258, 261, 263, 265, 268, 271, 304, 320], "9": [4, 9, 32, 66, 109, 120, 122, 134, 235, 239, 242, 247, 248, 252, 255, 258, 261, 264, 268, 271], "A": [73, 77, 99, 100, 122, 123], "For": [159, 175], "In": [7, 11, 43, 72, 159, 160, 174, 177], "It": [1, 11, 16, 72], "On": [7, 9, 47, 61], "The": [7, 43, 44, 45, 155, 159, 167, 173, 174, 278, 279, 281], "These": [295, 297], "To": [80, 82], "about": [1, 2, 16, 21, 306, 329], "access": [4, 5, 32, 36, 80, 82, 304, 319], "accomplish": [287, 294, 295, 302], "action": [242, 243, 247], "activ": [4, 5, 11, 32, 36, 72, 304, 319], "actor": [2, 9, 17, 25, 62, 146, 178, 191, 306, 330], "ad": [0, 235, 236], "adapt": [295, 298], "add": [11, 72], "addit": [136, 149], "admin": 339, "administr": [73, 74, 135], "advanc": [2, 17, 287, 293, 295, 296, 297, 302, 307, 336], "after": [159, 174], "again": [159, 175], "aggreg": [8, 9, 54, 64, 306, 331], "ai": [3, 7, 26, 27, 44, 303, 308, 309, 310, 311], "alert": [142, 145], "align": [295, 301], "all": [1, 8, 9, 16, 54, 64, 159, 175, 261, 267], "alloc": [2, 22], "also": [159, 174], "altern": [278, 279, 282], "an": [2, 3, 10, 18, 26, 28, 70, 73, 75, 109, 110, 152, 159, 164, 174, 242, 247, 287, 292, 295, 301, 303, 307, 310, 311, 335], "annot": [305, 325], "anti": [1, 16], "anyscal": [73, 74, 75, 78, 79, 80, 81, 82, 84, 85, 86, 87, 91, 92, 95, 96, 99, 100, 101, 103, 104, 109, 110, 112, 115, 116, 122, 123, 126, 130, 131, 132, 135, 137, 138, 140, 142, 143, 145, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269, 278, 279, 284, 287, 292, 339], "apach": [7, 43], "api": [91, 94, 122, 125, 146], "appli": [178, 196], "applic": [7, 10, 46, 69, 142, 145, 151, 154, 161, 163, 166], "approach": [295, 297], "ar": [73, 76, 235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269], "architectur": [73, 78, 146, 205, 208, 221, 224, 229, 232, 278, 279, 284], "area": [304, 319], "argument": [2, 18, 159, 175], "arm": [11, 72], "arrow": [7, 43, 255, 258], "artifact": [261, 267], "assist": [295, 298, 300], "assumpt": [136, 149], "attach": [109, 113], "auroc": [304, 319], "authent": [91, 94, 122, 125], "auto": [160, 177], "autom": [155, 159, 167, 175], "automat": [178, 200], "autosc": [9, 62, 307, 336], "autoscal": [99, 102, 109, 114], "avail": [2, 22, 122, 127, 146, 159, 175], "aw": [73, 77, 84, 85, 99, 100, 102, 109, 110, 114], "awai": [178, 179, 192, 198, 235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269], "backend": [80, 82], "balanc": [99, 102, 109, 114, 255, 257], "base": [8, 9, 54, 64, 248, 249, 255, 258, 306, 331], "basic": [159, 174], "batch": [2, 3, 7, 8, 9, 24, 28, 46, 54, 64, 205, 207, 210, 211, 255, 257, 259, 261, 263, 267, 268, 271, 277, 278, 279, 282, 306, 331, 339], "below": [159, 160, 174, 177], "benchmark": [295, 301], "benefit": [160, 176, 295, 298, 299, 300], "best": [235, 241], "block": [8, 9, 54, 59, 64, 306, 331], "book": 0, "bound": [2, 22], "breakdown": [287, 290], "build": [0, 4, 31, 178, 186, 194, 255, 258, 304, 313], "cach": [278, 279, 282], "california": [255, 257], "call": [1, 16, 205, 211, 295, 300], "can": [159, 174, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277], "car": [295, 299], "case": [11, 72, 295, 301], "caus": [2, 24], "chain": [2, 19], "challeng": [7, 46, 278, 279, 283], "characterist": [156, 168], "check": [159, 160, 174, 175, 177, 235, 237, 261, 263, 268, 270], "checkout": [160, 177], "checkpoint": [4, 5, 31, 32, 36, 178, 188, 191, 199, 200, 202, 235, 240, 241, 242, 246, 248, 252, 253, 255, 260, 261, 266, 304, 313, 317], "choos": [287, 289, 295, 301], "class": [205, 210, 255, 257], "classif": [10, 70, 255, 256, 268, 269], "classifi": [307, 335], "clean": [3, 11, 28, 72, 99, 107, 109, 120, 154, 155, 156, 166, 167, 168, 178, 191, 203, 235, 241, 242, 247, 248, 254, 255, 260, 268, 277, 307, 338], "cleanup": [84, 89, 91, 98, 122, 134, 159, 174, 261, 267], "cli": [159, 161, 175], "client": [229, 234], "clone": [157, 160, 161, 169, 177], "cloud": [73, 75, 76, 79, 84, 87, 91, 94, 96, 99, 103, 109, 115, 122, 125, 131, 153, 158, 165, 170, 171], "cluster": [2, 22, 99, 100, 102, 109, 110, 114, 122, 123, 136, 149, 153, 165, 178, 203, 205, 212, 221, 228, 229, 234], "code": [3, 28, 159, 160, 174, 177, 295, 298], "collabor": [157, 161, 169], "command": [91, 95, 159, 160, 174, 177], "comparison": [287, 289], "compon": [80, 83, 99, 102, 109, 114, 287, 290], "compos": [307, 336], "comput": [7, 11, 44, 46, 72, 91, 92, 152, 164, 235, 236, 268, 269, 304, 319], "concept": [4, 5, 6, 32, 36, 41, 278, 279, 282, 305, 325], "conclus": [84, 90, 99, 108, 109, 121, 295, 302], "concurr": [9, 61, 287, 293], "conda": [11, 72], "config": [152, 164], "configur": [2, 4, 5, 22, 32, 36, 91, 94, 122, 125, 128, 142, 145, 146, 152, 160, 164, 177, 178, 184, 188, 197, 200, 255, 258, 278, 279, 285, 287, 290, 292, 295, 298, 304, 315, 318], "confus": [255, 259], "consid": [9, 10, 57, 68], "consider": [278, 279, 282, 295, 301], "consol": [159, 174], "contain": [152, 160, 164, 177], "content": [0, 159, 173, 339], "context": [278, 279, 282, 295, 301], "continu": [255, 260, 278, 279, 282], "control": [73, 78, 80, 82, 99, 102, 109, 114, 122, 129], "convert": [213, 217, 220], "core": [1, 2, 7, 12, 17, 47], "cost": [278, 279, 283, 295, 301], "count": [261, 263], "cours": [0, 11, 72, 178, 204], "cover": [155, 156, 167, 168, 255, 256, 257, 295, 297], "cpu": [255, 259], "creat": [1, 4, 5, 11, 14, 31, 35, 36, 72, 73, 78, 84, 86, 91, 95, 99, 101, 109, 112, 122, 126, 152, 155, 159, 164, 167, 174, 175, 178, 189, 205, 211, 213, 217, 248, 250], "creation": [73, 77], "cursor": [151, 163], "curv": [235, 239, 248, 252, 268, 275, 304, 319], "custom": [0, 8, 9, 54, 64, 73, 78, 268, 271, 306, 307, 331, 336], "cv": 339, "d1": [235, 236], "d2": [242, 243], "dashboard": [142, 144, 154, 166], "data": [2, 3, 4, 5, 6, 7, 8, 9, 19, 24, 26, 28, 32, 36, 39, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 63, 64, 65, 66, 80, 82, 142, 144, 178, 179, 188, 192, 193, 194, 195, 196, 197, 204, 205, 206, 207, 211, 213, 214, 215, 222, 230, 235, 237, 248, 250, 255, 259, 261, 267, 268, 277, 304, 305, 306, 313, 314, 322, 327, 328, 329, 330, 331, 332, 339], "databas": [7, 43], "datafram": [178, 190, 213, 220], "dataload": [4, 5, 31, 35, 178, 186, 194, 221, 226, 261, 263, 268, 271, 272], "dataset": [4, 8, 31, 32, 51, 178, 180, 195, 205, 209, 212, 213, 216, 217, 218, 219, 242, 243, 244, 248, 250, 255, 257, 261, 263, 303, 304, 306, 310, 316, 329], "ddp": [178, 179, 182], "de": [235, 236], "debug": [154, 166], "decod": [235, 237, 278, 279, 281], "deeper": [6, 41, 305, 325], "default": [160, 177, 305, 325], "defin": [5, 35, 73, 76, 178, 181, 182, 183, 193, 196, 248, 251, 252, 255, 258, 304, 313], "definit": [73, 78], "demand": [261, 262, 278, 279, 283], "demonstr": [73, 77, 268, 276], "depend": [2, 11, 21, 72, 80, 82, 122, 130], "deploi": [80, 81, 83, 84, 85, 91, 92, 99, 100, 109, 110, 122, 123, 126, 160, 161, 176, 177, 205, 211, 229, 233, 287, 288, 292, 295, 298], "deploy": [10, 69, 73, 76, 80, 81, 83, 135, 146, 229, 233, 278, 279, 285, 287, 291, 307, 334, 336], "descript": [295, 299], "detail": [142, 143], "develop": [10, 11, 71, 72, 146, 151, 160, 161, 163, 177, 339], "devic": [99, 102, 109, 114], "diagnost": [255, 259], "differ": [287, 294], "diffus": [5, 35, 235, 236, 238, 241, 242, 243, 247], "diffusionpolici": [242, 245], "digit": [178, 180], "directori": [304, 313], "disabl": 0, "displai": [159, 174, 235, 241], "distribut": [3, 4, 5, 7, 28, 32, 36, 46, 178, 179, 188, 206, 214, 221, 222, 223, 230, 235, 236, 239, 242, 246, 248, 249, 252, 255, 256, 258, 261, 262, 267, 268, 269, 274, 304, 314, 318, 339], "dive": [6, 41, 305, 325], "document": [159, 175], "doe": [268, 269], "domain": [295, 301], "down": [11, 72, 287, 291, 292], "download": [160, 177, 178, 180], "dual": [73, 78], "duplic": [157, 169], "each": [80, 82], "ec2": [73, 77, 84, 85], "editor": [159, 174], "ef": [73, 78], "effici": [255, 258], "ek": [99, 100, 109, 110, 113], "els": [160, 177], "embed": [248, 249], "enabl": [0, 91, 94, 122, 125, 178, 199, 287, 293, 339], "encod": [235, 237, 248, 250, 268, 270], "end": [3, 28, 178, 191, 303, 310], "endpoint": [146, 160, 177], "engin": [7, 44, 91, 92, 122, 123, 278, 279, 284], "ensembl": [3, 28], "enter": [160, 177], "entir": [205, 212], "environ": [2, 11, 21, 72, 152, 160, 164, 177, 242, 243], "error": [205, 212], "evalu": [255, 258], "everyth": [160, 177], "exampl": [0, 3, 11, 26, 28, 72, 73, 77, 80, 83, 135, 137, 141, 146, 159, 160, 174, 177, 206, 214, 222, 230, 287, 289, 294, 295, 298, 299, 300, 303, 304, 310, 313], "execut": [0, 1, 8, 9, 15, 52, 60, 80, 82, 152, 159, 164, 174, 306, 330], "exercis": [6, 41, 305, 325], "exist": [73, 78, 109, 110, 113], "expect": [295, 299], "experi": [303, 305, 310, 311, 325], "explor": [154, 166], "extern": [80, 82], "extra": [178, 200], "face": [221, 223], "factor": [248, 249, 251], "failur": [2, 24], "failureconfig": [178, 200], "fastapi": [229, 231, 233, 307, 336], "fault": [178, 198, 201, 204, 268, 276], "featur": [0, 8, 55, 159, 173, 255, 259, 295, 296, 297, 303, 307, 310, 334, 336], "fetch": [2, 24], "file": [8, 9, 54, 64, 153, 159, 160, 165, 174, 177, 255, 257, 287, 292, 306, 331], "filter": [213, 218], "find": [159, 174], "first": [155, 156, 159, 160, 167, 168, 173, 174, 177], "fit": [5, 36, 178, 189], "flask": [229, 231], "flow": [7, 48, 146], "folder": [159, 175], "follow": [155, 156, 159, 160, 167, 168, 173, 174, 177], "food": [235, 237, 268, 269, 270], "food101dataset": [268, 271], "forc": [261, 265], "forecast": [261, 262], "forest": [255, 256], "format": [7, 43], "forward": [235, 236], "foundat": [278, 279, 280, 339], "fraction": [2, 22, 307, 336], "framework": [7, 46, 295, 301], "from": [159, 174, 178, 194, 202, 235, 240, 241, 242, 247, 248, 250, 253, 255, 260, 261, 266], "full": [159, 175, 178, 200], "function": [1, 7, 14, 15, 44, 73, 75, 221, 226, 227], "gce": [91, 92], "gcp": [91, 92], "gener": [0, 4, 5, 7, 31, 32, 36, 46, 235, 236, 241, 242, 244, 278, 279, 281, 339], "get": [1, 2, 6, 12, 15, 16, 24, 41, 146, 161, 278, 279, 285, 295, 299, 305, 324], "gettingstart": 339, "github": [160, 177], "give": [160, 177], "gke": [122, 123], "global": [8, 9, 54, 64, 306, 331], "go": [178, 204], "googl": [91, 94, 122, 123, 125], "gpu": [4, 5, 31, 32, 35, 36, 122, 127, 261, 265, 267, 304, 307, 313, 314, 315, 336], "grafana": [136, 149], "group": [8, 9, 54, 64, 73, 78, 306, 331], "groupbi": [8, 9, 54, 64, 306, 331], "guid": [160, 176], "handl": [159, 174], "hardwar": [287, 293, 295, 301], "harm": [1, 16], "head": [136, 149, 159, 160, 174, 177], "hello_world": [159, 174], "helper": [242, 247, 261, 267, 268, 272], "hourli": [261, 263], "how": [0, 8, 9, 50, 58, 73, 76, 80, 82, 178, 179, 235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269, 287, 294, 295, 301, 303, 310], "hug": [221, 223], "huggingfac": [206, 214, 222, 230], "hyperparamet": [3, 6, 28, 41, 305, 326], "i": [7, 8, 47, 48, 50, 73, 75, 159, 174, 229, 231, 278, 279, 281, 287, 292], "iam": [73, 78, 109, 113], "id": [1, 16, 73, 78, 151, 163, 248, 250, 254], "imag": [10, 70, 152, 164, 178, 196, 235, 236, 237, 268, 269, 270, 271], "implement": [3, 10, 26, 70, 307, 335], "import": [178, 180, 205, 208, 213, 216, 221, 224, 229, 232, 235, 237, 242, 244, 248, 250, 255, 257, 259, 261, 263, 268, 270], "improv": [287, 293], "industri": [7, 43], "infer": [3, 28, 178, 191, 205, 207, 210, 212, 248, 249, 254, 255, 259, 260, 261, 267, 268, 277, 278, 279, 281, 284, 287, 291, 292, 339], "infrastructur": [73, 76, 80, 83, 122, 126, 278, 279, 284], "ingress": [99, 102, 109, 114, 122, 129], "initi": [213, 216], "input": [235, 236, 248, 249, 268, 269], "inspect": [160, 177, 178, 190, 255, 257, 261, 263, 268, 271, 303, 310], "instal": [0, 11, 72, 91, 94, 99, 102, 104, 105, 109, 114, 116, 117, 122, 125, 129, 132, 136, 146, 149, 161, 206, 214, 222, 230], "instanc": [73, 78, 84, 85, 91, 92, 229, 234], "instruct": [155, 159, 167, 173], "integr": [178, 192, 204, 235, 239, 307, 336], "intro": [6, 9, 41, 56, 305, 306, 307, 321, 327, 333, 339], "introduct": [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 17, 26, 29, 33, 38, 43, 49, 67, 72, 84, 85, 91, 92, 99, 100, 109, 110, 122, 123, 136, 137, 138, 147, 150, 151, 152, 153, 154, 155, 156, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 173, 176, 178, 179, 191, 204, 278, 279, 280, 303, 304, 305, 308, 312, 324], "introductori": [304, 313], "invert": [242, 243], "io": [2, 22], "irvin": [255, 257], "item": [248, 249, 250, 254], "job": [4, 32, 155, 159, 161, 167, 173, 174, 175, 304, 318], "join": [213, 219, 248, 254], "json": [295, 299], "jupyt": [0, 11, 72], "just": [229, 231], "jvm": [7, 46], "k8": [80, 82], "kei": [4, 5, 32, 36, 73, 75, 156, 159, 168, 173, 278, 279, 282, 286, 287, 290, 294, 295, 298, 299, 300, 302, 304, 307, 315, 334], "kubectl": [122, 128], "kubernet": [80, 81, 82, 83, 99, 102, 109, 114, 122, 123, 278, 279, 283], "kv": [278, 279, 282], "label": [268, 269], "lake": [7, 43], "lakehous": [7, 43], "landscap": [7, 43], "languag": [278, 279, 280], "larg": [2, 22, 278, 279, 280], "last": [161, 178, 202], "latenc": [278, 279, 283], "latest": [235, 240, 255, 260], "launch": [4, 11, 32, 72, 136, 142, 145, 149, 150, 162, 178, 189, 197, 201, 235, 239, 242, 246, 248, 252, 261, 265, 268, 274, 287, 291, 292, 304, 318], "layer": [7, 43, 44, 45, 80, 82], "lazi": [9, 60], "learn": [7, 44, 135, 161, 178, 179, 192, 198, 235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269, 295, 297, 298, 299, 300, 339], "leav": [160, 177], "legend": [11, 72], "level": [2, 18], "librari": [3, 26, 27, 205, 208, 213, 216, 221, 224, 229, 232, 303, 308, 309, 310, 311], "lifecycl": [4, 32, 160, 176, 178, 188], "lightn": [5, 33, 35, 36, 235, 239], "lightningmodul": [235, 238, 242, 245], "limit": [9, 61], "list": [159, 175], "lite": [268, 269], "ll": [135, 178, 179, 192, 198, 295, 297], "llama": [287, 289], "llm": [278, 279, 280, 281, 283, 284, 285, 287, 288, 290, 293, 295, 296, 298, 301, 339], "load": [4, 5, 6, 8, 9, 31, 36, 39, 51, 59, 99, 102, 109, 114, 178, 191, 195, 199, 205, 209, 213, 216, 235, 237, 248, 250, 255, 257, 261, 263, 268, 270, 304, 305, 306, 313, 322, 329], "loader": [304, 313], "local": [0, 11, 72, 136, 146, 149, 151, 153, 163, 165, 287, 291, 304, 313], "log": [142, 144, 145, 154, 159, 166, 174], "loop": [1, 4, 5, 16, 31, 32, 35, 178, 182, 193, 199, 242, 246, 248, 252, 255, 258, 261, 265, 304, 313, 315, 319], "lora": [295, 298], "loss": [235, 239, 242, 246, 248, 252, 261, 265, 268, 275], "mac": [11, 72], "machin": [7, 44, 80, 81, 82], "main": [159, 160, 174, 177, 221, 227], "manag": [2, 11, 22, 72, 80, 82, 160, 176, 248, 252, 278, 279, 283], "mani": [2, 24], "manual": [178, 202], "materi": [9, 63, 306, 330], "matrix": [248, 249, 251, 255, 259], "matter": [295, 297, 299, 300], "max_model_len": [287, 293], "medium": [287, 288, 289, 290], "memori": [7, 43, 205, 212, 255, 258, 278, 279, 283], "memorydb": [73, 78], "metric": [4, 31, 32, 142, 144, 145, 154, 166, 178, 187, 188, 190, 221, 225, 248, 252, 304, 313, 317], "migrat": [4, 5, 32, 36, 235, 236, 248, 249, 255, 256, 261, 262, 268, 269, 304, 316], "min": [261, 263], "mini": [255, 257], "miniforg": [11, 72], "ml": [206, 214, 222, 230], "mnist": [178, 180, 181, 304, 313], "mode": [8, 9, 52, 60, 306, 330], "model": [3, 4, 5, 6, 26, 28, 31, 32, 35, 36, 41, 178, 181, 185, 205, 211, 229, 231, 233, 248, 249, 251, 255, 258, 261, 264, 268, 269, 278, 279, 280, 282, 287, 289, 290, 293, 295, 301, 304, 305, 313, 316, 323, 326], "modifi": [178, 199], "modul": [178, 191, 204], "monitor": [154, 166, 287, 293], "more": [4, 5, 32, 36, 80, 83, 287, 293, 295, 298, 299, 300, 302, 306, 329], "movi": [248, 254], "movielen": [248, 250], "multi": [146, 261, 262], "multipl": [304, 314], "n": [248, 254], "name": [159, 160, 174, 177], "navig": [0, 159, 174], "need": [80, 82, 160, 177], "nest": [2, 23], "new": [0, 11, 72, 73, 78, 99, 100, 122, 123, 146, 159, 174, 235, 236, 303, 310], "next": [136, 149, 159, 174, 178, 204, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277, 278, 279, 286, 287, 294, 295, 302], "nginx": [99, 102, 109, 114, 122, 129], "node": [109, 113, 136, 149, 152, 160, 164, 177, 261, 262], "nois": [235, 236], "normal": [242, 244, 261, 263], "note": [1, 2, 4, 5, 9, 16, 21, 22, 24, 32, 36, 59, 62, 178, 188], "notebook": [0, 11, 72, 151, 155, 156, 159, 163, 167, 168, 175, 213, 215, 303, 308], "now": [136, 149, 159, 174], "nvidia": [99, 102, 109, 114], "nyc": [261, 262, 263, 303, 310], "o": [11, 72], "object": [2, 17, 18, 24, 80, 82, 153, 161, 165, 235, 236, 242, 243, 248, 249], "observ": [136, 137, 138, 139, 140, 142, 143, 144, 145, 147, 148, 149, 339], "onc": [2, 24, 159, 174], "one": [261, 263], "onli": [142, 145, 178, 188], "onlin": [229, 231, 339], "open": [159, 174], "oper": [8, 9, 54, 64, 80, 81, 99, 104, 109, 116, 122, 132, 306, 331], "optim": [278, 279, 282, 283, 287, 293], "option": [11, 72, 73, 78, 80, 81, 83, 99, 102, 109, 114, 122, 130, 136, 149, 153, 161, 165, 255, 257], "orchestr": [7, 45, 278, 279, 284], "order": [8, 9, 54, 64, 306, 331], "organ": [158, 170, 171], "other": [287, 294], "our": [159, 174, 287, 289, 304, 313], "out": [6, 40, 159, 174, 175, 205, 212], "outlin": [205, 207, 213, 215, 221, 223, 229, 231], "outlook": [73, 74, 80, 83, 287, 294, 306, 332], "output": [0, 295, 299], "over": [7, 47, 159, 160, 174, 177, 261, 263], "overview": [1, 3, 4, 5, 10, 11, 13, 27, 31, 32, 35, 69, 72, 73, 74, 136, 146, 148, 158, 161, 171, 287, 289, 295, 297, 303, 304, 307, 309, 315, 334], "packag": [136, 149], "panda": [213, 220], "parallel": [1, 4, 5, 16, 32, 36, 178, 179, 188, 278, 279, 282, 287, 293, 304, 314], "parquet": [235, 237, 248, 250, 255, 257, 261, 263, 268, 270, 271], "part": [155, 156, 159, 160, 167, 168, 174, 175, 177], "pass": [2, 18, 19, 160, 177], "passeng": [261, 263], "past": [159, 174], "path": 161, "pattern": [1, 2, 16, 18, 24, 235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269, 339], "pendulum": [242, 243, 244], "per": [178, 182, 221, 226, 248, 249], "persist": [9, 65, 178, 188, 235, 237, 268, 270, 304, 306, 318, 332], "phase": [278, 279, 281], "pip": [2, 21], "pipelin": [2, 24, 142, 144, 161, 287, 293], "pixel": [235, 238], "plane": [73, 78], "platform": [153, 165], "plot": [235, 239, 242, 246, 248, 252, 261, 265, 268, 275], "plugin": [99, 102, 109, 114], "point": [248, 250, 304, 315], "polici": [109, 113, 235, 236, 242, 243, 247, 339], "positionalencod": [261, 264], "post": [255, 260], "practic": [295, 301], "predict": [4, 5, 31, 32, 36, 178, 191, 303, 310], "predictor": [261, 267], "prefil": [278, 279, 281], "prepar": [178, 195, 261, 263, 268, 272], "prepare_data_load": [178, 186], "prepare_model": [178, 185], "preprocess": [213, 220], "prerequisit": [11, 72, 84, 85, 91, 93, 99, 100, 109, 111, 122, 124, 135, 136, 142, 143, 146, 149, 287, 291], "preview": [295, 297], "problem": [235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269], "process": [2, 7, 24, 46, 47, 206, 213, 214, 215, 222, 230, 235, 236, 278, 279, 281, 295, 301, 339], "product": [4, 5, 6, 8, 9, 32, 37, 42, 55, 66, 304, 306, 307, 320, 332, 337], "profil": 146, "project": [157, 158, 169, 171], "prometheu": [136, 149], "provid": [73, 79, 99, 100], "publish": [0, 160, 177], "purpos": [7, 46, 73, 75], "put": [1, 16], "py": [159, 160, 174, 177], "python": [159, 174, 175], "pytorch": [4, 5, 6, 10, 29, 31, 32, 33, 35, 36, 40, 41, 67, 206, 214, 221, 222, 223, 230, 261, 263, 304, 305, 313, 314, 323, 326], "qualiti": [295, 301], "quantiz": [287, 293], "queri": [278, 279, 285], "quick": [3, 28, 261, 263], "rai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 16, 17, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 36, 37, 38, 41, 42, 43, 47, 48, 49, 50, 55, 56, 57, 58, 62, 66, 67, 68, 69, 72, 80, 82, 84, 85, 91, 92, 99, 100, 109, 110, 122, 123, 136, 137, 138, 139, 142, 143, 144, 145, 146, 149, 154, 159, 161, 166, 174, 178, 179, 182, 188, 191, 192, 193, 194, 195, 196, 197, 198, 204, 205, 206, 207, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 228, 229, 230, 231, 234, 235, 236, 237, 239, 242, 243, 246, 248, 249, 250, 252, 255, 256, 257, 258, 259, 261, 262, 263, 265, 267, 268, 269, 272, 277, 278, 279, 280, 284, 285, 287, 288, 290, 291, 295, 296, 298, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314, 315, 316, 320, 321, 324, 325, 326, 327, 328, 332, 333, 334, 336, 337, 339], "random": [261, 263], "rank": [178, 188, 248, 249], "rate": [248, 249, 250], "read": [8, 9, 54, 64, 306, 331], "real": [242, 244], "rec": 339, "recap": [303, 310, 311], "recommend": [11, 72, 161, 248, 249, 254, 295, 301], "reduc": [287, 293], "regist": [73, 79, 84, 87, 91, 96, 99, 103, 109, 115, 122, 131, 146], "registr": 146, "regress": [3, 26], "relat": [287, 289, 294], "remot": [1, 4, 14, 15, 32], "remov": [261, 267], "replica": [10, 69, 287, 293], "report": [4, 32, 178, 187, 188, 304, 313, 317], "repositori": [160, 161, 177], "request": [2, 22, 146, 229, 234, 287, 291], "requir": [11, 72, 91, 94, 99, 100, 109, 113, 122, 125, 136, 149, 278, 279, 283, 295, 301], "resampl": [261, 263], "resiz": [235, 237, 268, 270], "resnet": [178, 181], "resourc": [2, 9, 22, 61, 62, 73, 76, 77, 84, 86, 91, 95, 99, 101, 109, 112, 122, 126, 157, 169, 278, 279, 286, 287, 294, 295, 302], "restor": [178, 202], "result": [1, 2, 4, 5, 15, 24, 32, 36, 178, 190, 261, 267, 304, 319], "resum": [178, 202, 235, 240, 248, 253, 261, 266], "resumpt": [268, 276], "retri": [2, 20, 178, 200], "retriev": 146, "revers": [235, 236, 241, 242, 243, 247], "roc": [304, 319], "role": [73, 78, 109, 113, 158, 171], "row": [8, 9, 54, 64, 306, 331], "run": [4, 5, 31, 32, 36, 80, 82, 91, 95, 142, 144, 155, 159, 160, 167, 173, 174, 177, 178, 191, 205, 212, 261, 267, 268, 277, 287, 292, 303, 308, 310, 311], "runconfig": [178, 188], "runtim": [2, 21], "s3": [73, 78], "same": [159, 175], "sampl": [146, 178, 180, 235, 236, 241, 242, 243, 247], "sampler": [235, 241], "saniti": [235, 237, 261, 263, 268, 270], "save": [178, 188, 200, 304, 313], "scalabl": [278, 279, 283], "scale": [4, 5, 32, 36, 178, 184, 205, 211, 229, 233, 242, 243, 287, 293, 304, 315], "scalingconfig": [178, 184], "schedul": [155, 159, 167, 175, 304, 313], "script": [159, 174], "sdk": [159, 175], "second": [213, 217], "secur": [73, 78], "select": [160, 177, 295, 301], "send": [229, 234, 287, 291], "sequenc": [261, 262], "seri": [261, 262, 339], "serv": [0, 3, 7, 10, 26, 28, 48, 67, 68, 69, 142, 145, 146, 229, 231, 234, 278, 279, 280, 281, 283, 284, 285, 287, 288, 290, 291, 295, 296, 298, 307, 333, 334, 336, 337, 339], "server": [11, 72], "servic": [10, 70, 156, 159, 160, 161, 168, 173, 176, 177, 287, 292, 307, 335], "set": [11, 72, 136, 149, 287, 290, 292, 295, 300, 305, 323, 325], "setup": [136, 146, 149, 158, 170, 221, 225, 235, 236, 237, 242, 244, 248, 249, 255, 256, 261, 262, 268, 269], "share": [153, 165, 235, 241, 248, 254], "shuffl": [8, 9, 54, 64, 235, 237, 306, 331], "shut": [11, 72, 287, 291, 292], "shutdown": [205, 212, 213, 220, 221, 228, 229, 234, 279, 285], "sign": 161, "simpl": [11, 72, 142, 144], "simul": [229, 234], "singl": [4, 5, 31, 35, 304, 313], "size": [255, 257, 287, 288, 289, 290, 294], "slide": [261, 263], "solv": [235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269], "sourc": [160, 176], "spark": [7, 47], "specif": [1, 9, 16, 61, 62, 159, 174], "spin": [160, 177], "split": [0, 235, 237, 242, 244, 248, 250, 255, 257, 268, 271], "stabl": [5, 35], "start": [1, 6, 12, 40, 41, 136, 149, 156, 160, 161, 168, 177, 221, 228, 255, 258, 278, 279, 285, 305, 324], "starter": [160, 177], "state": [9, 62, 178, 200, 242, 243, 306, 330], "statu": [159, 174], "step": [136, 149, 178, 204, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277, 278, 279, 285, 286, 287, 294, 295, 302, 303, 310, 311], "storag": [4, 32, 80, 82, 153, 165, 178, 188, 203, 235, 241, 248, 254, 304, 318], "store": [2, 17, 18, 80, 82, 153, 165], "strategi": [287, 293], "stream": [7, 46], "structur": [7, 8, 43, 49, 135, 146, 158, 171, 295, 299], "style": [242, 243], "submit": [155, 159, 167, 174, 175], "subnet": [73, 78], "successfulli": [159, 174], "summari": [73, 78, 158, 172, 205, 212, 213, 220, 221, 228, 229, 234, 287, 294], "support": [73, 76], "sy": 339, "system": [248, 249], "tab": [154, 159, 160, 166, 174, 177], "tabl": [305, 325], "tabular": [255, 256, 339], "take": [178, 179, 192, 198, 235, 236, 241, 242, 243, 247, 248, 249, 254, 255, 256, 260, 261, 262, 267, 268, 269, 277], "takeawai": [278, 279, 286, 287, 294, 295, 302], "task": [2, 17, 19, 20, 21, 22, 23, 295, 301, 303, 310], "taxi": [261, 262, 263, 303, 310], "teacher": [261, 265], "team": 161, "templat": [160, 177, 287, 294], "tensor": [235, 236], "termin": [136, 149, 159, 160, 174, 177], "terraform": [84, 86, 91, 95, 99, 101, 109, 112, 122, 126], "test": [0, 84, 88, 91, 97, 99, 106, 109, 118, 122, 133, 152, 164, 229, 234], "text": [278, 279, 281], "tfvar": [91, 95, 122, 126], "thi": [11, 72, 155, 156, 160, 167, 168, 176, 177, 235, 236, 241, 242, 243, 247, 248, 249, 254, 255, 256, 260, 261, 262, 267, 268, 269, 277, 303, 308, 310], "through": [155, 159, 160, 167, 173, 176], "time": [261, 262, 339], "tip": [303, 310], "titl": [248, 254], "togeth": [1, 16], "token": [213, 220, 221, 226], "toler": [178, 198, 201, 204, 268, 276], "too": [2, 24], "tool": [295, 300], "top": [2, 18, 248, 254], "topic": [287, 293, 295, 302], "torch": [5, 35], "torchtrain": [5, 36, 178, 189, 197, 235, 239, 242, 246, 268, 274, 304, 318], "trace": [142, 145, 146], "track": [159, 174], "train": [3, 4, 5, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 178, 179, 182, 187, 188, 189, 190, 191, 192, 193, 197, 198, 199, 201, 202, 204, 206, 214, 221, 222, 223, 226, 227, 228, 230, 235, 236, 237, 239, 242, 243, 246, 247, 248, 249, 250, 252, 253, 255, 256, 257, 258, 260, 261, 262, 265, 266, 267, 268, 269, 271, 274, 275, 304, 312, 313, 314, 315, 316, 318, 319, 320, 339], "train_loop": [235, 239], "train_loop_config": [178, 183], "train_loop_per_work": [268, 273], "trainer": [178, 189, 255, 258], "transform": [8, 9, 52, 61, 62, 178, 196, 261, 262, 264, 268, 271, 306, 330], "tripl": [248, 249], "troubleshoot": [109, 119, 122, 127], "tune": [3, 6, 26, 28, 38, 41, 42, 305, 321, 324, 325, 326], "tupl": [242, 243], "tutori": [178, 204], "two": [213, 219, 278, 279, 281], "type": [73, 76, 255, 257, 295, 299], "uci": [255, 257], "under": [304, 319], "univers": [255, 257], "unstructur": [9, 56], "up": [3, 11, 28, 72, 99, 107, 109, 120, 136, 149, 154, 155, 156, 160, 161, 166, 167, 168, 177, 178, 191, 203, 204, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277, 287, 290, 292, 295, 300, 305, 307, 323, 338], "upcom": [8, 55], "updat": [160, 161, 176, 304, 319], "upgrad": [122, 130, 287, 293], "uri": [248, 250], "url": [159, 174], "us": [3, 4, 5, 6, 7, 8, 9, 11, 26, 30, 31, 32, 34, 41, 47, 50, 54, 55, 64, 72, 73, 78, 80, 82, 142, 145, 159, 160, 173, 175, 176, 177, 178, 179, 229, 231, 235, 236, 242, 243, 248, 249, 250, 255, 256, 261, 262, 268, 269, 287, 293, 295, 298, 299, 300, 301, 303, 305, 306, 307, 310, 326, 328, 331, 334, 336], "usag": 0, "user": [146, 158, 171, 248, 249, 250, 254], "uv": [11, 72], "v": [7, 47, 48, 80, 81, 82, 159, 174], "val": [235, 237, 242, 246], "valid": [248, 250, 252, 255, 257, 261, 265, 268, 271, 275], "valu": [278, 279, 282], "vanilla": [3, 6, 28, 40], "verifi": [11, 72, 99, 105, 109, 117, 159, 174, 255, 260], "view": [178, 190], "viewer": [154, 166], "virtual": [80, 81, 82], "vision": [235, 236, 268, 269, 339], "visual": [178, 180, 191, 235, 237, 248, 250, 255, 257, 259, 261, 263, 267, 268, 270, 277, 304, 305, 313, 322], "vllm": [278, 279, 284], "vm": [80, 82], "vpc": [73, 78], "vscode": [151, 160, 163, 177], "wa": [159, 174], "wait": [2, 24], "walk": [155, 159, 160, 167, 173, 176], "warehous": [7, 43], "we": [11, 72, 159, 160, 174, 177, 287, 294, 295, 297, 302, 303, 310], "weather": [295, 300], "web": [142, 145], "webservic": [229, 233], "welcom": [11, 72, 135], "what": [7, 8, 47, 48, 50, 73, 75, 80, 82, 135, 136, 149, 178, 179, 192, 198, 229, 231, 235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269, 278, 279, 281, 287, 292, 294, 295, 297, 302], "when": [4, 5, 7, 8, 9, 10, 30, 34, 47, 55, 57, 68, 80, 82, 178, 179, 306, 307, 328, 334], "where": [178, 204, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277], "which": [80, 82], "why": [7, 11, 47, 48, 72, 161, 229, 231, 235, 236, 278, 279, 283, 287, 289, 295, 297, 298, 299, 300], "window": [261, 263, 278, 279, 282, 295, 301], "wl": 339, "work": [0, 9, 58, 178, 179, 235, 236], "worker": [4, 5, 32, 36, 136, 149, 152, 160, 164, 177, 178, 182, 221, 226, 255, 258], "workflow": [0, 10, 71, 152, 159, 164, 174], "workload": [80, 82, 142, 144, 158, 171, 235, 236, 242, 243, 248, 249, 255, 256, 261, 262, 268, 269], "workspac": [150, 159, 160, 161, 162, 174, 175, 177], "wrap": [178, 185, 204, 235, 241, 242, 247, 248, 254, 255, 260, 261, 267, 268, 277], "write": [8, 53, 255, 257], "xgboost": [3, 26, 28, 255, 256, 258], "yaml": [160, 177], "york": [303, 310], "you": [135, 155, 159, 160, 167, 173, 174, 176, 178, 179, 192, 198, 235, 236, 241, 242, 243, 247, 248, 249, 254, 255, 256, 260, 261, 262, 267, 268, 269, 277], "your": [11, 72, 73, 79, 80, 83, 109, 113, 154, 155, 156, 159, 160, 161, 166, 167, 168, 173, 174, 177]}})