Search.setIndex({"alltitles": {"0. Overview": [[2, "overview"], [17, "overview"], [18, "overview"], [153, "overview"], [155, null]], "0. What is Ray Data?": [[9, "what-is-ray-data"], [176, "what-is-ray-data"], [178, null]], "01 \u00b7 Define Training Loop with Ray Data": [[202, "define-training-loop-with-ray-data"], [217, null]], "01 \u00b7 Imports": [[202, "imports"], [204, null]], "01 \u00b7 Modify Training Loop to Enable Checkpoint Loading": [[202, "modify-training-loop-to-enable-checkpoint-loading"], [223, null]], "02 \u00b7 Build DataLoader from Ray Data": [[202, "build-dataloader-from-ray-data"], [218, null]], "02 \u00b7 Download MNIST Dataset": [[202, "download-mnist-dataset"], [204, "download-mnist-dataset"]], "02 \u00b7 Save Full Checkpoint with Extra State": [[202, "save-full-checkpoint-with-extra-state"], [224, null]], "03 \u00b7 Configure Automatic Retries with FailureConfig": [[202, "configure-automatic-retries-with-failureconfig"], [224, "configure-automatic-retries-with-failureconfig"]], "03 \u00b7 Prepare Dataset for Ray Data": [[202, "prepare-dataset-for-ray-data"], [219, null]], "03 \u00b7 Visualize Sample Digits": [[202, "visualize-sample-digits"], [204, "visualize-sample-digits"]], "04 \u00b7 Define ResNet-18 Model for MNIST": [[202, "define-resnet-18-model-for-mnist"], [205, null]], "04 \u00b7 Launch Fault-Tolerant Training": [[202, "launch-fault-tolerant-training"], [225, null]], "04 \u00b7 Load Dataset into Ray Data": [[202, "load-dataset-into-ray-data"], [219, "load-dataset-into-ray-data"]], "04-d1 Generative computer-vision pattern with Ray Train": [[305, null], [306, null]], "04-d2 Diffusion-Policy Pattern with Ray Train": [[312, null], [313, null]], "04a Computer-vision pattern with Ray Train": [[338, null], [339, null]], "04b Tabular workload pattern with Ray Train": [[325, null], [326, null]], "04c Time-Series workload pattern with Ray Train": [[331, null], [332, null]], "04e Recommendation system pattern with Ray Train": [[318, null], [319, null]], "05 \u00b7 Define Image Transformation": [[202, "define-image-transformation"], [220, null]], "05 \u00b7 Define the Ray Train Loop (DDP per-worker)": [[202, "define-the-ray-train-loop-ddp-per-worker"], [206, null]], "05 \u00b7 Manual Restoration from Checkpoints": [[202, "manual-restoration-from-checkpoints"], [226, null]], "06 \u00b7 Apply Transformations with Ray Data": [[202, "apply-transformations-with-ray-data"], [220, "apply-transformations-with-ray-data"]], "06 \u00b7 Define train_loop_config": [[202, "define-train-loop-config"], [207, null]], "06 \u00b7 Resume Training from the Last Checkpoint": [[202, "resume-training-from-the-last-checkpoint"], [226, "resume-training-from-the-last-checkpoint"]], "07 \u00b7 Clean Up Cluster Storage": [[202, "clean-up-cluster-storage"], [227, null]], "07 \u00b7 Configure Scaling with ScalingConfig": [[202, "configure-scaling-with-scalingconfig"], [208, null]], "07 \u00b7 Configure TorchTrainer with Ray Data": [[202, "configure-torchtrainer-with-ray-data"], [221, null]], "08 \u00b7 Launch Training with Ray Data": [[202, "launch-training-with-ray-data"], [221, "launch-training-with-ray-data"]], "08 \u00b7 Wrap the Model with prepare_model()": [[202, "wrap-the-model-with-prepare-model"], [209, null]], "09 \u00b7 Build the DataLoader with prepare_data_loader()": [[202, "build-the-dataloader-with-prepare-data-loader"], [210, null]], "1. Architecture": [[275, "architecture"], [278, null]], "1. Cloud Object Store": [[83, "cloud-object-store"], [95, "cloud-object-store"]], "1. Create Anyscale Resources with Terraform": [[28, "create-anyscale-resources-with-terraform"], [30, null], [43, "create-anyscale-resources-with-terraform"], [45, null], [53, "create-anyscale-resources-with-terraform"], [56, null]], "1. Creating Remote Functions": [[2, "creating-remote-functions"], [153, "creating-remote-functions"], [156, null]], "1. Dataset tuples": [[312, "dataset-tuples"], [313, "dataset-tuples"]], "1. Deploy to Kubernetes with Anyscale Operator": [[24, "deploy-to-kubernetes-with-anyscale-operator"], [25, "deploy-to-kubernetes-with-anyscale-operator"]], "1. How to Use Ray Data?": [[9, "how-to-use-ray-data"], [176, "how-to-use-ray-data"], [178, "how-to-use-ray-data"]], "1. Imports": [[318, "imports"], [320, null], [325, "imports"], [327, null], [331, "imports"], [333, null], [338, "imports"], [340, null]], "1. Imports and setup": [[305, "imports-and-setup"], [307, null], [312, "imports-and-setup"], [314, null]], "1. In the Anyscale Console, open (or create) a Workspace.": [[89, "in-the-anyscale-console-open-or-create-a-workspace"], [104, "in-the-anyscale-console-open-or-create-a-workspace"]], "1. Installation": [[35, "installation"], [38, null], [66, "installation"], [69, null]], "1. Key-Value (KV) Caching": [[108, "key-value-kv-caching"], [109, "key-value-kv-caching"], [112, "key-value-kv-caching"]], "1. Loading and visualizing data": [[14, "loading-and-visualizing-data"]], "1. Loading the data": [[7, "loading-the-data"], [229, "loading-the-data"], [231, null]], "1. Memory Management": [[108, "memory-management"], [109, "memory-management"], [113, "memory-management"]], "1. Model Quality Benchmarks": [[125, "model-quality-benchmarks"], [131, "model-quality-benchmarks"]], "1. Object store": [[3, "object-store"], [159, "object-store"], [161, null]], "1. Overview of Ray Serve": [[16, "overview-of-ray-serve"]], "1. Overview of the Ray AI Libraries": [[4, "overview-of-the-ray-ai-libraries"], [12, "overview-of-the-ray-ai-libraries"], [147, "overview-of-the-ray-ai-libraries"], [149, null]], "1. PyTorch introductory example (single GPU)": [[13, "pytorch-introductory-example-single-gpu"]], "1. Ray Serve for Orchestration": [[108, "ray-serve-for-orchestration"], [109, "ray-serve-for-orchestration"], [114, "ray-serve-for-orchestration"]], "1. Reduce max_model_len": [[117, "reduce-max-model-len"], [123, "reduce-max-model-len"]], "1. Sign Up for Anyscale": [[91, "sign-up-for-anyscale"]], "1. Spin up a Anyscale Workspace, we will use this as the environment to develop and publish the Anyscale Service. Give this workspace a name, check the Auto-Select Worker Nodes and leave everything else as default.": [[90, "spin-up-a-anyscale-workspace-we-will-use-this-as-the-environment-to-develop-and-publish-the-anyscale-service-give-this-workspace-a-name-check-the-auto-select-worker-nodes-and-leave-everything-else-as-default"], [107, "spin-up-a-anyscale-workspace-we-will-use-this-as-the-environment-to-develop-and-publish-the-anyscale-service-give-this-workspace-a-name-check-the-auto-select-worker-nodes-and-leave-everything-else-as-default"]], "1. Split Notebooks and Generate Navigation": [[0, "split-notebooks-and-generate-navigation"]], "1. User Profile Retrieval": [[146, "user-profile-retrieval"]], "1. Using the same workspace, create a notebook folder": [[89, "using-the-same-workspace-create-a-notebook-folder"], [105, "using-the-same-workspace-create-a-notebook-folder"]], "1. What is an Anyscale Cloud?": [[17, "what-is-an-anyscale-cloud"], [19, null]], "1. When to Consider Ray Data": [[10, "when-to-consider-ray-data"], [184, "when-to-consider-ray-data"], [186, null]], "1. When to Consider Ray Serve": [[11, "when-to-consider-ray-serve"], [196, "when-to-consider-ray-serve"], [198, null]], "1. When to use Ray Data": [[15, "when-to-use-ray-data"]], "1. When to use Ray Train": [[5, "when-to-use-ray-train"], [6, "when-to-use-ray-train"], [235, "when-to-use-ray-train"], [237, null]], "1.1 Configure Google Cloud Authentication": [[35, "configure-google-cloud-authentication"], [38, "configure-google-cloud-authentication"]], "1.1. Configure Google Cloud Authentication": [[66, "configure-google-cloud-authentication"], [69, "configure-google-cloud-authentication"]], "1.1. Pattern: pass an object as a top-level argument": [[3, "pattern-pass-an-object-as-a-top-level-argument"], [159, "pattern-pass-an-object-as-a-top-level-argument"], [161, "pattern-pass-an-object-as-a-top-level-argument"]], "1.2 Enable Required APIs": [[35, "enable-required-apis"], [38, "enable-required-apis"]], "1.2: Enable Required APIs": [[66, "enable-required-apis"], [69, "enable-required-apis"]], "10 \u00b7 Report Training Metrics": [[202, "report-training-metrics"], [211, null]], "10. Clean up": [[312, "clean-up"], [317, "clean-up"]], "10. Conclusion": [[53, "conclusion"], [65, null]], "10. Helper: Ray-prepared DataLoaders": [[338, "helper-ray-prepared-dataloaders"], [342, null]], "10. Launch distributed Training with TorchTrainer": [[305, "launch-distributed-training-with-torchtrainer"], [309, "launch-distributed-training-with-torchtrainer"]], "10. Plot train and validation loss curves": [[318, "plot-train-and-validation-loss-curves"], [322, "plot-train-and-validation-loss-curves"]], "10. Ray Train training loop (with teacher forcing)": [[331, "ray-train-training-loop-with-teacher-forcing"], [335, null]], "10. Start distributed training": [[325, "start-distributed-training"], [328, "start-distributed-training"]], "101 - Anyscale Organization and Cloud Setup": [[88, null], [100, null]], "101 Introduction to Anyscale Services": [[89, "introduction-to-anyscale-services"], [90, "introduction-to-anyscale-services"], [103, "introduction-to-anyscale-services"], [106, "introduction-to-anyscale-services"]], "101 \u2013  Introduction to Anyscale Services": [[86, null], [98, null]], "101 \u2013 Collaboration on Anyscale": [[87, null], [99, null]], "101 \u2013 Compute Configs and Execution Environments in Anyscale": [[82, null], [94, null]], "101 \u2013 Debug and Monitor Your Anyscale Application": [[84, null], [96, null]], "101 \u2013 Developing Application with Anyscale": [[81, null], [93, null]], "101 \u2013 Introduction to Anyscale Jobs": [[85, null], [97, null]], "101 \u2013 Storage Options in the Anyscale Platform": [[83, null], [95, null]], "101 \u2014 Introduction to Anyscale Workspaces": [[80, null], [92, null]], "11 \u00b7 Save Checkpoints and Report Metrics": [[202, "save-checkpoints-and-report-metrics"], [212, null]], "11. Evaluate the trained model": [[325, "evaluate-the-trained-model"], [328, "evaluate-the-trained-model"]], "11. Launch training on 8 GPUs": [[331, "launch-training-on-8-gpus"], [335, "launch-training-on-8-gpus"]], "11. Plot loss curves": [[305, "plot-loss-curves"], [309, "plot-loss-curves"]], "11. Resume training from checkpoint": [[318, "resume-training-from-checkpoint"], [323, null]], "11. train_loop_per_worker": [[338, "train-loop-per-worker"], [343, null]], "12 \u00b7 Save Checkpoints on Rank-0 Only": [[202, "save-checkpoints-on-rank-0-only"], [212, "save-checkpoints-on-rank-0-only"]], "12. Confusion matrix visualization": [[325, "confusion-matrix-visualization"], [329, null]], "12. Inference: recommend top-N items for a user": [[318, "inference-recommend-top-n-items-for-a-user"], [324, null]], "12. Launch distributed training with TorchTrainer": [[338, "launch-distributed-training-with-torchtrainer"], [344, null]], "12. Plot training and validation loss": [[331, "plot-training-and-validation-loss"], [335, "plot-training-and-validation-loss"]], "12. Resume from latest checkpoint": [[305, "resume-from-latest-checkpoint"], [310, null]], "13 \u00b7 Configure Persistent Storage with RunConfig": [[202, "configure-persistent-storage-with-runconfig"], [212, "configure-persistent-storage-with-runconfig"]], "13. CPU batch inference with Ray Data": [[325, "cpu-batch-inference-with-ray-data"], [329, "cpu-batch-inference-with-ray-data"]], "13. Join top-N item IDs with movie titles": [[318, "join-top-n-item-ids-with-movie-titles"], [324, "join-top-n-item-ids-with-movie-titles"]], "13. Plot training and validation loss curves": [[338, "plot-training-and-validation-loss-curves"], [345, null]], "13. Resume training from checkpoint": [[331, "resume-training-from-checkpoint"], [336, null]], "13. Reverse diffusion sampler": [[305, "reverse-diffusion-sampler"], [311, null]], "14 \u00b7 Create the TorchTrainer": [[202, "create-the-torchtrainer"], [213, null]], "14. Clean up shared storage": [[318, "clean-up-shared-storage"], [324, "clean-up-shared-storage"]], "14. Demonstrate fault-tolerant resumption": [[338, "demonstrate-fault-tolerant-resumption"], [346, null]], "14. Feature-importance diagnostics": [[325, "feature-importance-diagnostics"], [329, "feature-importance-diagnostics"]], "14. Generate and display samples from the best checkpoint": [[305, "generate-and-display-samples-from-the-best-checkpoint"], [311, "generate-and-display-samples-from-the-best-checkpoint"]], "14. Inference helper \u2014 Ray Data batch predictor on GPU": [[331, "inference-helper-ray-data-batch-predictor-on-gpu"], [337, null]], "15 \u00b7 Launch Training with trainer.fit()": [[202, "launch-training-with-trainer-fit"], [213, "launch-training-with-trainer-fit"]], "15. Batch inference with Ray Data": [[338, "batch-inference-with-ray-data"], [347, null]], "15. Clean up shared storage": [[305, "clean-up-shared-storage"], [311, "clean-up-shared-storage"]], "15. Continue training from the latest checkpoint": [[325, "continue-training-from-the-latest-checkpoint"], [330, null]], "15. Run distributed inference and visualize results": [[331, "run-distributed-inference-and-visualize-results"], [337, "run-distributed-inference-and-visualize-results"]], "16 \u00b7 Inspect the Training Results": [[202, "inspect-the-training-results"], [214, null]], "16. Cleanup: remove all training artifacts": [[331, "cleanup-remove-all-training-artifacts"], [337, "cleanup-remove-all-training-artifacts"]], "16. Run and visualize Ray Data inference": [[338, "run-and-visualize-ray-data-inference"], [347, "run-and-visualize-ray-data-inference"]], "16. Verify post-training inference": [[325, "verify-post-training-inference"], [330, "verify-post-training-inference"]], "17 \u00b7 View Metrics as a DataFrame": [[202, "view-metrics-as-a-dataframe"], [214, "view-metrics-as-a-dataframe"]], "17. Clean up": [[325, "clean-up"], [330, "clean-up"], [338, "clean-up"], [347, "clean-up"]], "18 \u00b7 Load a Checkpoint for Inference": [[202, "load-a-checkpoint-for-inference"], [215, null]], "19 \u00b7 Run Inference and Visualize Predictions": [[202, "run-inference-and-visualize-predictions"], [215, "run-inference-and-visualize-predictions"]], "2. Attach Required IAM Policies to Your existing EKS\u2019s Node Role": [[53, "attach-required-iam-policies-to-your-existing-eks-s-node-role"], [57, null]], "2. Build the Book": [[0, "build-the-book"]], "2. Chaining Tasks and Passing Data": [[3, "chaining-tasks-and-passing-data"], [159, "chaining-tasks-and-passing-data"], [162, null]], "2. Clone the Repository (Optional)": [[91, "clone-the-repository-optional"]], "2. Cloud Deployment Types": [[17, "cloud-deployment-types"], [20, null]], "2. Continuous Batching": [[108, "continuous-batching"], [109, "continuous-batching"], [112, "continuous-batching"]], "2. Create Anyscale Resources with Terraform": [[35, "create-anyscale-resources-with-terraform"], [39, null], [66, "create-anyscale-resources-with-terraform"], [70, null]], "2. Distributed Data Parallel Training with Ray Train and PyTorch (multiple GPUs)": [[13, "distributed-data-parallel-training-with-ray-train-and-pytorch-multiple-gpus"]], "2. End-to-end example: predicting taxi tips in New York": [[12, "end-to-end-example-predicting-taxi-tips-in-new-york"]], "2. Executing Remote Functions": [[2, "executing-remote-functions"], [153, "executing-remote-functions"], [157, null]], "2. Generate a real pendulum dataset": [[312, "generate-a-real-pendulum-dataset"], [314, "generate-a-real-pendulum-dataset"]], "2. How to work with Ray Data": [[10, "how-to-work-with-ray-data"], [184, "how-to-work-with-ray-data"], [187, null]], "2. Implement an Classifier service": [[16, "implement-an-classifier-service"]], "2. Install Kubernetes Components": [[43, "install-kubernetes-components"], [46, null]], "2. Latency Requirements": [[108, "latency-requirements"], [109, "latency-requirements"], [113, "latency-requirements"]], "2. Library Imports": [[275, "library-imports"], [278, "library-imports"]], "2. Load 10 % of Food-101": [[305, "load-10-of-food-101"], [307, "load-10-of-food-101"], [338, "load-10-of-food-101"], [340, "load-10-of-food-101"]], "2. Load MovieLens 100K dataset": [[318, "load-movielens-100k-dataset"], [320, "load-movielens-100k-dataset"]], "2. Load NYC taxi passenger counts (30-min)": [[331, "load-nyc-taxi-passenger-counts-30-min"], [333, "load-nyc-taxi-passenger-counts-30-min"]], "2. Load the University of California, Irvine (UCI) Cover type dataset": [[325, "load-the-university-of-california-irvine-uci-cover-type-dataset"], [327, "load-the-university-of-california-irvine-uci-cover-type-dataset"]], "2. Loading Data": [[9, "loading-data"], [15, "loading-data"], [176, "loading-data"], [179, null]], "2. Once in the workspace, navigate to the VS Code Editor": [[89, "once-in-the-workspace-navigate-to-the-vs-code-editor"], [104, "once-in-the-workspace-navigate-to-the-vs-code-editor"]], "2. Overview of Ray Serve": [[11, "overview-of-ray-serve"], [196, "overview-of-ray-serve"], [199, null]], "2. Quick end-to-end example": [[4, "quick-end-to-end-example"], [147, "quick-end-to-end-example"], [150, null]], "2. Register the Anyscale Cloud": [[28, "register-the-anyscale-cloud"], [31, null]], "2. Setting up a PyTorch model": [[14, "setting-up-a-pytorch-model"]], "2. Shared File Storage": [[83, "shared-file-storage"], [95, "shared-file-storage"]], "2. Single GPU Training with PyTorch": [[5, "single-gpu-training-with-pytorch"]], "2. Single GPU Training with PyTorch Lightning": [[6, "single-gpu-training-with-pytorch-lightning"], [235, "single-gpu-training-with-pytorch-lightning"], [238, null]], "2. Starting out with vanilla PyTorch": [[7, "starting-out-with-vanilla-pytorch"], [229, "starting-out-with-vanilla-pytorch"], [232, null]], "2. Task and Domain Alignment": [[125, "task-and-domain-alignment"], [131, "task-and-domain-alignment"]], "2. Training objective": [[312, "training-objective"], [313, "training-objective"]], "2. Use Quantized Models": [[117, "use-quantized-models"], [123, "use-quantized-models"]], "2. Use the Anyscale CLI to submit the Anyscale Job. For a full list of all available arguments, check out the Anyscale Job CLI documentation.": [[89, "use-the-anyscale-cli-to-submit-the-anyscale-job-for-a-full-list-of-all-available-arguments-check-out-the-anyscale-job-cli-documentation"], [105, "use-the-anyscale-cli-to-submit-the-anyscale-job-for-a-full-list-of-all-available-arguments-check-out-the-anyscale-job-cli-documentation"]], "2. User Registration": [[146, "user-registration"]], "2. Virtual Machines (VM) vs. Kubernetes (K8s)": [[24, "virtual-machines-vm-vs-kubernetes-k8s"], [26, null]], "2. vLLM as the inference engine": [[108, "vllm-as-the-inference-engine"], [109, "vllm-as-the-inference-engine"], [114, "vllm-as-the-inference-engine"]], "2.1 Control Layer: What Anyscale Manages or Needs Access To": [[24, "control-layer-what-anyscale-manages-or-needs-access-to"], [26, "control-layer-what-anyscale-manages-or-needs-access-to"]], "2.1 Create terraform.tfvars": [[35, "create-terraform-tfvars"], [39, "create-terraform-tfvars"]], "2.1 Install the Cluster Autoscaler": [[43, "install-the-cluster-autoscaler"], [46, "install-the-cluster-autoscaler"]], "2.1 Overview": [[6, "overview"], [235, "overview"], [238, "overview"]], "2.1 Vanilla XGboost code": [[4, "vanilla-xgboost-code"], [147, "vanilla-xgboost-code"], [150, "vanilla-xgboost-code"]], "2.1. Overview": [[5, "overview"]], "2.1: Create terraform.tfvars": [[66, "create-terraform-tfvars"], [70, "create-terraform-tfvars"]], "2.2 Data Layer: Storage, Object Stores, and External Dependencies": [[24, "data-layer-storage-object-stores-and-external-dependencies"], [26, "data-layer-storage-object-stores-and-external-dependencies"]], "2.2 Hyperparameter tuning with Ray Tune": [[4, "hyperparameter-tuning-with-ray-tune"], [147, "hyperparameter-tuning-with-ray-tune"], [150, "hyperparameter-tuning-with-ray-tune"]], "2.2 Install the AWS Load Balancer Controller": [[43, "install-the-aws-load-balancer-controller"], [46, "install-the-aws-load-balancer-controller"]], "2.2 Note on blocks": [[10, "note-on-blocks"], [184, "note-on-blocks"], [188, "note-on-blocks"]], "2.2 Run Terraform Commands": [[35, "run-terraform-commands"], [39, "run-terraform-commands"]], "2.2. Build model and load it on the GPU": [[5, "build-model-and-load-it-on-the-gpu"]], "2.2. Create a torch dataloader": [[6, "create-a-torch-dataloader"], [235, "create-a-torch-dataloader"], [238, "create-a-torch-dataloader"]], "2.2: Deploy Infrastructure": [[66, "deploy-infrastructure"], [70, "deploy-infrastructure"]], "2.3 Define a stable diffusion model": [[6, "define-a-stable-diffusion-model"], [235, "define-a-stable-diffusion-model"], [238, "define-a-stable-diffusion-model"]], "2.3 Install the Nginx Ingress Controller": [[43, "install-the-nginx-ingress-controller"], [46, "install-the-nginx-ingress-controller"]], "2.3 Workload Execution Layer: How Ray Runs on Each Backend": [[24, "workload-execution-layer-how-ray-runs-on-each-backend"], [26, "workload-execution-layer-how-ray-runs-on-each-backend"]], "2.3. Create Dataset and DataLoader": [[5, "create-dataset-and-dataloader"]], "2.3. Distributed training with Ray Train": [[4, "distributed-training-with-ray-train"], [147, "distributed-training-with-ray-train"], [150, "distributed-training-with-ray-train"]], "2.4 (Optional) Install the Nvidia Device Plugin": [[43, "optional-install-the-nvidia-device-plugin"], [46, "optional-install-the-nvidia-device-plugin"]], "2.4 Serving an ensemble model with Ray Serve": [[4, "serving-an-ensemble-model-with-ray-serve"], [147, "serving-an-ensemble-model-with-ray-serve"], [150, "serving-an-ensemble-model-with-ray-serve"]], "2.4 When to use which": [[24, "when-to-use-which"], [26, "when-to-use-which"]], "2.4. Create metrics and checkpointing": [[5, "create-metrics-and-checkpointing"]], "2.4. Define a PyTorch Lightning training loop": [[6, "define-a-pytorch-lightning-training-loop"], [235, "define-a-pytorch-lightning-training-loop"], [238, "define-a-pytorch-lightning-training-loop"]], "2.5 Batch inference with Ray Data": [[4, "batch-inference-with-ray-data"], [147, "batch-inference-with-ray-data"], [150, "batch-inference-with-ray-data"]], "2.5. Run the training loop": [[5, "run-the-training-loop"]], "2.6 Clean up": [[4, "clean-up"], [147, "clean-up"], [150, "clean-up"]], "2.6. Use checkpointed model to generate predictions": [[5, "use-checkpointed-model-to-generate-predictions"]], "2.Download starter template. Clone a github repository containing the files needed to deploy a Anyscale Service. Head over to the VSCode Tab (In Anyscale Workspace) and enter the following command into the terminal.": [[90, "download-starter-template-clone-a-github-repository-containing-the-files-needed-to-deploy-a-anyscale-service-head-over-to-the-vscode-tab-in-anyscale-workspace-and-enter-the-following-command-into-the-terminal"], [107, "download-starter-template-clone-a-github-repository-containing-the-files-needed-to-deploy-a-anyscale-service-head-over-to-the-vscode-tab-in-anyscale-workspace-and-enter-the-following-command-into-the-terminal"]], "20 \u00b7 Clean Up the Ray Actor": [[202, "clean-up-the-ray-actor"], [215, "clean-up-the-ray-actor"]], "3. (Optional) More Kubernetes Deployments Components": [[24, "optional-more-kubernetes-deployments-components"], [27, null]], "3. A Demonstrative Example of Resource Creation with AWS EC2": [[17, "a-demonstrative-example-of-resource-creation-with-aws-ec2"], [21, null]], "3. Advanced features of Ray Serve": [[16, "advanced-features-of-ray-serve"]], "3. Anyscale for Infrastructure": [[108, "anyscale-for-infrastructure"], [109, "anyscale-for-infrastructure"], [114, "anyscale-for-infrastructure"]], "3. Context Window Requirements": [[125, "context-window-requirements"], [131, "context-window-requirements"]], "3. Distributed Data Parallel Training with Ray Train and PyTorch": [[5, "distributed-data-parallel-training-with-ray-train-and-pytorch"]], "3. Distributed Training with Ray Train and PyTorch Lightning": [[6, "distributed-training-with-ray-train-and-pytorch-lightning"], [235, "distributed-training-with-ray-train-and-pytorch-lightning"], [239, null]], "3. Enable Pipeline Parallelism": [[117, "enable-pipeline-parallelism"], [123, "enable-pipeline-parallelism"]], "3. Getting Results": [[2, "getting-results"], [153, "getting-results"], [157, "getting-results"]], "3. Hyperparameter tuning with Ray Tune": [[7, "hyperparameter-tuning-with-ray-tune"], [229, "hyperparameter-tuning-with-ray-tune"], [233, null]], "3. Implement an image classification service": [[11, "implement-an-image-classification-service"], [196, "implement-an-image-classification-service"], [200, null]], "3. Inspect the code for the Service Endpoint (./examples/02_service_hello_world/main.py)": [[90, "inspect-the-code-for-the-service-endpoint-examples-02-service-hello-world-main-py"], [107, "inspect-the-code-for-the-service-endpoint-examples-02-service-hello-world-main-py"]], "3. Install Kubernetes Components": [[53, "install-kubernetes-components"], [58, null]], "3. Install Ray and the Anyscale CLI (Recommended)": [[91, "install-ray-and-the-anyscale-cli-recommended"]], "3. Introduction to Ray Tune": [[14, "introduction-to-ray-tune"]], "3. Lazy execution mode": [[10, "lazy-execution-mode"], [184, "lazy-execution-mode"], [189, null]], "3. Loading data": [[10, "loading-data"], [184, "loading-data"], [188, null]], "3. Local Cluster Storage": [[83, "local-cluster-storage"], [95, "local-cluster-storage"]], "3. Metrics Setup": [[275, "metrics-setup"], [279, null]], "3. Model parallelization or alternatives": [[108, "model-parallelization-or-alternatives"], [109, "model-parallelization-or-alternatives"], [112, "model-parallelization-or-alternatives"]], "3. Next, create a new file. You can name it hello_world.py": [[89, "next-create-a-new-file-you-can-name-it-hello-world-py"], [104, "next-create-a-new-file-you-can-name-it-hello-world-py"]], "3. Normalize and split": [[312, "normalize-and-split"], [314, "normalize-and-split"]], "3. Overview of the training loop in Ray Train": [[13, "overview-of-the-training-loop-in-ray-train"]], "3. Point to Parquet dataset URI": [[318, "point-to-parquet-dataset-uri"], [320, "point-to-parquet-dataset-uri"]], "3. Register the Anyscale Cloud": [[35, "register-the-anyscale-cloud"], [40, null], [43, "register-the-anyscale-cloud"], [47, null]], "3. Resample to hourly, then normalize": [[331, "resample-to-hourly-then-normalize"], [333, "resample-to-hourly-then-normalize"]], "3. Resize and encode images": [[305, "resize-and-encode-images"], [307, "resize-and-encode-images"], [338, "resize-and-encode-images"], [340, "resize-and-encode-images"]], "3. Reverse diffusion (sampling)": [[312, "reverse-diffusion-sampling"], [313, "reverse-diffusion-sampling"]], "3. Running an experiment with Ray AI libraries": [[12, "running-an-experiment-with-ray-ai-libraries"]], "3. Scalability Demands": [[108, "scalability-demands"], [109, "scalability-demands"], [113, "scalability-demands"]], "3. Serve Locally for Testing": [[0, "serve-locally-for-testing"]], "3. Submit the job again using the Anyscale Python SDK": [[89, "submit-the-job-again-using-the-anyscale-python-sdk"], [105, "submit-the-job-again-using-the-anyscale-python-sdk"]], "3. Task retries": [[3, "task-retries"], [159, "task-retries"], [163, null]], "3. Test": [[28, "test"], [32, null]], "3. Transforming Data": [[9, "transforming-data"], [15, "transforming-data"], [176, "transforming-data"], [180, null]], "3. Troubleshooting GPU Availability": [[66, "troubleshooting-gpu-availability"], [71, null]], "3. Visualize class balance": [[325, "visualize-class-balance"], [327, "visualize-class-balance"]], "3.1 Distributed Data Parallel Training": [[6, "distributed-data-parallel-training"], [235, "distributed-data-parallel-training"], [239, "distributed-data-parallel-training"]], "3.1 IAM Role Definition": [[17, "iam-role-definition"], [22, null]], "3.1 Install the Cluster Autoscaler": [[53, "install-the-cluster-autoscaler"], [58, "install-the-cluster-autoscaler"]], "3.1. Overview of the training loop in Ray Train": [[5, "overview-of-the-training-loop-in-ray-train"]], "3.1.1\u202f\u202fAnyscale Control Plane Role (anyscale-iam-role-id)": [[17, "anyscale-control-plane-role-anyscale-iam-role-id"], [22, "anyscale-control-plane-role-anyscale-iam-role-id"]], "3.1.2\u202f\u202fInstance Role (instance-iam-role-id)": [[17, "instance-role-instance-iam-role-id"], [22, "instance-role-instance-iam-role-id"]], "3.10. Activity: Run the distributed training with more workers": [[5, "activity-run-the-distributed-training-with-more-workers"]], "3.2 Install the AWS Load Balancer Controller": [[53, "install-the-aws-load-balancer-controller"], [58, "install-the-aws-load-balancer-controller"]], "3.2 Ray Train Migration": [[6, "ray-train-migration"], [235, "ray-train-migration"], [239, "ray-train-migration"]], "3.2. Configure scale and GPUs": [[5, "configure-scale-and-gpus"]], "3.2.1. Note on Ray Train key concepts": [[5, "note-on-ray-train-key-concepts"]], "3.2\u202fVPC": [[17, "vpc"], [22, "vpc"]], "3.3 Install the Nginx Ingress Controller": [[53, "install-the-nginx-ingress-controller"], [58, "install-the-nginx-ingress-controller"]], "3.3 Subnets": [[17, "subnets"], [22, "subnets"]], "3.3. Configure scale and GPUs": [[6, "configure-scale-and-gpus"], [235, "configure-scale-and-gpus"], [239, "configure-scale-and-gpus"]], "3.3. Migrating the model to Ray Train": [[5, "migrating-the-model-to-ray-train"]], "3.3.1. Note on Ray Train key concepts": [[6, "note-on-ray-train-key-concepts"], [235, "note-on-ray-train-key-concepts"], [239, "note-on-ray-train-key-concepts"]], "3.4 (Optional) Install the Nvidia Device Plugin": [[53, "optional-install-the-nvidia-device-plugin"], [58, "optional-install-the-nvidia-device-plugin"]], "3.4 Create and fit a Ray Train TorchTrainer": [[6, "create-and-fit-a-ray-train-torchtrainer"], [235, "create-and-fit-a-ray-train-torchtrainer"], [239, "create-and-fit-a-ray-train-torchtrainer"]], "3.4. Migrating the dataset to Ray Train": [[5, "migrating-the-dataset-to-ray-train"]], "3.4\u202fSecurity Groups": [[17, "security-groups"], [22, "security-groups"]], "3.5. Access the training results": [[6, "access-the-training-results"], [235, "access-the-training-results"], [239, "access-the-training-results"]], "3.5. Reporting checkpoints and metrics": [[5, "reporting-checkpoints-and-metrics"]], "3.5.1. Note on the checkpoint lifecycle": [[5, "note-on-the-checkpoint-lifecycle"]], "3.5\u202fS3": [[17, "s3"], [22, "s3"]], "3.6. Configure remote storage": [[5, "configure-remote-storage"]], "3.6. Load the checkpointed model to generate predictions": [[6, "load-the-checkpointed-model-to-generate-predictions"], [235, "load-the-checkpointed-model-to-generate-predictions"], [239, "load-the-checkpointed-model-to-generate-predictions"]], "3.6\u202fEFS (Optional)": [[17, "efs-optional"], [22, "efs-optional"]], "3.7. Activity: Run the distributed training with more workers": [[6, "activity-run-the-distributed-training-with-more-workers"], [235, "activity-run-the-distributed-training-with-more-workers"], [239, "activity-run-the-distributed-training-with-more-workers"]], "3.7. Launching the distributed training job": [[5, "launching-the-distributed-training-job"]], "3.7\u202fMemoryDB (Optional)": [[17, "memorydb-optional"], [22, "memorydb-optional"]], "3.8 Summary": [[17, "summary"], [22, "summary"]], "3.8. Access the training results": [[5, "access-the-training-results"]], "3.9. Use checkpointed model to generate predictions": [[5, "id1"]], "4. Checkout the service.yaml file.": [[90, "checkout-the-service-yaml-file"], [107, "checkout-the-service-yaml-file"]], "4. Cleanup": [[28, "cleanup"], [33, null]], "4. Context Window Considerations": [[108, "context-window-considerations"], [109, "context-window-considerations"], [112, "context-window-considerations"]], "4. Cost Optimization": [[108, "cost-optimization"], [109, "cost-optimization"], [113, "cost-optimization"]], "4. Data Operations: Grouping, Aggregation, and Shuffling": [[15, "data-operations-grouping-aggregation-and-shuffling"]], "4. Development workflow": [[11, "development-workflow"], [196, "development-workflow"], [201, null]], "4. DiffusionPolicy LightningModule": [[312, "diffusionpolicy-lightningmodule"], [315, null]], "4. Diving deeper into Ray Tune concepts": [[14, "diving-deeper-into-ray-tune-concepts"]], "4. Examples Outlook: Deploying to Your Infrastructure": [[24, "examples-outlook-deploying-to-your-infrastructure"], [27, "examples-outlook-deploying-to-your-infrastructure"]], "4. Hardware and Cost Considerations": [[125, "hardware-and-cost-considerations"], [131, "hardware-and-cost-considerations"]], "4. Install the Anyscale Operator": [[43, "install-the-anyscale-operator"], [48, null]], "4. Local File Store": [[83, "local-file-store"], [95, "local-file-store"]], "4. Migrating the model and dataset to Ray Train": [[13, "migrating-the-model-and-dataset-to-ray-train"]], "4. Paste the basic Ray example below into the file.": [[89, "paste-the-basic-ray-example-below-into-the-file"], [104, "paste-the-basic-ray-example-below-into-the-file"]], "4. Putting It All Together": [[2, "putting-it-all-together"], [153, "putting-it-all-together"], [158, null]], "4. Quick visual sanity-check": [[331, "quick-visual-sanity-check"], [333, "quick-visual-sanity-check"]], "4. Ray Serve in Production": [[16, "ray-serve-in-production"]], "4. Ray Train in Production": [[5, "ray-train-in-production"], [6, "ray-train-in-production"], [235, "ray-train-in-production"], [240, null]], "4. Ray Tune in Production": [[7, "ray-tune-in-production"], [229, "ray-tune-in-production"], [234, null]], "4. Register Anyscale Cloud to Your Cloud Provider": [[17, "register-anyscale-cloud-to-your-cloud-provider"], [23, null]], "4. Register the Anyscale Cloud": [[53, "register-the-anyscale-cloud"], [59, null]], "4. Scale with More Replicas": [[117, "scale-with-more-replicas"], [123, "scale-with-more-replicas"]], "4. Task Runtime Environments": [[3, "task-runtime-environments"], [159, "task-runtime-environments"], [164, null]], "4. Test": [[35, "test"], [41, null]], "4. Training function per worker": [[275, "training-function-per-worker"], [280, null]], "4. Transforming data": [[10, "transforming-data"], [184, "transforming-data"], [190, null]], "4. Visual sanity check": [[305, "visual-sanity-check"], [307, "visual-sanity-check"], [338, "visual-sanity-check"], [340, "visual-sanity-check"]], "4. Visualize dataset: ratings, users, and items": [[318, "visualize-dataset-ratings-users-and-items"], [320, "visualize-dataset-ratings-users-and-items"]], "4. Write train / validation Parquet files": [[325, "write-train-validation-parquet-files"], [327, "write-train-validation-parquet-files"]], "4. Writing Data": [[9, "writing-data"], [176, "writing-data"], [181, null]], "4. kubectl Configuration": [[66, "kubectl-configuration"], [72, null]], "4.1 On resource specification": [[10, "on-resource-specification"], [184, "on-resource-specification"], [190, "on-resource-specification"]], "4.1. Note about Ray ID Specification": [[2, "note-about-ray-id-specification"], [153, "note-about-ray-id-specification"], [158, "note-about-ray-id-specification"]], "4.1. Note about pip dependencies": [[3, "note-about-pip-dependencies"], [159, "note-about-pip-dependencies"], [164, "note-about-pip-dependencies"]], "4.2 On concurrency limiting": [[10, "on-concurrency-limiting"], [184, "on-concurrency-limiting"], [190, "on-concurrency-limiting"]], "4.2. Anti-pattern: Calling ray.get in a loop harms parallelism": [[2, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"], [153, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"], [158, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"]], "5. Cleanup": [[35, "cleanup"], [42, null]], "5. Conclusion": [[28, "conclusion"], [34, null]], "5. Create Ray Dataset from Parquet and encode IDs": [[318, "create-ray-dataset-from-parquet-and-encode-ids"], [320, "create-ray-dataset-from-parquet-and-encode-ids"]], "5. Data Operations: Shuffling, Grouping and Aggregation": [[9, "data-operations-shuffling-grouping-and-aggregation"], [176, "data-operations-shuffling-grouping-and-aggregation"], [182, null]], "5. Deploy the Anyscale Service by running the command below in the terminal and passing in the service configuration yaml file.": [[90, "deploy-the-anyscale-service-by-running-the-command-below-in-the-terminal-and-passing-in-the-service-configuration-yaml-file"], [107, "deploy-the-anyscale-service-by-running-the-command-below-in-the-terminal-and-passing-in-the-service-configuration-yaml-file"]], "5. Distributed Train loop with checkpointing": [[312, "distributed-train-loop-with-checkpointing"], [316, null]], "5. Hyperparameter tuning the PyTorch model using Ray Tune": [[14, "hyperparameter-tuning-the-pytorch-model-using-ray-tune"]], "5. Install NGINX Ingress Controller": [[66, "install-nginx-ingress-controller"], [73, null]], "5. Install the Anyscale Operator": [[53, "install-the-anyscale-operator"], [60, null]], "5. Load the train and validation splits as Ray Datasets": [[325, "load-the-train-and-validation-splits-as-ray-datasets"], [327, "load-the-train-and-validation-splits-as-ray-datasets"]], "5. Main Training Function": [[275, "main-training-function"], [281, null]], "5. Open the terminal and run the following command to submit the Ray workflow as an Anyscale Job.": [[89, "open-the-terminal-and-run-the-following-command-to-submit-the-ray-workflow-as-an-anyscale-job"], [104, "open-the-terminal-and-run-the-following-command-to-submit-the-ray-workflow-as-an-anyscale-job"]], "5. Persist to Parquet": [[305, "persist-to-parquet"], [307, "persist-to-parquet"], [338, "persist-to-parquet"], [340, "persist-to-parquet"]], "5. Persisting Data": [[15, "persisting-data"]], "5. Reporting checkpoints and metrics": [[13, "reporting-checkpoints-and-metrics"]], "5. Resource allocation and management": [[3, "resource-allocation-and-management"], [159, "resource-allocation-and-management"], [165, null]], "5. Sliding-window dataset to Parquet": [[331, "sliding-window-dataset-to-parquet"], [333, "sliding-window-dataset-to-parquet"]], "5. Stateful transformations with Ray Actors": [[10, "stateful-transformations-with-ray-actors"], [184, "stateful-transformations-with-ray-actors"], [191, null]], "5. Upgrade Hardware": [[117, "upgrade-hardware"], [123, "upgrade-hardware"]], "5. Verify the Installation": [[43, "verify-the-installation"], [49, null]], "5.1 Resource specification for stateful transformations": [[10, "resource-specification-for-stateful-transformations"], [184, "resource-specification-for-stateful-transformations"], [191, "resource-specification-for-stateful-transformations"]], "5.1. Note on resources requests, available resources, configuring large clusters": [[3, "note-on-resources-requests-available-resources-configuring-large-clusters"], [159, "note-on-resources-requests-available-resources-configuring-large-clusters"], [165, "note-on-resources-requests-available-resources-configuring-large-clusters"]], "5.2 Note on autoscaling for stateful transformations": [[10, "note-on-autoscaling-for-stateful-transformations"], [184, "note-on-autoscaling-for-stateful-transformations"], [191, "note-on-autoscaling-for-stateful-transformations"]], "5.2. Fractional resources": [[3, "fractional-resources"], [159, "fractional-resources"], [165, "fractional-resources"]], "5.3. IO bound tasks and fractional resources": [[3, "io-bound-tasks-and-fractional-resources"], [159, "io-bound-tasks-and-fractional-resources"], [165, "io-bound-tasks-and-fractional-resources"]], "6. (Optional) Upgrade Anyscale Dependencies": [[66, "optional-upgrade-anyscale-dependencies"], [74, null]], "6. Custom Food101Dataset for Parquet": [[338, "custom-food101dataset-for-parquet"], [341, null]], "6. Inspect dataset sizes (optional)": [[325, "inspect-dataset-sizes-optional"], [327, "inspect-dataset-sizes-optional"]], "6. Launch Ray TorchTrainer": [[312, "launch-ray-torchtrainer"], [316, "launch-ray-torchtrainer"]], "6. Launching the distributed training job": [[13, "launching-the-distributed-training-job"]], "6. Load and decode with Ray Data": [[305, "load-and-decode-with-ray-data"], [307, "load-and-decode-with-ray-data"]], "6. Materializing data": [[10, "materializing-data"], [184, "materializing-data"], [192, null]], "6. Nested Tasks": [[3, "nested-tasks"], [159, "nested-tasks"], [166, null]], "6. PyTorch Dataset over Parquet": [[331, "pytorch-dataset-over-parquet"], [333, "pytorch-dataset-over-parquet"]], "6. Start Training": [[275, "start-training"], [282, null]], "6. Test": [[43, "test"], [50, null]], "6. Track the status of the job, head over to the Jobs tab and find the submitted Anyscale Job. The url is also displayed in the terminal.": [[89, "track-the-status-of-the-job-head-over-to-the-jobs-tab-and-find-the-submitted-anyscale-job-the-url-is-also-displayed-in-the-terminal"], [104, "track-the-status-of-the-job-head-over-to-the-jobs-tab-and-find-the-submitted-anyscale-job-the-url-is-also-displayed-in-the-terminal"]], "6. Train/validation split using Ray Data": [[318, "train-validation-split-using-ray-data"], [320, "train-validation-split-using-ray-data"]], "6. Verify the Installation": [[53, "verify-the-installation"], [61, null]], "6. When to use Ray Data": [[9, "when-to-use-ray-data"], [176, "when-to-use-ray-data"], [183, null]], "7. Accessing the training results": [[13, "accessing-the-training-results"]], "7. Clean up": [[43, "clean-up"], [51, null]], "7. Data Operations: grouping, aggregation, and shuffling": [[10, "data-operations-grouping-aggregation-and-shuffling"], [184, "data-operations-grouping-aggregation-and-shuffling"], [193, null]], "7. Define matrix factorization model": [[318, "define-matrix-factorization-model"], [321, null]], "7. Image transform": [[338, "image-transform"], [341, "image-transform"]], "7. In the Anyscale Jobs console, we can check out the status of the submitted job. From the logs, we can verify that our job was successfully executed and Anyscale will now handle the cleanup.": [[89, "in-the-anyscale-jobs-console-we-can-check-out-the-status-of-the-submitted-job-from-the-logs-we-can-verify-that-our-job-was-successfully-executed-and-anyscale-will-now-handle-the-cleanup"], [104, "in-the-anyscale-jobs-console-we-can-check-out-the-status-of-the-submitted-job-from-the-logs-we-can-verify-that-our-job-was-successfully-executed-and-anyscale-will-now-handle-the-cleanup"]], "7. Inspect a mini-batch": [[325, "inspect-a-mini-batch"], [327, "inspect-a-mini-batch"]], "7. Inspect one random batch": [[331, "inspect-one-random-batch"], [333, "inspect-one-random-batch"]], "7. Pattern: Pipeline data processing and waiting for results": [[3, "pattern-pipeline-data-processing-and-waiting-for-results"], [159, "pattern-pipeline-data-processing-and-waiting-for-results"], [167, null]], "7. Plot train / val loss": [[312, "plot-train-val-loss"], [316, "plot-train-val-loss"]], "7. Ray Data in Production": [[9, "ray-data-in-production"], [176, "ray-data-in-production"], [183, "ray-data-in-production"]], "7. Register the Anyscale Cloud": [[66, "register-the-anyscale-cloud"], [75, null]], "7. Shuffle and Train/Val split": [[305, "shuffle-and-train-val-split"], [307, "shuffle-and-train-val-split"]], "7. Shutdown Ray Cluster": [[275, "shutdown-ray-cluster"], [282, "shutdown-ray-cluster"]], "7. Test": [[53, "test"], [62, null]], "7.1 Batch Processing Pattern": [[3, "batch-processing-pattern"], [159, "batch-processing-pattern"], [167, "batch-processing-pattern"]], "7.1. Custom batching using groupby.": [[10, "custom-batching-using-groupby"], [184, "custom-batching-using-groupby"], [193, "custom-batching-using-groupby"]], "7.2 Note on fetching too many objects at once with ray.get causes failure": [[3, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"], [159, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"], [167, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"]], "7.2. Aggregations": [[10, "aggregations"], [184, "aggregations"], [193, "aggregations"]], "7.3. Shuffling data": [[10, "shuffling-data"], [184, "shuffling-data"], [193, "shuffling-data"]], "7.3.1. File based shuffle on read": [[10, "file-based-shuffle-on-read"], [184, "file-based-shuffle-on-read"], [193, "file-based-shuffle-on-read"]], "7.3.2. Shuffling block order": [[10, "shuffling-block-order"], [184, "shuffling-block-order"], [193, "shuffling-block-order"]], "7.3.3. Shuffle all rows globally": [[10, "shuffle-all-rows-globally"], [184, "shuffle-all-rows-globally"], [193, "shuffle-all-rows-globally"]], "8. Conclusion": [[43, "conclusion"], [52, null]], "8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)": [[318, "define-ray-train-loop-with-validation-checkpointing-and-ray-managed-metrics"], [322, null]], "8. Define the Ray Train worker loop (Arrow-based, memory-efficient)": [[325, "define-the-ray-train-worker-loop-arrow-based-memory-efficient"], [328, null]], "8. Install the Anyscale Operator": [[66, "install-the-anyscale-operator"], [76, null]], "8. Persisting data": [[10, "persisting-data"], [184, "persisting-data"], [194, null]], "8. Pixel diffusion LightningModule": [[305, "pixel-diffusion-lightningmodule"], [308, null]], "8. Ray Actors": [[3, "ray-actors"], [159, "ray-actors"], [168, null]], "8. Ray Train in Production": [[13, "ray-train-in-production"]], "8. Ray-prepared DataLoader": [[331, "ray-prepared-dataloader"], [333, "ray-prepared-dataloader"]], "8. Reverse diffusion helper": [[312, "reverse-diffusion-helper"], [317, null]], "8. Summary": [[275, "summary"], [282, "summary"]], "8. Test": [[66, "test"], [77, null]], "8. Train/validation split": [[338, "train-validation-split"], [341, "train-validation-split"]], "8. Troubleshooting": [[53, "troubleshooting"], [63, null]], "8. Upcoming Features in Ray Data": [[9, "upcoming-features-in-ray-data"], [176, "upcoming-features-in-ray-data"], [183, "upcoming-features-in-ray-data"]], "9. Clean up": [[53, "clean-up"], [64, null]], "9. Cleanup": [[66, "cleanup"], [78, null]], "9. Configure XGBoost and build the Trainer": [[325, "configure-xgboost-and-build-the-trainer"], [328, "configure-xgboost-and-build-the-trainer"]], "9. Inspect a DataLoader batch": [[338, "inspect-a-dataloader-batch"], [341, "inspect-a-dataloader-batch"]], "9. Launch distributed training with Ray Train": [[318, "launch-distributed-training-with-ray-train"], [322, "launch-distributed-training-with-ray-train"]], "9. PositionalEncoding and Transformer model": [[331, "positionalencoding-and-transformer-model"], [334, null]], "9. Ray Data in production": [[10, "ray-data-in-production"], [184, "ray-data-in-production"], [195, null]], "9. Ray Train train_loop (Lightning + Ray integration)": [[305, "ray-train-train-loop-lightning-ray-integration"], [309, null]], "9. Sample an action from the trained policy": [[312, "sample-an-action-from-the-trained-policy"], [317, "sample-an-action-from-the-trained-policy"]], "API Endpoints": [[146, "api-endpoints"]], "Activity: Update the training loop to compute the area under the curve of ROC (AUROC)": [[13, "activity-update-the-training-loop-to-compute-the-area-under-the-curve-of-roc-auroc"]], "Adding New Notebooks or Courses": [[0, "adding-new-notebooks-or-courses"]], "Additional Setup (Optional)": [[133, "additional-setup-optional"], [136, "additional-setup-optional"]], "Advanced LLM Features with Ray Serve LLM": [[125, null], [126, null]], "Advanced Topics: Monitoring & Optimization": [[117, "advanced-topics-monitoring-optimization"], [123, null]], "After navigating to a specific Anyscale Workspace, you can submit your main python script as a Anyscale Job.": [[89, "after-navigating-to-a-specific-anyscale-workspace-you-can-submit-your-main-python-script-as-a-anyscale-job"], [104, "after-navigating-to-a-specific-anyscale-workspace-you-can-submit-your-main-python-script-as-a-anyscale-job"]], "Aggregations": [[15, "aggregations"]], "Annotated experiment table": [[14, "annotated-experiment-table"]], "Anyscale 101 Learning Path": [[91, "anyscale-101-learning-path"]], "Anyscale Administrator Overview": [[17, null], [18, null]], "Anyscale For Admins": [[348, "anyscale-for-admins"]], "Anyscale Getting Started": [[348, "anyscale-getting-started"]], "Anyscale Observability": [[137, "anyscale-observability"], [140, null]], "Anyscale Projects": [[87, "anyscale-projects"], [99, "anyscale-projects"]], "Anyscale Ray Serve Observability": [[142, "anyscale-ray-serve-observability"], [145, "anyscale-ray-serve-observability"]], "Anyscale Service Lifecycle": [[90, "anyscale-service-lifecycle"], [106, "anyscale-service-lifecycle"]], "Apache Arrow": [[8, "apache-arrow"], [169, "apache-arrow"], [170, "apache-arrow"]], "Applications": [[11, "applications"], [196, "applications"], [199, "applications"]], "Architecture": [[146, "architecture"], [244, "architecture"], [247, "architecture"], [262, null], [288, "architecture"], [292, "architecture"], [302, null]], "Architecture Overview": [[247, null], [288, null]], "Architecture### Import librariesIn addition to ray and serve, we also import FastAPI to create webservice and Hugging Face transformers to download ML models.# Import ray serve and FastAPI librariesimport rayfrom ray import servefrom fastapi import FastAPI# library for pre-trained modelsfrom transformers import pipeline": [[286, null], [287, null]], "ArchitectureArchitecture Diagram": [[245, null], [246, null]], "Available Endpoints": [[146, "available-endpoints"]], "Batch Inference Class": [[244, "batch-inference-class"], [253, "batch-inference-class"], [264, null]], "Batch Inference ClassMany machine learning models are optimized for processing a batch of inputs at once. When working with a large dataset, there could be many batches of data. Instead of loading machine learning models repeatedly to run each batch of data, you want to spin up a number of actor processes that are initialized once with your model and reused to process multiple batches. To implement this, you can use the map_batches API with a \u201cCallable\u201d class method that implements:- __init__: Initialize any expensive state.- __call__: Perform the stateful transformation.In this example, a lightweight sentence transformer model, all-MiniLM-L6-v2 is used to generate embeddings of text data.": [[251, null], [252, null]], "Batch Inference with Ray Data": [[243, null], [243, "id1"], [244, null], [261, null]], "Batch Inference with Ray Data\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb Launch Locally: You can run this notebook locally.\ud83d\ude80 Launch on Cloud: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)This example shows how to do batch inference with Ray Data.Batch inference with Ray Data enables you to efficiently generate predictions from machine learning models on large datasets by processing multiple data points at once. Instead of running inference on one row at a time, which can be slow and resource-inefficient, batch inference leverages vectorized computation and parallelism to maximize throughput. This is especially useful when working with modern deep learning models, which are optimized for batch processing on CPUs, GPUs, or Apple Silicon devices.The typical workflow begins by loading your dataset\u2014such as a public dataset from Hugging Face\u2014into a Ray Dataset. Ray Data can automatically partition the data for parallel processing, or you can repartition it explicitly to control the number of data blocks. Once the data is loaded, you define a callable class (such as a text embedding model) that loads the machine learning model in its constructor and implements a __call__ method to process each batch. Ray Data\u2019s map_batches API is then used to apply this callable to each batch of data, with options to control concurrency and resource allocation (e.g., number of GPUs).This approach allows you to spin up multiple concurrent model instances, each processing different batches of data in parallel. The result is a significant speedup in inference time, especially for large datasets. After inference, you can materialize the results, inspect the output, and shut down the Ray cluster to free up resources. Batch inference with Ray Data is scalable, flexible, and integrates seamlessly with modern ML workflows, making it a powerful tool for production and research environments alike.": [[241, null], [242, null]], "Batch Processing": [[8, "batch-processing"], [169, "batch-processing"], [173, "batch-processing"]], "Benefits of using Anyscale Services": [[90, "benefits-of-using-anyscale-services"], [106, "benefits-of-using-anyscale-services"]], "Build and load our model on a single GPU": [[13, "build-and-load-our-model-on-a-single-gpu"]], "Building a FastAPI Web Service and Deploying a Model": [[291, null]], "Challenges in LLM Serving": [[108, "challenges-in-llm-serving"], [109, "challenges-in-llm-serving"], [113, null]], "Challenges with JVM": [[8, "challenges-with-jvm"], [169, "challenges-with-jvm"], [173, "challenges-with-jvm"]], "Clean Up": [[84, "clean-up"], [85, "clean-up"], [86, "clean-up"], [96, "clean-up"], [97, "clean-up"], [98, "clean-up"]], "Clean up": [[16, "clean-up"]], "Cloning/Duplicating Resources": [[87, "cloning-duplicating-resources"], [99, "cloning-duplicating-resources"]], "Cloud": [[88, "cloud"], [101, "cloud"]], "Collaborating with Your Team": [[91, "collaborating-with-your-team"]], "Composing Deployments": [[16, "composing-deployments"]], "Compute by Function": [[8, "compute-by-function"], [169, "compute-by-function"], [171, "compute-by-function"]], "Conclusion: Next Steps": [[125, "conclusion-next-steps"], [132, null]], "Concurrency Optimization Strategies": [[117, "concurrency-optimization-strategies"], [123, "concurrency-optimization-strategies"]], "Configuration Breakdown": [[117, "configuration-breakdown"], [120, "configuration-breakdown"]], "Configuration for Medium-Sized Models": [[117, "configuration-for-medium-sized-models"], [120, "configuration-for-medium-sized-models"]], "Configure Ray Serve LLM with LoRA": [[125, "configure-ray-serve-llm-with-lora"], [128, "configure-ray-serve-llm-with-lora"]], "Configure persistent storage": [[13, "configure-persistent-storage"]], "Configure scale and GPUs": [[13, "configure-scale-and-gpus"]], "Configure the Worker Node(s)": [[82, "configure-the-worker-node-s"], [94, "configure-the-worker-node-s"]], "Content Used": [[89, null], [103, null]], "Convert the Hugging Face dataset to a Ray Dataset": [[250, "convert-the-hugging-face-dataset-to-a-ray-dataset"]], "Convert to Pandas DataFrame": [[267, "convert-to-pandas-dataframe"], [274, "convert-to-pandas-dataframe"]], "Convert to Ray Dataset": [[267, "convert-to-ray-dataset"], [271, null]], "Course Welcome and Overview": [[1, "course-welcome-and-overview"], [151, "course-welcome-and-overview"], [152, "course-welcome-and-overview"]], "Create a Second Dataset": [[267, "create-a-second-dataset"], [271, "create-a-second-dataset"]], "Create a batch data and call the model": [[244, "create-a-batch-data-and-call-the-model"], [256, "create-a-batch-data-and-call-the-model"], [265, null]], "Create a batch data and call the modelDefine a Ray Data map_batches function to embed text using the SentenceTransformer model. This function will be applied to each batch of data in the Ray Data dataset. It will take a batch of sentences, encode them into embeddings, and return the batch with the embeddings added.Showcasing two options of to do batch inference based on if the ray cluster has have GPU nodes or if it has just CPU nodes. The second option also works on a local ray cluster on an Apple Silicon Mac with MPS.# setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPSworker_device = \u201ccpu\u201d # or \u201ccuda\u201d if you have a nvidia gpu on worker nodes# batch_size should be set based on VRAM if worker_device == \u201ccuda\u201d: # if you have a nvidia gpu on worker nodes    # adjust batch_size based on the VRAM available on the GPU    ds = ds.map_batches(TextEmbedder, num_gpus=1, concurrency=2, batch_size=64) # 2 nodes with 1 GPU eachelse:    ds = ds.map_batches(TextEmbedder, concurrency=2, batch_size=64) # either cpu or mps (on a mac)": [[254, null], [255, null]], "Create custom security group": [[17, "create-custom-security-group"], [22, "create-custom-security-group"]], "Creating Anyscale Resources": [[28, "creating-anyscale-resources"], [30, "creating-anyscale-resources"], [35, "creating-anyscale-resources"], [39, "creating-anyscale-resources"], [43, "creating-anyscale-resources"], [45, "creating-anyscale-resources"]], "Creating a Compute Config": [[82, "creating-a-compute-config"], [94, "creating-a-compute-config"]], "Creating a Container Image": [[82, "creating-a-container-image"], [94, "creating-a-container-image"]], "Creating a Data Batch and Calling the Model": [[256, null]], "Custom batching using groupby": [[15, "custom-batching-using-groupby"]], "Custom batching using groupby and aggregations": [[9, "custom-batching-using-groupby-and-aggregations"], [176, "custom-batching-using-groupby-and-aggregations"], [182, "custom-batching-using-groupby-and-aggregations"]], "Customization": [[0, "customization"]], "Customizing autoscaling": [[16, "customizing-autoscaling"]], "Data Engineering Compute": [[8, "data-engineering-compute"], [169, "data-engineering-compute"], [171, "data-engineering-compute"]], "Data Pipeline Observability (Ray Data)": [[142, "data-pipeline-observability-ray-data"], [144, null]], "Data Processing and ML examples with Ray": [[260, null], [268, null], [276, null], [300, null]], "Data Processing with Ray Data": [[8, "data-processing-with-ray-data"], [169, "data-processing-with-ray-data"], [174, null], [267, null], [269, null]], "Data flow": [[8, "data-flow"], [169, "data-flow"], [175, "data-flow"]], "Data lakes": [[8, "data-lakes"], [169, "data-lakes"], [170, "data-lakes"]], "Data warehouses": [[8, "data-warehouses"], [169, "data-warehouses"], [170, "data-warehouses"]], "Databases": [[8, "databases"], [169, "databases"], [170, "databases"]], "Dataloaders": [[275, "dataloaders"], [280, "dataloaders"]], "Dataset": [[9, "dataset"], [176, "dataset"], [179, "dataset"]], "Decode Phase": [[108, "decode-phase"], [109, "decode-phase"], [111, "decode-phase"]], "Default settings for Ray Tune": [[14, "default-settings-for-ray-tune"]], "Defining a data loader": [[13, "defining-a-data-loader"]], "Defining the Batch Inference Class": [[253, null]], "Deploy a Medium-Sized LLM with Ray Serve LLM": [[117, null], [118, null]], "Deploy the model": [[292, "deploy-the-model"], [296, "deploy-the-model"], [303, "deploy-the-model"]], "Deploy the modelserve.run(MySentimentModel.bind()) # Bind the deployment to the Ray Serve runtimeDeploymentHandle(deployment=\u2019MySentimentModel\u2019)": [[293, null], [294, null]], "Deploying Applications with Services": [[91, "deploying-applications-with-services"]], "Deploying Our Model and Testing it": [[296, null]], "Deploying Pipelines with Jobs": [[91, "deploying-pipelines-with-jobs"]], "Deploying at scale": [[244, "deploying-at-scale"], [256, "deploying-at-scale"], [265, "deploying-at-scale"]], "Deploying at scale- The batch size for encoding can be adjusted based on the available memory and performance requirements.- The device parameter ensures that the model runs on the correct device (CPU, GPU, or MPS).- The concurrency parameter controls how many batches are processed in parallel. If there are 2 nodes with 1 GPU each or 1 node with 2 GPUs, then set concurrency = 2 and num_gpus=1.- map_batches() is a lazy function and not executed until needed (example, using take or show).Run inference on a batch of 128 rows. This will return a batch of 128 rows with the embeddings added to the caller\u2019s machine.# Run inference on a batch of 128 rows for testing.ds.take_batch(128)": [[254, "deploying-at-scale-the-batch-size-for-encoding-can-be-adjusted-based-on-the-available-memory-and-performance-requirements-the-device-parameter-ensures-that-the-model-runs-on-the-correct-device-cpu-gpu-or-mps-the-concurrency-parameter-controls-how-many-batches-are-processed-in-parallel-if-there-are-2-nodes-with-1-gpu-each-or-1-node-with-2-gpus-then-set-concurrency-2-and-num-gpus-1-map-batches-is-a-lazy-function-and-not-executed-until-needed-example-using-take-or-show-run-inference-on-a-batch-of-128-rows-this-will-return-a-batch-of-128-rows-with-the-embeddings-added-to-the-caller-s-machine-run-inference-on-a-batch-of-128-rows-for-testing-ds-take-batch-128"], [255, "deploying-at-scale-the-batch-size-for-encoding-can-be-adjusted-based-on-the-available-memory-and-performance-requirements-the-device-parameter-ensures-that-the-model-runs-on-the-correct-device-cpu-gpu-or-mps-the-concurrency-parameter-controls-how-many-batches-are-processed-in-parallel-if-there-are-2-nodes-with-1-gpu-each-or-1-node-with-2-gpus-then-set-concurrency-2-and-num-gpus-1-map-batches-is-a-lazy-function-and-not-executed-until-needed-example-using-take-or-show-run-inference-on-a-batch-of-128-rows-this-will-return-a-batch-of-128-rows-with-the-embeddings-added-to-the-caller-s-machine-run-inference-on-a-batch-of-128-rows-for-testing-ds-take-batch-128"]], "Deploying to Anyscale Services": [[117, "deploying-to-anyscale-services"], [122, null]], "Deployment": [[146, "deployment"]], "Deployment Example Structure": [[79, "deployment-example-structure"]], "Deployment Options: Virtual Machines vs. Kubernetes": [[24, null], [25, null]], "Deployments": [[11, "deployments"], [16, "deployments"], [196, "deployments"], [199, "deployments"]], "Developing in Anyscale Workspaces": [[91, "developing-in-anyscale-workspaces"]], "Disabling Notebook Execution and Outputs": [[0, "disabling-notebook-execution-and-outputs"]], "Distributed Computing Frameworks": [[8, "distributed-computing-frameworks"], [169, "distributed-computing-frameworks"], [173, null]], "Distributed Data-Parallel Training with Ray Train": [[202, "distributed-data-parallel-training-with-ray-train"], [212, "distributed-data-parallel-training-with-ray-train"]], "Distributed training with Ray Train, PyTorch and Hugging Face": [[275, null], [277, null]], "Distributed training with Ray Train, PyTorch and HuggingFace": [[260, "distributed-training-with-ray-train-pytorch-and-huggingface"], [268, "distributed-training-with-ray-train-pytorch-and-huggingface"], [276, "distributed-training-with-ray-train-pytorch-and-huggingface"], [300, "distributed-training-with-ray-train-pytorch-and-huggingface"]], "Diving deeper into Ray Tune concepts": [[7, "diving-deeper-into-ray-tune-concepts"], [229, "diving-deeper-into-ray-tune-concepts"], [233, "diving-deeper-into-ray-tune-concepts"]], "Dual-Subnet Architecture": [[17, "dual-subnet-architecture"], [22, "dual-subnet-architecture"]], "Enabling LLM Monitoring": [[117, "enabling-llm-monitoring"], [123, "enabling-llm-monitoring"]], "End of Module 01 \u00b7 Introduction to Ray Train": [[202, "end-of-module-01-introduction-to-ray-train"], [215, "end-of-module-01-introduction-to-ray-train"]], "Environment state and action": [[312, "environment-state-and-action"], [313, "environment-state-and-action"]], "Example": [[137, "example"], [141, null]], "Example Workflow": [[0, "example-workflow"]], "Example: Car type description": [[125, "example-car-type-description"], [129, "example-car-type-description"]], "Example: Code Assistant LoRA": [[125, "example-code-assistant-lora"], [128, "example-code-assistant-lora"]], "Example: Deploying LoRA Adapters": [[125, "example-deploying-lora-adapters"], [128, null]], "Example: Getting Structured JSON Output": [[125, "example-getting-structured-json-output"], [129, null]], "Example: Setting up Tool Calling": [[125, "example-setting-up-tool-calling"], [130, null]], "Example: Weather Assistant with Tool Calling": [[125, "example-weather-assistant-with-tool-calling"], [130, "example-weather-assistant-with-tool-calling"]], "Execution mode": [[9, "execution-mode"], [15, "execution-mode"], [176, "execution-mode"], [180, "execution-mode"]], "Exercise": [[7, "exercise"], [14, "exercise"], [229, "exercise"], [233, "exercise"]], "Expected Output": [[125, "expected-output"], [129, "expected-output"]], "Exploring the Anyscale Log Viewer": [[84, "exploring-the-anyscale-log-viewer"], [96, "exploring-the-anyscale-log-viewer"]], "Exploring the Anyscale Metrics Tab": [[84, "exploring-the-anyscale-metrics-tab"], [96, "exploring-the-anyscale-metrics-tab"]], "Exploring the Ray Dashboard": [[84, "exploring-the-ray-dashboard"], [96, "exploring-the-ray-dashboard"]], "FastAPI webservice and deploy a model": [[291, "fastapi-webservice-and-deploy-a-model"], [292, "fastapi-webservice-and-deploy-a-model"], [303, null]], "FastAPI webservice and deploy a modelFastAPI is used to create a webservice \u2018app\u2019 to accept HTTP requests.MySentimentModel class loads the ML model and defines predict function for online inference. @serve.deployment decorator defines the Ray Serve deployment.@app.get() is used to create a GET \u2018/predict\u2019 route. Similarly, @app.post() can be used POST requests. See https://docs.ray.io/en/latest/serve/http-guide.html for more details.In this example, application_logic() function is used to define a sample transformation or business logic that can be applied before sending the input to the ML model for inference. See inline comments for further explanation.### Scaling deploymentnum_replicas parameter sets the number of instances of the deployment. FastAPI and RayServe automatically load balances to send requests to each instance. There are more options to set the accelerator_type to GPU and even use fractional GPUs. See configuration options here: https://docs.ray.io/en/latest/serve/configure-serve-deployment.html .": [[289, null], [290, null]], "Features": [[0, "features"]], "File based shuffle on read": [[9, "file-based-shuffle-on-read"], [15, "file-based-shuffle-on-read"], [176, "file-based-shuffle-on-read"], [182, "file-based-shuffle-on-read"]], "Filter Ray Dataset": [[267, "filter-ray-dataset"], [272, null]], "Forward process: adding noise": [[305, "forward-process-adding-noise"], [306, "forward-process-adding-noise"]], "Foundations": [[348, "foundations"]], "General-Purpose Distributed Computing": [[8, "general-purpose-distributed-computing"], [169, "general-purpose-distributed-computing"], [173, "general-purpose-distributed-computing"]], "Get User Profile": [[146, "get-user-profile"]], "Getting Started": [[91, "getting-started"]], "Getting Started with Ray Serve LLM": [[108, "getting-started-with-ray-serve-llm"], [109, "getting-started-with-ray-serve-llm"], [115, null]], "Getting started": [[7, "getting-started"], [14, "getting-started"], [229, "getting-started"], [233, "getting-started"]], "How Navigation Works": [[0, "how-navigation-works"]], "How Other Sizes Differ": [[117, "how-other-sizes-differ"], [124, "how-other-sizes-differ"]], "How Resources are defined": [[17, "how-resources-are-defined"], [20, "how-resources-are-defined"]], "How to Choose an LLM?": [[125, "how-to-choose-an-llm"], [131, null]], "How to migrate this computer vision workload to a distributed setup using Ray on Anyscale": [[338, "how-to-migrate-this-computer-vision-workload-to-a-distributed-setup-using-ray-on-anyscale"], [339, "how-to-migrate-this-computer-vision-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this diffusion-policy workload to a distributed setup using Ray on Anyscale": [[305, "how-to-migrate-this-diffusion-policy-workload-to-a-distributed-setup-using-ray-on-anyscale"], [306, "how-to-migrate-this-diffusion-policy-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this recommendation system workload to a distributed setup using Ray on Anyscale": [[318, "how-to-migrate-this-recommendation-system-workload-to-a-distributed-setup-using-ray-on-anyscale"], [319, "how-to-migrate-this-recommendation-system-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this tabular workload to a distributed setup using Ray on Anyscale": [[325, "how-to-migrate-this-tabular-workload-to-a-distributed-setup-using-ray-on-anyscale"], [326, "how-to-migrate-this-tabular-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this time-series workload to a distributed multi-node setup using Ray on Anyscale": [[331, "how-to-migrate-this-time-series-workload-to-a-distributed-multi-node-setup-using-ray-on-anyscale"], [332, "how-to-migrate-this-time-series-workload-to-a-distributed-multi-node-setup-using-ray-on-anyscale"]], "How to scale this policy learning workload using Ray on Anyscale": [[312, "how-to-scale-this-policy-learning-workload-using-ray-on-anyscale"], [313, "how-to-scale-this-policy-learning-workload-using-ray-on-anyscale"]], "How we use Ray AI Libraries for this task": [[12, "how-we-use-ray-ai-libraries-for-this-task"]], "Hyperparameter tune the PyTorch model using Ray Tune": [[7, "hyperparameter-tune-the-pytorch-model-using-ray-tune"], [229, "hyperparameter-tune-the-pytorch-model-using-ray-tune"], [233, "hyperparameter-tune-the-pytorch-model-using-ray-tune"]], "Import Libraries": [[244, "import-libraries"], [247, "import-libraries"], [262, "import-libraries"]], "Import Librariesimport rayimport torchfrom typing import Dictimport numpy as npfrom sentence_transformers import SentenceTransformer # huggingface sentence transformersfrom datasets import load_dataset # huggingface datasets": [[245, "import-librariesimport-rayimport-torchfrom-typing-import-dictimport-numpy-as-npfrom-sentence-transformers-import-sentencetransformer-huggingface-sentence-transformersfrom-datasets-import-load-dataset-huggingface-datasets"], [246, "import-librariesimport-rayimport-torchfrom-typing-import-dictimport-numpy-as-npfrom-sentence-transformers-import-sentencetransformer-huggingface-sentence-transformersfrom-datasets-import-load-dataset-huggingface-datasets"]], "Import libraries": [[288, "import-libraries"], [292, "import-libraries"], [302, "import-libraries"]], "Import ray serve and FastAPI libraries": [[288, "import-ray-serve-and-fastapi-libraries"]], "Improving Concurrency": [[117, "improving-concurrency"], [123, "improving-concurrency"]], "In-memory data formats": [[8, "in-memory-data-formats"], [169, "in-memory-data-formats"], [170, "in-memory-data-formats"]], "Inference: ranking items per user": [[318, "inference-ranking-items-per-user"], [319, "inference-ranking-items-per-user"]], "Initialize Ray and Load a Dataset": [[267, "initialize-ray-and-load-a-dataset"], [270, "initialize-ray-and-load-a-dataset"]], "Input: Images as tensors": [[305, "input-images-as-tensors"], [306, "input-images-as-tensors"]], "Input: user\u2013item\u2013rating triples": [[318, "input-useritemrating-triples"], [319, "input-useritemrating-triples"]], "Inputs": [[338, "inputs"], [339, "inputs"]], "Inspecting the features of the NYC taxi dataset": [[12, "inspecting-the-features-of-the-nyc-taxi-dataset"]], "Installation": [[0, "installation"], [260, "installation"], [268, "installation"], [276, "installation"], [300, "installation"]], "Integrating with FastAPI": [[16, "integrating-with-fastapi"]], "Intro to Ray Data": [[15, null]], "Intro to Ray Data:  Ray Data + Unstructured Data": [[10, null], [184, null], [185, null]], "Intro to Ray Serve": [[16, null]], "Intro to Ray Tune": [[7, "intro-to-ray-tune"], [14, null], [229, "intro-to-ray-tune"], [233, "intro-to-ray-tune"]], "Introduction": [[81, "introduction"], [82, "introduction"], [83, "introduction"], [84, "introduction"], [93, "introduction"], [94, "introduction"], [95, "introduction"], [96, "introduction"]], "Introduction to Ray Core (Advancement): Object store, Tasks, Actors": [[3, null], [159, null], [160, null]], "Introduction to Ray Core: Getting Started": [[2, null], [153, null], [154, null]], "Introduction to Ray Data: Industry Landscape": [[8, null], [169, null], [170, null]], "Introduction to Ray Data: Ray Data + Structured Data": [[9, null], [176, null], [177, null]], "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving": [[108, null], [109, null], [110, null]], "Introduction to Ray Serve with PyTorch": [[11, null], [196, null], [197, null]], "Introduction to Ray Train": [[13, null]], "Introduction to Ray Train + PyTorch": [[5, null]], "Introduction to Ray Train: Ray Train + PyTorch Lightning": [[6, null], [235, null], [236, null]], "Introduction to Ray Tune": [[7, null], [229, null], [230, null]], "Introduction to Ray: Developer": [[1, null], [151, null], [152, null]], "Introduction to the Ray AI Libraries": [[12, null]], "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model": [[4, null], [147, null], [148, null]], "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster": [[43, null], [44, null]], "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster": [[66, null], [67, null]], "Introduction: Deploy Anyscale Ray on AWS EC2 Instances": [[28, null], [29, null]], "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster": [[53, null], [54, null]], "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)": [[35, null], [36, null]], "Introduction: Why Anyscale?": [[91, "introduction-why-anyscale"]], "Join Two Ray Datasets": [[267, "join-two-ray-datasets"], [273, null]], "Key Benefits": [[125, "key-benefits"], [125, "id1"], [125, "id3"], [128, "key-benefits"], [129, "key-benefits"], [130, "key-benefits"]], "Key Components": [[117, "key-components"], [120, "key-components"]], "Key Concepts and Optimizations": [[108, "key-concepts-and-optimizations"], [109, "key-concepts-and-optimizations"], [112, null]], "Key Features": [[89, "key-features"], [103, "key-features"]], "Key Functions": [[17, "key-functions"], [19, "key-functions"]], "Key Ray Serve Features": [[16, "key-ray-serve-features"]], "Key Takeaways": [[108, "key-takeaways"], [109, "key-takeaways"], [116, null], [117, "key-takeaways"], [124, "key-takeaways"], [125, "key-takeaways"], [132, "key-takeaways"]], "Key characteristics of Anyscale Services": [[86, "key-characteristics-of-anyscale-services"], [98, "key-characteristics-of-anyscale-services"]], "Key points": [[13, "key-points"]], "Labels": [[338, "labels"], [339, "labels"]], "Lakehouses": [[8, "lakehouses"], [169, "lakehouses"], [170, "lakehouses"]], "Last Updated 6/19": [[91, null]], "Launch Grafana": [[133, "launch-grafana"], [136, "launch-grafana"]], "Launching Ray Serve": [[117, "launching-ray-serve"], [121, "launching-ray-serve"]], "Launching a Anyscale Workspace": [[80, "launching-a-anyscale-workspace"], [92, "launching-a-anyscale-workspace"]], "Launching a Web Application using Ray Serve": [[142, "launching-a-web-application-using-ray-serve"], [145, "launching-a-web-application-using-ray-serve"]], "Launching a distributed training job with a TorchTrainer.": [[13, "launching-a-distributed-training-job-with-a-torchtrainer"]], "Launching the Service": [[117, "launching-the-service"], [122, "launching-the-service"]], "Learn More": [[125, "learn-more"], [125, "id2"], [125, "id4"], [128, "learn-more"], [129, "learn-more"], [130, "learn-more"]], "Learning Approach": [[125, "learning-approach"], [127, "learning-approach"]], "Learning Path Overview and Objectives": [[91, "learning-path-overview-and-objectives"]], "Library Imports": [[267, "library-imports"], [270, null]], "Llm Serving": [[348, "llm-serving"]], "Load a dataset": [[244, "load-a-dataset"], [250, "load-a-dataset"], [263, null]], "Load a datasetLoad a dataset from hugging face or local and convert into Ray Dataset. A Ray cluster automatically initialized on local or on Anyscale platform. You can also use ray.init() To explicitly create or connect to an existing Ray cluster.https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html#ray.init# load a Hugging Face datasethf_dataset = load_dataset(\u201ccardiffnlp/tweet_eval\u201d, \u201csentiment\u201d, split=\u201dtrain\u201d)# Convert the Hugging Face dataset to a Ray Datasetds = ray.data.from_huggingface(hf_dataset).repartition(2) # repartition to 2 blocks for parallel processing. Not necessary if already partitioned due to the size of the dataset.": [[248, null], [249, null]], "Loading a Dataset": [[250, null]], "Loading and visualizing MNIST data": [[13, "loading-and-visualizing-mnist-data"]], "Local Deployment & Inference": [[117, "local-deployment-inference"], [121, null]], "Local Development": [[146, "local-development"]], "Local IDE (VSCode / Cursor)": [[81, "local-ide-vscode-cursor"], [93, "local-ide-vscode-cursor"]], "Logging Configuration": [[142, "logging-configuration"], [145, "logging-configuration"]], "Machine Learning and AI Compute": [[8, "machine-learning-and-ai-compute"], [169, "machine-learning-and-ai-compute"], [171, "machine-learning-and-ai-compute"]], "Managing Dependencies": [[1, "managing-dependencies"], [151, "managing-dependencies"], [152, "managing-dependencies"]], "Materializing Data": [[15, "materializing-data"]], "Model Recommendations by Use Case": [[125, "model-recommendations-by-use-case"], [131, "model-recommendations-by-use-case"]], "Model Selection Framework": [[125, "model-selection-framework"], [131, "model-selection-framework"]], "Model Size Comparison": [[117, "model-size-comparison"], [119, "model-size-comparison"]], "Model: embedding-based matrix factorization": [[318, "model-embedding-based-matrix-factorization"], [319, "model-embedding-based-matrix-factorization"]], "More Advanced Topics": [[125, "more-advanced-topics"], [132, "more-advanced-topics"]], "More about Datasets": [[15, "more-about-datasets"]], "Multi-Actor Ray Serve Tracing Example": [[146, null]], "Next Steps": [[108, "next-steps"], [109, "next-steps"], [116, "next-steps"], [117, "next-steps"], [124, "next-steps"], [125, "next-steps"], [132, "next-steps"]], "Note on the Checkpoint Lifecycle": [[202, "note-on-the-checkpoint-lifecycle"], [212, "note-on-the-checkpoint-lifecycle"]], "Note that this does not mutate the original Dataset.": [[259, "note-that-this-does-not-mutate-the-original-dataset"]], "Notebook": [[81, "notebook"], [93, "notebook"]], "Now launch a Ray worker node in the terminal:": [[133, "now-launch-a-ray-worker-node-in-the-terminal"], [136, "now-launch-a-ray-worker-node-in-the-terminal"]], "Observability": [[348, "observability"]], "Observability Introduction": [[133, null], [134, null]], "Observability Overview": [[133, "observability-overview"], [135, null]], "On Ray Data vs Spark": [[8, "on-ray-data-vs-spark"], [169, "on-ray-data-vs-spark"], [174, "on-ray-data-vs-spark"]], "Online Model Serving with Ray Serve": [[285, null], [285, "id1"], [292, null], [301, null]], "Online Model Serving with Ray Serve\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb Launch Locally: You can run this notebook locally.\ud83d\ude80 Launch on Cloud: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)Model serving is the process of deploying machine learning models to production so that they can be accessed and used by applications or users. It involves creating an API or interface that allows users to send requests to the modeland receive predictions in response. There are several libraries and frameworks available for model serving, each with its own features and capabilities. In this notebook, we showcase Ray Serve and FastAPI to deploy a sentiment analysismachine learning (ML) model.": [[283, null], [284, null]], "Option 1: Create a New VPC": [[17, "option-1-create-a-new-vpc"], [22, "option-1-create-a-new-vpc"]], "Option 2: Use Existing VPC": [[17, "option-2-use-existing-vpc"], [22, "option-2-use-existing-vpc"]], "Organization": [[88, "organization"], [101, "organization"]], "Our Example: Llama-3.1-70B": [[117, "our-example-llama-3-1-70b"], [119, "our-example-llama-3-1-70b"]], "Out of memory errors": [[244, "out-of-memory-errors"], [259, "out-of-memory-errors"], [266, "out-of-memory-errors"]], "Out of memory errorsGPU (or MPS or CPU) memory has to keep the machine learning model and the batch of data in memory during the inference. If the batch_size is too large, it can run out of memory and throw out of memory errors. In that case, reduce the batch_size.### Shutdown Ray cluster# avoids collisons with other notebooks running ray jobs on the same machineray.shutdown()": [[257, "out-of-memory-errorsgpu-or-mps-or-cpu-memory-has-to-keep-the-machine-learning-model-and-the-batch-of-data-in-memory-during-the-inference-if-the-batch-size-is-too-large-it-can-run-out-of-memory-and-throw-out-of-memory-errors-in-that-case-reduce-the-batch-size-shutdown-ray-cluster-avoids-collisons-with-other-notebooks-running-ray-jobs-on-the-same-machineray-shutdown"], [258, "out-of-memory-errorsgpu-or-mps-or-cpu-memory-has-to-keep-the-machine-learning-model-and-the-batch-of-data-in-memory-during-the-inference-if-the-batch-size-is-too-large-it-can-run-out-of-memory-and-throw-out-of-memory-errors-in-that-case-reduce-the-batch-size-shutdown-ray-cluster-avoids-collisons-with-other-notebooks-running-ray-jobs-on-the-same-machineray-shutdown"]], "Outline": [[243, "outline"], [244, "outline"], [261, "outline"], [275, "outline"], [277, "outline"], [285, "outline"], [292, "outline"], [301, "outline"]], "Outline    Architecture    Import Libraries    FastAPI service to accept HTTP requests and scaling with Ray Serve    Simulate Client: Send test requests    Shutdown the Serve app and the ray cluster</ul></div>": [[283, "outline-architecture-import-libraries-fastapi-service-to-accept-http-requests-and-scaling-with-ray-serve-simulate-client-send-test-requests-shutdown-the-serve-app-and-the-ray-cluster"], [284, "outline-architecture-import-libraries-fastapi-service-to-accept-http-requests-and-scaling-with-ray-serve-simulate-client-send-test-requests-shutdown-the-serve-app-and-the-ray-cluster"]], "Outline of the notebook": [[267, "outline-of-the-notebook"], [269, "outline-of-the-notebook"]], "Outline<b>In this notebook, we go through a typical ML batch inference workflow:</b>    Architecture    Import Libraries    Load a public dataset from Hugging Face and move it into Ray Data object store.    Batch Inference Class        - Create a Ray actor class to load a ML model. In this example, we use SentenceTransformer library from Hugging Face to load a sentence embedding model.    Create batches of data to do inference.    Deploying at Scale    Inference on the entire dataset    Out of memory errors    Summary</ul></div>": [[241, "outlinein-this-notebook-we-go-through-a-typical-ml-batch-inference-workflow-architecture-import-libraries-load-a-public-dataset-from-hugging-face-and-move-it-into-ray-data-object-store-batch-inference-class-create-a-ray-actor-class-to-load-a-ml-model-in-this-example-we-use-sentencetransformer-library-from-hugging-face-to-load-a-sentence-embedding-model-create-batches-of-data-to-do-inference-deploying-at-scale-inference-on-the-entire-dataset-out-of-memory-errors-summary"], [242, "outlinein-this-notebook-we-go-through-a-typical-ml-batch-inference-workflow-architecture-import-libraries-load-a-public-dataset-from-hugging-face-and-move-it-into-ray-data-object-store-batch-inference-class-create-a-ray-actor-class-to-load-a-ml-model-in-this-example-we-use-sentencetransformer-library-from-hugging-face-to-load-a-sentence-embedding-model-create-batches-of-data-to-do-inference-deploying-at-scale-inference-on-the-entire-dataset-out-of-memory-errors-summary"]], "Outlook": [[17, "outlook"], [18, "outlook"]], "Outlook:  Ray Data in Production": [[15, "outlook-ray-data-in-production"]], "Overview": [[146, "overview"]], "Overview: Advanced Features Preview": [[125, "overview-advanced-features-preview"], [127, null]], "Overview: Why Medium-Sized Models?": [[117, "overview-why-medium-sized-models"], [119, null]], "Part 1. Creating and Submitting your first job": [[85, "part-1-creating-and-submitting-your-first-job"], [89, "part-1-creating-and-submitting-your-first-job"], [97, "part-1-creating-and-submitting-your-first-job"], [104, null]], "Part 1: Starting your first Anyscale Service": [[86, "part-1-starting-your-first-anyscale-service"], [90, "part-1-starting-your-first-anyscale-service"], [98, "part-1-starting-your-first-anyscale-service"], [107, null]], "Part 2. Automation and Scheduling": [[85, "part-2-automation-and-scheduling"], [89, "part-2-automation-and-scheduling"], [97, "part-2-automation-and-scheduling"], [105, null]], "Practical Selection Process": [[125, "practical-selection-process"], [131, "practical-selection-process"]], "Prefill Phase": [[108, "prefill-phase"], [109, "prefill-phase"], [111, "prefill-phase"]], "Preprocessing with a Tokenizer": [[267, "preprocessing-with-a-tokenizer"], [274, null]], "Prerequisites": [[1, "prerequisites"], [28, "prerequisites"], [29, "prerequisites"], [35, "prerequisites"], [37, null], [43, "prerequisites"], [44, "prerequisites"], [53, "prerequisites"], [55, null], [66, "prerequisites"], [68, null], [79, "prerequisites"], [117, "prerequisites"], [121, "prerequisites"], [142, "prerequisites"], [143, "prerequisites"], [146, "prerequisites"], [151, "prerequisites"], [152, "prerequisites"]], "Prerequisites and Assumptions": [[133, "prerequisites-and-assumptions"], [136, "prerequisites-and-assumptions"]], "Projects": [[88, "projects"], [101, "projects"]], "Providers": [[43, "providers"], [44, "providers"]], "Publishing": [[0, "publishing"]], "Purpose": [[17, "purpose"], [19, "purpose"]], "Pytorch Lightning": [[348, "pytorch-lightning"]], "Query the deployed model": [[296, "query-the-deployed-model"]], "Ray Ai Libs": [[348, "ray-ai-libs"]], "Ray Core": [[348, "ray-core"]], "Ray Data": [[348, "ray-data"]], "Ray Data Batch Inference": [[348, "ray-data-batch-inference"]], "Ray Data Logs": [[142, "ray-data-logs"], [144, "ray-data-logs"]], "Ray Data Metrics": [[142, "ray-data-metrics"], [144, "ray-data-metrics"]], "Ray Data Processing": [[348, "ray-data-processing"]], "Ray Distributed Training": [[348, "ray-distributed-training"]], "Ray Enablement Content": [[348, null]], "Ray Enablement Content: Jupyter Book Publishing": [[0, null]], "Ray Observability": [[137, "ray-observability"], [139, null]], "Ray Serve": [[8, "ray-serve"], [169, "ray-serve"], [175, null], [348, "ray-serve"]], "Ray Serve Alerts": [[142, "ray-serve-alerts"], [145, "ray-serve-alerts"]], "Ray Serve LLM + Anyscale Architecture": [[108, "ray-serve-llm-anyscale-architecture"], [109, "ray-serve-llm-anyscale-architecture"], [114, null]], "Ray Serve Logs": [[142, "ray-serve-logs"], [145, "ray-serve-logs"]], "Ray Serve Metrics": [[142, "ray-serve-metrics"], [145, "ray-serve-metrics"]], "Ray Serve Online Serving": [[348, "ray-serve-online-serving"]], "Ray Serve Tracing (Anyscale Only)": [[142, "ray-serve-tracing-anyscale-only"], [145, "ray-serve-tracing-anyscale-only"]], "Ray Serve vs Ray Data": [[8, "ray-serve-vs-ray-data"], [169, "ray-serve-vs-ray-data"], [175, "ray-serve-vs-ray-data"]], "Ray Train": [[348, "ray-train"]], "Ray Tune": [[348, "ray-tune"]], "Ray Workloads Data Dashboard": [[142, "ray-workloads-data-dashboard"], [144, "ray-workloads-data-dashboard"]], "Ray and Anyscale Observability Introduction": [[137, null], [138, null]], "Ray and Anyscale Observability in Detail": [[142, null], [143, null]], "Recap": [[12, "recap"]], "Register New User": [[146, "register-new-user"]], "Related Examples": [[117, "related-examples"], [119, "related-examples"]], "Related Examples & Templates": [[117, "related-examples-templates"], [124, "related-examples-templates"]], "Replicas": [[11, "replicas"], [196, "replicas"], [199, "replicas"]], "Reporting metrics": [[13, "reporting-metrics"]], "Request Flow": [[146, "request-flow"]], "Requirements": [[43, "requirements"], [44, "requirements"]], "Resources": [[108, "resources"], [109, "resources"], [116, "resources"], [117, "resources"], [124, "resources"], [125, "resources"], [132, "resources"]], "Reverse diffusion: sampling new images": [[305, "reverse-diffusion-sampling-new-images"], [306, "reverse-diffusion-sampling-new-images"]], "Run a simple Data Pipeline": [[142, "run-a-simple-data-pipeline"], [144, "run-a-simple-data-pipeline"]], "Run inference on a batch of 128 rows for testing.": [[256, "run-inference-on-a-batch-of-128-rows-for-testing"]], "Run inference on the entire dataset": [[244, "run-inference-on-the-entire-dataset"], [259, "run-inference-on-the-entire-dataset"], [259, "id1"], [266, null]], "Run inference on the entire datasetExecute and materialize this dataset into object store memory. This operation will trigger execution of the lazy transformations performed on this dataset. The embedding model \u2018TextEmbedder\u2019 in map_batches() is called on the entire dataset.# Run inference on the entire dataset# Note that this does not mutate the original Dataset.materialized_ds = ds.materialize()# metadata after inferenceprint(\u2018** Original dataset:\u2019, ds)print(\u2018\\n** Materialized dataset:\u2019, materialized_ds)# Show a few rows of the materialized dataset with embeddingsmaterialized_ds.show(3)": [[257, null], [258, null]], "Running Inference on Anyscale": [[117, "running-inference-on-anyscale"], [122, "running-inference-on-anyscale"]], "Running inference on the entire dataset": [[259, null]], "Running this notebook": [[12, "running-this-notebook"]], "Sample Requests": [[146, "sample-requests"]], "Saving a checkpoint in a local directory": [[13, "saving-a-checkpoint-in-a-local-directory"]], "Scaling deployment": [[291, "scaling-deployment"], [292, "scaling-deployment"], [303, "scaling-deployment"]], "Scheduling the training loop on a single GPU": [[13, "scheduling-the-training-loop-on-a-single-gpu"]], "Sending Requests": [[117, "sending-requests"], [121, "sending-requests"]], "Setting Up Local Ray Observability": [[133, "setting-up-local-ray-observability"], [136, null]], "Setting Up a Local Ray server using Jupyter Notebook": [[1, "setting-up-a-local-ray-server-using-jupyter-notebook"], [151, "setting-up-a-local-ray-server-using-jupyter-notebook"], [152, "setting-up-a-local-ray-server-using-jupyter-notebook"]], "Setting up Ray Serve LLM": [[117, "setting-up-ray-serve-llm"], [120, null]], "Setting up the Configuration File": [[117, "setting-up-the-configuration-file"], [122, "setting-up-the-configuration-file"]], "Setup and Installation": [[146, "setup-and-installation"]], "Show a few rows of the materialized dataset with embeddings": [[259, "show-a-few-rows-of-the-materialized-dataset-with-embeddings"]], "Shuffle all rows globally": [[9, "shuffle-all-rows-globally"], [176, "shuffle-all-rows-globally"], [182, "shuffle-all-rows-globally"]], "Shuffle rows globally": [[15, "shuffle-rows-globally"]], "Shuffling block order": [[9, "shuffling-block-order"], [15, "shuffling-block-order"], [176, "shuffling-block-order"], [182, "shuffling-block-order"]], "Shuffling data": [[9, "shuffling-data"], [15, "shuffling-data"], [176, "shuffling-data"], [182, "shuffling-data"]], "Shutdown Ray": [[267, "shutdown-ray"], [274, "shutdown-ray"]], "Shutdown Ray cluster": [[244, "shutdown-ray-cluster"], [259, "shutdown-ray-cluster"], [266, "shutdown-ray-cluster"]], "Shutdown and Summary": [[299, null]], "Shutdown the Ray Serve instances and Ray Cluster": [[292, "shutdown-the-ray-serve-instances-and-ray-cluster"], [299, "shutdown-the-ray-serve-instances-and-ray-cluster"], [304, "shutdown-the-ray-serve-instances-and-ray-cluster"]], "Shutdown the Ray Serve instances and Ray Cluster# stop ray serveserve.shutdown()  # Shutdown Ray Serve when done, ray cluster will still be runningray.shutdown()  # Shutdown Ray cluster": [[297, null], [298, null]], "Shutting Down": [[117, "shutting-down"], [121, "shutting-down"]], "Shutting Down the Service": [[117, "shutting-down-the-service"], [122, "shutting-down-the-service"]], "Simulate Client: Send test requests": [[292, "simulate-client-send-test-requests"], [296, "simulate-client-send-test-requests"], [304, null]], "Simulate Client: Send test requestsWe use requests library to send HTTP requests to the deployed model.Note: if you encounter any errors with serve not able to start, most likely it is due to previous instance of serve not being shutdown properly. Restart the notebook or see towards the end of notebook to see how to gracefully shutdown ray serve and the ray cluster.import requests # used to send HTTP requests to the deployed model# Query the deployed modelresponse = requests.get(\u201dhttp://localhost:8000/predict\u201d, params={\u201ctext\u201d: \u201cI love Ray Serve!\u201d})print(response.json())  # Should print the sentiment analysis result{\u2018text\u2019: \u2018i love ray serve!\u2019, \u2018sentiment\u2019: [{\u2018label\u2019: \u2018POSITIVE\u2019, \u2018score\u2019: 0.9998507499694824}]}": [[293, "simulate-client-send-test-requestswe-use-requests-library-to-send-http-requests-to-the-deployed-model-note-if-you-encounter-any-errors-with-serve-not-able-to-start-most-likely-it-is-due-to-previous-instance-of-serve-not-being-shutdown-properly-restart-the-notebook-or-see-towards-the-end-of-notebook-to-see-how-to-gracefully-shutdown-ray-serve-and-the-ray-cluster-import-requests-used-to-send-http-requests-to-the-deployed-model-query-the-deployed-modelresponse-requests-get-http-localhost-8000-predict-params-text-i-love-ray-serve-print-response-json-should-print-the-sentiment-analysis-result-text-i-love-ray-serve-sentiment-label-positive-score-0-9998507499694824"], [295, null]], "Sources": [[90, null], [106, null]], "Start by launching the Ray head node in the terminal:": [[133, "start-by-launching-the-ray-head-node-in-the-terminal"], [136, "start-by-launching-the-ray-head-node-in-the-terminal"]], "Stateful transformations with actors": [[15, "stateful-transformations-with-actors"]], "Step 1: Configuration": [[108, "step-1-configuration"], [109, "step-1-configuration"], [115, "step-1-configuration"]], "Step 1: Install Required Packages": [[133, "step-1-install-required-packages"], [136, "step-1-install-required-packages"]], "Step 2: Deployment": [[108, "step-2-deployment"], [109, "step-2-deployment"], [115, "step-2-deployment"]], "Step 2: Launch Prometheus": [[133, "step-2-launch-prometheus"], [136, "step-2-launch-prometheus"]], "Step 3: Launch Ray Cluster": [[133, "step-3-launch-ray-cluster"], [136, "step-3-launch-ray-cluster"]], "Step 3: Querying": [[108, "step-3-querying"], [109, "step-3-querying"], [115, "step-3-querying"]], "Step 4: Install and Launch Grafana": [[133, "step-4-install-and-launch-grafana"], [136, "step-4-install-and-launch-grafana"]], "Step 4: Shutdown": [[109, "step-4-shutdown"], [115, "step-4-shutdown"]], "Steps to run:": [[12, "steps-to-run"]], "Streaming Applications": [[8, "streaming-applications"], [169, "streaming-applications"], [173, "streaming-applications"]], "Structure of a data lake": [[8, "structure-of-a-data-lake"], [169, "structure-of-a-data-lake"], [170, "structure-of-a-data-lake"]], "Summary": [[244, "summary"], [259, "summary"], [266, "summary"], [267, "summary"], [274, "summary"], [292, "summary"], [299, "summary"], [304, "summary"]], "Summary & Outlook": [[117, "summary-outlook"], [124, null]], "SummaryIn this notebook, we deployed a sentiment analysis model from Hugging Face using Ray Serve and FastAPI. Using num_replicas we scaled the number of instances of the model. There are many more options to autoscale to increase the replicas when the traffic is high and downscale to zero when there is no traffic.": [[297, "summaryin-this-notebook-we-deployed-a-sentiment-analysis-model-from-hugging-face-using-ray-serve-and-fastapi-using-num-replicas-we-scaled-the-number-of-instances-of-the-model-there-are-many-more-options-to-autoscale-to-increase-the-replicas-when-the-traffic-is-high-and-downscale-to-zero-when-there-is-no-traffic"], [298, "summaryin-this-notebook-we-deployed-a-sentiment-analysis-model-from-hugging-face-using-ray-serve-and-fastapi-using-num-replicas-we-scaled-the-number-of-instances-of-the-model-there-are-many-more-options-to-autoscale-to-increase-the-replicas-when-the-traffic-is-high-and-downscale-to-zero-when-there-is-no-traffic"]], "SummaryThis notebook demonstrates how to perform efficient batch inference on large datasets using Ray Data. It walks through loading a public dataset from Hugging Face, converting it into a Ray Dataset, and defining a callable class to load and apply a machine learning model (SentenceTransformer) for embedding text. The notebook shows how to use Ray Data\u2019s map_batches API to process data in parallel batches, leveraging available CPUs or GPUs for high-throughput inference. It also covers best practices for scaling, handling memory constraints, and summarizes how Ray Data enables scalable, distributed batch inference for modern ML workflows.": [[257, "summarythis-notebook-demonstrates-how-to-perform-efficient-batch-inference-on-large-datasets-using-ray-data-it-walks-through-loading-a-public-dataset-from-hugging-face-converting-it-into-a-ray-dataset-and-defining-a-callable-class-to-load-and-apply-a-machine-learning-model-sentencetransformer-for-embedding-text-the-notebook-shows-how-to-use-ray-datas-map-batches-api-to-process-data-in-parallel-batches-leveraging-available-cpus-or-gpus-for-high-throughput-inference-it-also-covers-best-practices-for-scaling-handling-memory-constraints-and-summarizes-how-ray-data-enables-scalable-distributed-batch-inference-for-modern-ml-workflows"], [258, "summarythis-notebook-demonstrates-how-to-perform-efficient-batch-inference-on-large-datasets-using-ray-data-it-walks-through-loading-a-public-dataset-from-hugging-face-converting-it-into-a-ray-dataset-and-defining-a-callable-class-to-load-and-apply-a-machine-learning-model-sentencetransformer-for-embedding-text-the-notebook-shows-how-to-use-ray-datas-map-batches-api-to-process-data-in-parallel-batches-leveraging-available-cpus-or-gpus-for-high-throughput-inference-it-also-covers-best-practices-for-scaling-handling-memory-constraints-and-summarizes-how-ray-data-enables-scalable-distributed-batch-inference-for-modern-ml-workflows"]], "Supported Infrastructure Types": [[17, "supported-infrastructure-types"], [20, "supported-infrastructure-types"]], "Testing the Container Image and Compute Config with an Anyscale Workflow": [[82, "testing-the-container-image-and-compute-config-with-an-anyscale-workflow"], [94, "testing-the-container-image-and-compute-config-with-an-anyscale-workflow"]], "The Compute Layer": [[8, "the-compute-layer"], [169, "the-compute-layer"], [171, null]], "The LLM Text Generation Process": [[108, "the-llm-text-generation-process"], [109, "the-llm-text-generation-process"], [111, "the-llm-text-generation-process"]], "The Orchestration Layer": [[8, "the-orchestration-layer"], [169, "the-orchestration-layer"], [172, null]], "The data layer": [[8, "the-data-layer"], [169, "the-data-layer"], [170, "the-data-layer"]], "The following instructions will walk you through running your first job.": [[89, "the-following-instructions-will-walk-you-through-running-your-first-job"], [103, "the-following-instructions-will-walk-you-through-running-your-first-job"]], "The following instructions will walk you through running your first job. This notebook covers the following:": [[85, "the-following-instructions-will-walk-you-through-running-your-first-job-this-notebook-covers-the-following"], [97, "the-following-instructions-will-walk-you-through-running-your-first-job-this-notebook-covers-the-following"]], "This guide will walk you through deploying, updating, and managing Anyscale Services": [[90, "this-guide-will-walk-you-through-deploying-updating-and-managing-anyscale-services"], [106, "this-guide-will-walk-you-through-deploying-updating-and-managing-anyscale-services"]], "This notebook covers the following:": [[86, "this-notebook-covers-the-following"], [98, "this-notebook-covers-the-following"]], "Tokenizer": [[275, "tokenizer"], [280, "tokenizer"]], "Trace Structure": [[146, "trace-structure"]], "Tracing Configuration": [[146, "tracing-configuration"]], "Train Generative Cv": [[348, "train-generative-cv"]], "Train Policy Learning": [[348, "train-policy-learning"]], "Train Rec Sys": [[348, "train-rec-sys"]], "Train Tabular": [[348, "train-tabular"]], "Train Time Series": [[348, "train-time-series"]], "Train Vision Pattern": [[348, "train-vision-pattern"]], "Training objective": [[305, "training-objective"], [306, "training-objective"], [318, "training-objective"], [319, "training-objective"]], "Two Phases of LLM Inference": [[108, "two-phases-of-llm-inference"], [109, "two-phases-of-llm-inference"], [111, "two-phases-of-llm-inference"]], "Usage": [[0, "usage"]], "Users and Roles": [[88, "users-and-roles"], [101, "users-and-roles"]], "Using LoRA Adapters": [[125, "using-lora-adapters"], [128, "using-lora-adapters"]], "Using Structured Output": [[125, "using-structured-output"], [129, "using-structured-output"]], "Using Tool Calling": [[125, "using-tool-calling"], [130, "using-tool-calling"]], "Using fractions of a GPU": [[16, "using-fractions-of-a-gpu"]], "VSCode": [[81, "vscode"], [93, "vscode"]], "Web Application Observability (Ray Serve)": [[142, "web-application-observability-ray-serve"], [145, null]], "Welcome to Anyscale Administration": [[79, null]], "What We Accomplished": [[117, "what-we-accomplished"], [124, "what-we-accomplished"], [125, "what-we-accomplished"], [132, "what-we-accomplished"]], "What We\u2019ll Cover": [[125, "what-we-ll-cover"], [127, "what-we-ll-cover"]], "What You\u2019ll Learn": [[79, "what-you-ll-learn"]], "What does the model learn?": [[338, "what-does-the-model-learn"], [339, "what-does-the-model-learn"]], "What is LLM Serving?": [[108, "what-is-llm-serving"], [109, "what-is-llm-serving"], [111, null]], "What is Ray Data ?": [[8, "what-is-ray-data"], [169, "what-is-ray-data"], [174, "what-is-ray-data"]], "What is Ray Serve ?": [[8, "what-is-ray-serve"], [169, "what-is-ray-serve"], [175, "what-is-ray-serve"]], "What is Ray Serve?": [[285, "what-is-ray-serve"], [292, "what-is-ray-serve"], [301, "what-is-ray-serve"]], "What is Ray Serve?Ray Serve is a scalable model serving library that allows you to deploy and manage machine learning models in production.With Ray Serve, you can easily create a scalable and distributed serving architecture thatcan handle high traffic and large workloads. It is built on top of Ray, a distributed computing frameworkthat allows you to run Python code in parallel across multiple machines. Ray Serve provides a simple API for deploying and managing models, as well as features like autoscaling,load balancing, and versioning.Ray Serve is designed to be easy to use and integrate with existing machine learning workflows.It supports a wide range of machine learning frameworks, including TensorFlow, PyTorch, and Scikit-learn.Ray Serve also provides a simple way to deploy models as REST APIs, using FastAPI,making it easy to integrate with web applications and other services.More information: https://docs.ray.io/en/latest/serve/index.html": [[283, "what-is-ray-serve-ray-serve-is-a-scalable-model-serving-library-that-allows-you-to-deploy-and-manage-machine-learning-models-in-production-with-ray-serve-you-can-easily-create-a-scalable-and-distributed-serving-architecture-thatcan-handle-high-traffic-and-large-workloads-it-is-built-on-top-of-ray-a-distributed-computing-frameworkthat-allows-you-to-run-python-code-in-parallel-across-multiple-machines-ray-serve-provides-a-simple-api-for-deploying-and-managing-models-as-well-as-features-like-autoscaling-load-balancing-and-versioning-ray-serve-is-designed-to-be-easy-to-use-and-integrate-with-existing-machine-learning-workflows-it-supports-a-wide-range-of-machine-learning-frameworks-including-tensorflow-pytorch-and-scikit-learn-ray-serve-also-provides-a-simple-way-to-deploy-models-as-rest-apis-using-fastapi-making-it-easy-to-integrate-with-web-applications-and-other-services-more-information-https-docs-ray-io-en-latest-serve-index-html"], [284, "what-is-ray-serve-ray-serve-is-a-scalable-model-serving-library-that-allows-you-to-deploy-and-manage-machine-learning-models-in-production-with-ray-serve-you-can-easily-create-a-scalable-and-distributed-serving-architecture-thatcan-handle-high-traffic-and-large-workloads-it-is-built-on-top-of-ray-a-distributed-computing-frameworkthat-allows-you-to-run-python-code-in-parallel-across-multiple-machines-ray-serve-provides-a-simple-api-for-deploying-and-managing-models-as-well-as-features-like-autoscaling-load-balancing-and-versioning-ray-serve-is-designed-to-be-easy-to-use-and-integrate-with-existing-machine-learning-workflows-it-supports-a-wide-range-of-machine-learning-frameworks-including-tensorflow-pytorch-and-scikit-learn-ray-serve-also-provides-a-simple-way-to-deploy-models-as-rest-apis-using-fastapi-making-it-easy-to-integrate-with-web-applications-and-other-services-more-information-https-docs-ray-io-en-latest-serve-index-html"]], "What is an Anyscale Service?": [[117, "what-is-an-anyscale-service"], [122, "what-is-an-anyscale-service"]], "What problem are you solving? (Diffusion as image de-noising)": [[305, "what-problem-are-you-solving-diffusion-as-image-de-noising"], [306, "what-problem-are-you-solving-diffusion-as-image-de-noising"]], "What problem are you solving? (Forest cover classification with XGBoost)": [[325, "what-problem-are-you-solving-forest-cover-classification-with-xgboost"], [326, "what-problem-are-you-solving-forest-cover-classification-with-xgboost"]], "What problem are you solving? (Inverted Pendulum, Diffusion-Style)": [[312, "what-problem-are-you-solving-inverted-pendulum-diffusion-style"], [313, "what-problem-are-you-solving-inverted-pendulum-diffusion-style"]], "What problem are you solving? (NYC taxi demand forecasting with a Transformer)": [[331, "what-problem-are-you-solving-nyc-taxi-demand-forecasting-with-a-transformer"], [332, "what-problem-are-you-solving-nyc-taxi-demand-forecasting-with-a-transformer"]], "What problem are you solving? (image classification with Food-101-Lite)": [[338, "what-problem-are-you-solving-image-classification-with-food-101-lite"], [339, "what-problem-are-you-solving-image-classification-with-food-101-lite"]], "What problem are you solving? (matrix factorization for recommendations)": [[318, "what-problem-are-you-solving-matrix-factorization-for-recommendations"], [319, "what-problem-are-you-solving-matrix-factorization-for-recommendations"]], "What you learn and take away": [[305, "what-you-learn-and-take-away"], [306, "what-you-learn-and-take-away"], [312, "what-you-learn-and-take-away"], [313, "what-you-learn-and-take-away"], [318, "what-you-learn-and-take-away"], [319, "what-you-learn-and-take-away"], [325, "what-you-learn-and-take-away"], [326, "what-you-learn-and-take-away"], [331, "what-you-learn-and-take-away"], [332, "what-you-learn-and-take-away"], [338, "what-you-learn-and-take-away"], [339, "what-you-learn-and-take-away"]], "What you\u2019ll learn & take away": [[202, "what-youll-learn-take-away"], [202, "id1"], [202, "id3"], [203, "what-youll-learn-take-away"], [216, "what-youll-learn-take-away"], [222, "what-youll-learn-take-away"]], "What\u2019s Next": [[133, "what-s-next"], [136, "what-s-next"]], "What\u2019s XGBoost?": [[325, "what-s-xgboost"], [326, "what-s-xgboost"]], "What\u2019s a policy?": [[312, "what-s-a-policy"], [313, "what-s-a-policy"]], "What\u2019s a sequence-to-sequence Transformer?": [[331, "what-s-a-sequence-to-sequence-transformer"], [332, "what-s-a-sequence-to-sequence-transformer"]], "When to use Ray Core over Ray Data ?": [[8, "when-to-use-ray-core-over-ray-data"], [169, "when-to-use-ray-core-over-ray-data"], [174, "when-to-use-ray-core-over-ray-data"]], "When to use Ray Serve?": [[16, "when-to-use-ray-serve"]], "Where can you take this next?": [[305, "where-can-you-take-this-next"], [311, "where-can-you-take-this-next"], [312, "where-can-you-take-this-next"], [317, "where-can-you-take-this-next"], [318, "where-can-you-take-this-next"], [324, "where-can-you-take-this-next"], [325, "where-can-you-take-this-next"], [330, "where-can-you-take-this-next"], [331, "where-can-you-take-this-next"], [337, "where-can-you-take-this-next"], [338, "where-can-you-take-this-next"], [347, "where-can-you-take-this-next"]], "Why Choose Medium-Sized Models?": [[117, "why-choose-medium-sized-models"], [119, "why-choose-medium-sized-models"]], "Why Ray Data ?": [[8, "why-ray-data"], [169, "why-ray-data"], [174, "why-ray-data"]], "Why Ray Serve ?": [[8, "why-ray-serve"], [169, "why-ray-serve"], [175, "why-ray-serve"]], "Why Ray?": [[1, "why-ray"], [151, "why-ray"], [152, "why-ray"]], "Why Structured Output Matters": [[125, "why-structured-output-matters"], [129, "why-structured-output-matters"]], "Why These Features Matter": [[125, "why-these-features-matter"], [127, "why-these-features-matter"]], "Why Tool Calling Matters": [[125, "why-tool-calling-matters"], [130, "why-tool-calling-matters"]], "Why Use LoRA Adapters?": [[125, "why-use-lora-adapters"], [128, "why-use-lora-adapters"]], "Why not Kubernetes ?": [[108, "why-not-kubernetes"], [109, "why-not-kubernetes"], [113, "why-not-kubernetes"]], "Why not use just FastAPI or Flask?": [[285, "why-not-use-just-fastapi-or-flask"], [292, "why-not-use-just-fastapi-or-flask"], [301, "why-not-use-just-fastapi-or-flask"]], "Why not use just FastAPI or Flask?We could have simply used FastAPI or Flask to create a REST API for the model,but Ray Serve provides additional features like autoscaling and load balancing that make it a better choice for production deployments. Ray Serve also allows you to easilydeploy multiple models and manage their versions, which can be useful in a production environment where you may need to deploy multiple models or update existing ones.\u201d\u201d\u201d": [[283, "why-not-use-just-fastapi-or-flask-we-could-have-simply-used-fastapi-or-flask-to-create-a-rest-api-for-the-model-but-ray-serve-provides-additional-features-like-autoscaling-and-load-balancing-that-make-it-a-better-choice-for-production-deployments-ray-serve-also-allows-you-to-easilydeploy-multiple-models-and-manage-their-versions-which-can-be-useful-in-a-production-environment-where-you-may-need-to-deploy-multiple-models-or-update-existing-ones"], [284, "why-not-use-just-fastapi-or-flask-we-could-have-simply-used-fastapi-or-flask-to-create-a-rest-api-for-the-model-but-ray-serve-provides-additional-features-like-autoscaling-and-load-balancing-that-make-it-a-better-choice-for-production-deployments-ray-serve-also-allows-you-to-easilydeploy-multiple-models-and-manage-their-versions-which-can-be-useful-in-a-production-environment-where-you-may-need-to-deploy-multiple-models-or-update-existing-ones"]], "Why this works": [[305, "why-this-works"], [306, "why-this-works"]], "Workloads": [[88, "workloads"], [101, "workloads"], [348, "workloads"]], "Wrap up and next steps": [[305, "wrap-up-and-next-steps"], [311, "wrap-up-and-next-steps"], [312, "wrap-up-and-next-steps"], [317, "wrap-up-and-next-steps"], [318, "wrap-up-and-next-steps"], [324, "wrap-up-and-next-steps"], [325, "wrap-up-and-next-steps"], [330, "wrap-up-and-next-steps"], [331, "wrap-up-and-next-steps"], [337, "wrap-up-and-next-steps"], [338, "wrap-up-and-next-steps"], [347, "wrap-up-and-next-steps"]], "avoids collisons with other notebooks running ray jobs on the same machine": [[259, "avoids-collisons-with-other-notebooks-running-ray-jobs-on-the-same-machine"]], "batch_size should be set based on VRAM": [[256, "batch-size-should-be-set-based-on-vram"]], "library for pre-trained models": [[288, "library-for-pre-trained-models"]], "load a Hugging Face dataset": [[250, "load-a-hugging-face-dataset"]], "metadata after inference": [[259, "metadata-after-inference"]], "setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPS": [[256, "setting-manually-so-that-code-works-on-ray-clusters-with-both-cpu-or-gpu-workers-or-on-a-local-mac-with-mps"]], "stop ray serve": [[299, "stop-ray-serve"]], "\u25b6\ufe0f 3. Activate the Environment": [[1, "activate-the-environment"], [151, "activate-the-environment"], [152, "activate-the-environment"]], "\u2705 1. Install Conda": [[1, "install-conda"], [151, "install-conda"], [152, "install-conda"]], "\u2705 7. Verify Ray Installation with a Simple Example": [[1, "verify-ray-installation-with-a-simple-example"], [151, "verify-ray-installation-with-a-simple-example"], [152, "verify-ray-installation-with-a-simple-example"]], "\u2705 Module 01 \u00b7 Introduction to Ray Train": [[202, "module-01-introduction-to-ray-train"], [228, "module-01-introduction-to-ray-train"]], "\u2705 Module 02 \u00b7 Integrating Ray Train with Ray Data": [[202, "module-02-integrating-ray-train-with-ray-data"], [228, "module-02-integrating-ray-train-with-ray-data"]], "\u2705 Module 03 \u00b7 Fault Tolerance in Ray Train": [[202, "module-03-fault-tolerance-in-ray-train"], [228, "module-03-fault-tolerance-in-ray-train"]], "\ud83c\udf89 Wrapping Up & Next Steps": [[202, "wrapping-up-next-steps"], [228, null]], "\ud83d\udccb Notebook Compute Requirements Legend": [[1, "notebook-compute-requirements-legend"], [151, "notebook-compute-requirements-legend"], [152, "notebook-compute-requirements-legend"]], "\ud83d\udccc Overview of Structure": [[88, "overview-of-structure"], [101, null]], "\ud83d\udcda 01 \u00b7 Introduction to Ray Train": [[202, null], [203, null]], "\ud83d\udcda Next Tutorials in the Course": [[202, "next-tutorials-in-the-course"], [228, "next-tutorials-in-the-course"]], "\ud83d\udce6 4. Install UV and Dependencies": [[1, "install-uv-and-dependencies"], [151, "install-uv-and-dependencies"], [152, "install-uv-and-dependencies"]], "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data": [[202, "integrating-ray-train-with-ray-data"], [216, null]], "\ud83d\udd0e Integrating Ray Train with Ray Data": [[202, "id2"], [216, "id1"]], "\ud83d\udd0e When to use Ray Train": [[202, "when-to-use-ray-train"], [203, "when-to-use-ray-train"]], "\ud83d\udda5\ufe0f 5. (Optional but Recommended) Add Your Conda Environment to Jupyter": [[1, "optional-but-recommended-add-your-conda-environment-to-jupyter"], [151, "optional-but-recommended-add-your-conda-environment-to-jupyter"], [152, "optional-but-recommended-add-your-conda-environment-to-jupyter"]], "\ud83d\udda5\ufe0f How Distributed Data Parallel (DDP) Works": [[202, "how-distributed-data-parallel-ddp-works"], [203, "how-distributed-data-parallel-ddp-works"]], "\ud83d\ude80 6. Launch Jupyter Notebook": [[1, "launch-jupyter-notebook"], [151, "launch-jupyter-notebook"], [152, "launch-jupyter-notebook"]], "\ud83d\ude80 Where to go next": [[202, "where-to-go-next"], [228, "where-to-go-next"]], "\ud83d\udee0\ufe0f 2. Create a New Conda Environment": [[1, "create-a-new-conda-environment"], [151, "create-a-new-conda-environment"], [152, "create-a-new-conda-environment"]], "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train": [[202, "fault-tolerance-in-ray-train"], [222, null]], "\ud83e\udde0 Summary": [[88, "summary"], [102, null]], "\ud83e\udde9 Miniforge Installation (It depends on your OS. In this case, we use ARM Macs)": [[1, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"], [151, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"], [152, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"]], "\ud83e\uddf9 8. Shut Down and Clean Up": [[1, "shut-down-and-clean-up"], [151, "shut-down-and-clean-up"], [152, "shut-down-and-clean-up"]]}, "docnames": ["README", "courses/deprecated/Developer_Intro_to_Ray/00_Introduction", "courses/deprecated/Developer_Intro_to_Ray/00a_Intro_Ray_Core_Basics", "courses/deprecated/Developer_Intro_to_Ray/00b_Intro_Ray_Core_Advancement", "courses/deprecated/Developer_Intro_to_Ray/01_Intro_Ray_AI_Libs_Overview", "courses/deprecated/Developer_Intro_to_Ray/02a_Intro_Ray_Train_with_PyTorch", "courses/deprecated/Developer_Intro_to_Ray/02b_Intro_Ray_Train_with_PyTorch_Lightning", "courses/deprecated/Developer_Intro_to_Ray/03_Intro_Ray_Tune", "courses/deprecated/Developer_Intro_to_Ray/04a_Intro_Ray_Data_Industry_Landscape", "courses/deprecated/Developer_Intro_to_Ray/04b_Intro_Ray_Data_Structured", "courses/deprecated/Developer_Intro_to_Ray/04c_Intro_Ray_Data_Unstructured", "courses/deprecated/Developer_Intro_to_Ray/05_Intro_Ray_Serve_PyTorch", "courses/deprecated/ray-101/1_AI_Libs_Intro", "courses/deprecated/ray-101/2_Intro_Train", "courses/deprecated/ray-101/3_Intro_Tune", "courses/deprecated/ray-101/4_Intro_Data", "courses/deprecated/ray-101/5_Intro_Serve", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/anyscale_administrator_overview", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/anyscale_vm_vs_k8s", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/deploy_to_ec2", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/deploy_to_GCE", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/deploy_to_a_new_EKS", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/deploy_to_an_existing_EKS", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/deploy_to_new_GKE", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12", "courses/foundations/Anyscale_For_Admins/README", "courses/foundations/Anyscale_Getting_Started/101_01_anyscale_intro_workspace", "courses/foundations/Anyscale_Getting_Started/101_02_anyscale_development_intro", "courses/foundations/Anyscale_Getting_Started/101_03_anyscale_compute_runtime_intro", "courses/foundations/Anyscale_Getting_Started/101_04_anyscale_storage_options", "courses/foundations/Anyscale_Getting_Started/101_05_anyscale_logging_metrics", "courses/foundations/Anyscale_Getting_Started/101_06_anyscale_intro_jobs", "courses/foundations/Anyscale_Getting_Started/101_07_anyscale_intro_services", "courses/foundations/Anyscale_Getting_Started/101_08_anyscale_collaboration", "courses/foundations/Anyscale_Getting_Started/101_09_anyscale_org_setup", "courses/foundations/Anyscale_Getting_Started/101_anyscale_intro_jobs", "courses/foundations/Anyscale_Getting_Started/101_anyscale_intro_services", "courses/foundations/Anyscale_Getting_Started/README", "courses/foundations/Anyscale_Getting_Started/output/101_01_anyscale_intro_workspace_01", "courses/foundations/Anyscale_Getting_Started/output/101_02_anyscale_development_intro_01", "courses/foundations/Anyscale_Getting_Started/output/101_03_anyscale_compute_runtime_intro_01", "courses/foundations/Anyscale_Getting_Started/output/101_04_anyscale_storage_options_01", "courses/foundations/Anyscale_Getting_Started/output/101_05_anyscale_logging_metrics_01", "courses/foundations/Anyscale_Getting_Started/output/101_06_anyscale_intro_jobs_01", "courses/foundations/Anyscale_Getting_Started/output/101_07_anyscale_intro_services_01", "courses/foundations/Anyscale_Getting_Started/output/101_08_anyscale_collaboration_01", "courses/foundations/Anyscale_Getting_Started/output/101_09_anyscale_org_setup_01", "courses/foundations/Anyscale_Getting_Started/output/101_09_anyscale_org_setup_02", "courses/foundations/Anyscale_Getting_Started/output/101_09_anyscale_org_setup_03", "courses/foundations/Anyscale_Getting_Started/output/101_anyscale_intro_jobs_01", "courses/foundations/Anyscale_Getting_Started/output/101_anyscale_intro_jobs_02", "courses/foundations/Anyscale_Getting_Started/output/101_anyscale_intro_jobs_03", "courses/foundations/Anyscale_Getting_Started/output/101_anyscale_intro_services_01", "courses/foundations/Anyscale_Getting_Started/output/101_anyscale_intro_services_02", "courses/foundations/LLM_Serving/00_intro_serve_llm/README", "courses/foundations/LLM_Serving/00_intro_serve_llm/notebook", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_01", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_02", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_03", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_04", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_05", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_06", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_07", "courses/foundations/LLM_Serving/01_deploy_medium_llm/notebook", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_01", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_02", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_03", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_04", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_05", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_06", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_07", "courses/foundations/LLM_Serving/02_advanced_llm_features/notebook", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_01", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_02", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_03", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_04", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_05", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_06", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_07", "courses/foundations/Observability/01_Intro_and_setup/01_general_intro_and_setup", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_01", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_02", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_03", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/2_Ray_Anyscale_Observability_Overview", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/03_Ray_Anyscale_Observability_in_Details", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/tracing_example/README", "courses/foundations/Ray_AI_Libs/00_Overview/01_Intro_Ray_AI_Libs_Overview", "courses/foundations/Ray_AI_Libs/00_Overview/output/01_Intro_Ray_AI_Libs_Overview_01", "courses/foundations/Ray_AI_Libs/00_Overview/output/01_Intro_Ray_AI_Libs_Overview_02", "courses/foundations/Ray_AI_Libs/00_Overview/output/01_Intro_Ray_AI_Libs_Overview_03", "courses/foundations/Ray_AI_Libs/00_intro/README", "courses/foundations/Ray_AI_Libs/00_intro/output/README_01", "courses/foundations/Ray_Core/00_Basics/00_Intro_Ray_Core_Basics", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_01", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_02", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_03", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_04", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_05", "courses/foundations/Ray_Core/01_Advanced/00a_Intro_Ray_Core_Advancement", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_01", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_02", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_03", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_04", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_05", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_06", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_07", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_08", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_09", "courses/foundations/Ray_Data/00_Landscape/04a_Intro_Ray_Data_Industry_Landscape", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_01", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_02", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_03", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_04", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_05", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_06", "courses/foundations/Ray_Data/01_Structured/04b_Intro_Ray_Data_Structured", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_01", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_02", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_03", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_04", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_05", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_06", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_07", "courses/foundations/Ray_Data/02_Unstructured/04c_Intro_Ray_Data_Unstructured", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_01", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_02", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_03", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_04", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_05", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_06", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_07", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_08", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_09", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_10", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_11", "courses/foundations/Ray_Serve/00_Serve/05_Intro_Ray_Serve_PyTorch", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_01", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_02", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_03", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_04", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_05", "courses/foundations/Ray_Train/01_02_03_intro_to_ray_train", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_01", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_02", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_03", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_04", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_05", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_06", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_07", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_08", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_09", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_10", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_11", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_12", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_13", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_14", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_15", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_16", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_17", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_18", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_19", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_20", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_21", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_22", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_23", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_24", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_25", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_26", "courses/foundations/Ray_Tune/00_Tune/03_Intro_Ray_Tune", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_01", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_02", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_03", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_04", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_05", "courses/workloads/PyTorch_Lightning/00_workload/02b_Intro_Ray_Train_with_PyTorch_Lightning", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_Ray_Data_batch_inference", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/README", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_02", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_03", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_04", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_05", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_06", "courses/workloads/Ray_Data_Processing/00_workload/02_Ray_Data_data_processing", "courses/workloads/Ray_Data_Processing/00_workload/README", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_01", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_02", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_03", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_04", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_05", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_06", "courses/workloads/Ray_Distributed_Training/00_workload/04_Ray_Train_distributed_training", "courses/workloads/Ray_Distributed_Training/00_workload/README", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_01", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_02", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_03", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_04", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_05", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_06", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/output/lesson_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/output/lesson_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/output/lesson_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_Ray_Serve_online_serving", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/output/lesson_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/output/lesson_02", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/output/lesson_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/README", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_02", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_03", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_04", "courses/workloads/Train_Generative_CV/00_workload/04d1_generative_cv_pattern", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_01", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_02", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_03", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_04", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_05", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_06", "courses/workloads/Train_Policy_Learning/00_workload/04d2_policy_learning_pattern", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_01", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_02", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_03", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_04", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_05", "courses/workloads/Train_Rec_sys/00_workload/04e_rec_sys_workload_pattern", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_01", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_02", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_03", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_04", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_05", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_06", "courses/workloads/Train_Tabular/00_workload/04b_tabular_workload_pattern", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_01", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_02", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_03", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_04", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_05", "courses/workloads/Train_Time_Series/00_workload/04c_time_series_workload_pattern", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_01", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_02", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_03", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_04", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_05", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_06", "courses/workloads/Train_Vision_Pattern/00_workload/04a_vision_pattern", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_01", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_02", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_03", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_04", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_05", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_06", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_07", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_08", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_09", "index"], "envversion": {"sphinx": 62, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["README.md", "courses/deprecated/Developer_Intro_to_Ray/00_Introduction.ipynb", "courses/deprecated/Developer_Intro_to_Ray/00a_Intro_Ray_Core_Basics.ipynb", "courses/deprecated/Developer_Intro_to_Ray/00b_Intro_Ray_Core_Advancement.ipynb", "courses/deprecated/Developer_Intro_to_Ray/01_Intro_Ray_AI_Libs_Overview.ipynb", "courses/deprecated/Developer_Intro_to_Ray/02a_Intro_Ray_Train_with_PyTorch.ipynb", "courses/deprecated/Developer_Intro_to_Ray/02b_Intro_Ray_Train_with_PyTorch_Lightning.ipynb", "courses/deprecated/Developer_Intro_to_Ray/03_Intro_Ray_Tune.ipynb", "courses/deprecated/Developer_Intro_to_Ray/04a_Intro_Ray_Data_Industry_Landscape.ipynb", "courses/deprecated/Developer_Intro_to_Ray/04b_Intro_Ray_Data_Structured.ipynb", "courses/deprecated/Developer_Intro_to_Ray/04c_Intro_Ray_Data_Unstructured.ipynb", "courses/deprecated/Developer_Intro_to_Ray/05_Intro_Ray_Serve_PyTorch.ipynb", "courses/deprecated/ray-101/1_AI_Libs_Intro.ipynb", "courses/deprecated/ray-101/2_Intro_Train.ipynb", "courses/deprecated/ray-101/3_Intro_Tune.ipynb", "courses/deprecated/ray-101/4_Intro_Data.ipynb", "courses/deprecated/ray-101/5_Intro_Serve.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/anyscale_administrator_overview.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/anyscale_vm_vs_k8s.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/deploy_to_ec2.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/deploy_to_GCE.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/deploy_to_a_new_EKS.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/deploy_to_an_existing_EKS.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/deploy_to_new_GKE.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12.ipynb", "courses/foundations/Anyscale_For_Admins/README.md", "courses/foundations/Anyscale_Getting_Started/101_01_anyscale_intro_workspace.ipynb", "courses/foundations/Anyscale_Getting_Started/101_02_anyscale_development_intro.ipynb", "courses/foundations/Anyscale_Getting_Started/101_03_anyscale_compute_runtime_intro.ipynb", "courses/foundations/Anyscale_Getting_Started/101_04_anyscale_storage_options.ipynb", "courses/foundations/Anyscale_Getting_Started/101_05_anyscale_logging_metrics.ipynb", "courses/foundations/Anyscale_Getting_Started/101_06_anyscale_intro_jobs.ipynb", "courses/foundations/Anyscale_Getting_Started/101_07_anyscale_intro_services.ipynb", "courses/foundations/Anyscale_Getting_Started/101_08_anyscale_collaboration.ipynb", "courses/foundations/Anyscale_Getting_Started/101_09_anyscale_org_setup.ipynb", "courses/foundations/Anyscale_Getting_Started/101_anyscale_intro_jobs.ipynb", "courses/foundations/Anyscale_Getting_Started/101_anyscale_intro_services.ipynb", "courses/foundations/Anyscale_Getting_Started/README.md", "courses/foundations/Anyscale_Getting_Started/output/101_01_anyscale_intro_workspace_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_02_anyscale_development_intro_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_03_anyscale_compute_runtime_intro_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_04_anyscale_storage_options_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_05_anyscale_logging_metrics_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_06_anyscale_intro_jobs_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_07_anyscale_intro_services_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_08_anyscale_collaboration_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_09_anyscale_org_setup_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_09_anyscale_org_setup_02.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_09_anyscale_org_setup_03.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_anyscale_intro_jobs_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_anyscale_intro_jobs_02.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_anyscale_intro_jobs_03.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_anyscale_intro_services_01.ipynb", "courses/foundations/Anyscale_Getting_Started/output/101_anyscale_intro_services_02.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/README.md", "courses/foundations/LLM_Serving/00_intro_serve_llm/notebook.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_01.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_02.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_03.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_04.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_05.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_06.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_07.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/notebook.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_01.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_02.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_03.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_04.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_05.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_06.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_07.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/notebook.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_01.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_02.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_03.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_04.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_05.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_06.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_07.ipynb", "courses/foundations/Observability/01_Intro_and_setup/01_general_intro_and_setup.ipynb", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_01.ipynb", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_02.ipynb", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_03.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/2_Ray_Anyscale_Observability_Overview.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/03_Ray_Anyscale_Observability_in_Details.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/tracing_example/README.md", "courses/foundations/Ray_AI_Libs/00_Overview/01_Intro_Ray_AI_Libs_Overview.ipynb", "courses/foundations/Ray_AI_Libs/00_Overview/output/01_Intro_Ray_AI_Libs_Overview_01.ipynb", "courses/foundations/Ray_AI_Libs/00_Overview/output/01_Intro_Ray_AI_Libs_Overview_02.ipynb", "courses/foundations/Ray_AI_Libs/00_Overview/output/01_Intro_Ray_AI_Libs_Overview_03.ipynb", "courses/foundations/Ray_AI_Libs/00_intro/README.ipynb", "courses/foundations/Ray_AI_Libs/00_intro/output/README_01.ipynb", "courses/foundations/Ray_Core/00_Basics/00_Intro_Ray_Core_Basics.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_01.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_02.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_03.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_04.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_05.ipynb", "courses/foundations/Ray_Core/01_Advanced/00a_Intro_Ray_Core_Advancement.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_01.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_02.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_03.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_04.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_05.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_06.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_07.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_08.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_09.ipynb", "courses/foundations/Ray_Data/00_Landscape/04a_Intro_Ray_Data_Industry_Landscape.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_01.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_02.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_03.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_04.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_05.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_06.ipynb", "courses/foundations/Ray_Data/01_Structured/04b_Intro_Ray_Data_Structured.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_01.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_02.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_03.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_04.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_05.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_06.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_07.ipynb", "courses/foundations/Ray_Data/02_Unstructured/04c_Intro_Ray_Data_Unstructured.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_01.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_02.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_03.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_04.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_05.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_06.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_07.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_08.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_09.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_10.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_11.ipynb", "courses/foundations/Ray_Serve/00_Serve/05_Intro_Ray_Serve_PyTorch.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_01.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_02.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_03.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_04.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_05.ipynb", "courses/foundations/Ray_Train/01_02_03_intro_to_ray_train.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_01.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_02.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_03.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_04.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_05.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_06.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_07.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_08.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_09.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_10.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_11.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_12.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_13.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_14.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_15.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_16.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_17.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_18.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_19.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_20.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_21.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_22.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_23.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_24.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_25.ipynb", "courses/foundations/Ray_Train/output/01_02_03_intro_to_ray_train_26.ipynb", "courses/foundations/Ray_Tune/00_Tune/03_Intro_Ray_Tune.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_01.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_02.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_03.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_04.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_05.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/02b_Intro_Ray_Train_with_PyTorch_Lightning.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_Ray_Data_batch_inference.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/README.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_02.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_03.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_04.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_05.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_06.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/02_Ray_Data_data_processing.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/README.md", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_01.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_02.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_03.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_04.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_05.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_06.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/04_Ray_Train_distributed_training.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/README.md", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_01.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_02.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_03.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_04.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_05.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_06.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_Ray_Serve_online_serving.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/output/lesson_02.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/README.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_02.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_03.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_04.ipynb", "courses/workloads/Train_Generative_CV/00_workload/04d1_generative_cv_pattern.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_01.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_02.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_03.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_04.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_05.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_06.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/04d2_policy_learning_pattern.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_01.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_02.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_03.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_04.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_05.ipynb", "courses/workloads/Train_Rec_sys/00_workload/04e_rec_sys_workload_pattern.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_01.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_02.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_03.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_04.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_05.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_06.ipynb", "courses/workloads/Train_Tabular/00_workload/04b_tabular_workload_pattern.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_01.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_02.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_03.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_04.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_05.ipynb", "courses/workloads/Train_Time_Series/00_workload/04c_time_series_workload_pattern.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_01.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_02.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_03.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_04.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_05.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_06.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/04a_vision_pattern.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_01.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_02.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_03.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_04.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_05.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_06.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_07.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_08.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_09.ipynb", "index.md"], "indexentries": {}, "objects": {}, "objnames": {}, "objtypes": {}, "terms": {"": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 28, 30, 31, 34, 35, 39, 40, 43, 45, 46, 47, 56, 58, 59, 66, 75, 81, 83, 84, 86, 87, 91, 93, 95, 96, 98, 99, 108, 109, 111, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 137, 140, 141, 142, 144, 145, 147, 148, 150, 151, 152, 153, 158, 159, 161, 162, 163, 165, 167, 168, 169, 170, 174, 175, 176, 179, 180, 181, 182, 184, 186, 188, 189, 191, 193, 196, 198, 199, 200, 202, 203, 204, 205, 206, 210, 211, 212, 215, 216, 218, 228, 229, 231, 232, 233, 235, 237, 238, 239, 243, 244, 256, 259, 261, 263, 265, 266, 267, 270, 273, 274, 275, 277, 279, 280, 281, 305, 306, 307, 309, 311, 316, 317, 318, 319, 320, 321, 322, 323, 324, 327, 328, 329, 330, 333, 335, 337, 338, 339, 340, 341, 342, 343, 345, 347], "0": [1, 3, 4, 5, 6, 7, 10, 12, 13, 14, 15, 16, 22, 28, 29, 30, 35, 37, 40, 43, 44, 45, 46, 47, 53, 55, 56, 58, 59, 66, 68, 75, 80, 82, 84, 92, 94, 96, 108, 109, 115, 117, 121, 122, 125, 128, 129, 130, 133, 136, 137, 141, 146, 147, 150, 151, 152, 159, 163, 165, 167, 168, 177, 181, 184, 189, 190, 191, 203, 204, 205, 206, 210, 211, 213, 215, 219, 220, 223, 224, 228, 229, 231, 232, 233, 235, 238, 239, 244, 263, 265, 267, 270, 271, 274, 275, 280, 282, 292, 296, 304, 305, 306, 307, 308, 309, 311, 312, 313, 314, 316, 317, 318, 320, 322, 324, 325, 326, 327, 328, 331, 333, 334, 335, 337, 338, 339, 340, 341, 343, 347], "00": [12, 13, 14, 15, 83, 95, 146, 244, 265, 275, 282, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340, 348], "000": [7, 14, 202, 204, 229, 231, 312, 314, 318, 320, 325, 327], "0000000000000": [28, 30], "00000000000000000": [28, 30], "0001": [6, 10, 184, 191, 235, 239], "0003573892": 14, "0003590581": 14, "0003788471": 14, "0003824231": 14, "0004189012": 14, "00046369e": [244, 266], "00087994e": [244, 266], "00129196e": [244, 266], "00217544e": [244, 266], "00403815e": [244, 266], "0059": 12, "00596860e": [244, 266], "00641076e": [244, 266], "006742": 13, "00719017e": [244, 266], "00724374e": [244, 266], "00728178e": [244, 266], "00749106": [244, 265], "00753223": [244, 265], "007877049646500664": 14, "00787705": 14, "00844238": [244, 265], "00926834e": [244, 266], "0092816": [244, 265], "00958297e": [244, 266], "00974117e": [244, 266], "00982723e": [244, 266], "00994138e": [244, 266], "00_developer_intro_to_rai": [260, 268, 276, 300], "00a": 348, "00z": 146, "01": [4, 6, 13, 147, 150, 235, 239, 244, 266, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340, 348], "01000227e": [244, 266], "01103884e": [244, 266], "01104600e": [244, 266], "01141734e": [244, 266], "01148352e": [244, 266], "01190887": [244, 265], "01222771": [244, 265], "01231135": [244, 265], "01273207e": [244, 266], "01347007e": [244, 266], "01351717e": [244, 266], "01387227e": [244, 266], "01402104e": [244, 266], "01455652": [244, 265], "01504247": [244, 265], "01505721e": [244, 266], "01544438e": [244, 266], "01616676e": [244, 266], "01646197e": [244, 266], "01848297e": [244, 266], "01875377e": [244, 266], "01946776e": [244, 266], "01951000e": [244, 266], "01958193e": [244, 266], "02": [4, 147, 150, 244, 266, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340, 348], "02095584": [244, 265], "02202111": [244, 265], "02316421e": [244, 266], "02338964e": [244, 266], "02352677e": [244, 266], "0242": 12, "02448604e": [244, 266], "02480531e": [244, 266], "02481507e": [244, 266], "02496293": [244, 265], "02508835e": [244, 266], "02556132": [244, 265], "02750473e": [244, 266], "02791084": [244, 265], "02842702e": [244, 266], "02_service_hello_world": [86, 98], "02b": 348, "02d": [318, 320], "03": [4, 12, 14, 147, 150, 244, 266, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340, 348], "03000000000000000": [28, 30], "03162946e": [244, 266], "03241184e": [244, 266], "03302041": [244, 265], "03319024e": [244, 266], "03347346e": [244, 266], "03351809e": [244, 266], "03357503": [244, 265], "03620186e": [244, 266], "03722222e": [244, 266], "03901269e": [244, 266], "03915609e": [244, 266], "03924675": [244, 265], "03974594e": [244, 266], "03d": [305, 309, 312, 316], "04": [244, 266, 307, 315, 318, 320, 325, 327, 331, 333, 338, 340, 348], "04023737e": [244, 266], "04127836e": [244, 266], "04148921": [244, 265], "04188204e": [244, 266], "04267104e": [244, 266], "04279362e": [244, 266], "04313433e": [244, 266], "04401015e": [244, 266], "04419766e": [244, 266], "04514116e": [244, 266], "04760937e": [244, 266], "04781413e": [244, 266], "04831458": [244, 265], "04871886e": [244, 266], "04a": 348, "04b": 348, "04c": 348, "04d1": 348, "04d2": 348, "04e": 348, "05": [6, 9, 12, 13, 142, 144, 176, 179, 182, 235, 238, 275, 282, 305, 307, 312, 316, 318, 320, 325, 327, 331, 333, 338, 340, 348], "050227": [244, 265], "05029851e": [244, 266], "05031021e": [244, 266], "05048873e": [244, 266], "05110919e": [244, 266], "05117615e": [244, 266], "05286286": [244, 265], "05403318e": [244, 266], "05412337e": [244, 266], "0564279": 14, "05699580e": [244, 266], "0582891": 14, "05838008e": [244, 266], "05897461e": [244, 266], "05951829e": [244, 266], "05955295e": [244, 266], "05962829": [244, 265], "05964907e": [244, 266], "05_069833_2422": 13, "06": [1, 13, 83, 95, 151, 152, 305, 307, 312, 316, 318, 320, 331, 333, 338, 341, 348], "06012297e": [244, 266], "06039613e": [244, 266], "06096685": [244, 265], "06099542": [244, 265], "06128380e": [244, 266], "06155156e": [244, 266], "06164196e": [244, 266], "06194988": [244, 265], "06202352e": [244, 266], "06234232e": [244, 266], "06241195e": [244, 266], "06251295": [244, 265], "06282867e": [244, 266], "06317782e": [244, 266], "06359579e": [244, 266], "06367093324661255": 13, "063671": 13, "06383444e": [244, 266], "06465332e": [244, 266], "06659506e": [244, 266], "06678507e": [244, 266], "06857569e": [244, 266], "06872221": [244, 265], "06887527e": [244, 266], "07": [125, 130, 275, 282, 305, 307, 312, 316, 318, 321, 325, 327, 331, 333, 338, 341, 348], "07005756e": [244, 266], "07039157e": [244, 266], "07176238": [244, 265], "07316985e": [244, 266], "07334603e": [244, 266], "07348490e": [244, 266], "07420641e": [244, 266], "07510021": [244, 265], "07512747e": [244, 266], "07565679e": [244, 266], "07582638e": [244, 266], "07590961e": [244, 266], "07614997e": [244, 266], "07735191e": [244, 266], "07769895e": [244, 266], "07796153e": [244, 266], "07813133e": [244, 266], "07829855": [244, 265], "08": [117, 123, 146, 244, 266, 305, 308, 312, 317, 318, 322, 331, 333, 338, 341, 348], "08080895": [244, 265], "08113792": [244, 265], "08117312e": [244, 266], "08142687": [244, 265], "08161136e": [244, 266], "08306534e": [244, 266], "08318681e": [244, 266], "08386130e": [244, 266], "08393911e": [244, 266], "08423311e": [244, 266], "08562492e": [244, 266], "08593434e": [244, 266], "08834168e": [244, 266], "08847059e": [244, 266], "08849846e": [244, 266], "08855490e": [244, 266], "08866049e": [244, 266], "08888834e": [244, 266], "08900222e": [244, 266], "08926150e": [244, 266], "08967713e": [244, 266], "09": [13, 14, 305, 309, 312, 317, 318, 322, 325, 328, 331, 334, 338, 341, 348], "09058516e": [244, 266], "09158831e": [244, 266], "09305708e": [244, 266], "09318195e": [244, 266], "09376505e": [244, 266], "09640113e": [244, 266], "09668531e": [244, 266], "09694359e": [244, 266], "09729558e": [244, 266], "09788750e": [244, 266], "09841380e": [244, 266], "09954223e": [244, 266], "09_200164_18044": [275, 282], "0a000000000000000": [28, 30], "0bd7bde3f2c914b3": [28, 30], "0f8bb12ddf9a451e9": [28, 30, 43, 45, 53, 56], "0m": [13, 14, 16, 275, 282], "0x72c6d85fc9d0": 16, "0x72c6d85fcf90": 16, "0x72c6d85fd3d0": 16, "0x72c6d85fd550": 16, "0x72c6d85fe590": 16, "0x72c6d85ff250": 16, "0x72c6d85ff3d0": 16, "0x72c6d8608750": 16, "0x72c6d8609050": 16, "0x72c6d860b6d0": 16, "0x72c6d860b7d0": 16, "0x72c6d8610490": 16, "0x72c6d8610a50": 16, "0x72c6d8611310": 16, "0x72c6d8611ad0": 16, "0x72c6d8611b90": 16, "0x72c6d8612050": 16, "0x72c6d8613690": 16, "0x72c6d8620a90": 16, "0x72c6d8620e10": 16, "0x72c6d86218d0": 16, "0x72c6d8621b90": 16, "0x72c6d8622ad0": 16, "0x72c6d8623590": 16, "0x72c6d86281d0": 16, "0x72c6d8628710": 16, "0x72c6d862a1d0": 16, "0x72c6d862ac50": 16, "0x72c6d862b790": 16, "0x72c6d862b7d0": 16, "0x72c6d862c690": 16, "0x72c6d872db50": 16, "0x72c6d8747390": 16, "0x72c6d87500d0": 16, "0x72c6d8752290": 16, "0x72c6d8752e50": 16, "0x72c6d8757dd0": 16, "0x72c6d87793d0": 16, "0x72c6d8779b90": 16, "0x72c6d877a010": 16, "0x72c6d877a6d0": 16, "0x72c6d877b010": 16, "0x72c6d877bc10": 16, "0x72c6d8785e50": 16, "0x72c6d8785fd0": 16, "0x72c6d8786a50": 16, "0x72c6d8787c90": 16, "0x72c6d8794350": 16, "0x72c6d8795110": 16, "0x72c6d8796b50": 16, "0x72c6d8797150": 16, "0x72c6d8797ed0": 16, "0x72c6d87a0690": 16, "0x72c6d87a14d0": 16, "0x72c6d87a1b50": 16, "0x72c6d87a1f50": 16, "0x72c6d87a2c10": 16, "0x72c6d87a3d50": 16, "0x72c6d87a3ed0": 16, "0x72c6d87a8690": 16, "0x72c6d87a96d0": 16, "0x72c6d87a9cd0": 16, "0x72c6d87aa0d0": 16, "0x72c6d87aa4d0": 16, "0x72c6d87aad50": 16, "0x72c6d87ab690": 16, "0x72c6d87ac4d0": 16, "0x72c6d87adb90": 16, "0x72c6d87ae4d0": 16, "0x72c6d87ae710": 16, "0x72c6d87c0110": 16, "0x72c6d87c1110": 16, "0x72c6d87c1250": 16, "0x72c6d87c18d0": 16, "0x72c6d87c2350": 16, "0x72c6d87c3a50": 16, "0x72c6d87d0690": 16, "0x72c6d87d0d90": 16, "0x72c6d87d1c50": 16, "0x72c6d87d1cd0": 16, "0x72c6d87d3190": 16, "0x72c6d87d3fd0": 16, "0x72c6d87d8250": 16, "0x72c6d87d8e90": 16, "0x72c6d87d96d0": 16, "0x72c6d87da1d0": 16, "0x72c6d87e4810": 16, "0x72c6d87e4f90": 16, "0x72c6d87e62d0": 16, "0x72c6d87e64d0": 16, "0x72c6e01cbd50": 16, "0x72c6e034a510": 16, "0x72c6e034b950": 16, "0x72c6e0351e10": 16, "0x72c6e0353410": 16, "0x72c6e035ca50": 16, "0x72c6e035d5d0": 16, "0x72c6e03660d0": 16, "0x72c72000ddd0": 16, "0x72c73032a850": 16, "0xxxxxxxx": [28, 30], "0xxxxxxxxx": [28, 30], "0xxxxxxxxxx": [28, 30], "1": [29, 37, 42, 44, 50, 55, 62, 68, 73, 78, 84, 96, 120, 121, 122, 124, 128, 129, 130, 137, 141, 142, 143, 145, 157, 162, 166, 177, 197, 200, 202, 203, 204, 205, 208, 209, 213, 214, 215, 220, 221, 223, 224, 226, 230, 232, 233, 244, 256, 263, 265, 266, 267, 270, 272, 273, 274, 279, 281, 282, 306, 308, 309, 311, 315, 316, 317, 319, 321, 322, 324, 326, 328, 329, 330, 332, 334, 335, 337, 339, 341, 343, 344, 347], "10": [1, 3, 4, 7, 10, 11, 12, 13, 14, 15, 16, 17, 22, 35, 39, 43, 44, 45, 55, 56, 66, 68, 70, 83, 84, 95, 96, 137, 141, 142, 144, 146, 147, 150, 151, 152, 159, 161, 163, 167, 168, 184, 186, 190, 196, 200, 205, 229, 231, 233, 244, 248, 249, 250, 263, 265, 267, 273, 274, 275, 282, 306, 310, 311, 314, 316, 320, 324, 326, 327, 330, 333, 337, 339, 348], "100": [3, 7, 10, 14, 15, 16, 84, 96, 125, 128, 146, 159, 168, 184, 190, 191, 202, 209, 229, 233, 267, 271, 275, 280, 312, 317, 318, 320], "1000": [9, 10, 15, 84, 96, 137, 141, 176, 179, 184, 188, 305, 308, 312, 314, 315], "10000": [331, 334], "100000000000": [6, 235, 238], "100k": [319, 324], "100th": [267, 271], "101": [17, 22, 306, 311, 343, 347, 348], "1010": [267, 274], "10129036e": [244, 266], "101_01_anyscale_intro_workspac": 91, "101_02_anyscale_development_intro": 91, "101_03_anycale_compute_runtime_intro": 91, "101_04_anyscale_storage_opt": 91, "101_05_anyscale_logging_and_metr": 91, "101_06_anyscale_intro_job": 91, "101_07_anyscale_intro_servic": 91, "101_08_anyscale_collaboration_intro": 91, "101_09_anyscale_org_setup": 91, "102": [17, 22], "1024": [3, 6, 9, 10, 35, 39, 159, 161, 176, 179, 180, 184, 190, 235, 238, 331, 334], "10279503e": [244, 266], "10307": 14, "10526211e": [244, 266], "10536157": [244, 265], "10537948e": [244, 266], "105m": [142, 145, 146], "10776436e": [244, 266], "10807291e": [244, 266], "10863163e": [244, 266], "10879738e": [244, 266], "108934": 13, "10893423855304718": 13, "10954670e": [244, 266], "10956261e": [244, 266], "10_000": [6, 235, 238, 239, 312, 314], "10am": [244, 265], "10m": [318, 324], "11": [1, 12, 13, 14, 151, 152, 244, 265, 275, 282, 348], "11016287e": [244, 266], "11058047e": [244, 266], "11085677e": [244, 266], "110m": [142, 145, 146], "11493243e": [244, 266], "11712754e": [244, 266], "11721872": [244, 265], "11745796e": [244, 266], "11788076e": [244, 266], "11897744e": [244, 266], "11_10": [275, 282], "11th": [244, 265], "12": [1, 4, 12, 13, 43, 46, 53, 58, 66, 73, 83, 95, 147, 150, 151, 152, 260, 268, 276, 300, 320, 333, 348], "12014441e": [244, 266], "12174596e": [244, 266], "12183236e": [244, 266], "12234001e": [244, 266], "123": 146, "123456": [35, 38], "12468980e": [244, 266], "12480514e": [244, 266], "12500": [267, 270, 273, 274], "12501": [267, 274], "12502": [267, 274], "12503": [267, 274], "12504": [267, 274], "12587933e": [244, 266], "12685782e": [244, 266], "127": [1, 133, 136, 151, 152], "128": [5, 7, 13, 14, 202, 207, 229, 232, 233, 244, 265, 312, 315, 331, 335, 337], "12821269e": [244, 266], "12832280e": [244, 266], "12841654e": [244, 266], "128k": [108, 109, 112, 125, 131], "12907687e": [244, 266], "12912727e": [244, 266], "12939501e": [244, 266], "129887": 13, "12th": [244, 265], "12x": 14, "12xlarg": [137, 141], "13": [12, 14, 43, 46, 53, 58, 327, 348], "13000": [267, 273, 274], "13086134e": [244, 266], "13095595e": [244, 266], "13100": [267, 273, 274], "13238472e": [244, 266], "13547181e": [244, 266], "13586960e": [244, 266], "13600": [267, 273, 274], "13700": [267, 273, 274], "138": 13, "13803817e": [244, 266], "13828215e": [244, 266], "13841531e": [244, 266], "13b": [117, 119, 124, 125, 131], "14": [117, 119, 244, 265, 275, 282, 307, 312, 314, 320, 327, 333, 340, 348], "140": [117, 119], "14019522e": [244, 266], "140gb": [117, 119, 121], "14159100e": [244, 266], "14268738e": [244, 266], "14443852e": [244, 266], "14533243e": [244, 266], "14656349e": [244, 266], "14703774e": [244, 266], "14777484e": [244, 266], "14787792e": [244, 266], "14892137e": [244, 266], "14971709e": [244, 266], "14gb": [108, 109, 113], "14th": [244, 265], "15": [3, 5, 12, 13, 28, 29, 35, 39, 43, 44, 45, 53, 55, 56, 66, 70, 125, 130, 133, 136, 159, 167, 244, 263, 265, 275, 282, 329, 348], "150": [244, 263, 265], "15072963e": [244, 266], "150m": 146, "15157820e": [244, 266], "15247765e": [244, 266], "15391724e": [244, 266], "15394783": [244, 265], "15428728e": [244, 266], "15451038e": [244, 266], "155": [9, 176, 179], "15531293e": [244, 266], "15553670e": [244, 266], "15556864e": [244, 266], "15585802e": [244, 266], "155m": 146, "156": 12, "15658525e": [244, 266], "15747452e": [244, 266], "15786707e": [244, 266], "15844142e": [244, 266], "15874708e": [244, 266], "15_08": 12, "15_15": 12, "15x": 13, "16": [12, 13, 14, 117, 123, 207, 244, 265, 275, 282, 305, 307, 326, 328, 340, 341, 348], "160": [6, 117, 119, 235, 238], "16129310e": [244, 266], "16142738e": [244, 266], "16249922e": [244, 266], "16273381e": [244, 266], "16315296e": [244, 266], "163491": 12, "163492": 12, "16707636e": [244, 266], "168": [331, 332, 333], "16848189e": [244, 266], "1693310400": 146, "16946062e": [244, 266], "16970104e": [244, 266], "16th": [244, 263, 265], "16xlarg": [137, 141], "17": [12, 17, 22, 43, 46, 47, 53, 58, 59, 66, 75, 83, 95, 117, 123, 244, 265, 267, 270, 348], "17145060e": [244, 266], "172": [17, 22], "17218718e": [244, 266], "17278847e": [244, 266], "17306670e": [244, 266], "1732276209": 13, "1732276227": 13, "17342269e": [244, 266], "17458829e": [244, 266], "17503238e": [244, 266], "17549804e": [244, 266], "175m": [142, 145, 146], "17605399e": [244, 266], "17623755e": [244, 266], "17677706e": [244, 266], "17716263e": [244, 266], "17952654e": [244, 266], "17982033e": [244, 266], "17th": [244, 265], "18": [13, 35, 40, 43, 47, 53, 59, 66, 75, 83, 95, 117, 123, 203, 204, 209, 244, 265, 338, 339, 348], "18025970e": [244, 266], "180m": [142, 145, 146], "18165373e": [244, 266], "18200418e": [244, 266], "18252768e": [244, 266], "18264270e": [244, 266], "18500029e": [244, 266], "18707759e": [244, 266], "1879": [1, 151, 152], "18899436e": [244, 266], "18926680e": [244, 266], "18932852e": [244, 266], "18th": [244, 265], "19": [12, 14, 83, 95, 117, 123, 244, 265, 305, 307, 331, 333, 338, 340, 348], "19172417e": [244, 266], "19192507e": [244, 266], "19254841e": [244, 266], "19275388e": [244, 266], "19408388e": [244, 266], "19601890e": [244, 266], "19630387e": [244, 266], "1967": [267, 270], "19670653e": [244, 266], "19684739e": [244, 266], "19716156e": [244, 266], "19767813e": [244, 266], "19797611e": [244, 266], "19835320e": [244, 266], "19861914e": [244, 266], "19884178e": [244, 266], "1995": [244, 265], "19986786e": [244, 266], "1_000_000": [3, 159, 167], "1d": [312, 315], "1e": [5, 6, 7, 13, 14, 202, 206, 217, 223, 229, 232, 233, 235, 238, 275, 281, 312, 315, 318, 322, 331, 335, 338, 344], "1f": [331, 333], "1gb": [9, 176, 179], "1h34m20": 13, "1m": [318, 324], "1m10": 16, "1mb": [137, 141], "1pb": [137, 141], "1st": [3, 159, 162, 244, 263, 265], "1xt4": [12, 13, 14], "2": [29, 30, 42, 44, 45, 55, 56, 78, 80, 86, 92, 98, 119, 122, 124, 128, 129, 130, 137, 141, 142, 143, 145, 163, 166, 177, 181, 197, 200, 202, 203, 205, 207, 215, 218, 230, 231, 233, 244, 250, 256, 263, 265, 266, 267, 272, 273, 274, 281, 282, 289, 290, 291, 292, 303, 306, 311, 319, 322, 324, 328, 334, 335, 337, 339, 342, 343, 347, 348], "20": [1, 7, 12, 13, 14, 17, 22, 83, 95, 117, 123, 151, 152, 229, 231, 233, 275, 282, 305, 307, 312, 314, 318, 320, 322, 325, 327, 331, 335, 348], "200": [244, 263, 265, 267, 271], "20093006e": [244, 266], "200m": 146, "20103974e": [244, 266], "20113872e": [244, 266], "2012": [17, 22, 244, 265], "2014": [331, 332, 333], "2015": [244, 265], "20152864e": [244, 266], "2017": [244, 263, 265], "20175812e": [244, 266], "2021": [4, 12, 147, 150], "2023": [9, 176, 183], "2024": [9, 10, 12, 13, 14, 15, 146, 176, 183, 184, 195], "2025": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 69, 83, 89, 90, 91, 95, 103, 106, 108, 109, 110, 117, 118, 125, 126, 130, 133, 134, 137, 138, 142, 143, 147, 148, 151, 152, 153, 154, 159, 160, 169, 170, 176, 177, 184, 185, 229, 230, 235, 236, 243, 244, 261, 267, 269, 275, 277, 282, 285, 292, 301], "20302368700504303": 14, "20312032e": [244, 266], "20370263e": [244, 266], "20407227e": [244, 266], "20567256e": [244, 266], "20587808e": [244, 266], "205m": 146, "20716389e": [244, 266], "20yr": [244, 265], "21": [13, 17, 22, 275, 282, 348], "210m": 146, "21206143e": [244, 266], "21334969e": [244, 266], "21377383e": [244, 266], "21380231e": [244, 266], "21405767e": [244, 266], "2147483648": [3, 159, 165], "21503563e": [244, 266], "21508265e": [244, 266], "21584712e": [244, 266], "21600205e": [244, 266], "21637220e": [244, 266], "21745682e": [244, 266], "21776196e": [244, 266], "21796946e": [244, 266], "21971425e": [244, 266], "21st": [244, 265], "22": [13, 244, 265, 275, 282, 348], "22192677e": [244, 266], "222": 14, "22234142e": [244, 266], "22277103e": [244, 266], "22332841e": [244, 266], "224": [305, 307, 311, 338, 339, 340, 341], "22458421e": [244, 266], "225": [338, 341], "22552105e": [244, 266], "22804798e": [244, 266], "22867932e": [244, 266], "22891478e": [244, 266], "229": [338, 341], "22918031e": [244, 266], "22967193e": [244, 266], "22_06": 13, "22_11": 13, "22nd": [244, 265], "23": [12, 244, 265, 305, 307, 338, 340, 348], "23053056e": [244, 266], "23069037e": [244, 266], "23083012e": [244, 266], "23107583e": [244, 266], "23113478e": [244, 266], "23204021e": [244, 266], "23302741e": [244, 266], "23314434e": [244, 266], "23478852e": [244, 266], "235m": [142, 145, 146], "23639209e": [244, 266], "23694149e": [244, 266], "23694418e": [244, 266], "23702966e": [244, 266], "23793101e": [244, 266], "23815991e": [244, 266], "23960292e": [244, 266], "23982316e": [244, 266], "23x": [275, 282], "24": [3, 12, 17, 22, 159, 165, 331, 332, 333, 348], "24085984e": [244, 266], "240m": [142, 145, 146], "24192730e": [244, 266], "24219281e": [244, 266], "24473451e": [244, 266], "245m": [142, 145, 146], "24615501e": [244, 266], "24854468e": [244, 266], "24885803e": [244, 266], "24984046e": [244, 266], "24xlarg": [137, 141], "25": [12, 13, 14, 43, 44, 53, 55, 66, 68, 275, 282, 348], "250": [267, 271], "25000": [267, 270, 271, 272], "25175510e": [244, 266], "25248020e": [244, 266], "25258800e": [244, 266], "25268223e": [244, 266], "25294994e": [244, 266], "25387060e": [244, 266], "25401214e": [244, 266], "255": [7, 14, 15, 229, 231, 305, 307], "256": [6, 13, 137, 141, 235, 238, 239, 305, 307, 338, 340], "25669474e": [244, 266], "25707304e": [244, 266], "25723777e": [244, 266], "25795130e": [244, 266], "25806283e": [244, 266], "25912409e": [244, 266], "25934454e": [244, 266], "25_924022_2383": 12, "25th": [244, 265], "26": [13, 14, 117, 119, 275, 282, 348], "26000811e": [244, 266], "26009388e": [244, 266], "26156314e": [244, 266], "26208720e": [244, 266], "26244992e": [244, 266], "26258478e": [244, 266], "26309279e": [244, 266], "26314560e": [244, 266], "26320729e": [244, 266], "26351237e": [244, 266], "26371822e": [244, 266], "264131": 14, "26429361e": [244, 266], "26502474e": [244, 266], "26816351e": [244, 266], "26948217e": [244, 266], "26990714e": [244, 266], "27": [12, 13, 244, 263, 265], "27106623e": [244, 266], "27159020e": [244, 266], "27211": 14, "27212": 14, "27213278e": [244, 266], "27219909e": [244, 266], "27394149e": [244, 266], "27486494e": [244, 266], "27514724e": [244, 266], "27598614e": [244, 266], "27818017e": [244, 266], "2784426808357239": 13, "278443": 13, "27902825e": [244, 266], "27904241e": [244, 266], "27935463e": [244, 266], "27x": [275, 282], "28": [10, 11, 13, 14, 15, 16, 142, 145, 184, 190, 196, 200, 202, 204, 275, 282], "28050": 14, "28076579e": [244, 266], "28086493e": [244, 266], "28125295e": [244, 266], "28131025e": [244, 266], "28160176e": [244, 266], "28330866e": [244, 266], "28415197e": [244, 266], "28529142e": [244, 266], "28545947e": [244, 266], "28572544e": [244, 266], "28628640e": [244, 266], "28712449e": [244, 266], "28749549e": [244, 266], "28796948e": [244, 266], "28858958e": [244, 266], "28869668e": [244, 266], "28947487e": [244, 266], "28947702e": [244, 266], "28th": [244, 265], "28x28": [7, 14, 229, 231], "29": [12, 13, 14, 125, 130], "29297644e": [244, 266], "29473785e": [244, 266], "29535252e": [244, 266], "29715446e": [244, 266], "29792884e": [244, 266], "29807210e": [244, 266], "29825398e": [244, 266], "29964035e": [244, 266], "29_09": 14, "29t10": 146, "2a": [43, 45, 53, 56], "2b": [43, 45, 53, 56], "2cpu": [43, 50, 53, 62], "2d": [7, 14, 229, 231, 318, 324], "2e": [305, 308], "2f": [3, 84, 96, 159, 161, 318, 324, 325, 329], "2nd": [3, 159, 162, 244, 265], "2ykut_ijz8q8gwt5vphvitzshksddol6msszjxzwe5a": [117, 122], "3": [31, 42, 44, 51, 55, 64, 68, 78, 82, 94, 120, 121, 122, 127, 128, 129, 132, 137, 141, 142, 145, 146, 158, 167, 177, 190, 191, 197, 202, 203, 204, 205, 215, 217, 223, 224, 230, 232, 240, 244, 259, 260, 265, 266, 267, 268, 270, 274, 276, 281, 282, 300, 306, 308, 311, 315, 316, 317, 319, 322, 324, 328, 335, 337, 339, 344], "30": [13, 125, 130, 244, 265, 275, 282, 318, 320, 332, 337], "30478994e": [244, 266], "30517557e": [244, 266], "30540405e": [244, 266], "30551": 13, "30557770e": [244, 266], "30565623e": [244, 266], "30582720e": [244, 266], "30603": 13, "30618355e": [244, 266], "30638674e": [244, 266], "30715715e": [244, 266], "30746688e": [244, 266], "30834863e": [244, 266], "308870": 13, "30887049436569214": 13, "30980236e": [244, 266], "30981060e": [244, 266], "30999158e": [244, 266], "30min": [331, 333], "30th": [244, 265], "31": [1, 12, 13, 14, 151, 152, 275, 282], "31019164e": [244, 266], "31141504e": [244, 266], "31172134e": [244, 266], "31209707e": [244, 266], "31449399e": [244, 266], "31499174e": [244, 266], "31562141e": [244, 266], "31659403e": [244, 266], "31913936e": [244, 266], "31973000e": [244, 266], "31st": [244, 265], "32": [3, 6, 10, 117, 123, 125, 128, 159, 161, 168, 184, 190, 235, 238, 305, 308, 309, 312, 316, 325, 328], "320": [6, 235, 238], "32145682e": [244, 266], "32192443e": [244, 266], "32244647e": [244, 266], "32266051e": [244, 266], "32296453e": [244, 266], "32373542e": [244, 266], "32401919e": [244, 266], "32454751e": [244, 266], "32564947e": [244, 266], "326001912355423": 14, "32635012e": [244, 266], "32654747e": [244, 266], "3266499161421599": 14, "32665": 14, "32722983e": [244, 266], "32756231e": [244, 266], "32768": [117, 120, 123, 125, 130], "32768166e": [244, 266], "32890965e": [244, 266], "32902160e": [244, 266], "32918817e": [244, 266], "32b": [125, 130, 131], "32gb": 14, "32k": [108, 109, 112, 125, 131], "32m": [13, 14, 275, 282], "33": [244, 263, 265, 266], "33023707e": [244, 266], "33163792e": [244, 266], "33167297e": [244, 266], "33281359e": [244, 266], "33300245e": [244, 266], "33315668e": [244, 266], "33572224e": [244, 266], "33688403e": [244, 266], "33728483e": [244, 266], "33760041e": [244, 266], "33768886e": [244, 266], "33827804e": [244, 266], "33832851e": [244, 266], "33951919e": [244, 266], "34": [13, 14, 15, 244, 266], "34206163e": [244, 266], "34323069e": [244, 266], "34348310e": [244, 266], "34449054e": [244, 266], "34561165e": [244, 266], "34613437e": [244, 266], "34668782e": [244, 266], "34740751e": [244, 266], "34842591e": [244, 266], "34999743e": [244, 266], "35": [13, 14, 244, 266, 275, 282], "35016027e": [244, 266], "35024523e": [244, 266], "35026570e": [244, 266], "35185423e": [244, 266], "35189386e": [244, 266], "35268092e": [244, 266], "35481167e": [244, 266], "35665376e": [244, 266], "35665385e": [244, 266], "35833579e": [244, 266], "35873899e": [244, 266], "35890651e": [244, 266], "36": [14, 117, 123, 275, 282], "36150215e": [244, 266], "36240765e": [244, 266], "365191": 13, "36595646e": [244, 266], "36710434e": [244, 266], "36786141e": [244, 266], "36829392e": [244, 266], "36855166e": [244, 266], "36857450e": [244, 266], "36868265e": [244, 266], "36873224e": [244, 266], "36m": [13, 14, 16, 275, 282], "37": [13, 14, 117, 123], "37080820e": [244, 266], "37142141e": [244, 266], "37153175e": [244, 266], "37196398e": [244, 266], "37391533e": [244, 266], "37605290e": [244, 266], "37751494e": [244, 266], "37764162e": [244, 266], "37784477e": [244, 266], "37850042e": [244, 266], "37964366e": [244, 266], "38": [275, 282], "38109175e": [244, 266], "38115362e": [244, 266], "38281021e": [244, 266], "384": [117, 123, 244, 265, 266], "38448": 14, "384480": 14, "38451725e": [244, 266], "38496622e": [244, 266], "38554320e": [244, 266], "38563488e": [244, 266], "38687068e": [244, 266], "38798335e": [244, 266], "38858718e": [244, 266], "38910706e": [244, 266], "39053045e": [244, 266], "39066431e": [244, 266], "39268537e": [244, 266], "39287962e": [244, 266], "39421923e": [244, 266], "39532143e": [244, 266], "39561000e": [244, 266], "39590132e": [244, 266], "39660065e": [244, 266], "39745891e": [244, 266], "39747020e": [244, 266], "39796034e": [244, 266], "39855982e": [244, 266], "39905545e": [244, 266], "3b": [125, 129], "3d": [312, 315], "3f": [325, 328, 329, 330], "3rd": [244, 265], "3x3": [202, 204, 215], "4": [12, 42, 73, 78, 84, 96, 118, 119, 120, 121, 128, 129, 130, 142, 145, 146, 148, 163, 177, 191, 197, 202, 203, 230, 233, 236, 244, 265, 266, 267, 274, 308, 309, 316, 319, 322, 324, 328, 335, 337, 341], "40": [13, 146, 244, 265, 267, 270], "4000": [3, 159, 165], "40064341e": [244, 266], "40084913e": [244, 266], "400b": [117, 119, 124, 125, 131], "40222309e": [244, 266], "40240113e": [244, 266], "40254933e": [244, 266], "403": [13, 14], "40336857e": [244, 266], "40409318e": [244, 266], "40456108e": [244, 266], "40510444e": [244, 266], "40537590e": [244, 266], "406": [338, 341], "40600796e": [244, 266], "40880044e": [244, 266], "40937243e": [244, 266], "40g": [117, 120, 121], "41": [1, 151, 152, 275, 282], "41169238e": [244, 266], "41280317e": [244, 266], "41516277e": [244, 266], "41520910e": [244, 266], "41526775e": [244, 266], "41575071e": [244, 266], "41598": [275, 282], "41599": [275, 282], "415m": 146, "41709536e": [244, 266], "41786465e": [244, 266], "41920993e": [244, 266], "41922843e": [244, 266], "41933542e": [244, 266], "41968962e": [244, 266], "41985670e": [244, 266], "42": [4, 147, 150, 318, 320, 325, 327, 338, 341], "420m": 146, "42177847e": [244, 266], "422321": [275, 282], "42241838e": [244, 266], "42471355e": [244, 266], "42482564e": [244, 266], "42548003e": [244, 266], "42604055e": [244, 266], "42662169e": [244, 266], "42837034e": [244, 266], "42856956e": [244, 266], "42857780e": [244, 266], "42904809e": [244, 266], "42943636e": [244, 266], "43057": 12, "43168001e": [244, 266], "43248991e": [244, 266], "43267226e": [244, 266], "43328887e": [244, 266], "43732": 13, "43747482e": [244, 266], "43779554e": [244, 266], "43806068e": [244, 266], "43821533e": [244, 266], "43902507e": [244, 266], "43916206e": [244, 266], "43940079e": [244, 266], "43996522e": [244, 266], "44": [12, 14], "44072895e": [244, 266], "44087312e": [244, 266], "44237953e": [244, 266], "44269560e": [244, 266], "44299744e": [244, 266], "443": [17, 22], "44326049e": [244, 266], "44500265e": [244, 266], "44628939e": [244, 266], "44769201e": [244, 266], "44773570": 13, "44875658e": [244, 266], "44877301e": [244, 266], "44888324e": [244, 266], "44945610e": [244, 266], "45": [13, 28, 31, 244, 265], "45237213e": [244, 266], "45387161e": [244, 266], "45463008e": [244, 266], "45558545e": [244, 266], "45565206e": [244, 266], "45596695e": [244, 266], "456": [146, 338, 341], "45615": [244, 263, 266], "45630976e": [244, 266], "45667797e": [244, 266], "45803327e": [244, 266], "45804777e": [244, 266], "45845979e": [244, 266], "45928478e": [244, 266], "45977615e": [244, 266], "45981687e": [244, 266], "46": [43, 46, 53, 58, 275, 282], "46038486e": [244, 266], "46128924e": [244, 266], "46165411e": [244, 266], "46190545e": [244, 266], "46210968e": [244, 266], "46212946e": [244, 266], "46281177e": [244, 266], "46350823e": [244, 266], "46354072e": [244, 266], "46371266e": [244, 266], "46490431e": [244, 266], "46654081e": [244, 266], "46736982e": [244, 266], "46787928e": [244, 266], "46938870e": [244, 266], "46954274e": [244, 266], "47110615e": [244, 266], "47293536e": [244, 266], "47399181e": [244, 266], "47439016e": [244, 266], "47477984e": [244, 266], "475m": 146, "47602344e": [244, 266], "47635157e": [244, 266], "47678024e": [244, 266], "47783903e": [244, 266], "47834991e": [244, 266], "47896763e": [244, 266], "47925606e": [244, 266], "47997355e": [244, 266], "48": [13, 117, 123, 137, 141, 331, 332, 333], "48048115e": [244, 266], "480m": 146, "485": [338, 341], "485m": 146, "48663167e": [244, 266], "48813944e": [244, 266], "48855758e": [244, 266], "48856053e": [244, 266], "49": [13, 108, 109, 115, 117, 122, 275, 282], "49026504e": [244, 266], "49106100e": [244, 266], "49187856e": [244, 266], "49249397e": [244, 266], "49257514e": [244, 266], "49307863e": [244, 266], "49331436e": [244, 266], "49361154e": [244, 266], "49425897e": [244, 266], "49631714e": [244, 266], "49642932e": [244, 266], "49779003e": [244, 266], "49808554e": [244, 266], "49876371e": [244, 266], "49959813e": [244, 266], "4f": [318, 322], "4k": [108, 109, 112, 125, 131], "4m37": 14, "4th": [244, 265], "5": [2, 7, 12, 16, 29, 37, 44, 55, 68, 125, 128, 129, 131, 142, 145, 146, 153, 158, 162, 163, 168, 177, 190, 202, 206, 210, 215, 220, 229, 231, 232, 233, 244, 265, 266, 267, 273, 274, 280, 309, 311, 314, 319, 322, 335, 337, 344, 345, 347], "50": [10, 13, 15, 28, 30, 117, 123, 184, 188, 267, 273, 289, 290, 291, 292, 293, 295, 296, 303, 304, 305, 311, 312, 317, 325, 328, 330], "500": [7, 137, 141, 229, 233, 305, 307, 338, 340, 341], "50099332e": [244, 266], "50164117e": [244, 266], "50424745e": [244, 266], "50576949e": [244, 266], "50653918e": [244, 266], "50785267e": [244, 266], "50892793e": [244, 266], "50946071e": [244, 266], "50_per_index": [10, 15, 16, 184, 188, 190, 193], "51002133e": [244, 266], "51042950e": [244, 266], "51119480e": [244, 266], "512": [13, 202, 221, 224, 226, 318, 322], "51281480e": [244, 266], "51315774e": [244, 266], "51320172e": [244, 266], "51383309e": [244, 266], "51503164e": [244, 266], "51518283e": [244, 266], "51617597e": [244, 266], "51739728e": [244, 266], "51786283e": [244, 266], "51865722e": [244, 266], "52096841e": [244, 266], "52135583e": [244, 266], "52154651e": [244, 266], "52324782e": [244, 266], "525325868955": [17, 22], "52539432e": [244, 266], "52647242e": [244, 266], "53": 13, "53193595e": [244, 266], "53377999e": [244, 266], "53381238e": [244, 266], "53534813e": [244, 266], "53647423e": [244, 266], "53716086e": [244, 266], "53754605e": [244, 266], "53959617e": [244, 266], "53998228e": [244, 266], "54": [12, 275, 282, 325, 326, 327], "5404948": 12, "54113623e": [244, 266], "54230055e": [244, 266], "54291149e": [244, 266], "54304842e": [244, 266], "54450669e": [244, 266], "54546804e": [244, 266], "54573391e": [244, 266], "54621774e": [244, 266], "54645248e": [244, 266], "54685497e": [244, 266], "55": 13, "55025972e": [244, 266], "55099240e": [244, 266], "550_000": [6, 235, 239], "5529555": 12, "5552995": 12, "55601753e": [244, 266], "55613178e": [244, 266], "55635225e": [244, 266], "55699139e": [244, 266], "5588189": 12, "55900936e": [244, 266], "55923614e": [244, 266], "56011016e": [244, 266], "56041365e": [244, 266], "56115811e": [244, 266], "56188577e": [244, 266], "56274483e": [244, 266], "56389399e": [244, 266], "56662523e": [244, 266], "56688479e": [244, 266], "56718080e": [244, 266], "56896329e": [244, 266], "56896693e": [244, 266], "56920916e": [244, 266], "57": [117, 123], "57008413e": [244, 266], "57156566e": [244, 266], "57160601e": [244, 266], "57175567e": [244, 266], "57226282e": [244, 266], "57393692e": [244, 266], "57440994e": [244, 266], "57502690e": [244, 266], "57536250e": [244, 266], "57737350e": [244, 266], "57877821e": [244, 266], "57970206e": [244, 266], "58": 12, "580": [137, 141, 325, 326, 327], "58003160e": [244, 266], "58086956e": [244, 266], "580k": [325, 327], "580x580x3": [137, 141], "58155808e": [244, 266], "58189368e": [244, 266], "58261052e": [244, 266], "58301930e": [244, 266], "58452298e": [244, 266], "58531007e": [244, 266], "58614498e": [244, 266], "58806413e": [244, 266], "58818898e": [244, 266], "58824509e": [244, 266], "58904049e": [244, 266], "59": [12, 125, 130], "59060508e": [244, 266], "59105931e": [244, 266], "59260547e": [244, 266], "59349391e": [244, 266], "59438765e": [244, 266], "59552705e": [244, 266], "59555081e": [244, 266], "59728657e": [244, 266], "59733031e": [244, 266], "59831755e": [244, 266], "59980466e": [244, 266], "5_anyscalejob": [89, 103], "6": [14, 15, 16, 133, 136, 142, 145, 146, 177, 244, 265, 266, 314, 326, 335, 340], "60": [7, 14, 142, 145, 202, 204, 229, 231], "60254669e": [244, 266], "60417205": 12, "60559933e": [244, 266], "60700288e": [244, 266], "60769555e": [244, 266], "60900429e": [244, 266], "60926636e": [244, 266], "60929009e": [244, 266], "61124960e": [244, 266], "61173157e": [244, 266], "61210120e": [244, 266], "61377022e": [244, 266], "61495838e": [244, 266], "61626707e": [244, 266], "61644816e": [244, 266], "61783993e": [244, 266], "61808310e": [244, 266], "62014505e": [244, 266], "62131321e": [244, 266], "62209135e": [244, 266], "62317707e": [244, 266], "62408248e": [244, 266], "62470581e": [244, 266], "62676229e": [244, 266], "62782574e": [244, 266], "6290559": 12, "629055917263031": 12, "63": [275, 282], "63008086e": [244, 266], "63077107e": [244, 266], "63363028e": [244, 266], "63391770e": [244, 266], "63410094e": [244, 266], "63496330e": [244, 266], "63529071e": [244, 266], "63559180e": [244, 266], "63716504e": [244, 266], "63769671e": [244, 266], "6379": [133, 136], "64": [5, 6, 7, 13, 14, 202, 205, 229, 232, 233, 235, 238, 244, 256, 265, 318, 321, 322, 331, 334, 338, 344, 347], "640": [6, 235, 238], "64042781e": [244, 266], "641551": 13, "64353992e": [244, 266], "64574068e": [244, 266], "64824829e": [244, 266], "64927173e": [244, 266], "65101083e": [244, 266], "65288996e": [244, 266], "65297012e": [244, 266], "65683129e": [244, 266], "65831": 14, "65908886e": [244, 266], "66008": 12, "66033891e": [244, 266], "66034375e": [244, 266], "660569": 12, "662196": 12, "66246349e": [244, 266], "662499": 12, "66510823e": [244, 266], "66672035e": [244, 266], "66712171e": [244, 266], "66808441e": [244, 266], "66871315e": [244, 266], "66888866e": [244, 266], "67074795e": [244, 266], "67096058e": [244, 266], "67168414e": [244, 266], "67200039e": [244, 266], "67249476e": [244, 266], "67394597e": [244, 266], "67530221e": [244, 266], "67625724e": [244, 266], "67702921e": [244, 266], "68044616e": [244, 266], "682": [318, 320], "68261507e": [244, 266], "68279577e": [244, 266], "68410710e": [244, 266], "68623075e": [244, 266], "68863142e": [244, 266], "68928802e": [244, 266], "68990344e": [244, 266], "69230062e": [244, 266], "69485952e": [244, 266], "69655108e": [244, 266], "69689246e": [244, 266], "69690510e": [244, 266], "69849765e": [244, 266], "69882979e": [244, 266], "69974936e": [244, 266], "6_000108_000000": [6, 235, 238], "6_2024": 14, "6_anyscaleservic": [90, 106], "6f": [84, 96, 275, 280], "6th": [244, 265], "6x": 14, "7": [7, 12, 14, 15, 142, 145, 146, 177, 202, 205, 229, 232, 233, 244, 265, 266, 309, 322, 328, 335, 340], "70": [117, 119], "70078446e": [244, 266], "70130879e": [244, 266], "70160577e": [244, 266], "70176884e": [244, 266], "70207208e": [244, 266], "70480572e": [244, 266], "70483862e": [244, 266], "70502144e": [244, 266], "70606668e": [244, 266], "70623609e": [244, 266], "70640138e": [244, 266], "70758316e": [244, 266], "70849383e": [244, 266], "70991046e": [244, 266], "70b": [108, 109, 112, 120, 121, 122, 123, 124, 125, 131], "71": 12, "71107422e": [244, 266], "71276686e": [244, 266], "71327189e": [244, 266], "71361454e": [244, 266], "71367359e": [244, 266], "71599907e": [244, 266], "71614686e": [244, 266], "71677093e": [244, 266], "71802861e": [244, 266], "71831726e": [244, 266], "72043703e": [244, 266], "72060728e": [244, 266], "72087287e": [244, 266], "72232352e": [244, 266], "72505680e": [244, 266], "72602186e": [244, 266], "72664893e": [244, 266], "72793153e": [244, 266], "72918749e": [244, 266], "72964428e": [244, 266], "73005784e": [244, 266], "73056117e": [244, 266], "73160497e": [244, 266], "73338448e": [244, 266], "73339425e": [244, 266], "73386657e": [244, 266], "73471044e": [244, 266], "73475703e": [244, 266], "73528048e": [244, 266], "73541475e": [244, 266], "73842654e": [244, 266], "73909385e": [244, 266], "73954112e": [244, 266], "73962507e": [244, 266], "74006203e": [244, 266], "74092592e": [244, 266], "74106744e": [244, 266], "74198601e": [244, 266], "74435990e": [244, 266], "74639919e": [244, 266], "74725649e": [244, 266], "75": [117, 123, 338, 347], "75087003e": [244, 266], "75092961e": [244, 266], "75342406e": [244, 266], "75359203e": [244, 266], "75361482e": [244, 266], "75378935e": [244, 266], "75653571e": [244, 266], "75755769e": [244, 266], "75899668e": [244, 266], "76055871e": [244, 266], "76067518e": [244, 266], "76138058e": [244, 266], "76263633e": [244, 266], "76323075e": [244, 266], "76346910e": [244, 266], "76389585e": [244, 266], "76404205e": [244, 266], "76448804e": [244, 266], "76463524e": [244, 266], "764641": [275, 282], "76568236e": [244, 266], "76702512e": [244, 266], "76715726e": [244, 266], "76780378e": [244, 266], "76795331e": [244, 266], "768": [117, 123], "76875392e": [244, 266], "76900255e": [244, 266], "76936167e": [244, 266], "76972622e": [244, 266], "77": [6, 235, 238], "77095975e": [244, 266], "7721": 14, "7722": 14, "7725": 14, "77374096e": [244, 266], "774": [1, 151, 152], "77459675e": [244, 266], "77555919e": [244, 266], "77606642e": [244, 266], "77784879e": [244, 266], "77830246e": [244, 266], "78041089e": [244, 266], "78072055e": [244, 266], "78197911e": [244, 266], "78284839e": [244, 266], "7834368348121643": 13, "783437": 13, "78439620e": [244, 266], "78474542e": [244, 266], "78715137e": [244, 266], "78778227e": [244, 266], "78793629e": [244, 266], "78836194e": [244, 266], "78927866e": [244, 266], "79144512e": [244, 266], "79223316e": [244, 266], "79288665e": [244, 266], "79409343e": [244, 266], "79455946e": [244, 266], "79656491e": [244, 266], "79875319e": [244, 266], "79880509e": [244, 266], "79888725e": [244, 266], "799808": [275, 282], "79x": [117, 123], "7am": [244, 265], "7b": [108, 109, 113, 117, 119, 124, 125, 131], "7th": [244, 263, 265, 266], "7x": [5, 6, 13, 235, 240], "8": [6, 12, 15, 117, 118, 119, 120, 121, 123, 124, 125, 131, 146, 161, 165, 177, 182, 202, 204, 207, 208, 215, 235, 238, 239, 244, 265, 266, 267, 273, 281, 306, 307, 309, 313, 314, 316, 319, 320, 332, 334, 339, 340, 344, 345], "80": [117, 123, 305, 307, 312, 314, 318, 320, 325, 327], "800": [117, 119], "8000": [0, 4, 11, 12, 16, 108, 109, 115, 117, 121, 125, 128, 129, 130, 142, 145, 147, 150, 196, 200, 292, 296, 304], "80058852e": [244, 266], "80134752e": [244, 266], "80346647e": [244, 266], "80356482e": [244, 266], "80584863e": [244, 266], "80600139e": [244, 266], "80770779e": [244, 266], "80772168e": [244, 266], "80778313e": [244, 266], "80779386e": [244, 266], "8080": [133, 136], "80b": [117, 119, 125, 131], "81101489e": [244, 266], "81150190e": [244, 266], "81153491e": [244, 266], "81209888e": [244, 266], "81385726e": [244, 266], "8147535920143127": 13, "814754": 13, "81605020e": [244, 266], "81611199e": [244, 266], "81677291e": [244, 266], "81750199e": [244, 266], "8192": [108, 109, 115, 125, 128, 129], "81969379e": [244, 266], "81974494e": [244, 266], "81992236e": [244, 266], "82237032e": [244, 266], "82248098e": [244, 266], "82377563e": [244, 266], "82634446e": [244, 266], "8265": [1, 133, 136, 151, 152], "82670507e": [244, 266], "82704625e": [244, 266], "82709087e": [244, 266], "82714777e": [244, 266], "82736081e": [244, 266], "82819171e": [244, 266], "82876396e": [244, 266], "82900697e": [244, 266], "82929088e": [244, 266], "82934299e": [244, 266], "82985464e": [244, 266], "83": 13, "83045536e": [244, 266], "83186028e": [244, 266], "83414386e": [244, 266], "83462293e": [244, 266], "83483610e": [244, 266], "83486040e": [244, 266], "837": [117, 123], "83987024e": [244, 266], "84016372e": [244, 266], "84025085e": [244, 266], "84072098e": [244, 266], "84077434e": [244, 266], "84100178e": [244, 266], "84200376e": [244, 266], "84204574e": [244, 266], "84257627e": [244, 266], "84342591e": [244, 266], "84438897e": [244, 266], "84560393e": [244, 266], "84609653e": [244, 266], "84649059e": [244, 266], "84724203e": [244, 266], "84787306e": [244, 266], "84826868e": [244, 266], "85": [125, 128], "85011083e": [244, 266], "85030222e": [244, 266], "85491417e": [244, 266], "85630648e": [244, 266], "85674404e": [244, 266], "85712914e": [244, 266], "85737485e": [244, 266], "86": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 125, 130], "86025219e": [244, 266], "86067504e": [244, 266], "86110076e": [244, 266], "86166140e": [244, 266], "86302094e": [244, 266], "86359452e": [244, 266], "86543170e": [244, 266], "86763499e": [244, 266], "87068461e": [244, 266], "87136489e": [244, 266], "87145798e": [244, 266], "87155861e": [244, 266], "87222108e": [244, 266], "87272584e": [244, 266], "87305135e": [244, 266], "87395632e": [244, 266], "87503409e": [244, 266], "87622940e": [244, 266], "87687856e": [244, 266], "87722724e": [244, 266], "87823978e": [244, 266], "87826851e": [244, 266], "88060308e": [244, 266], "88125470e": [244, 266], "88136353e": [244, 266], "88193573e": [244, 266], "88231084e": [244, 266], "88282757e": [244, 266], "88339034e": [244, 266], "88483773e": [244, 266], "88547347e": [244, 266], "88624009e": [244, 266], "88834351e": [244, 266], "88852262e": [244, 266], "89050034e": [244, 266], "89146360e": [244, 266], "89227986e": [244, 266], "89326443e": [244, 266], "89384127e": [244, 266], "89393270e": [244, 266], "89414667e": [244, 266], "89564747e": [244, 266], "89613281e": [244, 266], "89739510e": [244, 266], "89740060e": [244, 266], "89891556e": [244, 266], "89910729e": [244, 266], "8b": [108, 109, 115, 125, 128, 131], "8cpu": 14, "8gb": [43, 50, 53, 62], "8k": [108, 109, 112, 125, 131], "8th": [244, 263, 265], "8xl40": [117, 123], "9": [3, 7, 12, 14, 15, 28, 29, 35, 37, 43, 44, 46, 47, 55, 58, 59, 68, 75, 137, 141, 146, 159, 168, 202, 204, 205, 219, 229, 231, 232, 244, 266, 275, 280, 307, 311, 333, 339, 340], "90": [43, 44, 125, 129, 137, 141, 244, 265], "90151963e": [244, 266], "90165711e": [244, 266], "90290013e": [244, 266], "90392175e": [244, 266], "90476887e": [244, 266], "90511677e": [244, 266], "90528610e": [244, 266], "90640": 14, "90656148e": [244, 266], "90659517e": [244, 266], "90668219e": [244, 266], "90833239e": [244, 266], "91": 13, "91010717e": [244, 266], "91011913e": [244, 266], "91205712e": [244, 266], "91437262e": [244, 266], "91462375e": [244, 266], "91614705e": [244, 266], "91802080e": [244, 266], "91938744e": [244, 266], "92106171e": [244, 266], "92230538e": [244, 266], "92250460e": [244, 266], "92267558e": [244, 266], "92452052e": [244, 266], "92608377e": [244, 266], "92633970e": [244, 266], "92704949e": [244, 266], "92848936e": [244, 266], "92968845e": [244, 266], "93108886e": [244, 266], "93218452e": [244, 266], "93256009e": [244, 266], "93267390e": [244, 266], "93274853e": [244, 266], "93358018e": [244, 266], "93396618e": [244, 266], "93505753e": [244, 266], "93599756e": [244, 266], "93615": 12, "93653901e": [244, 266], "93655512e": [244, 266], "93872128e": [244, 266], "93877090e": [244, 266], "93991698e": [244, 266], "94008095e": [244, 266], "94048835e": [244, 266], "94202918e": [244, 266], "942508": 13, "9425080418586731": 13, "943": [318, 320], "94449548e": [244, 266], "94474315e": [244, 266], "94507216e": [244, 266], "94511395e": [244, 266], "94559568e": [244, 266], "94806529e": [244, 266], "94919703e": [244, 266], "94921815e": [244, 266], "94926137e": [244, 266], "949393": [275, 282], "94980036e": [244, 266], "950654": 13, "9506543874740601": 13, "95128240e": [244, 266], "95309842e": [244, 266], "95409912e": [244, 266], "95493992e": [244, 266], "95530374e": [244, 266], "95612733e": [244, 266], "95642184e": [244, 266], "95678936e": [244, 266], "95717700e": [244, 266], "95755831e": [244, 266], "95777296e": [244, 266], "95807119e": [244, 266], "9590334296226501": [292, 293, 295, 296, 304], "959243851260": [28, 30], "96": 15, "96016713e": [244, 266], "96078059e": [244, 266], "96112816e": [244, 266], "96223371e": [244, 266], "96230914e": [244, 266], "96423475e": [244, 266], "96519734e": [244, 266], "96568063e": [244, 266], "96672040e": [244, 266], "96707787e": [244, 266], "96917129e": [244, 266], "97122377e": [244, 266], "97128675e": [244, 266], "97161290e": [244, 266], "97234564e": [244, 266], "97312343e": [244, 266], "97468323e": [244, 266], "97478577e": [244, 266], "97603959e": [244, 266], "97606084e": [244, 266], "97651729e": [244, 266], "97699012e": [244, 266], "97741865e": [244, 266], "97773625e": [244, 266], "97899076e": [244, 266], "97899440e": [244, 266], "97910169e": [244, 266], "97926176e": [244, 266], "97952076e": [244, 266], "97973699e": [244, 266], "98": 15, "98070179e": [244, 266], "98221380e": [244, 266], "98234466e": [244, 266], "98250581e": [244, 266], "98440845e": [244, 266], "98496": 14, "98530062e": [244, 266], "98645657e": [244, 266], "98712543e": [244, 266], "988": 15, "99006638e": [244, 266], "99074054e": [244, 266], "99094741e": [244, 266], "99340957e": [244, 266], "99445761e": [244, 266], "99487356e": [244, 266], "99530393e": [244, 266], "99537045e": [244, 266], "999": [312, 314], "99955577e": [244, 266], "9996353387832642": [292, 293, 295, 296, 304], "9998507499694824": [292, 296, 304], "A": [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 24, 26, 35, 37, 41, 53, 62, 81, 82, 88, 93, 94, 101, 108, 109, 110, 112, 113, 117, 118, 119, 125, 126, 128, 131, 146, 147, 148, 150, 153, 156, 159, 161, 165, 168, 176, 177, 179, 184, 185, 188, 196, 197, 199, 202, 205, 212, 214, 229, 230, 233, 235, 236, 239, 244, 250, 251, 252, 253, 263, 264, 265, 267, 273, 274, 275, 277, 305, 308, 311, 312, 313, 315, 317, 318, 320, 321, 322, 324, 325, 327, 331, 332, 335], "AND": [86, 98], "And": [244, 265, 267, 270], "As": [1, 15, 81, 85, 89, 93, 97, 105, 125, 128, 137, 141, 151, 152, 267, 270, 274, 331, 337], "At": [2, 3, 5, 13, 17, 22, 142, 145, 153, 155, 159, 163, 244, 265, 267, 274, 305, 306, 312, 313, 338, 339], "Be": [244, 265], "But": [7, 14, 229, 232, 244, 265, 267, 270, 273, 274], "By": [3, 8, 10, 85, 89, 91, 97, 103, 142, 145, 159, 165, 169, 173, 184, 188, 190, 202, 203, 204, 212, 325, 328, 331, 332, 338, 339, 340, 341, 345, 347], "For": [1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 17, 18, 20, 24, 25, 26, 27, 43, 47, 53, 59, 66, 69, 75, 80, 81, 82, 83, 84, 85, 86, 90, 92, 93, 94, 95, 96, 97, 98, 107, 108, 109, 113, 117, 120, 121, 122, 125, 128, 129, 130, 131, 133, 136, 142, 144, 145, 147, 150, 151, 152, 153, 155, 159, 164, 166, 169, 175, 176, 180, 181, 182, 183, 184, 191, 193, 196, 201, 202, 212, 216, 244, 260, 265, 267, 268, 269, 274, 275, 276, 278, 280, 300, 312, 313, 318, 319, 325, 329, 338, 339], "IN": [142, 145], "IT": [244, 265], "If": [1, 2, 3, 4, 5, 7, 8, 9, 14, 28, 29, 35, 42, 43, 44, 50, 53, 55, 62, 63, 66, 68, 71, 78, 81, 82, 83, 84, 91, 93, 94, 95, 96, 125, 128, 147, 148, 151, 152, 153, 157, 159, 164, 165, 169, 171, 174, 175, 176, 181, 202, 208, 211, 223, 225, 226, 229, 233, 244, 256, 259, 265, 266, 267, 270, 272, 273, 274, 275, 281, 305, 311, 318, 322, 324, 331, 335, 338, 339, 345, 346], "In": [3, 4, 5, 9, 10, 11, 15, 24, 27, 28, 30, 43, 45, 53, 56, 80, 81, 82, 83, 84, 85, 86, 88, 91, 92, 93, 94, 95, 96, 97, 98, 101, 108, 109, 116, 117, 119, 125, 127, 128, 137, 139, 147, 150, 159, 162, 165, 166, 174, 175, 176, 182, 184, 189, 190, 191, 193, 196, 201, 202, 203, 204, 207, 212, 214, 216, 219, 222, 228, 243, 244, 253, 259, 261, 263, 264, 265, 266, 267, 270, 271, 272, 273, 274, 275, 280, 285, 288, 291, 292, 299, 301, 302, 303, 304, 305, 311, 312, 313, 317, 318, 324, 325, 326, 328, 331, 332, 335, 338, 339], "It": [0, 4, 5, 6, 7, 8, 9, 10, 15, 16, 17, 19, 22, 28, 29, 43, 44, 47, 53, 54, 59, 66, 67, 75, 81, 82, 88, 93, 94, 100, 117, 119, 125, 129, 142, 145, 147, 150, 154, 169, 174, 176, 178, 179, 184, 187, 190, 191, 202, 206, 207, 210, 229, 233, 235, 238, 244, 256, 259, 265, 266, 267, 270, 274, 275, 277, 280, 281, 282, 285, 292, 301, 312, 315, 318, 321, 325, 326, 331, 334], "Its": [8, 169, 174, 267, 274], "NOT": [244, 265], "No": [0, 9, 24, 26, 117, 124, 133, 136, 176, 181, 202, 217, 244, 265, 305, 311, 312, 313, 317, 318, 319, 338, 345, 347], "Not": [3, 5, 6, 17, 18, 159, 168, 202, 227, 235, 236, 238, 244, 250, 263, 265, 338, 347], "OR": [35, 38, 66, 69], "Of": [267, 273], "On": [13, 17, 20, 24, 27, 66, 69, 82, 87, 94, 99, 108, 109, 112, 117, 123, 318, 322, 338, 339], "One": [81, 88, 93, 101, 125, 128, 267, 270, 274, 338, 344], "Or": [0, 83, 95, 108, 117, 120, 125, 128], "Such": [267, 274], "THE": [244, 265, 267, 273], "TO": [244, 265], "That": [3, 91, 159, 161, 244, 265, 267, 273, 274], "The": [0, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 17, 20, 22, 23, 24, 25, 26, 27, 28, 31, 35, 39, 40, 43, 46, 47, 53, 58, 59, 63, 66, 69, 75, 81, 82, 83, 84, 86, 87, 88, 90, 91, 93, 94, 95, 96, 98, 99, 102, 105, 107, 112, 113, 114, 115, 117, 120, 122, 123, 125, 128, 129, 130, 133, 135, 136, 137, 141, 142, 144, 145, 146, 147, 149, 150, 153, 156, 157, 159, 162, 168, 173, 175, 176, 179, 180, 182, 184, 189, 190, 192, 202, 203, 205, 207, 208, 212, 213, 214, 215, 216, 217, 219, 220, 223, 226, 229, 231, 233, 234, 235, 239, 243, 244, 256, 259, 261, 265, 266, 267, 270, 273, 274, 275, 277, 279, 280, 281, 282, 305, 306, 312, 313, 317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 331, 334, 337, 338, 339, 347], "Their": [244, 265], "Then": [0, 1, 8, 11, 28, 31, 35, 40, 43, 47, 53, 57, 59, 66, 75, 151, 152, 169, 174, 196, 200, 244, 263, 265, 266, 267, 273, 318, 324], "There": [9, 10, 15, 85, 89, 97, 105, 176, 179, 181, 182, 184, 193, 244, 265, 267, 273, 274, 285, 291, 292, 299, 301, 303, 304], "These": [0, 3, 4, 8, 10, 15, 17, 22, 35, 39, 80, 82, 84, 92, 94, 96, 108, 109, 112, 117, 119, 133, 135, 147, 149, 159, 161, 165, 169, 173, 184, 189, 202, 204, 213, 260, 267, 268, 274, 276, 300, 318, 320], "To": [1, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 22, 25, 28, 30, 43, 45, 53, 56, 81, 83, 84, 91, 93, 95, 96, 108, 109, 115, 133, 135, 136, 142, 144, 145, 147, 150, 151, 152, 159, 162, 166, 169, 172, 176, 180, 182, 184, 189, 190, 191, 192, 193, 196, 201, 202, 212, 214, 220, 223, 224, 229, 233, 244, 250, 253, 263, 264, 265, 275, 278, 281, 318, 324, 338, 341, 346], "With": [1, 5, 16, 88, 102, 108, 109, 112, 125, 131, 137, 141, 151, 152, 202, 203, 215, 216, 221, 222, 225, 228, 267, 269, 274, 285, 292, 301, 318, 319, 325, 326, 338, 339], "_": [3, 7, 14, 16, 84, 96, 125, 128, 159, 163, 229, 233, 244, 265, 305, 306, 308, 311, 312, 313, 314, 318, 319, 331, 332, 338, 339, 347], "_2": [305, 306, 312, 313], "__call__": [4, 10, 11, 12, 15, 16, 142, 145, 147, 150, 184, 191, 196, 200, 243, 244, 253, 261, 264, 325, 329, 331, 337, 338, 347], "__file__": [83, 95], "__getitem__": [6, 235, 238, 331, 333, 338, 341], "__index_level_0__": [325, 328], "__init__": [3, 4, 6, 10, 11, 12, 15, 16, 83, 95, 147, 150, 159, 168, 184, 191, 196, 200, 202, 215, 235, 238, 244, 253, 264, 289, 290, 291, 292, 303, 305, 308, 312, 315, 318, 321, 325, 329, 331, 333, 334, 337, 338, 341, 347], "__len__": [6, 235, 238, 331, 333, 338, 341], "__main__": [84, 96, 137, 141], "__name__": [84, 96, 137, 141], "_arrow_table_from_shard": [325, 328], "_build": 0, "_class_nam": [6, 235, 238], "_config": 0, "_diffusers_vers": [6, 235, 238], "_dmat_from_arrow": [325, 328], "_k": [312, 313], "_model": [4, 12, 147, 150], "_sample_timestep": [6, 235, 238], "_shared_step": [305, 308, 312, 315], "_static": 0, "_toc": 0, "a10": [338, 339], "a100": [117, 120, 121, 123, 338, 339], "a10g": [312, 313, 316, 317], "a_random_job_nam": [35, 41, 53, 62], "abandon": [267, 274], "abil": [3, 5, 6, 80, 82, 92, 94, 137, 139, 159, 166, 202, 203, 235, 237, 267, 270], "abl": [5, 8, 9, 28, 29, 43, 44, 53, 55, 63, 66, 68, 133, 136, 169, 174, 176, 182, 292, 296, 304], "abortmultipartupload": [17, 22], "about": [5, 7, 12, 13, 14, 16, 24, 26, 28, 34, 82, 83, 84, 94, 95, 96, 108, 109, 113, 117, 122, 125, 128, 130, 137, 139, 141, 154, 155, 160, 162, 165, 166, 167, 202, 204, 206, 214, 229, 233, 243, 244, 261, 265, 267, 269, 270, 273, 274, 275, 277, 285, 292, 301, 338, 339], "abov": [3, 5, 6, 108, 109, 113, 114, 125, 128, 133, 136, 137, 141, 142, 145, 159, 165, 202, 203, 204, 205, 212, 214, 235, 239, 244, 265, 267, 274, 331, 335], "absenc": [142, 145, 244, 265, 325, 330], "absent": [267, 274], "absolut": [5, 331, 333], "abstract": [4, 8, 15, 17, 19, 91, 108, 109, 113, 114, 117, 120, 147, 149, 169, 170, 312, 313, 325, 326, 331, 337], "absurd": [267, 274], "academ": [125, 131, 331, 332], "academi": [267, 274], "acc": [3, 13, 159, 168, 325, 328], "acc_metr": [338, 343], "acceler": [1, 2, 3, 6, 8, 10, 11, 28, 34, 66, 71, 82, 91, 94, 151, 152, 153, 155, 159, 165, 169, 170, 184, 191, 196, 198, 235, 238, 239, 305, 309, 312, 316], "accelerator_shap": [12, 13, 14], "accelerator_typ": [10, 12, 13, 14, 108, 109, 115, 117, 120, 123, 125, 128, 129, 130, 184, 191, 291, 292, 303], "accept": [1, 2, 7, 10, 11, 14, 15, 151, 152, 153, 158, 184, 190, 191, 196, 198, 202, 205, 215, 229, 233, 285, 291, 292, 301, 303, 331, 334], "access": [3, 8, 17, 20, 21, 22, 27, 53, 63, 81, 83, 84, 87, 88, 91, 93, 95, 96, 99, 100, 101, 117, 118, 120, 121, 125, 126, 128, 129, 130, 133, 136, 137, 141, 142, 145, 146, 159, 161, 168, 169, 170, 202, 207, 212, 214, 221, 275, 280, 282, 285, 292, 301, 325, 327, 338, 339, 340], "accident": [325, 328], "acclaim": [267, 274], "accomplish": [202, 228], "accord": [2, 9, 10, 15, 82, 94, 153, 158, 176, 182, 184, 193, 202, 213, 221], "accordingli": [125, 131, 275, 280, 312, 317], "account": [3, 8, 17, 21, 22, 24, 26, 28, 29, 30, 35, 36, 39, 43, 44, 45, 53, 55, 56, 66, 68, 70, 78, 91, 159, 168, 169, 174, 267, 270], "account_id": [17, 22], "accross": [108, 109, 112, 113], "accumul": [5, 13, 202, 206], "accur": [108, 109, 112, 331, 332], "accuraci": [7, 10, 13, 14, 15, 117, 119, 125, 131, 184, 193, 229, 232, 275, 277, 279, 281, 325, 326, 328, 329, 330, 338, 343, 347], "accuracy_scor": [325, 327, 328], "achiev": [3, 5, 6, 9, 10, 11, 15, 108, 109, 112, 159, 162, 167, 176, 182, 184, 191, 193, 196, 198, 202, 203, 235, 237], "acid": [8, 169, 170], "acl": [88, 101], "across": [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 17, 19, 22, 24, 26, 79, 82, 83, 84, 88, 94, 95, 96, 102, 108, 109, 113, 114, 117, 120, 123, 124, 125, 128, 131, 137, 139, 141, 146, 147, 150, 151, 152, 153, 155, 159, 161, 164, 166, 169, 170, 173, 176, 182, 183, 184, 186, 188, 192, 193, 195, 202, 203, 204, 205, 206, 207, 209, 212, 213, 214, 216, 219, 220, 221, 228, 235, 239, 267, 271, 274, 275, 277, 281, 282, 285, 292, 301, 305, 306, 307, 311, 312, 313, 314, 317, 318, 319, 320, 322, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 337, 338, 339, 343, 345, 347], "act": [17, 22, 202, 207, 267, 274], "act_dim": [312, 315, 317], "act_fn": [6, 235, 238], "action": [16, 17, 22, 125, 128, 130, 137, 141, 267, 273, 274, 314, 315], "action_spac": [312, 314], "activ": [0, 2, 3, 7, 9, 10, 14, 80, 85, 89, 92, 97, 103, 108, 109, 113, 146, 153, 158, 159, 168, 176, 182, 184, 190, 229, 233, 318, 320], "actor": [5, 6, 8, 9, 11, 13, 16, 108, 109, 113, 137, 139, 161, 165, 169, 174, 176, 179, 185, 196, 198, 199, 205, 228, 235, 239, 243, 244, 253, 261, 264, 325, 329, 330, 331, 332, 337, 338, 347], "actorpoolmapoper": [10, 184, 192], "actorpoolstrategi": [325, 327, 329, 330], "actress": [267, 270], "actual": [2, 7, 10, 14, 15, 28, 30, 35, 38, 43, 45, 48, 53, 56, 60, 66, 69, 72, 76, 125, 130, 153, 157, 184, 191, 202, 212, 229, 233, 244, 265, 267, 274, 312, 314, 318, 319], "ad": [6, 16, 17, 22, 28, 30, 34, 82, 87, 91, 94, 99, 202, 216, 228, 235, 239, 244, 256, 265, 312, 313, 325, 328, 329], "adam": [5, 7, 13, 14, 202, 204, 206, 217, 223, 229, 230, 232, 233, 305, 308, 312, 315, 318, 322, 331, 335, 338, 343], "adamw": [6, 235, 238], "adapt": [108, 109, 112, 126, 127, 132, 202, 204, 209, 275, 281, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "adapter_config": [125, 128], "adapter_model": [125, 128], "adapter_nam": [125, 128], "adaptiveavgpool2d": 13, "add": [2, 3, 5, 6, 8, 9, 10, 11, 13, 16, 17, 22, 35, 39, 43, 46, 48, 51, 53, 58, 60, 63, 64, 66, 69, 73, 76, 80, 81, 82, 86, 90, 92, 93, 94, 98, 107, 108, 109, 114, 125, 128, 130, 137, 141, 142, 144, 153, 156, 157, 158, 159, 161, 162, 164, 168, 169, 170, 176, 180, 181, 184, 190, 196, 200, 202, 204, 205, 207, 210, 215, 228, 235, 238, 239, 260, 267, 268, 271, 276, 300, 305, 306, 311, 312, 314, 317, 318, 324, 325, 330, 331, 335, 337, 338, 347], "add_label": [10, 15, 184, 190, 193], "add_nois": [6, 235, 238], "add_ref": [3, 159, 166], "add_subplot": [5, 202, 204, 215], "addit": [5, 9, 17, 20, 80, 81, 82, 84, 91, 92, 93, 94, 96, 125, 128, 132, 137, 140, 176, 180, 202, 209, 222, 223, 285, 288, 292, 301, 302, 318, 319, 324, 325, 330, 338, 347], "addition": [9, 10, 16, 17, 22, 28, 34, 137, 141, 176, 182, 184, 190, 275, 281, 318, 319], "addr": [305, 309], "address": [2, 8, 17, 22, 28, 30, 43, 45, 53, 56, 133, 136, 153, 155, 169, 170, 202, 213, 244, 265], "adebayor": [244, 265], "adher": [267, 274], "adjust": [6, 9, 125, 131, 176, 181, 202, 205, 235, 238, 244, 256, 265, 267, 273, 275, 282, 325, 328, 338, 347], "adjust_total_amount": [9, 176, 180], "adjusted_data": [9, 176, 181, 183], "adjusted_data_rai": [9, 176, 181, 183], "adjusted_total_amount": [9, 142, 144, 176, 180], "admin": [17, 18, 22, 88, 101], "administr": [267, 274, 348], "admittedli": [267, 270], "adopt": [9, 176, 179], "adv": [244, 263, 265], "advanc": [6, 7, 8, 14, 24, 26, 27, 28, 34, 82, 94, 108, 109, 116, 118, 124, 128, 142, 145, 169, 170, 174, 229, 232, 235, 238, 338, 347, 348], "advantag": [8, 117, 119, 169, 174], "adventureland": [244, 265], "adversari": [305, 306], "affect": [11, 196, 199], "affin": 13, "afford": [117, 119, 244, 265], "after": [1, 2, 3, 4, 5, 6, 11, 13, 16, 43, 47, 50, 53, 57, 59, 62, 66, 75, 81, 82, 84, 90, 93, 94, 96, 106, 108, 109, 112, 137, 141, 142, 145, 146, 147, 150, 151, 152, 153, 158, 159, 161, 167, 196, 201, 202, 204, 206, 212, 215, 235, 238, 243, 244, 261, 265, 266, 267, 273, 275, 277, 282, 305, 306, 311, 312, 313, 318, 319, 320, 322, 324, 325, 327, 330, 331, 333, 335, 336, 338, 339, 343, 345], "afterward": [338, 339], "again": [3, 4, 9, 28, 33, 82, 85, 94, 97, 147, 148, 159, 165, 176, 180, 267, 274, 305, 310, 318, 323, 325, 326, 338, 346], "against": [117, 122, 125, 128, 202, 225, 244, 263, 265, 267, 273, 274, 312, 317, 318, 319, 322, 324, 331, 337, 338, 345], "agent": [108, 109, 112, 125, 131, 312, 313, 317], "aggreg": [8, 13, 169, 173, 177, 185, 189, 202, 203, 305, 308, 309, 325, 329, 330, 338, 343], "aggress": [10, 142, 145, 184, 190], "agil": [312, 313], "agnost": [331, 333], "ago": [267, 270], "agre": [267, 273], "aguero": [244, 265], "ahead": [117, 124, 125, 132, 267, 273], "ai": [1, 3, 9, 10, 11, 15, 16, 83, 85, 87, 89, 90, 91, 95, 97, 99, 103, 106, 108, 109, 115, 117, 124, 125, 127, 128, 130, 151, 152, 159, 168, 176, 183, 184, 188, 190, 191, 193, 196, 200], "air": 12, "aj": [244, 263, 265], "ak": [17, 20], "ako": [244, 265], "alaska": [267, 273], "alb": [24, 27], "alberta": [267, 273], "album": [244, 265], "alciato": [244, 263, 265], "ald": [244, 265], "alert": [86, 89, 90, 98, 103, 106, 143, 202, 228], "algorithm": [7, 12, 13, 14, 24, 27, 91, 108, 109, 114, 229, 233, 275, 282, 325, 326], "alic": 146, "align": [8, 169, 173, 202, 204, 305, 309, 338, 340, 343], "alik": [243, 244, 261], "all": [0, 1, 3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 17, 20, 22, 24, 26, 28, 29, 35, 36, 38, 43, 44, 49, 50, 51, 53, 54, 61, 62, 64, 66, 67, 69, 71, 81, 82, 83, 84, 85, 87, 88, 91, 93, 94, 95, 96, 97, 99, 101, 108, 109, 110, 111, 112, 117, 118, 125, 126, 128, 129, 132, 133, 134, 136, 137, 138, 141, 142, 143, 146, 147, 148, 149, 151, 152, 154, 159, 160, 161, 166, 167, 169, 170, 177, 179, 183, 185, 190, 202, 203, 204, 206, 207, 211, 212, 214, 221, 224, 225, 226, 229, 230, 233, 235, 236, 239, 243, 244, 253, 260, 261, 264, 265, 267, 268, 269, 270, 273, 274, 275, 276, 277, 281, 285, 292, 300, 301, 305, 309, 312, 314, 317, 318, 319, 320, 322, 324, 325, 327, 328, 329, 330, 332, 333, 338, 339, 342, 343, 345, 347], "all_fil": [83, 95], "all_results_at_onc": [3, 159, 167], "alleg": [267, 270], "allegori": [267, 274], "alli": [267, 273], "allobjectact": [17, 22], "alloc": [7, 8, 10, 14, 17, 22, 24, 25, 28, 30, 43, 45, 53, 56, 160, 169, 175, 184, 186, 190, 202, 208, 215, 229, 233, 243, 244, 261, 325, 328, 331, 332], "allow": [3, 4, 5, 6, 8, 15, 16, 17, 22, 53, 63, 80, 82, 84, 87, 88, 92, 94, 96, 99, 101, 117, 123, 125, 127, 128, 142, 145, 147, 149, 150, 159, 161, 165, 169, 175, 202, 203, 208, 209, 212, 221, 224, 226, 235, 237, 243, 244, 261, 265, 267, 274, 275, 277, 280, 281, 282, 285, 289, 290, 291, 292, 301, 303, 305, 309, 318, 323, 325, 328, 331, 335, 337, 338, 339], "allowedhead": [17, 22, 53, 63], "allowedmethod": [17, 22, 53, 63], "allowedorigin": [17, 22, 53, 63], "allreduc": [202, 203], "alltoallapi": [10, 184, 193], "allus": [267, 274], "almost": [267, 274, 325, 327], "alon": [9, 10, 15, 176, 182, 184, 193], "along": [82, 94, 202, 211, 244, 265, 338, 339], "alongsid": [202, 222, 228, 325, 327, 338, 347], "alonso": [244, 265], "alphas_cumprod": [6, 235, 238], "alreadi": [1, 9, 10, 11, 15, 43, 46, 53, 58, 81, 83, 91, 93, 95, 137, 141, 151, 152, 176, 179, 184, 193, 196, 198, 202, 204, 206, 215, 226, 244, 250, 263, 265, 318, 320, 324, 325, 329, 331, 333, 338, 346], "also": [3, 5, 9, 10, 11, 13, 15, 16, 28, 29, 35, 37, 43, 44, 50, 53, 55, 62, 66, 68, 81, 83, 84, 85, 86, 88, 90, 93, 95, 96, 97, 98, 100, 106, 108, 109, 112, 137, 141, 142, 143, 145, 159, 162, 176, 183, 184, 193, 196, 200, 202, 204, 214, 224, 244, 250, 256, 259, 263, 265, 266, 267, 269, 270, 273, 274, 285, 288, 292, 301, 302, 305, 309, 318, 320, 321, 322, 325, 327, 331, 335], "altern": [1, 28, 29, 43, 44, 53, 55, 66, 68, 151, 152, 275, 280, 325, 330], "alwai": [0, 3, 125, 128, 129, 137, 140, 159, 165, 325, 330, 331, 333], "am": [267, 270], "amaz": [125, 132, 244, 265, 267, 273], "amazon": [8, 17, 20, 28, 29, 43, 44, 53, 55, 83, 84, 95, 96, 169, 170, 244, 265], "amazonaw": [13, 14, 17, 22], "amazonelasticfilesystemclientreadwriteaccess": [53, 57, 63], "ambush": [267, 273], "america": [267, 270], "american": [267, 270, 273, 274], "ami": [24, 26, 244, 265], "amnt": [244, 265], "among": [9, 10, 15, 117, 120, 176, 182, 184, 193, 267, 273, 274], "amount": [3, 4, 5, 6, 8, 9, 10, 12, 83, 95, 147, 150, 159, 168, 169, 170, 176, 179, 184, 190, 202, 203, 235, 237, 331, 335, 337, 338, 339], "amp": [244, 265, 305, 311, 338, 347], "amus": [267, 274], "an": [0, 2, 5, 8, 9, 10, 13, 15, 18, 21, 22, 23, 24, 27, 28, 29, 35, 41, 43, 44, 52, 55, 65, 81, 83, 85, 86, 87, 88, 91, 93, 95, 97, 98, 99, 100, 101, 103, 108, 109, 112, 114, 115, 120, 121, 123, 126, 128, 133, 136, 137, 138, 141, 142, 144, 145, 149, 153, 155, 157, 160, 162, 163, 164, 168, 169, 170, 174, 175, 176, 177, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 197, 199, 201, 202, 203, 204, 244, 250, 251, 252, 253, 256, 260, 263, 264, 265, 267, 268, 270, 272, 273, 274, 275, 276, 281, 285, 289, 290, 291, 292, 300, 301, 303, 305, 306, 311, 313, 318, 319, 325, 326, 328, 329, 330, 331, 332, 338, 339, 341, 348], "anal": [267, 274], "analys": [267, 274], "analysi": [8, 79, 84, 96, 108, 109, 112, 125, 128, 131, 169, 173, 267, 274, 285, 289, 290, 291, 292, 296, 299, 301, 303, 304], "analyt": [8, 169, 170, 171, 173], "analyz": [8, 125, 128, 133, 135, 169, 170, 202, 214], "anatom": [267, 270], "anatomi": [244, 265], "angel": [267, 273, 274], "angelbr": [267, 274], "anger": [244, 265], "anggrek": [244, 265], "angl": [312, 313], "angular": [312, 313], "ani": [1, 2, 3, 4, 7, 9, 10, 11, 14, 15, 16, 28, 30, 35, 39, 42, 43, 45, 53, 56, 66, 78, 80, 81, 87, 91, 92, 93, 99, 125, 128, 133, 136, 137, 141, 142, 144, 147, 150, 151, 152, 153, 155, 156, 159, 165, 176, 179, 184, 191, 194, 196, 197, 200, 202, 207, 220, 227, 229, 230, 232, 233, 244, 253, 264, 267, 269, 270, 273, 274, 289, 290, 291, 292, 296, 303, 304, 305, 306, 309, 312, 317, 318, 319, 322, 324, 325, 326, 327, 328, 330, 331, 337, 338, 339, 347], "anniversari": [244, 265], "annot": [7, 24, 27, 229, 233, 325, 329], "anon": [6, 235, 238], "anonym": [12, 142, 144], "anoth": [9, 10, 24, 27, 82, 85, 89, 94, 97, 105, 176, 180, 184, 189, 244, 263, 265, 267, 271, 273, 274, 305, 311], "answer": [108, 109, 112, 244, 265, 267, 270], "ant": [244, 265], "anthoni": [267, 273, 274], "anti": [3, 154, 159, 162, 167, 244, 265, 267, 274], "antiwoman": [267, 274], "anymor": [305, 311, 318, 324], "anyon": [244, 265, 267, 270], "anyscal": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 20, 21, 27, 32, 33, 34, 37, 38, 41, 42, 46, 49, 50, 51, 52, 55, 57, 58, 61, 62, 63, 64, 65, 68, 69, 72, 77, 78, 101, 102, 110, 115, 116, 118, 119, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 141, 144, 146, 147, 148, 150, 151, 152, 153, 154, 159, 160, 169, 170, 176, 177, 179, 182, 184, 185, 188, 190, 191, 193, 196, 197, 200, 202, 203, 204, 228, 229, 230, 235, 236, 240, 243, 244, 250, 261, 263, 267, 269, 275, 277, 285, 292, 301, 311, 317, 324, 330, 337, 344], "anyscale_101": [89, 90, 103, 106], "anyscale_artifact_storag": [6, 83, 95, 137, 141, 235, 238], "anyscale_cloud_id": [35, 39], "anyscale_cloud_nam": [17, 23, 28, 29, 30, 31, 32, 33, 35, 36, 39, 40, 41, 42, 43, 44, 45, 47, 50, 51, 53, 54, 56, 59, 62, 64, 66, 67, 69, 70, 75, 77, 78], "anyscale_cloud_storage_bucket": [83, 95], "anyscale_cloud_storage_bucket_region": [83, 95], "anyscale_iam_rol": [53, 57], "anyscale_iam_role_arn": [17, 23], "anyscale_iam_s3_policy_arn": [53, 57], "anyscale_registration_command": [28, 30, 35, 39, 43, 45, 53, 56, 66, 70], "anyscale_s3_bucket_nam": [28, 30, 33, 43, 45, 51, 53, 56, 64], "anyscale_security_group": [17, 22], "anyscale_vpc": [17, 22], "anyscale_vpc_nam": [17, 22], "anyscalerai": [43, 51, 53, 64], "anyscaleuserdata": [117, 122], "anyth": [24, 27, 83, 95, 202, 207, 267, 270, 274, 338, 346], "anywher": [3, 85, 97, 159, 161], "apach": 173, "aperitif": [244, 265], "api": [3, 8, 9, 10, 11, 13, 15, 16, 24, 25, 26, 27, 37, 68, 78, 85, 86, 88, 89, 90, 91, 97, 98, 101, 103, 105, 107, 108, 109, 114, 115, 117, 120, 125, 127, 129, 130, 132, 159, 168, 169, 172, 173, 174, 175, 176, 178, 183, 184, 190, 191, 196, 198, 243, 244, 250, 253, 259, 261, 263, 264, 266, 267, 272, 285, 292, 301, 305, 309, 318, 320, 325, 330, 331, 337, 338, 339, 347], "api_kei": [108, 109, 115, 117, 121, 122, 125, 128, 129, 130], "apigatewai": [142, 145, 146], "app": [2, 4, 11, 16, 53, 63, 84, 86, 90, 96, 98, 106, 107, 108, 109, 113, 115, 117, 120, 121, 122, 123, 125, 128, 129, 130, 142, 145, 146, 147, 150, 153, 155, 196, 201, 285, 291, 292, 301, 303, 305, 311], "app1": [11, 142, 145, 196, 201], "app_build": [11, 196, 201], "appapp": [289, 290], "apparatu": [312, 317], "appear": [108, 109, 113, 244, 265, 267, 270, 274], "append": [2, 3, 10, 83, 95, 125, 130, 153, 158, 159, 161, 167, 184, 190, 305, 307, 312, 314, 318, 322, 331, 333, 338, 340, 347], "appl": [1, 151, 152, 243, 244, 256, 261, 265, 275, 277, 280], "appli": [0, 3, 4, 5, 6, 9, 10, 11, 12, 15, 16, 28, 30, 31, 35, 39, 40, 42, 43, 45, 47, 53, 56, 59, 66, 70, 75, 78, 147, 150, 159, 168, 176, 178, 179, 180, 182, 184, 187, 191, 193, 196, 200, 203, 204, 206, 210, 213, 215, 235, 238, 243, 244, 256, 259, 261, 265, 266, 267, 269, 272, 274, 275, 280, 291, 292, 303, 305, 307, 312, 313, 314, 318, 319, 320, 331, 335, 338, 339, 341], "applic": [1, 3, 4, 9, 12, 15, 16, 24, 27, 28, 34, 35, 38, 66, 69, 80, 82, 85, 86, 87, 89, 90, 92, 94, 97, 98, 99, 103, 106, 107, 108, 109, 113, 115, 117, 119, 120, 122, 124, 125, 127, 129, 130, 132, 133, 135, 137, 139, 140, 143, 146, 147, 149, 151, 152, 159, 163, 170, 174, 175, 176, 178, 197, 200, 201, 285, 289, 290, 291, 292, 293, 295, 296, 301, 303, 304, 318, 324], "application_log": [291, 292, 303], "approach": [3, 8, 79, 159, 162, 169, 175, 243, 244, 261, 267, 269, 274, 275, 277, 281, 305, 306, 318, 319, 320, 325, 327, 338, 339], "appropri": [43, 50, 53, 62, 108, 109, 113, 117, 120], "approv": [28, 30, 33, 35, 39, 42, 43, 45, 51, 53, 64, 66, 70, 78], "approx": [3, 159, 161, 312, 313], "approxim": [108, 109, 113, 318, 319, 320], "april": [244, 265], "ar": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 22, 24, 26, 27, 28, 29, 30, 33, 35, 38, 39, 43, 44, 45, 49, 50, 51, 53, 55, 56, 57, 61, 62, 63, 64, 66, 68, 69, 71, 78, 80, 82, 83, 84, 85, 87, 88, 89, 92, 94, 95, 96, 97, 99, 101, 105, 108, 109, 113, 117, 119, 120, 122, 123, 124, 125, 128, 129, 130, 132, 133, 136, 137, 141, 142, 143, 144, 145, 146, 151, 152, 153, 155, 157, 158, 159, 160, 161, 165, 167, 168, 169, 170, 172, 173, 174, 175, 176, 179, 180, 181, 182, 183, 184, 188, 189, 190, 191, 193, 196, 198, 199, 202, 203, 204, 205, 206, 207, 209, 210, 212, 213, 214, 215, 217, 220, 222, 226, 229, 233, 234, 235, 239, 240, 243, 244, 253, 256, 260, 261, 264, 265, 267, 268, 270, 273, 274, 275, 276, 280, 285, 291, 292, 293, 295, 296, 299, 300, 301, 303, 304, 307, 311, 314, 317, 322, 324, 330, 337, 340, 343, 347], "arang": [331, 334, 337], "arbitrari": [2, 125, 129, 153, 155], "architectur": [5, 6, 11, 13, 16, 21, 24, 26, 79, 110, 116, 196, 199, 202, 205, 235, 238, 239, 243, 245, 246, 261, 277, 285, 301, 305, 311, 318, 321, 331, 332, 334, 338, 347], "archuleta": [244, 265], "area": [9, 125, 132, 176, 182], "aree": 108, "aren": [53, 63, 331, 335], "arena": [125, 131], "arg": [125, 129, 331, 334, 338, 347], "argmax": [5, 10, 11, 13, 15, 16, 184, 191, 196, 200, 202, 215, 275, 279, 325, 328, 329, 338, 343, 347], "argu": [267, 274], "arguabl": [267, 270], "argument": [2, 5, 6, 7, 10, 11, 14, 15, 16, 85, 97, 125, 128, 130, 153, 158, 160, 162, 165, 184, 191, 196, 200, 202, 207, 209, 229, 233, 235, 239, 244, 265, 267, 274, 275, 280], "arm": [267, 273], "arm64": [1, 151, 152], "arn": [17, 22, 28, 30, 43, 45, 53, 56, 57], "around": [5, 6, 86, 90, 98, 106, 125, 128, 202, 205, 235, 239, 267, 270, 273, 274, 338, 347], "arr": [202, 215, 305, 307], "arrai": [7, 9, 10, 11, 12, 14, 16, 125, 129, 176, 180, 184, 190, 196, 200, 202, 204, 215, 219, 220, 229, 231, 233, 244, 265, 266], "arrang": [3, 159, 166], "array_equ": [3, 159, 161], "array_split": [318, 320], "arriv": [8, 169, 173, 312, 317], "arrow_ref": [325, 328], "arsen": [244, 265], "art": [108, 109, 114, 267, 273], "articl": 16, "artifact": [5, 12, 13, 83, 95, 202, 204, 212, 213, 227, 275, 282, 325, 330, 338, 347], "artifact_dir": [325, 330], "artifact_storag": [83, 95], "artifact_storage_path": [6, 235, 238], "artifici": [312, 313], "artist": [267, 270], "as_directori": [5, 6, 13, 202, 215, 223, 235, 239, 305, 309, 311, 312, 316, 317, 318, 322, 324, 325, 328, 331, 335, 337, 338, 343, 347], "as_fram": [325, 327], "as_index": [318, 322, 331, 335, 338, 345], "as_tensor": [6, 202, 215, 235, 238], "asarrai": [305, 307], "asgi": 146, "ask": [3, 159, 168, 267, 270, 305, 309, 338, 344], "aspen": [325, 326], "assert": [3, 10, 15, 16, 159, 167, 184, 190, 305, 311, 312, 317, 325, 327], "assess": [331, 335, 337, 338, 345], "assign": [9, 10, 15, 66, 73, 87, 99, 176, 181, 184, 188, 202, 208, 213, 218, 318, 319, 325, 328, 338, 339], "assist": [131, 244, 265], "associ": [8, 81, 93, 137, 139, 169, 175, 202, 214, 318, 324], "assum": [3, 17, 22, 24, 26, 133, 136, 159, 161, 202, 204, 325, 329], "assumerol": [17, 22], "astral": [1, 151, 152], "astyp": [6, 11, 196, 200, 235, 238, 312, 314, 318, 320, 325, 329, 331, 337, 338, 347], "async": [4, 11, 12, 16, 142, 145, 147, 150, 196, 200], "asynchron": [7, 229, 232], "asyncio": [4, 8, 147, 148, 150, 169, 175], "athen": [244, 265], "atom": [8, 169, 170, 244, 265], "attach": [24, 26, 27, 63, 202, 212, 215, 224, 318, 322, 331, 335, 338, 343], "attempt": [10, 184, 189, 190, 202, 223, 331, 335], "attend": [267, 274], "attent": [108, 109, 116, 267, 270, 275, 280, 331, 332, 337], "attention_head_dim": [6, 235, 238], "attribut": [305, 309, 325, 330], "audienc": [267, 273, 274, 318, 324], "audio": [244, 265], "audit": [108, 109, 114], "augment": [202, 216, 228, 338, 347], "august": [244, 265], "auschwitz": [267, 274], "auth": [35, 38, 66, 69, 72, 331, 333], "authent": [17, 22, 24, 27, 36, 37, 68, 81, 83, 93, 95, 117, 121, 122], "author": [117, 121, 125, 128, 146], "auto": [6, 16, 24, 26, 28, 30, 33, 35, 39, 42, 43, 45, 51, 53, 64, 66, 70, 78, 86, 98, 125, 130, 142, 144, 235, 238, 239, 305, 309, 312, 313, 316, 318, 322, 338, 339, 347], "auto_select_worker_config": [108, 109, 115, 117, 122], "autocal": [24, 26], "autocomplet": [125, 131], "autodiscoveri": [43, 46, 53, 58], "autograd": [202, 215], "autom": [24, 25, 26, 81, 93, 103, 125, 130, 132, 325, 326, 331, 337], "automat": [0, 3, 5, 6, 9, 10, 13, 16, 17, 22, 24, 26, 27, 35, 39, 81, 85, 86, 89, 90, 93, 97, 98, 103, 106, 108, 109, 114, 117, 122, 159, 163, 176, 179, 184, 186, 191, 203, 204, 205, 206, 207, 209, 210, 212, 214, 217, 218, 219, 221, 222, 225, 228, 235, 237, 238, 243, 244, 250, 251, 252, 253, 261, 263, 264, 267, 270, 275, 277, 280, 291, 292, 303, 305, 306, 309, 311, 312, 313, 314, 316, 317, 318, 319, 322, 323, 324, 325, 326, 328, 330, 331, 332, 335, 336, 337, 338, 339, 342, 343, 344, 345, 346, 347], "automodelforsequenceclassif": [275, 278, 280], "autosc": [8, 11, 24, 26, 28, 34, 35, 42, 82, 84, 86, 90, 94, 96, 98, 106, 107, 108, 109, 113, 115, 169, 175, 186, 196, 199, 202, 203, 285, 292, 301], "autoscal": [3, 5, 6, 10, 13, 14, 16, 24, 26, 27, 28, 34, 49, 50, 51, 52, 61, 62, 63, 64, 65, 80, 82, 92, 94, 137, 139, 159, 165, 184, 191, 235, 237, 292, 299, 304], "autoscaling_config": [16, 108, 109, 115, 117, 120, 123, 125, 130], "autotoken": [275, 278, 280], "autotun": [7, 229, 234], "auxiliari": [312, 317], "avail": [1, 5, 6, 8, 9, 10, 17, 22, 28, 30, 43, 45, 53, 56, 81, 85, 86, 90, 93, 97, 98, 106, 108, 109, 111, 114, 117, 121, 125, 128, 131, 133, 136, 137, 140, 141, 142, 145, 151, 152, 160, 161, 162, 166, 167, 169, 175, 176, 182, 184, 190, 191, 202, 204, 205, 207, 212, 221, 223, 235, 239, 244, 256, 259, 265, 266, 267, 272, 275, 277, 280, 285, 292, 301, 305, 311, 312, 316, 318, 320, 322, 331, 336, 337, 338, 343, 347], "available_resourc": [3, 159, 165], "availi": [80, 86, 90, 92, 98, 106], "avalanch": [267, 273], "averag": [5, 6, 9, 176, 182, 202, 203, 206, 235, 239, 267, 270, 318, 322, 325, 329, 338, 343], "avg_loss": 5, "avg_train_loss": [318, 322, 331, 335], "avg_val_loss": [318, 322, 331, 335], "avgpool": 13, "avoid": [2, 3, 5, 6, 8, 9, 10, 11, 84, 85, 86, 96, 97, 98, 108, 109, 115, 142, 145, 153, 158, 159, 166, 169, 173, 176, 180, 181, 184, 186, 190, 196, 198, 202, 203, 212, 215, 224, 227, 235, 237, 238, 244, 266, 267, 270, 273, 312, 313, 317, 318, 320, 325, 327, 328, 329, 331, 333, 338, 347], "aw": [8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 30, 33, 34, 45, 48, 50, 51, 52, 55, 56, 57, 60, 63, 64, 65, 66, 76, 79, 83, 87, 88, 95, 99, 101, 125, 128, 169, 170, 176, 179, 184, 188, 191, 196, 200, 267, 274], "awai": [10, 81, 91, 93, 184, 189, 244, 265, 267, 270, 274, 337], "await": [4, 11, 12, 16, 147, 150, 196, 200], "awak": [267, 274], "awar": [312, 313, 318, 324, 331, 333], "award": [244, 265, 267, 274], "aws_region": [17, 23, 28, 30, 43, 45, 46, 48, 53, 56, 58, 60, 125, 128], "aws_role_nam": [35, 39], "awsregion": [43, 46, 53, 58], "ax": [7, 13, 14, 202, 204, 229, 231, 305, 307, 311, 338, 340], "axi": [5, 7, 10, 11, 13, 14, 15, 16, 184, 189, 191, 196, 200, 202, 204, 215, 229, 231, 275, 279, 305, 307, 311, 325, 328, 329, 331, 337, 338, 340, 347], "axvlin": [331, 337], "aydin": [125, 128], "azur": [17, 20, 202, 212, 228, 338, 347], "b": [2, 3, 11, 153, 156, 158, 159, 162, 165, 196, 200, 202, 215, 244, 265, 305, 307, 308, 318, 324, 331, 334, 335, 337, 338, 347], "babi": [244, 263, 265], "back": [3, 4, 12, 16, 86, 98, 142, 145, 147, 150, 159, 168, 202, 218, 219, 244, 263, 265, 266, 267, 273, 274, 305, 311, 312, 313, 317, 318, 320, 325, 328, 329, 338, 347], "backbon": [202, 205, 305, 311], "backdrop": [267, 274], "backend": [117, 123, 125, 128, 137, 140, 141, 142, 143, 275, 280, 281, 338, 347], "background": [2, 153, 157, 267, 274], "backpressur": [8, 108, 109, 113, 142, 144, 169, 174, 175], "backpropag": [275, 280], "backward": [5, 6, 7, 13, 14, 202, 203, 206, 217, 223, 229, 232, 233, 235, 239, 267, 274, 275, 280, 305, 306, 318, 322, 331, 335, 338, 343], "bad": [244, 265, 267, 274], "bai": [267, 274], "bake": [3, 159, 164], "balanc": [3, 11, 17, 22, 24, 27, 28, 34, 51, 52, 64, 65, 86, 90, 98, 106, 107, 108, 109, 114, 117, 119, 120, 122, 125, 131, 159, 168, 196, 198, 285, 291, 292, 301, 303, 312, 313], "bale": [244, 265], "ball": [244, 265], "band": [244, 265], "bandwidth": [108, 109, 111], "bank": [267, 274], "bar": [318, 320, 325, 327, 338, 340], "barca": [244, 265], "barcelona": [125, 128, 244, 265], "barh": [325, 329], "barr": [267, 274], "barrier": [305, 309, 312, 316], "base": [1, 4, 5, 6, 7, 8, 14, 16, 24, 25, 26, 27, 43, 44, 53, 54, 66, 67, 79, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 101, 106, 108, 109, 113, 114, 125, 127, 128, 131, 132, 137, 141, 142, 145, 147, 150, 151, 152, 169, 170, 172, 173, 175, 181, 187, 188, 191, 202, 203, 212, 215, 229, 233, 235, 237, 238, 239, 244, 265, 267, 269, 273, 275, 280, 282, 289, 290, 291, 292, 303, 305, 306, 307, 312, 313, 314, 324, 326, 327, 329, 331, 337, 338, 339, 345, 347], "base_dir": [338, 347], "base_model_id": [125, 128], "base_s3_path": [125, 128], "base_url": [86, 98, 108, 109, 115, 117, 121, 122, 125, 128, 129, 130], "baselin": [7, 14, 202, 204, 229, 232, 312, 317, 318, 321], "basemodel": [4, 125, 129, 147, 148, 150], "bash": [1, 133, 136, 151, 152], "bash_profil": [1, 151, 152], "basic": [2, 5, 7, 10, 11, 13, 14, 15, 17, 21, 24, 27, 79, 85, 91, 97, 125, 126, 127, 132, 133, 136, 142, 143, 153, 154, 184, 193, 196, 201, 229, 230, 232, 233, 331, 333, 348], "basicblock": 13, "basicvariantgener": [7, 14, 229, 233], "bastion": [17, 22], "batch": [5, 6, 7, 11, 12, 13, 14, 16, 84, 85, 88, 89, 96, 97, 101, 103, 113, 114, 116, 142, 144, 148, 160, 165, 174, 175, 180, 183, 186, 189, 190, 191, 196, 198, 200, 202, 203, 206, 207, 210, 212, 215, 216, 217, 218, 221, 223, 228, 229, 231, 235, 238, 239, 259, 260, 266, 268, 275, 276, 277, 278, 280, 281, 300, 305, 307, 308, 312, 314, 315, 316, 317, 318, 319, 320, 322, 326, 328, 330, 332, 339], "batch_df": [305, 307], "batch_first": [331, 334], "batch_format": [9, 176, 180, 305, 307, 312, 314, 318, 320, 325, 327, 329, 330, 331, 337, 338, 347], "batch_idx": [6, 235, 238, 305, 308, 312, 315], "batch_pr": [10, 15, 184, 191], "batch_siz": [3, 5, 6, 7, 9, 10, 13, 14, 15, 16, 159, 167, 176, 180, 184, 189, 190, 191, 202, 206, 210, 217, 218, 223, 229, 231, 232, 233, 235, 238, 239, 244, 259, 265, 266, 275, 280, 305, 309, 312, 316, 318, 322, 325, 327, 331, 333, 335, 337, 338, 341, 342, 343, 344, 347], "batch_size_per_work": [6, 235, 239, 275, 280, 281], "batchnorm2d": 13, "bathtub": [267, 274], "bathtuby": [267, 274], "batman": [244, 265], "batteri": 12, "battl": [244, 263, 265, 266, 267, 273, 274], "battleship": [267, 274], "bayesian": [7, 14, 229, 233], "bbc": [244, 265], "bc": [312, 317], "bd1": [244, 265], "beach": [125, 128], "bearer": 146, "beast": [244, 265], "beauti": [244, 265, 267, 273, 274, 292, 293, 295, 296, 304], "bebr": [267, 274], "becaus": [3, 7, 9, 14, 24, 26, 28, 33, 43, 51, 53, 63, 64, 81, 93, 108, 109, 112, 115, 137, 141, 159, 163, 176, 181, 229, 233, 267, 270, 273, 274, 305, 307, 309, 310, 325, 330, 338, 340, 345], "becom": [3, 8, 9, 137, 141, 159, 161, 167, 169, 175, 176, 179, 202, 207, 215, 244, 265, 305, 306], "bee": [244, 263, 265], "been": [8, 81, 82, 87, 93, 94, 99, 169, 174, 267, 273, 274, 325, 328], "befor": [3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 69, 78, 80, 91, 92, 108, 109, 112, 142, 143, 147, 148, 150, 159, 167, 176, 182, 184, 193, 202, 204, 212, 216, 217, 220, 221, 223, 224, 226, 229, 232, 233, 235, 239, 267, 270, 291, 292, 303, 305, 307, 318, 320, 325, 327, 331, 333, 338, 339, 340, 341], "beforehand": [81, 93, 125, 128], "begin": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 81, 91, 93, 125, 131, 142, 143, 243, 244, 261, 267, 274, 318, 322], "beginn": [260, 268, 276, 300], "behalf": [133, 135], "behavior": [3, 82, 94, 125, 127, 128, 133, 135, 137, 139, 142, 145, 159, 162, 202, 222, 312, 317, 318, 320, 338, 340, 343, 345], "behaviour": [305, 307, 325, 329], "behind": [24, 27, 108, 109, 113, 267, 273, 274, 318, 319], "being": [5, 8, 13, 16, 82, 84, 94, 96, 169, 174, 267, 270, 273, 274, 292, 296, 304, 305, 307, 338, 340], "believ": [267, 273], "belong": [338, 339], "below": [2, 3, 5, 7, 8, 14, 24, 26, 81, 82, 84, 85, 86, 91, 93, 94, 96, 97, 98, 105, 108, 109, 113, 117, 123, 142, 144, 153, 158, 159, 163, 168, 169, 170, 202, 206, 214, 229, 233, 305, 311, 325, 330, 338, 347], "ben": [244, 263, 265, 266, 267, 273], "benchmark": [129, 325, 330, 338, 339], "benefit": [1, 3, 4, 7, 10, 12, 14, 17, 22, 132, 147, 149, 151, 152, 159, 162, 184, 190, 229, 232], "bergman": [267, 270], "berni": [244, 265], "bert": [260, 268, 275, 276, 277, 280, 281, 282, 300], "besok": [244, 265], "best": [4, 5, 7, 9, 10, 12, 14, 15, 17, 20, 35, 39, 82, 94, 108, 125, 128, 131, 137, 140, 147, 150, 176, 182, 184, 193, 202, 203, 212, 214, 219, 228, 229, 233, 244, 259, 265, 266, 267, 273, 274, 275, 277, 312, 317, 318, 324, 325, 328, 330, 331, 335, 337, 338, 339, 345, 347], "best_ckpt": [305, 309, 311, 312, 316, 317, 325, 328, 329, 330, 331, 335, 338, 344], "best_ckpt_path": [331, 337, 338, 347], "best_result": [7, 14, 229, 233], "better": [3, 7, 8, 10, 14, 84, 86, 96, 98, 125, 127, 159, 165, 169, 173, 184, 190, 202, 228, 229, 232, 244, 265, 267, 270, 272, 273, 285, 292, 301, 305, 311, 312, 317, 318, 324, 325, 329], "between": [3, 7, 8, 9, 10, 11, 14, 15, 17, 18, 19, 22, 24, 26, 53, 63, 79, 83, 95, 108, 109, 113, 117, 119, 125, 127, 128, 137, 140, 141, 159, 162, 169, 170, 172, 173, 176, 179, 182, 184, 186, 190, 193, 196, 198, 202, 207, 229, 231, 233, 267, 270, 271, 318, 319, 325, 327, 331, 333], "beyonc": [244, 265], "beyond": [87, 99, 125, 126, 130, 244, 265, 267, 273], "bf16": [6, 235, 238, 239], "bfloat16": [338, 347], "bia": [5, 7, 13, 14, 202, 205, 229, 232, 233, 325, 327], "bias": [202, 228, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "bidder": [267, 273], "bieber": [244, 265], "big": [8, 169, 170, 174, 267, 273, 275, 277, 305, 306, 312, 313, 318, 319], "bigger": [244, 265], "bigl": [338, 339], "bigqueri": [8, 169, 170], "bigr": [338, 339], "bill": [35, 37, 66, 68, 88, 101], "billion": [117, 119, 137, 141], "bin": [0, 66, 69, 318, 320], "binari": [12, 325, 326, 330], "bind": [4, 8, 11, 12, 16, 86, 90, 98, 107, 142, 145, 147, 150, 169, 170, 196, 200, 292, 296, 303], "birthdai": [244, 265], "bit": [3, 159, 167, 267, 273, 274], "bitten": [267, 273], "bjork": [267, 274], "bjp": [244, 265], "black": [7, 14, 229, 231, 244, 265, 267, 274, 318, 320], "blair": [244, 265], "blank": [80, 92], "bless": [244, 265], "blind": [267, 274], "blob": [202, 212, 318, 320, 338, 347], "block": [2, 3, 5, 7, 11, 13, 16, 43, 51, 53, 64, 108, 109, 115, 117, 121, 123, 125, 128, 129, 130, 142, 144, 145, 153, 157, 158, 159, 160, 166, 167, 179, 180, 181, 186, 189, 190, 191, 196, 199, 200, 201, 202, 213, 216, 217, 229, 232, 243, 244, 250, 261, 263, 267, 272, 305, 307, 318, 319, 320, 325, 328], "block_out_channel": [6, 235, 238], "blockbust": [267, 274], "blog": [5, 6, 7, 13, 229, 234, 235, 240], "blow": [244, 265], "blue": [108, 109, 112, 244, 263, 265, 305, 306, 338, 339], "bn1": 13, "bn2": 13, "board": [267, 274], "boat": [267, 273], "bob": [244, 265], "bodi": [11, 16, 196, 200, 267, 270, 274], "boi": [267, 270, 274], "boilerpl": [202, 209, 212, 305, 306, 312, 313, 325, 326, 338, 347], "bomb": [267, 274], "bon": [244, 265], "book": [244, 263, 265, 266, 267, 273], "bookkeep": [338, 342], "bool": [338, 347], "boolean": 12, "boost": [4, 12, 147, 150, 325, 326, 327, 328, 330], "booster": [4, 12, 147, 150, 325, 328, 329, 330], "boot": [244, 265], "booth": [244, 265], "bootstrap": [24, 26], "border": [267, 273], "bore": [267, 273], "both": [0, 8, 10, 17, 22, 24, 26, 53, 57, 88, 91, 101, 117, 118, 125, 129, 133, 135, 137, 139, 142, 145, 169, 170, 175, 184, 186, 202, 212, 214, 216, 219, 222, 226, 244, 265, 275, 277, 281, 305, 307, 325, 327, 328], "boto3": [83, 95, 125, 128], "bottleneck": [133, 135, 137, 141, 325, 327], "bottom": [3, 87, 99, 159, 167], "bound": [17, 22, 160], "boundari": [8, 169, 173], "bouquet": [244, 265], "bout": [244, 263, 265, 266], "box": [5, 84, 87, 96, 99, 202, 203], "br": [267, 270, 273, 274], "brain": [267, 273], "branch": 0, "brand": [125, 129], "braun": [244, 265], "break": [6, 7, 14, 84, 96, 229, 231, 235, 238, 267, 274, 338, 341], "breakdown": [8, 169, 170], "breakneck": [267, 274], "breakpoint": [275, 280], "breez": [267, 270], "brennan": [267, 273], "brew": [4, 28, 29, 43, 44, 53, 55, 66, 68, 133, 136, 147, 148], "bridg": [8, 15, 16, 169, 170, 202, 207, 267, 274], "brief": [125, 128], "bring": [4, 147, 149, 202, 213, 244, 263, 265, 267, 273, 274, 325, 327], "brit": [244, 265], "british": [244, 265, 267, 274], "broadcast": [202, 203], "broader": [8, 169, 174], "brock": [244, 265], "brought": [267, 273], "brown": [244, 265, 267, 270], "brows": 146, "browser": [0, 1, 35, 38, 81, 93, 151, 152], "bryant": [244, 265], "bst": [4, 147, 150], "bubbl": [244, 265], "bucket": [17, 21, 22, 23, 28, 30, 33, 34, 35, 36, 39, 42, 43, 45, 51, 53, 56, 63, 64, 66, 70, 78, 83, 95, 125, 128], "bucket_nam": [35, 39, 125, 128], "budget": [125, 131], "buf": [305, 307, 338, 340], "buffalo": [244, 265], "buffer": [8, 108, 109, 113, 169, 170, 202, 218], "bug": [133, 135], "bui": [244, 265, 267, 273], "build": [1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 16, 17, 21, 80, 82, 91, 92, 94, 117, 124, 125, 127, 130, 132, 147, 149, 150, 151, 152, 153, 158, 159, 160, 169, 170, 173, 175, 176, 178, 184, 186, 189, 196, 197, 199, 200, 201, 204, 206, 209, 217, 219, 235, 238, 267, 274, 305, 306, 307, 312, 313, 318, 319, 320, 324, 326, 327, 331, 333, 337, 338, 341, 343, 347], "build_app": [11, 196, 201], "build_data_load": [7, 14, 229, 231, 232], "build_data_loader_ray_train": [5, 13, 202, 206, 210], "build_data_loader_ray_train_ray_data": [202, 217, 218, 223], "build_data_loader_torch": [5, 13], "build_dataload": [331, 333, 335, 338, 342, 343], "build_inference_dataset": [338, 347], "build_openai_app": [108, 109, 115, 117, 120, 123, 125, 128, 129, 130], "build_resnet18": [5, 13, 202, 205, 209, 215], "builder": [11, 196, 201], "built": [0, 2, 4, 5, 6, 8, 9, 12, 16, 24, 27, 28, 34, 35, 39, 81, 82, 84, 93, 94, 96, 108, 109, 113, 133, 136, 137, 139, 146, 147, 149, 153, 155, 169, 170, 176, 178, 202, 203, 205, 217, 235, 237, 239, 285, 292, 301, 312, 313, 316, 318, 323, 325, 327, 328, 329, 330, 331, 337, 338, 339, 343, 347], "bulk": [85, 89, 97, 103], "bundl": [82, 94], "bunni": [267, 270], "burden": 91, "burrito": [338, 339], "bursti": [80, 82, 92, 94, 108, 109, 113], "busi": [8, 11, 125, 130, 169, 170, 196, 199, 291, 292, 303], "button": [81, 82, 86, 87, 93, 94, 98, 99], "bx1": [305, 308], "bx3xhxw": [305, 308], "bypass": [3, 159, 162], "bystand": [267, 274], "byte": [10, 142, 144, 184, 190, 305, 307, 311, 338, 340], "bytesio": [305, 307, 338, 340, 341, 347], "byth": [83, 95], "c": [1, 3, 11, 82, 83, 94, 95, 151, 152, 159, 165, 196, 200, 202, 215, 244, 265, 267, 273, 318, 322, 325, 327, 328, 331, 335, 338, 345, 347], "cab": [4, 9, 12, 142, 144, 147, 150, 176, 179, 182], "cabin": [267, 274], "cabl": [267, 270], "cach": [3, 15, 17, 21, 82, 94, 111, 113, 116, 117, 123, 159, 161, 202, 228, 305, 307, 312, 314, 318, 320, 324, 325, 327, 331, 333, 338, 340, 341, 347], "cactu": [244, 263, 265], "caesar": [338, 339], "cahse": [244, 265], "calcul": [5, 9, 13, 84, 96, 176, 180, 202, 206, 275, 279, 280], "call": [3, 4, 5, 6, 7, 9, 10, 13, 14, 15, 86, 98, 109, 116, 126, 127, 132, 142, 145, 146, 147, 150, 154, 157, 159, 162, 167, 168, 176, 182, 183, 184, 189, 190, 193, 202, 204, 206, 209, 212, 213, 217, 221, 223, 224, 225, 226, 229, 233, 235, 239, 259, 266, 267, 274, 275, 282, 305, 307, 309, 310, 318, 322, 325, 327, 330, 338, 340, 344], "call_id": [125, 130], "callabl": [10, 11, 15, 125, 130, 184, 191, 196, 201, 243, 244, 253, 259, 261, 264, 266], "callback": [6, 235, 239, 305, 308, 309, 312, 316, 317, 325, 328, 330], "caller": [244, 256, 265], "came": [244, 265], "camera": [267, 273, 274], "campu": [244, 265], "can": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 22, 24, 26, 27, 28, 29, 30, 32, 33, 34, 35, 39, 41, 42, 43, 44, 45, 50, 51, 52, 53, 55, 56, 57, 62, 63, 64, 65, 66, 68, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119, 121, 122, 123, 125, 126, 127, 128, 130, 133, 135, 136, 137, 139, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 171, 173, 174, 175, 176, 177, 179, 180, 181, 182, 184, 185, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 207, 208, 211, 213, 214, 215, 216, 219, 220, 222, 223, 224, 226, 228, 229, 230, 232, 233, 235, 237, 238, 239, 243, 244, 250, 253, 256, 259, 261, 263, 264, 265, 266, 267, 269, 270, 272, 273, 274, 275, 277, 280, 281, 282, 285, 291, 292, 301, 303, 306, 307, 308, 309, 313, 315, 319, 320, 326, 327, 332, 333, 336, 339, 340, 341], "canadian": [267, 273], "cancel": [267, 273], "candid": [125, 128, 305, 309, 312, 316, 317], "cannon": [267, 274], "cannot": [1, 3, 28, 31, 33, 35, 40, 43, 47, 53, 59, 66, 75, 82, 94, 151, 152, 159, 161, 267, 270], "canopi": [325, 330], "cant": [267, 274], "canva": [5, 6, 13, 235, 240], "capabl": [8, 11, 24, 27, 81, 83, 93, 95, 117, 119, 124, 125, 126, 127, 130, 131, 132, 137, 138, 140, 146, 169, 170, 174, 196, 199, 275, 281, 285, 292, 301], "capac": [28, 30, 43, 45, 53, 56, 82, 94, 108, 109, 111, 112, 318, 319], "capit": [108, 109, 115, 125, 128], "captain": [267, 273], "caption_lat": [6, 235, 238], "captur": [87, 99, 142, 145, 305, 309, 318, 322, 331, 332, 337], "car_typ": [125, 129], "card": [9, 117, 120, 176, 179], "cardescript": [125, 129], "cardiffnlp": [244, 250, 263], "care": [202, 212, 267, 273, 274, 331, 333], "carli": [244, 265], "carriag": [267, 274], "carrow": [244, 265], "cartograph": [325, 326], "cartpol": [312, 317], "cartyp": [125, 129], "case": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 24, 26, 28, 30, 43, 45, 53, 56, 82, 84, 94, 96, 108, 109, 112, 117, 119, 124, 127, 128, 132, 137, 140, 147, 149, 150, 153, 155, 159, 162, 168, 169, 173, 176, 182, 183, 184, 191, 193, 196, 201, 202, 224, 229, 233, 234, 235, 239, 240, 244, 259, 265, 266, 267, 274, 275, 280, 318, 319, 325, 326, 331, 335], "cash": [9, 176, 179], "castl": [267, 273], "casual": [244, 265], "cat": [305, 308, 312, 315, 331, 335], "catalog": [318, 319], "catch": [244, 265], "categor": [8, 84, 96, 169, 171, 173], "categori": [10, 24, 26, 184, 189, 275, 277, 338, 339], "cattl": [267, 273], "cattleman": [267, 273], "caus": [5, 6, 28, 30, 43, 45, 53, 56, 137, 141, 160, 202, 203, 235, 237], "cd": [0, 11, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 85, 86, 89, 90, 91, 97, 98, 103, 107, 125, 132, 137, 141, 142, 145, 196, 201, 338, 347], "cdot": [305, 306, 312, 313], "ceil_mod": 13, "cell": [4, 5, 6, 7, 9, 10, 11, 13, 81, 82, 83, 85, 89, 93, 94, 95, 97, 105, 147, 150, 176, 183, 184, 195, 196, 201, 202, 204, 215, 229, 234, 235, 236, 240, 318, 320, 331, 333], "celsiu": [3, 125, 130, 159, 168], "cena": [244, 263, 265], "center": [202, 210, 244, 265, 267, 270, 305, 307, 338, 339, 340], "center_input_sampl": [6, 235, 238], "centercrop": [305, 307, 338, 340], "central": [125, 128, 137, 140, 202, 212], "centric": [8, 169, 173], "ceph": [8, 169, 170], "cerebr": [267, 273], "cert": [24, 27], "certain": [5, 6, 7, 9, 14, 176, 180, 229, 233, 235, 236, 267, 270, 289, 290, 291, 292, 303], "certif": [24, 27], "chain": [15, 160], "chair": [244, 265], "chalk": [267, 274], "challeng": [5, 6, 10, 11, 91, 110, 116, 184, 186, 196, 198, 202, 203, 216, 235, 237], "chanc": [3, 159, 163, 244, 265], "chang": [3, 5, 6, 7, 8, 10, 11, 14, 28, 30, 35, 39, 43, 45, 53, 56, 63, 83, 84, 86, 90, 95, 96, 98, 107, 117, 122, 124, 125, 128, 137, 139, 142, 145, 159, 161, 169, 170, 184, 194, 196, 198, 201, 202, 203, 229, 233, 235, 237, 239, 244, 265, 267, 273, 275, 281, 305, 306, 309, 311, 312, 313, 318, 319, 325, 326, 331, 337, 338, 339, 346, 347], "channel": [10, 13, 15, 16, 137, 141, 142, 145, 184, 190, 202, 204, 205, 215, 267, 274, 305, 306, 307, 308, 331, 337, 338, 339], "chao": [267, 274], "chap": [28, 29, 43, 44, 53, 55], "charact": [108, 109, 111, 267, 274, 292, 293, 295, 296, 304], "characterist": [8, 9, 11, 15, 108, 109, 111, 169, 170, 176, 179, 196, 199], "charg": [9, 176, 179], "charli": [244, 265], "charm": [244, 265, 267, 273], "chart": [43, 46, 48, 53, 58, 60, 66, 76, 325, 327], "chase": [244, 263, 265], "chat": [108, 109, 112, 115, 117, 121, 122, 125, 128, 129, 130, 131], "chatbot": [125, 130, 131], "cheap": [244, 265], "cheaper": [8, 169, 172], "cheapli": [267, 270], "cheat": [267, 273], "check": [3, 5, 6, 9, 10, 11, 13, 28, 32, 35, 41, 43, 49, 50, 53, 61, 62, 63, 66, 71, 77, 83, 84, 85, 86, 87, 95, 96, 97, 98, 99, 106, 108, 109, 113, 125, 130, 142, 144, 146, 159, 165, 176, 179, 181, 184, 190, 191, 196, 200, 202, 204, 212, 213, 223, 235, 239, 244, 265, 275, 280, 309, 312, 316, 318, 320, 325, 327, 330], "check_cal": [305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "check_val_every_n_epoch": [305, 309, 312, 316], "checkout": [84, 86, 96, 98], "checkpoint": [9, 10, 12, 14, 83, 95, 176, 183, 184, 186, 203, 204, 205, 206, 213, 214, 217, 221, 222, 225, 227, 228, 237, 238, 275, 282, 306, 307, 309, 313, 314, 317, 319, 320, 324, 326, 328, 332, 333, 335, 337, 338, 339, 340, 343, 344, 345, 346, 347], "checkpoint_": [338, 347], "checkpoint_000000": 13, "checkpoint_000001": 13, "checkpoint_at_end": [331, 335], "checkpoint_config": [305, 309, 312, 316, 318, 322, 325, 328, 331, 335, 338, 344], "checkpoint_dir": [338, 347], "checkpoint_dir_nam": 13, "checkpoint_frequ": [305, 309, 312, 316, 325, 328, 338, 344], "checkpoint_nam": [325, 328], "checkpoint_path": [5, 6, 13, 235, 239, 331, 337, 338, 347], "checkpoint_root": [338, 347], "checkpoint_score_": [331, 335], "checkpoint_score_attribut": [305, 309, 312, 316, 325, 328, 331, 335, 338, 344], "checkpoint_score_ord": [305, 309, 312, 316, 325, 328, 331, 335, 338, 344], "checkpointconfig": [305, 306, 307, 309, 312, 313, 314, 316, 318, 319, 320, 322, 325, 326, 327, 328, 331, 333, 335, 338, 340, 344], "cheekbon": [244, 265], "chelsea": [244, 265], "cheri": [244, 265], "chill": [244, 265], "chip": [1, 151, 152], "chloe": [267, 270], "chmod": [133, 136], "choic": [8, 9, 24, 26, 108, 109, 115, 117, 121, 122, 125, 128, 129, 130, 169, 175, 176, 183, 285, 292, 301, 331, 332], "choos": [1, 10, 15, 17, 18, 20, 53, 63, 66, 73, 80, 81, 82, 92, 93, 94, 108, 109, 113, 126, 132, 151, 152, 184, 192, 267, 274, 318, 324], "choreo": [244, 265], "chose": [5, 13], "chown": [133, 136], "chri": [244, 265], "chrisbrown": [244, 265], "christian": [244, 265], "chromadb": [8, 169, 170], "chronolog": [137, 139], "chuck": [244, 265], "chunk": [8, 10, 15, 84, 96, 108, 109, 115, 117, 121, 122, 125, 128, 169, 173, 184, 193], "church": [244, 265], "churn": [331, 337], "chw": [305, 307], "ci": [85, 89, 97, 103, 125, 132, 202, 228, 338, 347], "ciara": [244, 265], "cidr": [17, 22], "cidr_block": [17, 22], "cif": [244, 265], "cifar": [202, 221, 224, 226], "cifar10": [202, 219, 227], "cinema": [267, 270, 273, 274], "cinemat": [267, 274], "cinematograph": [267, 274], "cinematographi": [267, 273, 274], "cineworld": [244, 265], "citi": [4, 9, 12, 125, 130, 147, 150, 176, 179, 244, 265, 267, 274, 331, 332], "citizenship": [244, 265], "ckpt": [5, 6, 13, 235, 239, 305, 306, 309, 311, 312, 313, 316, 317, 318, 322, 325, 328, 329, 331, 335, 337, 338, 343], "ckpt_dir": [5, 6, 13, 202, 215, 223, 235, 239, 305, 311, 312, 317, 318, 322, 324, 331, 335, 337, 338, 343, 347], "ckpt_file": [305, 311, 312, 317], "ckpt_out": [318, 322, 331, 335, 338, 343], "ckpt_path": [6, 235, 239, 305, 309, 312, 316], "ckpt_root": [305, 309, 312, 316], "claim": [8, 169, 174, 267, 270, 273], "clamp": [305, 311], "clarifi": [88, 100], "class": [3, 4, 5, 6, 7, 10, 11, 12, 14, 15, 16, 125, 129, 142, 145, 147, 150, 159, 168, 184, 188, 191, 196, 200, 202, 205, 213, 215, 219, 229, 231, 233, 235, 238, 239, 243, 259, 261, 265, 266, 291, 292, 303, 305, 306, 307, 308, 311, 312, 315, 318, 321, 326, 329, 331, 333, 334, 337, 338, 339, 340, 341, 347], "class_nam": [3, 159, 168], "classic": [312, 313, 317, 318, 319, 325, 326], "classif": [4, 12, 15, 147, 150, 197, 202, 204, 205, 206, 275, 277, 278, 280, 282, 343], "classifi": [7, 11, 14, 196, 200, 229, 232, 275, 277, 338, 339], "classmat": [267, 270], "claud": [125, 131], "cld": [83, 95, 117, 122], "cld_g54aiirwj1s8t9ktgzikqur41k": [83, 95], "cldrsrc_12345abcdefgh67890ijklmnop": [43, 47, 48, 53, 59, 60, 66, 75, 76], "clean": [0, 11, 28, 33, 35, 42, 66, 78, 196, 199, 212, 267, 274, 306, 308, 309, 320, 331, 335, 337, 339, 340, 343, 346], "cleaner": [202, 204], "cleanli": [202, 212, 318, 320], "cleanup": [4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 24, 26, 79, 85, 97, 147, 150, 176, 183, 184, 195, 196, 201, 202, 204, 227, 229, 234, 235, 240, 305, 311, 312, 317, 318, 324, 325, 330, 338, 347], "clear": [0, 88, 102, 267, 273, 312, 317, 325, 330, 331, 337, 338, 339, 347], "clearli": [142, 143, 267, 274], "cli": [17, 18, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 74, 81, 83, 84, 85, 93, 95, 96, 97, 142, 145, 146], "click": [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 17, 22, 28, 29, 43, 44, 53, 55, 63, 66, 68, 80, 81, 82, 83, 84, 86, 87, 92, 93, 94, 95, 96, 98, 99, 108, 109, 110, 117, 118, 123, 125, 126, 133, 136, 142, 144, 145, 147, 148, 153, 154, 158, 159, 160, 168, 176, 177, 184, 185, 190, 196, 197, 229, 230, 233, 235, 236, 239, 243, 244, 261, 267, 269, 275, 277, 285, 292, 301], "client": [83, 95, 108, 109, 114, 115, 117, 121, 122, 125, 128, 129, 130, 285, 301], "cliff": [244, 265], "clipboard": [86, 98], "clitori": [267, 270], "clock": [267, 274], "clog": [267, 274], "clone": [0, 86, 98, 312, 317], "close": [202, 212, 244, 265, 267, 274], "cloud": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 18, 21, 22, 24, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 41, 42, 44, 45, 48, 49, 50, 51, 52, 54, 56, 60, 61, 62, 64, 65, 67, 68, 70, 76, 77, 78, 79, 80, 82, 87, 90, 91, 92, 94, 99, 102, 106, 108, 109, 110, 114, 115, 117, 118, 122, 125, 126, 128, 137, 141, 147, 148, 151, 152, 153, 154, 159, 160, 176, 177, 184, 185, 196, 197, 202, 203, 212, 228, 229, 230, 235, 236, 243, 244, 261, 267, 269, 275, 277, 281, 285, 292, 301, 305, 311, 318, 324, 325, 326, 338, 339], "cloud_deployment_id": [43, 48, 53, 60, 66, 76], "cloud_nam": [43, 50, 53, 62], "clouddeploymentid": [43, 45, 48, 53, 56, 60, 66, 76], "cloudflar": [8, 169, 170], "cloudform": [17, 22], "cloudfound": [17, 22, 35, 39], "cloudprovid": [43, 45, 48, 53, 56, 60, 66, 76], "cloudresourcemanag": [35, 38, 66, 69], "cloudwatch": [17, 22], "club": [244, 263, 265, 267, 273], "cluster": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 35, 41, 42, 45, 49, 50, 51, 52, 55, 56, 57, 61, 62, 63, 64, 65, 72, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 89, 90, 92, 93, 94, 96, 97, 98, 99, 103, 105, 106, 108, 109, 110, 114, 115, 117, 118, 122, 125, 126, 135, 137, 139, 140, 142, 143, 147, 148, 150, 151, 152, 153, 154, 156, 157, 160, 161, 162, 164, 166, 168, 169, 173, 176, 177, 181, 183, 184, 185, 186, 188, 189, 190, 191, 192, 195, 196, 197, 199, 203, 204, 205, 208, 210, 212, 213, 214, 215, 216, 219, 220, 221, 224, 228, 229, 230, 235, 236, 237, 243, 250, 261, 263, 265, 267, 269, 277, 281, 285, 296, 301, 305, 306, 307, 311, 312, 313, 314, 317, 318, 319, 320, 322, 324, 325, 326, 327, 330, 331, 332, 338, 339, 344, 347], "cluster_id": [84, 96], "cluster_storag": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 83, 95, 142, 144, 147, 150, 176, 181, 184, 191, 196, 200, 202, 204, 205, 210, 212, 214, 219, 227, 235, 239, 240, 305, 307, 309, 311, 312, 316, 317, 318, 320, 322, 324, 325, 327, 328, 330, 331, 332, 333, 338, 339, 340, 341, 343, 344, 347], "clusternam": [43, 46, 53, 58], "clusteronc": [28, 32], "cm": [325, 329], "cm_norm": [325, 329], "cmap": [5, 7, 10, 13, 14, 16, 184, 189, 202, 204, 215, 229, 231, 325, 329], "cnn": [202, 204, 305, 308, 311, 338, 339], "co": [125, 128, 312, 313, 314, 317, 331, 334], "coach": [244, 265], "coars": [11, 196, 199], "cocki": [267, 273], "code": [0, 1, 2, 3, 5, 6, 7, 9, 11, 13, 14, 16, 17, 18, 79, 81, 82, 83, 84, 85, 86, 87, 91, 93, 94, 95, 96, 97, 98, 99, 103, 105, 117, 122, 124, 127, 131, 137, 141, 142, 145, 148, 151, 152, 153, 155, 159, 165, 168, 176, 179, 196, 200, 201, 202, 203, 206, 207, 208, 209, 210, 211, 216, 228, 229, 233, 235, 237, 239, 244, 265, 285, 292, 301, 305, 306, 312, 313, 318, 319, 324, 325, 326, 327, 331, 332, 333, 337, 338, 339, 347], "coder": [125, 131], "cogent": [267, 270], "coher": [267, 273, 274], "coi": [267, 273], "col": [5, 6, 202, 204, 215, 235, 238, 318, 322, 331, 335, 338, 345], "colbert": [244, 265], "cold": [108, 109, 115], "collabor": [88, 101, 102, 318, 319, 321, 324, 348], "collat": [6, 235, 238, 275, 280], "collate_fn": [275, 280], "colleagu": [244, 265], "collect": [2, 3, 9, 10, 16, 17, 19, 133, 135, 136, 153, 155, 158, 159, 161, 176, 179, 184, 188, 193, 202, 204, 211, 213, 215, 244, 265, 312, 316, 317, 325, 328, 329, 338, 343, 348], "collector": [202, 215], "collison": [244, 266], "colour": [267, 274], "column": [4, 6, 9, 10, 81, 93, 147, 150, 176, 179, 182, 184, 189, 202, 214, 219, 235, 238, 244, 251, 252, 253, 264, 267, 271, 274, 305, 307, 309, 311, 312, 316, 318, 320, 322, 324, 325, 327, 328, 331, 333, 335, 337, 338, 340, 341, 345, 347], "column_nam": [325, 328], "column_stack": [325, 328], "columnar": [8, 169, 170, 202, 219, 305, 307, 325, 327, 331, 333, 338, 340], "com": [0, 1, 13, 14, 17, 22, 24, 27, 28, 29, 35, 37, 38, 39, 43, 44, 53, 55, 63, 66, 68, 69, 70, 84, 86, 89, 90, 91, 96, 98, 103, 106, 107, 117, 122, 146, 151, 152, 331, 333], "combin": [3, 8, 159, 165, 169, 170, 202, 212, 267, 273, 274, 305, 306], "combur": [244, 265], "comcast": [244, 265], "come": [3, 24, 26, 82, 84, 91, 94, 96, 159, 164, 244, 265, 267, 270, 273, 274, 325, 330, 331, 337, 338, 347], "comedi": [267, 274], "comfort": [338, 347], "command": [17, 20, 23, 28, 30, 31, 40, 42, 43, 45, 46, 47, 51, 53, 56, 58, 59, 64, 66, 69, 75, 78, 81, 84, 85, 86, 93, 96, 97, 98, 105, 133, 136, 137, 141, 142, 145, 312, 317], "commend": [267, 270], "comment": [86, 90, 98, 107, 291, 292, 303], "commerci": [8, 169, 170, 172], "commiss": [4, 9, 12, 147, 150, 176, 179], "commit": [305, 307, 338, 340], "common": [2, 3, 4, 9, 10, 53, 63, 66, 71, 125, 132, 133, 135, 142, 145, 147, 149, 153, 155, 159, 168, 176, 179, 181, 184, 188, 191, 244, 265, 267, 271, 273, 274], "common_prefix": [35, 39], "commonli": [8, 10, 169, 170, 173, 184, 187, 318, 321], "commun": [3, 8, 9, 10, 15, 17, 22, 108, 109, 112, 117, 123, 124, 125, 132, 159, 168, 169, 174, 176, 182, 184, 193, 331, 333], "compact": [11, 196, 198, 305, 307, 311, 338, 339, 340], "compani": [24, 27, 88, 100], "compar": [7, 8, 14, 15, 82, 94, 108, 109, 111, 117, 119, 137, 138, 169, 173, 229, 233, 305, 311, 312, 317, 331, 335, 338, 345, 347], "comparison": [267, 274], "compat": [108, 109, 114, 117, 120, 125, 128, 142, 143, 202, 218, 338, 339], "compet": [8, 169, 174], "competit": [244, 265], "compil": [1, 151, 152, 260, 268, 276, 300], "complet": [1, 3, 4, 5, 6, 13, 24, 26, 28, 30, 43, 50, 53, 62, 81, 84, 85, 89, 91, 93, 96, 97, 103, 108, 109, 111, 112, 115, 117, 118, 121, 122, 125, 128, 129, 130, 131, 132, 137, 141, 146, 147, 150, 151, 152, 159, 166, 202, 203, 213, 223, 224, 226, 228, 235, 237, 267, 274, 275, 282, 305, 309, 310, 311, 312, 316, 317, 338, 345], "complex": [4, 8, 11, 16, 24, 26, 91, 108, 109, 112, 114, 117, 119, 124, 125, 127, 131, 147, 150, 169, 170, 173, 174, 175, 196, 198, 305, 306], "compli": [6, 235, 239], "complianc": [17, 20, 117, 122, 125, 132], "compliant": [17, 22], "compon": [2, 6, 7, 8, 11, 14, 17, 20, 22, 25, 52, 65, 66, 72, 82, 84, 94, 96, 108, 109, 113, 114, 116, 137, 139, 140, 146, 153, 155, 169, 170, 196, 198, 202, 204, 229, 233, 235, 239, 331, 333, 338, 340], "compos": [5, 6, 7, 10, 11, 13, 14, 15, 184, 185, 190, 196, 198, 199, 202, 204, 210, 220, 229, 230, 231, 233, 235, 238, 305, 307, 338, 340, 341, 347], "composit": [8, 169, 175, 267, 274], "comprehens": [1, 9, 10, 15, 17, 21, 66, 67, 79, 108, 109, 110, 117, 123, 124, 125, 127, 128, 129, 130, 132, 146, 151, 152, 176, 179, 181, 184, 186, 194], "compress": [325, 327, 331, 337], "comput": [2, 3, 4, 5, 6, 9, 10, 11, 12, 15, 16, 17, 20, 22, 24, 26, 27, 28, 34, 38, 39, 42, 43, 45, 50, 53, 56, 62, 66, 69, 70, 71, 78, 79, 80, 81, 84, 85, 86, 88, 89, 90, 91, 92, 93, 96, 97, 98, 101, 102, 103, 107, 108, 109, 111, 112, 114, 115, 147, 150, 153, 155, 157, 158, 159, 160, 161, 165, 168, 170, 172, 174, 176, 182, 183, 184, 186, 190, 192, 193, 195, 196, 198, 202, 203, 205, 206, 217, 235, 237, 239, 243, 244, 261, 267, 271, 275, 277, 279, 280, 281, 285, 292, 301, 307, 318, 319, 322, 324, 325, 328, 329, 330, 343, 347, 348], "computation": [7, 14, 229, 232], "compute_accuraci": [10, 15, 184, 193], "compute_config": [43, 50, 53, 62, 108, 109, 115, 117, 122, 137, 141], "compute_metr": [275, 279], "compute_nodes_service_account_email": [35, 39, 66, 70], "compute_tip_percentag": [9, 176, 180], "computeconfig": [43, 50, 53, 62], "con": [244, 265], "conc": [35, 41], "concat_t": [325, 328], "concept": [8, 10, 24, 26, 79, 110, 142, 143, 169, 172, 184, 185, 202, 205], "conceptu": 5, "concern": [2, 11, 153, 155, 196, 199, 267, 273], "concert": [244, 265], "concis": [125, 128], "conclus": [126, 267, 269], "concomit": [244, 265], "concret": [17, 18], "concurr": [3, 4, 8, 9, 11, 12, 15, 16, 108, 109, 112, 124, 147, 150, 159, 165, 169, 175, 176, 183, 191, 196, 199, 202, 206, 243, 244, 256, 261, 265, 267, 272, 331, 337, 338, 347], "concurrency_limit": [10, 184, 190], "concuss": [244, 263, 265, 266], "conda": [133, 136], "condit": [3, 6, 17, 22, 125, 128, 159, 161, 235, 238, 267, 269, 305, 311], "conductor": [267, 274], "confid": [202, 222, 305, 311, 312, 317, 318, 324, 325, 330], "config": [4, 5, 6, 7, 11, 12, 13, 14, 16, 35, 38, 66, 69, 133, 136, 142, 145, 147, 150, 196, 201, 202, 205, 206, 207, 208, 213, 217, 223, 226, 229, 233, 235, 239, 275, 280, 305, 309, 312, 313, 316, 318, 322, 325, 328, 331, 335, 338, 340, 343], "configur": [7, 10, 11, 14, 16, 17, 19, 20, 21, 22, 24, 25, 27, 28, 29, 30, 34, 37, 39, 43, 44, 45, 50, 53, 55, 56, 62, 68, 70, 71, 80, 84, 85, 86, 89, 92, 96, 97, 98, 103, 105, 106, 114, 116, 118, 124, 127, 132, 133, 136, 137, 141, 160, 184, 186, 190, 196, 199, 201, 203, 204, 205, 207, 222, 226, 228, 229, 233, 238, 275, 277, 278, 280, 281, 282, 291, 292, 303, 305, 309, 312, 316, 318, 319, 322, 323, 326, 331, 332, 335, 338, 339, 343, 347], "configure_optim": [6, 235, 238, 305, 308, 312, 315], "confirm": [86, 98, 125, 128, 202, 204, 213, 226, 305, 307, 309, 318, 320, 322, 325, 327, 330, 331, 333, 338, 340], "conflict": [202, 215], "confluent": [8, 169, 173], "confus": [10, 184, 190, 338, 347], "confusion_matrix": [325, 327, 329], "congratul": [117, 124, 125, 132], "congress": [244, 265], "conjur": [267, 274], "connect": [3, 5, 6, 10, 17, 22, 24, 27, 28, 34, 43, 46, 53, 58, 66, 72, 80, 81, 83, 88, 92, 93, 95, 101, 159, 162, 184, 187, 202, 203, 221, 235, 237, 244, 250, 263, 267, 274, 325, 329], "connector": [9, 10, 176, 183, 184, 187], "consecut": [244, 263, 265], "consid": [2, 3, 5, 6, 13, 117, 124, 125, 131, 142, 143, 153, 158, 159, 161, 163, 164, 165, 167, 197, 235, 239, 267, 270, 272, 273, 305, 307], "consider": [24, 26, 91], "consist": [7, 8, 14, 15, 79, 80, 82, 92, 94, 125, 127, 129, 132, 169, 170, 202, 203, 204, 205, 216, 229, 231, 260, 268, 276, 300, 312, 317, 325, 328, 330, 331, 333, 334], "consol": [28, 31, 32, 35, 40, 41, 43, 47, 50, 53, 59, 62, 63, 66, 75, 77, 81, 85, 86, 93, 97, 98, 105, 125, 132, 133, 135, 142, 143, 146, 318, 322], "conspicu": [244, 265], "constant": [6, 108, 109, 112, 235, 238, 267, 274, 331, 337], "constraint": [6, 108, 109, 113, 125, 131, 235, 238, 244, 259, 266, 267, 274], "construct": [3, 6, 159, 168, 202, 209, 210, 235, 238, 312, 313, 331, 335, 338, 341, 342], "constructor": [3, 10, 11, 15, 16, 159, 168, 184, 191, 196, 200, 243, 244, 261], "consum": [7, 9, 10, 14, 15, 28, 30, 43, 45, 53, 56, 176, 179, 181, 184, 187, 189, 202, 216, 221, 228, 229, 233, 331, 333], "consumptionapi": [10, 184, 189, 193], "contain": [0, 3, 4, 5, 6, 10, 12, 13, 15, 24, 26, 27, 43, 51, 53, 64, 66, 69, 72, 80, 81, 83, 86, 92, 93, 95, 98, 125, 130, 147, 150, 159, 162, 184, 188, 202, 205, 214, 226, 235, 239, 267, 274, 318, 320, 325, 327, 328, 331, 333, 337, 338, 339, 345, 348], "container": [11, 79, 196, 198], "containerfil": [108, 109, 115, 117, 122], "content": [10, 83, 91, 95, 108, 109, 115, 117, 121, 122, 125, 128, 129, 130, 146, 184, 190, 318, 324, 338, 347], "context": [111, 113, 117, 120, 123, 137, 139, 141, 305, 309, 312, 316, 318, 324, 331, 332, 338, 339], "contextu": [133, 135, 137, 140, 141], "contigu": [10, 184, 188, 318, 319, 320], "continu": [5, 8, 13, 80, 83, 92, 95, 114, 116, 125, 131, 137, 141, 169, 175, 202, 223, 224, 225, 226, 244, 265, 267, 274, 305, 307, 312, 317, 318, 323, 338, 340, 343, 345, 346], "contrast": [8, 169, 173, 174, 267, 273, 274], "control": [4, 8, 9, 16, 19, 20, 21, 25, 27, 51, 52, 64, 65, 82, 87, 88, 91, 94, 99, 101, 108, 109, 113, 115, 125, 128, 137, 141, 142, 145, 147, 150, 169, 170, 176, 180, 202, 206, 218, 243, 244, 256, 261, 265, 312, 313, 317, 318, 319, 325, 328, 338, 339], "controversi": [267, 270], "conv1": [5, 7, 13, 14, 202, 205, 229, 232, 233], "conv2": 13, "conv2d": [5, 7, 13, 14, 202, 205, 229, 232, 233, 305, 308], "convei": [267, 274], "conveni": [1, 4, 5, 147, 149, 151, 152, 202, 204], "convent": [202, 203, 267, 274], "converg": [305, 309, 311, 312, 316, 318, 322, 331, 335, 338, 345], "convers": [125, 128, 130, 131, 202, 220], "convert": [3, 6, 9, 13, 108, 109, 111, 159, 168, 176, 180, 202, 210, 215, 218, 219, 220, 235, 238, 244, 259, 263, 266, 269, 275, 280, 289, 290, 291, 292, 303, 305, 307, 309, 318, 319, 320, 325, 326, 328, 331, 333, 338, 340, 341, 347], "convnext": [338, 347], "convolut": [202, 205, 305, 306], "cool": [267, 274], "coordin": [17, 22, 305, 306, 312, 313, 325, 326, 328, 338, 339, 347], "cop": [89, 90, 103, 106], "copi": [3, 5, 8, 81, 82, 85, 86, 89, 93, 94, 97, 98, 105, 142, 144, 159, 161, 169, 170, 202, 203, 210, 244, 265, 318, 320, 322, 325, 328, 331, 335, 337, 338, 345, 347], "cor": [17, 22, 53, 63], "core": [4, 6, 7, 10, 11, 12, 14, 84, 96, 147, 149, 155, 165, 184, 186, 196, 198, 202, 204, 229, 232, 233, 235, 238, 244, 250, 263, 305, 306, 307, 309, 311, 312, 317, 318, 319, 322, 325, 328, 338, 339, 340], "corner": [81, 93, 244, 265], "correct": [5, 8, 13, 137, 141, 169, 175, 202, 205, 206, 209, 210, 212, 217, 223, 224, 244, 256, 265, 318, 323, 325, 326, 329, 330, 331, 333, 338, 339, 342, 343], "correct_squar": [3, 159, 163], "correct_square_mod": [3, 159, 163], "correctli": [1, 53, 63, 88, 100, 151, 152, 202, 204, 213, 215, 226, 305, 307, 318, 320, 322, 325, 327, 331, 333, 338, 340, 341], "correl": [7, 14, 229, 233], "correspond": [2, 8, 13, 153, 157, 169, 175, 202, 214, 318, 321], "corrupt": [305, 306, 307], "cost": [2, 5, 6, 8, 13, 82, 84, 85, 86, 94, 96, 97, 98, 112, 114, 115, 116, 117, 119, 124, 128, 137, 141, 153, 155, 169, 170, 172, 174, 235, 240, 312, 317], "costum": [244, 263, 265], "could": [3, 9, 108, 109, 113, 159, 163, 167, 176, 182, 244, 253, 264, 265, 267, 273, 274, 285, 292, 301], "couldn": [244, 265], "count": [2, 9, 10, 15, 133, 135, 153, 155, 176, 182, 183, 184, 193, 267, 274, 305, 307, 312, 314, 318, 320, 322, 325, 327, 329, 332], "countri": [125, 130, 244, 265, 267, 270, 318, 324], "countrymen": [267, 270], "coup": [125, 129], "coupl": [267, 274], "cours": [17, 20, 79, 125, 132, 133, 135, 136, 142, 143, 267, 273, 274, 348], "cover": [17, 20, 80, 81, 82, 84, 91, 92, 93, 94, 96, 108, 109, 116, 117, 118, 124, 132, 133, 134, 135, 244, 259, 260, 266, 267, 268, 274, 275, 276, 282, 300, 329, 330], "cover_typ": [325, 327], "covtyp": [325, 327, 328, 330], "covtype_xgb_cpu": [325, 328], "coward": [267, 274], "cp": [10, 11, 15, 16, 184, 191, 196, 200], "cpu": [3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 80, 82, 84, 92, 94, 96, 133, 135, 136, 137, 139, 147, 150, 159, 165, 166, 168, 176, 181, 183, 184, 186, 188, 190, 191, 196, 200, 201, 202, 205, 208, 209, 210, 212, 213, 215, 216, 221, 228, 229, 232, 233, 235, 238, 239, 243, 244, 251, 252, 253, 259, 261, 264, 265, 266, 267, 269, 275, 277, 280, 281, 282, 305, 311, 312, 313, 317, 318, 322, 324, 326, 327, 328, 330, 331, 335, 337, 338, 343, 347], "cpus_per_work": [325, 328], "craft": [267, 273, 274], "crappi": [267, 270], "crash": [8, 169, 173, 202, 225, 244, 263, 265, 266, 325, 326, 338, 346], "crazi": [244, 263, 265, 267, 274], "cream": [244, 265], "creat": [3, 4, 7, 8, 9, 10, 12, 13, 14, 15, 16, 19, 20, 21, 23, 24, 26, 27, 31, 32, 34, 36, 37, 40, 41, 42, 46, 47, 48, 50, 52, 57, 58, 59, 60, 62, 65, 68, 72, 73, 75, 76, 77, 78, 80, 81, 83, 84, 86, 87, 88, 90, 92, 93, 95, 96, 98, 99, 101, 106, 108, 109, 112, 115, 117, 120, 121, 122, 124, 125, 128, 129, 130, 132, 133, 136, 137, 141, 147, 149, 150, 154, 158, 159, 161, 168, 169, 171, 172, 176, 178, 180, 184, 187, 204, 212, 215, 218, 219, 226, 228, 229, 233, 243, 250, 251, 252, 253, 260, 261, 263, 264, 268, 269, 276, 285, 288, 291, 292, 300, 301, 302, 303, 305, 307, 319, 325, 330, 331, 332, 338, 339, 341, 342, 347], "create_us": 146, "creation": [3, 79, 159, 161], "cred": [244, 263, 265], "credenti": [28, 29, 35, 38, 43, 44, 53, 55, 66, 69, 72], "credit": [9, 176, 179, 267, 274], "creepi": [267, 274], "creepybr": [267, 274], "creighton": [244, 265], "crime": [267, 274], "criteria": [10, 11, 108, 109, 111, 184, 186, 196, 198], "criterion": [5, 7, 14, 202, 206, 217, 223, 229, 232, 233, 338, 343], "critic": [8, 83, 95, 169, 175, 244, 265], "crop": [305, 307, 338, 339, 340], "cross": [17, 21, 22, 24, 26, 53, 63, 202, 204, 331, 333, 338, 339], "cross_attention_dim": [6, 235, 238], "crossattndownblock2d": [6, 235, 238], "crossattnupblock2d": [6, 235, 238], "crossentropyloss": [5, 7, 13, 14, 202, 204, 206, 217, 223, 229, 230, 232, 233, 338, 343], "crouch": [267, 274], "crow": [267, 273], "crucial": [125, 131, 267, 269, 275, 280], "crush": [244, 265], "cry": [267, 270], "css": 0, "csv": [5, 8, 10, 13, 83, 95, 169, 170, 184, 188, 202, 204, 216, 318, 320, 324, 325, 330, 331, 332, 333, 338, 345, 347], "csv_path": [331, 333], "ctor": [338, 347], "ctrl": [1, 151, 152], "cu128": [108, 109, 115, 117, 122, 137, 141], "cub": [244, 265], "cuda": [5, 7, 10, 13, 14, 15, 16, 108, 109, 114, 184, 191, 202, 206, 208, 209, 212, 213, 215, 217, 229, 232, 244, 251, 252, 253, 256, 264, 265, 275, 280, 305, 311, 312, 317, 331, 337, 338, 347], "cultur": [267, 270], "cumprod": [6, 235, 238], "cumul": [318, 324], "cup": [267, 274], "cure": [267, 274], "curiou": [202, 228, 267, 270], "curl": [1, 146, 151, 152], "current": [3, 5, 8, 9, 12, 13, 14, 16, 43, 46, 53, 58, 66, 69, 86, 90, 98, 107, 125, 130, 142, 144, 159, 165, 169, 174, 176, 182, 202, 211, 212, 218, 224, 275, 282, 305, 309, 312, 313, 316, 318, 320], "current_training_step": [6, 235, 238], "curti": [244, 263, 265, 266], "curv": [202, 204, 214, 308, 331, 335, 339, 340], "custom": [3, 4, 6, 11, 12, 20, 21, 24, 26, 27, 28, 29, 35, 36, 39, 43, 44, 53, 54, 66, 67, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 101, 107, 108, 109, 115, 117, 122, 124, 125, 128, 132, 137, 139, 142, 145, 147, 149, 159, 164, 165, 190, 196, 198, 201, 202, 205, 235, 238, 267, 269, 270, 274, 305, 306, 318, 319, 322, 325, 326, 330, 339, 347], "custom_hid": 0, "custom_light": 0, "custom_nam": [125, 128], "customer_ingress_cidr_rang": [17, 22], "cut": [5, 6, 13, 235, 240, 331, 333], "cv": [125, 128], "cv_job_match": [125, 128], "cybersecur": [125, 128], "d": [9, 10, 15, 16, 137, 141, 142, 144, 146, 176, 179, 180, 182, 184, 188, 189, 190, 244, 248, 249, 250, 256, 259, 263, 265, 266, 267, 273, 274, 305, 307, 309, 312, 314, 316, 317, 318, 319, 324, 325, 328, 329, 331, 333, 337, 338, 340, 347], "d3a9a7d0": [35, 39, 66, 70], "d89d0_00000": 13, "d_": [5, 13], "d_model": [331, 334, 335, 337], "da": [244, 265], "dag": [8, 169, 172, 175], "dai": [125, 130, 244, 265, 267, 274], "daili": [325, 330], "dalla": [244, 265], "damn": [244, 265], "dancer": [267, 274], "daniel": [244, 265, 267, 273], "dark": [146, 267, 273, 274], "darwin": [244, 265], "dash": [267, 274], "dashboard": [1, 5, 6, 8, 16, 24, 27, 81, 86, 90, 93, 98, 106, 117, 123, 124, 133, 135, 136, 137, 139, 140, 141, 143, 145, 151, 152, 169, 171, 173, 202, 203, 228, 235, 237, 312, 317], "dask": [8, 169, 173, 174], "data": [2, 11, 12, 16, 17, 21, 22, 43, 51, 53, 64, 81, 82, 83, 91, 93, 94, 95, 109, 115, 125, 129, 130, 131, 132, 133, 135, 137, 141, 143, 146, 153, 155, 160, 161, 172, 173, 189, 191, 196, 198, 199, 204, 206, 208, 210, 223, 227, 230, 233, 234, 236, 238, 240, 250, 253, 259, 263, 264, 266, 270, 271, 274, 275, 277, 278, 280, 282, 306, 309, 311, 312, 313, 314, 316, 317, 319, 322, 324, 326, 327, 328, 330, 332, 333, 335, 339, 340, 341], "data_dir": [331, 333, 335, 337], "data_load": [5, 6, 7, 13, 14, 202, 206, 217, 218, 223, 229, 231, 232, 233, 235, 238], "data_path": [9, 176, 179, 182], "data_url": [318, 320], "databas": [10, 125, 127, 130, 146, 184, 187], "databaseservic": [142, 145, 146], "databrick": [8, 10, 169, 170, 184, 188], "datadog": [133, 135], "datafram": [4, 8, 9, 12, 147, 150, 169, 173, 176, 180, 203, 204, 219, 269, 305, 307, 309, 318, 320, 322, 324, 325, 329, 330, 331, 333, 338, 347], "dataload": [7, 13, 14, 204, 206, 212, 216, 217, 220, 229, 230, 231, 233, 236, 239, 278, 305, 307, 312, 314, 335, 339, 340, 343], "dataset": [4, 6, 7, 8, 10, 14, 16, 83, 95, 142, 144, 147, 150, 169, 170, 171, 172, 178, 180, 182, 183, 184, 186, 187, 188, 189, 190, 192, 193, 194, 203, 205, 207, 210, 212, 215, 216, 217, 218, 220, 221, 224, 226, 227, 228, 229, 230, 231, 232, 233, 235, 236, 238, 239, 243, 247, 253, 256, 261, 262, 264, 265, 269, 274, 275, 277, 278, 280, 281, 282, 305, 306, 307, 309, 311, 316, 317, 319, 322, 324, 326, 328, 330, 332, 335, 337, 338, 339, 340, 341, 342, 347], "dataset_": [142, 144], "dataset_iter": [202, 218], "dataset_uri": [318, 320], "datasourc": [9, 10, 15, 176, 181, 184, 188], "date": [13, 125, 130, 244, 265], "datetim": [5, 13, 202, 204, 331, 333], "david": [146, 244, 263, 265], "dawson": [267, 273], "day_of_week": 12, "db": 146, "ddim": [305, 311], "ddp": [5, 6, 204, 205, 208, 209, 212, 213, 224, 228, 235, 239, 305, 309, 318, 320, 321, 324, 331, 332, 335, 337, 338, 343, 347], "ddpmschedul": [6, 235, 236, 238], "ddpstrategi": [6, 235, 238], "de": [3, 159, 161, 244, 265, 308, 311, 312, 313, 317, 331, 337], "deactiv": [1, 151, 152], "dead": [267, 274], "deadlock": [3, 159, 166], "deal": [8, 169, 175], "dear": [244, 265], "debat": [244, 265], "debug": [8, 11, 81, 87, 91, 93, 99, 125, 132, 133, 135, 136, 137, 139, 141, 142, 143, 145, 169, 173, 196, 201, 202, 204, 211, 215, 305, 306], "debut": [244, 265], "decemb": [244, 265], "decid": [2, 7, 14, 15, 125, 130, 153, 155, 202, 205, 213, 229, 233, 244, 265, 267, 274, 312, 313, 331, 335], "decis": [24, 26, 325, 326, 327], "declar": [202, 208, 305, 306, 312, 313, 318, 319, 325, 326, 338, 339], "decod": [116, 306, 311, 331, 332, 333, 334, 335, 337], "decode_and_norm": [305, 307], "decoder_input": [331, 334, 335], "decor": [2, 3, 10, 11, 16, 153, 156, 159, 165, 168, 184, 189, 193, 196, 200, 291, 292, 303], "decoupl": [11, 196, 201, 202, 228], "decreas": [202, 214, 331, 335, 338, 345], "dedic": [7, 14, 85, 86, 89, 90, 97, 98, 103, 106, 117, 122, 125, 131, 133, 136, 202, 215, 229, 233, 267, 274], "dedupl": [14, 275, 282], "deep": [9, 117, 123, 176, 179, 202, 204, 243, 244, 261, 267, 274, 275, 277, 278, 281, 282, 318, 320, 324, 338, 340], "deeper": [8, 108, 109, 116, 125, 127, 132, 169, 171], "deepli": [137, 141], "deepseek": [125, 131], "deepspe": [5, 6, 13, 202, 228, 235, 240], "def": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 81, 84, 85, 89, 93, 96, 97, 104, 125, 130, 137, 141, 142, 144, 145, 147, 150, 151, 152, 153, 156, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 176, 180, 184, 190, 191, 193, 196, 200, 202, 205, 206, 209, 210, 211, 212, 215, 217, 218, 220, 223, 224, 229, 231, 232, 233, 235, 238, 239, 244, 251, 252, 253, 264, 267, 274, 275, 279, 280, 281, 289, 290, 291, 292, 303, 305, 307, 308, 309, 311, 312, 314, 315, 316, 317, 318, 320, 321, 322, 325, 328, 329, 331, 333, 334, 335, 337, 338, 341, 342, 343, 347], "default": [0, 3, 5, 6, 7, 9, 10, 11, 12, 28, 30, 35, 38, 39, 43, 45, 53, 56, 66, 69, 80, 82, 83, 84, 86, 87, 92, 94, 95, 96, 98, 99, 125, 130, 137, 141, 142, 145, 159, 163, 165, 176, 179, 180, 184, 188, 190, 196, 201, 202, 205, 223, 229, 233, 235, 238, 267, 272, 275, 282, 305, 307, 309, 325, 327, 331, 332, 338, 347], "default_cluster_storag": [142, 144], "default_data_col": [275, 280], "default_root_dir": [6, 235, 238, 305, 309, 312, 316], "default_tracing_servic": 146, "defens": [244, 265, 331, 335], "defin": [2, 3, 4, 5, 7, 10, 11, 12, 14, 19, 43, 50, 53, 62, 66, 69, 80, 82, 85, 86, 89, 90, 92, 94, 97, 98, 105, 107, 108, 109, 112, 117, 120, 125, 129, 131, 137, 139, 147, 150, 153, 158, 159, 163, 165, 168, 184, 190, 196, 200, 201, 203, 208, 209, 210, 212, 215, 216, 221, 229, 233, 243, 244, 256, 259, 261, 265, 266, 267, 274, 275, 277, 280, 281, 291, 292, 303, 305, 307, 309, 320, 326, 329, 330, 331, 332, 333, 337, 338, 341, 342, 343, 347], "definit": [82, 94, 125, 129, 130, 312, 314], "degre": [9, 10, 15, 176, 182, 184, 193], "del": [202, 215], "delai": [16, 267, 273], "deleg": [4, 147, 150], "delet": [5, 17, 22, 28, 30, 33, 35, 42, 43, 45, 51, 53, 56, 64, 66, 78, 83, 95, 202, 204, 227, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "delete_object": [83, 95], "deleteobject": [17, 22], "delhi": [244, 265], "deliv": [137, 141, 267, 273], "deloy": [86, 98], "delta": [8, 108, 109, 115, 117, 121, 122, 125, 128, 169, 170, 325, 330], "demand": [1, 16, 24, 25, 27, 80, 82, 86, 90, 92, 94, 98, 106, 151, 152, 305, 311, 333], "demo": [17, 20, 202, 206, 305, 311, 312, 317, 318, 324], "demonstr": [6, 20, 83, 95, 117, 118, 125, 127, 137, 141, 142, 143, 144, 146, 202, 215, 235, 236, 244, 259, 266, 267, 271, 273, 274, 275, 277, 282, 305, 307, 310, 312, 316, 318, 320, 323, 324, 331, 336, 339, 340], "denizen": [267, 270], "denni": [244, 265], "deped": [82, 94], "depend": [0, 8, 10, 17, 22, 28, 30, 43, 45, 53, 56, 73, 80, 81, 82, 91, 92, 93, 94, 108, 109, 111, 113, 146, 160, 166, 169, 172, 173, 174, 184, 190, 202, 216, 260, 268, 275, 276, 277, 300, 305, 307, 312, 314, 318, 320, 325, 327, 331, 332, 333, 338, 340], "depict": [267, 274], "deploi": [4, 11, 12, 16, 17, 19, 20, 21, 22, 30, 39, 45, 56, 79, 82, 86, 94, 98, 108, 109, 110, 111, 113, 114, 115, 116, 119, 121, 124, 126, 127, 129, 130, 132, 142, 145, 146, 147, 150, 196, 197, 201, 243, 261, 285, 299, 301, 304, 312, 317, 325, 330, 331, 337, 348], "deploy": [0, 4, 8, 12, 18, 19, 22, 23, 26, 28, 30, 35, 36, 38, 39, 43, 45, 47, 48, 53, 56, 59, 60, 63, 66, 69, 70, 75, 76, 86, 87, 90, 98, 99, 106, 107, 113, 114, 116, 118, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 137, 140, 142, 145, 147, 150, 169, 175, 200, 201, 285, 296, 301, 318, 324, 331, 337, 338, 347], "deployment_config": [108, 109, 115, 117, 120, 123, 125, 130], "deploymenthandl": [292, 296, 303], "deploymentrespons": 16, "depress": [267, 270], "depth": 13, "deriv": [318, 322], "derp": [28, 30, 43, 45, 53, 56], "desc": [5, 338, 340], "descent": [267, 274], "describ": [28, 30, 43, 45, 53, 56, 57, 267, 274, 312, 313], "descript": [1, 9, 17, 21, 22, 108, 109, 113, 128, 130, 151, 152, 176, 179, 312, 314], "deseri": [2, 8, 153, 155, 169, 173], "design": [1, 2, 8, 9, 10, 16, 79, 108, 109, 112, 125, 128, 132, 151, 152, 153, 155, 169, 170, 173, 174, 176, 178, 181, 184, 189, 267, 274, 275, 277, 281, 282, 285, 292, 301, 325, 328, 331, 332, 338, 343], "desir": [13, 338, 347], "desktop": [81, 93], "despit": [267, 274], "destin": [125, 128], "destroi": [28, 30, 33, 35, 42, 43, 51, 53, 64, 66, 78, 244, 265, 267, 274], "destruct": [267, 274], "detach": [202, 215, 331, 337], "detail": [3, 5, 6, 7, 9, 10, 11, 13, 14, 16, 17, 20, 22, 24, 25, 35, 39, 43, 45, 53, 56, 66, 70, 83, 84, 88, 95, 96, 101, 117, 123, 125, 128, 144, 145, 146, 159, 165, 176, 182, 184, 186, 196, 198, 201, 202, 203, 208, 212, 216, 217, 229, 231, 235, 237, 239, 260, 267, 268, 274, 276, 291, 292, 300, 303, 348], "detailsbr": [267, 274], "detect": [8, 125, 128, 137, 141, 169, 173, 202, 226, 244, 251, 252, 253, 264, 305, 310, 338, 345], "determin": [10, 108, 109, 111, 184, 189, 267, 272], "determinist": [325, 327], "determint": [10, 184, 193], "dev": [28, 30, 43, 45, 53, 56, 312, 313], "deval": [325, 328], "develop": [2, 4, 8, 9, 12, 16, 28, 29, 30, 35, 37, 43, 44, 45, 53, 55, 56, 66, 68, 80, 83, 84, 85, 86, 88, 89, 92, 95, 96, 97, 98, 101, 103, 117, 118, 133, 135, 142, 145, 147, 149, 153, 155, 169, 173, 174, 175, 176, 182, 197, 198, 244, 265, 275, 281, 338, 339, 348], "deviat": [338, 341], "devic": [2, 5, 6, 7, 10, 11, 13, 24, 27, 51, 52, 64, 65, 108, 109, 115, 153, 155, 184, 191, 196, 200, 201, 202, 205, 206, 208, 209, 210, 212, 213, 215, 217, 218, 229, 232, 233, 235, 238, 239, 243, 244, 256, 261, 265, 275, 277, 280, 305, 308, 309, 311, 312, 313, 316, 317, 331, 335, 337, 338, 339, 342, 343, 347], "devop": [79, 91], "df": [4, 6, 9, 147, 150, 176, 179, 180, 181, 182, 202, 219, 235, 238, 305, 309, 312, 316, 318, 320, 322, 324, 325, 327, 329, 330, 331, 333, 335, 338, 341, 345], "di": [3, 159, 163, 267, 273], "diagnos": [133, 135], "diagnost": [331, 335], "diagon": [325, 329], "diagram": [4, 6, 7, 8, 13, 14, 17, 21, 24, 26, 91, 108, 109, 114, 147, 150, 169, 173, 175, 202, 203, 212, 229, 233, 235, 238, 239, 275, 277], "diari": [244, 265], "dict": [3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 108, 109, 115, 117, 120, 123, 125, 128, 130, 137, 141, 147, 150, 159, 162, 184, 190, 191, 193, 196, 200, 202, 206, 212, 217, 218, 220, 221, 223, 224, 228, 229, 233, 235, 238, 239, 244, 247, 251, 252, 253, 262, 264, 275, 278, 280, 305, 307, 312, 314, 331, 337], "dictat": [267, 274], "dictionari": [3, 4, 7, 14, 147, 150, 159, 165, 202, 207, 211, 217, 218, 229, 233, 275, 280, 318, 322], "did": [3, 159, 163, 244, 265, 267, 273, 274, 305, 309, 311, 312, 316, 317], "didn": [244, 265], "diego": [244, 265], "differ": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 22, 24, 26, 66, 71, 79, 81, 83, 93, 95, 108, 109, 111, 112, 113, 119, 125, 127, 128, 133, 136, 137, 140, 142, 143, 145, 147, 150, 151, 152, 153, 158, 169, 172, 176, 182, 184, 190, 193, 196, 199, 202, 203, 206, 208, 216, 217, 218, 229, 233, 235, 237, 243, 244, 261, 265, 267, 270, 275, 280, 281, 305, 311, 312, 317, 325, 328, 331, 335, 338, 339, 347], "differenti": [325, 326], "difficult": [8, 137, 141, 169, 173], "diffus": [5, 13, 236, 239, 240, 309, 314, 316], "diffusionpolici": [316, 317], "digit": [7, 11, 14, 196, 200, 205, 219, 229, 231, 232], "dii": [108, 109, 113], "dilat": 13, "dim": [202, 215, 305, 308, 312, 313, 315, 318, 321, 338, 343, 347], "dimens": [202, 215, 318, 319, 322, 331, 333], "dimension": [325, 326, 327], "dinger": [244, 265], "dir": [82, 83, 84, 94, 95, 96, 202, 204, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340, 343, 347], "direct": [7, 14, 17, 22, 24, 26, 125, 131, 229, 233, 267, 274, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "directli": [3, 8, 17, 22, 24, 26, 27, 81, 84, 88, 93, 96, 101, 125, 129, 159, 161, 162, 169, 170, 202, 216, 305, 306, 309, 312, 313, 317, 318, 319, 320, 322, 325, 327, 328, 331, 332, 338, 339, 345], "director": [267, 273, 274], "directori": [0, 5, 9, 43, 46, 53, 58, 81, 83, 86, 90, 93, 95, 98, 107, 146, 176, 181, 202, 204, 212, 215, 227, 305, 311, 318, 320, 324, 325, 326, 330, 331, 333, 337, 338, 343, 347], "dirpath": [305, 309, 312, 316], "disabl": [6, 13, 14, 16, 202, 215, 235, 239, 275, 282], "disaggreg": [10, 184, 195], "disappoint": [244, 263, 265], "discern": [267, 270], "disconnect": [318, 322], "discontinu": [312, 313], "discount": [318, 324], "discov": [267, 274], "discret": [8, 169, 173], "discuss": [7, 12, 14, 229, 232, 233], "disengag": [9, 176, 179], "disjoint": [202, 206, 267, 273, 274], "disk": [5, 10, 15, 83, 84, 95, 96, 133, 135, 137, 139, 142, 144, 145, 184, 186, 192, 202, 204, 219, 224, 305, 311, 318, 324, 325, 330, 331, 337, 338, 347], "dismiss": [244, 265], "displai": [1, 5, 13, 17, 22, 85, 97, 117, 120, 142, 145, 151, 152, 202, 204, 214, 215, 226, 267, 270, 274, 309, 338, 345], "disrupt": [267, 273], "dist": [202, 221], "dist_val_acc": [338, 343], "distanc": [4, 7, 9, 12, 14, 147, 150, 176, 179, 182, 229, 233, 305, 311, 325, 330], "distil": [108, 109, 112, 331, 337], "distilbert": [289, 290, 291, 292, 303], "distinct": [24, 26, 108, 109, 111, 137, 140, 202, 205, 318, 320], "distract": [267, 274], "distribut": [1, 2, 3, 7, 9, 10, 12, 14, 15, 17, 22, 82, 84, 91, 94, 96, 108, 109, 113, 114, 117, 123, 124, 146, 148, 149, 151, 152, 153, 155, 159, 160, 161, 166, 170, 174, 175, 176, 177, 178, 179, 181, 183, 184, 185, 186, 188, 189, 192, 204, 205, 206, 208, 210, 213, 214, 216, 219, 220, 221, 228, 229, 232, 236, 237, 244, 259, 266, 267, 269, 271, 274, 278, 280, 281, 282, 285, 292, 301, 307, 311, 313, 314, 317, 320, 324, 327, 330, 333, 335, 340, 343, 347], "distributeddataparallel": [5, 6, 13, 202, 203, 206, 209, 212, 213, 235, 239, 318, 321], "distributedsampl": [5, 13, 202, 206, 210, 212, 338, 342, 343], "div_term": [331, 334], "dive": [91, 108, 109, 116, 125, 126, 127, 132, 331, 333], "divers": [8, 169, 170, 173], "divid": [8, 17, 22, 169, 173], "dl_dw": [7, 14, 229, 233], "dmatrix": [4, 12, 147, 150, 325, 327, 328, 329], "dn": [24, 26], "do": [3, 4, 7, 9, 12, 13, 14, 35, 37, 66, 68, 81, 83, 93, 95, 125, 128, 130, 147, 148, 159, 162, 165, 176, 180, 181, 182, 229, 232, 243, 244, 256, 261, 265, 267, 270, 273, 274], "doc": [1, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 22, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 84, 88, 89, 90, 96, 101, 103, 106, 108, 109, 115, 125, 128, 142, 145, 151, 152, 176, 179, 181, 182, 184, 193, 194, 196, 201, 202, 208, 214, 235, 239, 244, 250, 263, 275, 282, 285, 291, 292, 301, 303], "docker": [24, 26], "dockerfil": [108, 109, 115, 117, 122], "document": [7, 8, 14, 16, 17, 22, 24, 25, 81, 85, 93, 97, 108, 109, 112, 116, 117, 124, 125, 129, 131, 132, 142, 145, 169, 170, 174, 175, 229, 233], "documentari": [267, 270], "doe": [3, 4, 6, 7, 8, 9, 10, 14, 24, 27, 84, 85, 89, 96, 97, 103, 108, 109, 112, 113, 133, 136, 137, 141, 147, 150, 159, 165, 166, 169, 174, 176, 179, 184, 190, 202, 220, 229, 233, 235, 238, 244, 265, 266, 267, 270, 274, 305, 311, 312, 317, 318, 324], "doesn": [3, 5, 9, 10, 15, 24, 26, 83, 95, 108, 109, 113, 159, 162, 176, 182, 184, 193, 202, 204, 244, 265, 267, 270, 273, 274, 331, 333], "doesnt": [267, 274], "dog": [338, 339], "dogma": [267, 274], "dolocationid": [9, 176, 179], "domain": [325, 329, 330], "domin": [8, 169, 173, 325, 329], "don": [1, 2, 3, 5, 9, 10, 15, 91, 151, 152, 153, 158, 159, 164, 176, 180, 184, 190, 202, 203, 204, 208, 244, 265, 267, 270, 274, 305, 307, 311, 318, 324, 338, 339, 342], "donald": [244, 265], "done": [5, 9, 13, 28, 33, 35, 42, 66, 78, 81, 84, 85, 86, 93, 96, 97, 98, 108, 109, 115, 117, 121, 122, 142, 144, 176, 182, 202, 206, 215, 267, 273, 274, 292, 299, 304, 318, 320, 325, 326], "dont": [267, 274], "dool": [244, 265], "dorset": [244, 265], "dot": [3, 159, 165, 244, 263, 265, 305, 306, 312, 313, 318, 319, 321, 324, 325, 326, 338, 339], "dot_product": [318, 321], "doubl": [244, 263, 265, 267, 270], "down": [5, 6, 13, 16, 24, 26, 27, 53, 63, 85, 89, 97, 103, 108, 109, 113, 123, 202, 212, 235, 240, 243, 244, 261, 265, 267, 269, 274, 275, 277, 312, 313], "down_block_typ": [6, 235, 238], "downblock2d": [6, 235, 238], "download": [1, 5, 7, 13, 14, 15, 16, 66, 69, 81, 83, 84, 86, 91, 93, 95, 96, 98, 125, 128, 151, 152, 210, 227, 229, 231, 233, 288, 292, 302, 318, 320, 331, 332, 333], "downsampl": 13, "downsample_pad": [6, 235, 238], "downscal": [16, 292, 299, 304], "downscale_delay_": 16, "downstream": [10, 15, 125, 127, 129, 184, 192, 267, 269, 318, 324], "downtim": [86, 90, 98, 106, 142, 145], "draft": [244, 263, 265, 266], "drag": [244, 265], "dragon": [267, 274], "drama": [267, 270], "draw": [267, 274], "dread": [244, 265], "dream": [267, 273, 274], "dreambr": [267, 274], "dreamnightmar": [267, 274], "dress": [244, 265], "drill": [8, 169, 171], "drive": [267, 273], "driver": [3, 10, 24, 27, 137, 139, 142, 145, 159, 162, 166, 184, 186, 191, 244, 265, 318, 320, 325, 329, 331, 337, 338, 347], "driver_artifact": [12, 13, 275, 282], "drop": [108, 109, 114, 202, 210, 215, 244, 265, 305, 307, 325, 328, 331, 333, 338, 347], "drop_column": [12, 305, 307], "drop_last": [5, 7, 13, 14, 202, 210, 229, 231, 233, 331, 333], "dropdown": [81, 93, 117, 123, 142, 144], "dropna": [318, 322, 331, 335, 338, 345], "dropout": [331, 334], "ds_adjust": [9, 176, 180, 181], "ds_block_based_shuffl": [9, 176, 182], "ds_file_shuffl": [9, 176, 182], "ds_iter": [325, 328], "ds_label": [10, 184, 190], "ds_limit": [9, 176, 181], "ds_meta": [267, 271, 273], "ds_normal": [10, 15, 184, 190, 191], "ds_pred": [10, 15, 184, 191, 192, 193, 194], "ds_randomized_block": [10, 15, 184, 193], "ds_randomized_row": [10, 15, 184, 193], "ds_review": [267, 271, 272], "ds_row_based_shuffl": [9, 176, 182], "ds_tip": [9, 176, 180], "ds_tmp": [338, 347], "dsl": [8, 169, 172], "dtest": [4, 147, 150], "dtrain": [4, 147, 150, 325, 328], "dtype": [6, 12, 16, 137, 141, 202, 220, 235, 238, 244, 265, 266, 305, 307, 312, 314, 317, 318, 322, 331, 333, 334], "due": [3, 5, 6, 8, 10, 11, 108, 109, 112, 117, 123, 137, 141, 159, 163, 169, 173, 184, 186, 190, 196, 198, 202, 203, 235, 237, 244, 250, 263, 267, 273, 274, 292, 296, 304], "dummi": [108, 109, 115, 125, 130], "dummy_data_1000_500": [83, 95], "dummy_data_1000_720": [83, 95], "dummy_data_xxl": [83, 95], "dummy_kei": [109, 115], "dump": [11, 16, 125, 130, 142, 145, 196, 200], "duplic": [202, 203, 212, 224, 331, 333, 338, 339], "durabl": [8, 169, 170], "durat": [84, 96, 142, 145, 146], "dure": [0, 3, 5, 6, 13, 28, 30, 43, 45, 53, 56, 83, 95, 108, 109, 112, 113, 142, 145, 159, 165, 202, 203, 211, 214, 221, 235, 239, 244, 259, 266, 267, 270, 274, 275, 277, 280, 305, 306, 307, 309, 318, 322, 324, 325, 327, 328, 331, 332, 334, 337, 338, 340, 345], "dustin": [244, 265], "dvd": [267, 273], "dynam": [8, 9, 11, 16, 17, 22, 24, 25, 125, 128, 169, 175, 176, 182, 196, 198, 331, 332], "dynamic_lora_loading_path": [125, 128], "e": [0, 2, 3, 5, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 22, 24, 26, 27, 28, 29, 30, 35, 36, 43, 44, 45, 49, 50, 51, 53, 54, 56, 61, 62, 64, 66, 67, 81, 83, 84, 86, 87, 90, 93, 95, 96, 98, 99, 106, 107, 108, 109, 112, 117, 123, 133, 135, 137, 139, 142, 144, 153, 155, 159, 162, 163, 165, 169, 170, 173, 176, 180, 181, 184, 186, 188, 190, 196, 198, 199, 201, 202, 206, 207, 212, 213, 214, 216, 218, 222, 225, 228, 229, 233, 243, 244, 261, 305, 306, 312, 313, 318, 319, 322, 325, 328, 331, 337, 338, 340], "each": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 22, 43, 46, 53, 58, 79, 82, 83, 84, 86, 88, 90, 94, 95, 96, 98, 101, 106, 108, 109, 111, 112, 117, 120, 123, 125, 127, 128, 132, 137, 139, 140, 141, 142, 144, 145, 146, 147, 150, 151, 152, 153, 158, 159, 161, 169, 175, 176, 179, 180, 181, 184, 188, 190, 191, 196, 199, 202, 203, 204, 205, 206, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 224, 229, 231, 233, 235, 238, 239, 243, 244, 248, 249, 250, 253, 256, 261, 263, 264, 265, 267, 271, 274, 275, 277, 280, 281, 285, 291, 292, 301, 303, 305, 306, 307, 312, 313, 316, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 329, 331, 332, 333, 335, 337, 338, 339, 340, 341, 343, 345, 347], "earli": [7, 14, 229, 233, 244, 265, 305, 311, 325, 330, 331, 333, 337, 338, 347], "earlier": [81, 82, 84, 93, 94, 96, 267, 273, 305, 307, 338, 346], "early_stopping_round": [325, 330], "earn": [244, 265], "earth": [267, 273], "eas": [8, 169, 172], "easi": [0, 8, 9, 16, 91, 108, 109, 113, 117, 123, 125, 129, 130, 132, 137, 141, 142, 145, 169, 175, 176, 178, 202, 214, 216, 275, 280, 281, 285, 292, 301, 305, 306], "easier": [6, 8, 24, 26, 169, 175, 235, 238, 267, 274, 275, 277, 318, 324], "easili": [2, 3, 4, 5, 6, 7, 9, 10, 11, 86, 90, 98, 106, 108, 109, 110, 114, 117, 118, 125, 126, 137, 141, 147, 148, 153, 154, 159, 160, 176, 177, 184, 185, 196, 197, 229, 230, 235, 236, 243, 244, 261, 267, 269, 275, 277, 281, 285, 292, 301, 312, 313], "eastern": [267, 273, 274], "eat": [267, 273], "ec2": [18, 20, 22, 23, 24, 26, 27, 30, 31, 34, 43, 45, 53, 56, 84, 96, 348], "echo": [66, 69, 244, 265], "eclips": [244, 265], "ecolog": [325, 330], "ecosystem": [4, 5, 6, 8, 12, 147, 149, 169, 173, 174, 202, 203, 235, 237], "ed": [244, 265], "eddi": [244, 265, 267, 273, 274], "edg": [244, 265, 331, 337], "edgecolor": [318, 320], "edit": [81, 82, 93, 94, 244, 265], "editor": [35, 37, 66, 68, 81, 82, 85, 93, 94, 97], "educ": [338, 339], "ef": [15, 16, 21, 23, 24, 26, 28, 30, 34, 53, 57], "effect": [3, 4, 17, 22, 83, 95, 117, 119, 125, 128, 147, 150, 159, 162, 202, 207, 267, 273, 274, 318, 321, 322, 331, 337], "effici": [1, 8, 10, 11, 82, 88, 94, 102, 108, 109, 111, 117, 119, 125, 128, 151, 152, 169, 170, 173, 174, 184, 186, 196, 198, 199, 202, 203, 215, 216, 219, 228, 243, 244, 259, 261, 266, 267, 269, 274, 275, 277, 280, 281, 282, 305, 306, 307, 318, 319, 320, 322, 324, 331, 332, 333, 337, 338, 339, 341, 347], "efs_id": [17, 23, 28, 30], "egress": [24, 26], "eid": [244, 265], "eight": [312, 316, 338, 344], "eip": [28, 30, 43, 45, 53, 56], "eipalloc": [28, 30, 43, 45, 53, 56], "either": [9, 10, 15, 85, 89, 97, 105, 108, 109, 113, 142, 145, 176, 180, 184, 187, 202, 215, 244, 256, 265, 267, 274, 318, 320], "eject": [244, 265], "ek": [17, 18, 20, 22, 24, 27, 45, 46, 47, 52, 55, 56, 58, 59, 65, 348], "eks_cluster_nam": [43, 45, 46, 53, 56, 57, 58], "elam": [267, 273], "elaps": 12, "elast": [17, 22, 28, 30, 43, 45, 53, 56], "element": [16, 267, 274], "elev": [325, 326, 329], "elif": [275, 280, 305, 311, 331, 333], "elimin": [8, 108, 109, 112, 169, 170, 325, 327], "ellipsi": [87, 99], "els": [5, 7, 10, 86, 98, 125, 130, 184, 191, 202, 215, 227, 229, 232, 244, 256, 265, 275, 280, 289, 290, 291, 292, 303, 305, 311, 312, 317, 318, 320, 322, 324, 325, 328, 331, 333, 335, 337, 338, 343, 347], "elt": [8, 169, 170], "email": [35, 39, 66, 70, 88, 89, 101, 103, 142, 145, 146], "emb": [244, 251, 252, 253, 256, 264, 265, 318, 319, 324], "embed": [0, 85, 89, 97, 103, 243, 244, 253, 256, 260, 261, 264, 265, 266, 268, 276, 300, 305, 311, 312, 317, 320, 321, 324], "embedd": 0, "embedding_dim": [202, 207, 318, 321, 322, 324], "emit": [108, 109, 112, 305, 309], "emmanuel": [244, 265], "emotion": [267, 273, 274], "emploi": [8, 169, 170], "empti": [28, 30, 33, 35, 42, 43, 45, 51, 53, 64, 66, 78, 86, 90, 98, 107], "emption": [331, 332], "en": [1, 14, 108, 109, 115, 151, 152, 244, 250, 263, 267, 274, 275, 282, 285, 291, 292, 301, 303], "enabl": [2, 3, 4, 8, 9, 10, 11, 12, 17, 21, 22, 24, 25, 27, 37, 68, 78, 82, 83, 84, 86, 90, 91, 94, 95, 96, 98, 106, 124, 125, 127, 128, 130, 132, 133, 136, 137, 141, 142, 145, 146, 147, 149, 153, 155, 159, 160, 165, 169, 170, 173, 175, 176, 181, 183, 184, 186, 196, 199, 216, 222, 224, 225, 228, 243, 244, 259, 261, 266, 267, 269, 271, 274, 275, 281, 305, 306, 307, 311, 312, 313, 317, 318, 319, 322, 325, 326, 331, 332, 333, 337, 338, 339, 344, 347], "enable_access_log": [142, 145], "enable_auto_tool_choic": [125, 130], "enable_checkpoint": [6, 235, 239], "enable_filestor": [35, 39], "enable_lora": [125, 128], "enable_progress_bar": [305, 309, 312, 316], "encapsul": [202, 213, 338, 339], "encod": [108, 109, 111, 142, 145, 244, 251, 252, 253, 256, 264, 265, 267, 269, 311, 312, 313, 319, 324, 331, 333, 334], "encode_batch": [318, 320], "encount": [3, 5, 6, 66, 71, 159, 163, 235, 236, 292, 296, 304], "encourag": [305, 306, 318, 319], "end": [3, 8, 16, 24, 26, 79, 84, 96, 108, 109, 111, 112, 113, 115, 117, 121, 122, 124, 125, 128, 131, 142, 145, 148, 149, 159, 165, 169, 173, 174, 221, 226, 228, 244, 265, 267, 273, 274, 292, 296, 304, 305, 306, 311, 312, 313, 317, 318, 324, 325, 326, 330, 331, 332, 333, 337, 338, 339, 347], "endpoint": [11, 16, 24, 26, 86, 98, 108, 109, 113, 115, 117, 121, 122, 124, 196, 200, 289, 290, 291, 292, 303, 305, 311, 318, 324, 325, 330], "enforc": [0, 3, 8, 10, 82, 94, 125, 129, 159, 165, 169, 170, 184, 190], "engag": [9, 125, 128, 176, 179], "engin": [2, 5, 6, 9, 10, 11, 16, 17, 20, 24, 27, 39, 69, 70, 78, 79, 87, 99, 115, 117, 123, 125, 128, 142, 143, 153, 155, 170, 172, 173, 174, 176, 183, 184, 186, 196, 198, 202, 203, 235, 237, 275, 277, 282, 325, 329, 330, 331, 332, 333], "engine_arg": [108, 109, 115], "engine_kwarg": [108, 109, 115, 117, 120, 123, 125, 128, 129, 130], "english": [289, 290, 291, 292, 303], "enhanc": [8, 125, 127, 130, 133, 135, 137, 140, 141, 169, 170, 174], "enjoi": [244, 265, 267, 274], "enough": [3, 10, 15, 82, 94, 159, 168, 184, 193, 202, 212, 305, 307, 318, 320, 338, 340], "ensembl": [148, 325, 326], "ensu": [267, 274], "ensur": [1, 3, 5, 6, 8, 9, 13, 28, 29, 31, 35, 37, 40, 43, 44, 47, 53, 55, 59, 63, 66, 68, 75, 80, 82, 92, 94, 125, 129, 133, 136, 142, 143, 151, 152, 159, 165, 168, 169, 170, 176, 181, 202, 203, 206, 209, 212, 215, 216, 218, 220, 224, 235, 237, 244, 256, 260, 265, 268, 275, 276, 280, 300, 305, 309, 312, 317, 318, 320, 322, 325, 327, 328, 330, 331, 333, 338, 343], "enter": [81, 85, 86, 89, 93, 97, 98, 105, 146, 267, 270], "enterpris": [86, 90, 98, 106, 108, 109, 114, 117, 122, 124, 125, 132], "entir": [3, 5, 6, 8, 9, 10, 15, 24, 25, 66, 67, 87, 99, 108, 109, 112, 159, 165, 169, 173, 176, 179, 184, 189, 192, 193, 202, 203, 206, 235, 238, 243, 261, 267, 274, 275, 280, 305, 311, 318, 320, 324, 325, 328, 331, 333, 338, 339], "entiti": [87, 99], "entri": [146, 202, 220, 331, 337], "entropi": [338, 339], "entrypoint": [16, 85, 89, 97, 105], "enum": [125, 129, 130], "enumer": [7, 14, 229, 231, 267, 271, 318, 320, 324], "env": [3, 82, 94, 117, 122, 159, 164, 202, 204, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "env_var": [3, 117, 120, 123, 125, 128, 129, 130, 159, 164, 165], "environ": [0, 4, 5, 6, 8, 9, 10, 11, 17, 19, 22, 28, 29, 34, 35, 36, 43, 44, 52, 53, 54, 65, 66, 67, 69, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 95, 96, 97, 98, 99, 101, 103, 108, 109, 111, 117, 120, 123, 125, 128, 130, 133, 134, 136, 137, 141, 142, 143, 144, 145, 147, 150, 160, 165, 169, 173, 176, 181, 184, 191, 196, 199, 200, 235, 238, 243, 244, 261, 275, 277, 278, 281, 285, 292, 301, 305, 306, 307, 309, 314, 316, 317, 318, 320, 325, 326, 327, 331, 333, 338, 339, 340], "environment": [82, 94], "eot": [28, 30, 35, 39, 43, 45, 53, 56, 66, 70], "ep": 13, "ephemer": [202, 204], "epic": [244, 265], "episod": [312, 317], "epoch": [5, 7, 13, 14, 202, 206, 211, 212, 214, 216, 217, 222, 223, 224, 226, 228, 229, 232, 233, 275, 280, 281, 282, 305, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 322, 323, 324, 331, 332, 335, 337, 338, 339, 343, 344, 345, 346, 347], "epoch_loss": 5, "equival": [9, 176, 179, 181], "ergonom": [312, 314], "ernst": [267, 274], "ernsthugo": [267, 274], "erotica": [267, 270], "errno": [9, 176, 181], "error": [3, 5, 6, 8, 9, 10, 13, 14, 43, 51, 53, 64, 66, 71, 82, 94, 125, 129, 133, 135, 137, 141, 142, 145, 159, 163, 169, 173, 176, 181, 184, 186, 190, 235, 236, 243, 261, 292, 296, 304, 305, 306, 318, 319, 324, 325, 326], "erupt": [267, 274], "escap": [267, 274], "especi": [3, 5, 9, 15, 16, 82, 94, 159, 162, 165, 176, 183, 202, 210, 216, 243, 244, 261, 267, 273, 275, 280, 305, 307, 331, 335, 338, 340], "essenti": [8, 17, 21, 108, 109, 116, 117, 124, 169, 173, 202, 222, 275, 280], "establish": [7, 14, 17, 19, 125, 131, 229, 232], "estim": [5, 84, 96, 202, 206], "estimate_pi": [84, 96], "eta": [4, 147, 150, 305, 306, 312, 313, 325, 328, 330], "etc": [2, 3, 4, 5, 6, 9, 10, 12, 17, 18, 19, 22, 23, 24, 27, 117, 120, 142, 144, 145, 147, 150, 153, 155, 159, 168, 176, 179, 181, 184, 188, 202, 203, 207, 226, 235, 237, 325, 328], "ether": [267, 274], "etl": [8, 15, 169, 170, 173], "euler": [305, 311], "europ": [267, 273, 274], "europa": [244, 265, 267, 274], "ev": [244, 265], "eval": [4, 5, 6, 10, 11, 13, 15, 16, 147, 150, 184, 191, 196, 200, 202, 215, 235, 239, 275, 280, 305, 311, 312, 317, 318, 322, 324, 325, 328, 331, 335, 337, 338, 343, 347], "eval_arrow": [325, 328], "eval_dataset": [275, 280], "eval_metr": [4, 147, 150, 325, 328], "eval_pr": [275, 279], "evals_result": [4, 147, 150, 325, 328], "evalu": [3, 7, 14, 16, 17, 20, 79, 125, 131, 159, 165, 202, 215, 229, 232, 275, 277, 278, 279, 280, 305, 306, 311, 312, 313, 317, 318, 319, 324, 326, 327, 330, 338, 339, 343, 347], "evan": [244, 265], "even": [3, 16, 84, 96, 108, 109, 112, 125, 132, 137, 141, 159, 165, 202, 212, 244, 265, 267, 270, 274, 291, 292, 293, 295, 296, 303, 304, 331, 337, 338, 343], "evenli": [202, 206, 207], "event": [5, 6, 8, 84, 96, 133, 135, 137, 139, 140, 169, 173, 202, 203, 235, 237], "eventu": [202, 215], "ever": [4, 12, 147, 149, 267, 270, 273], "everi": [0, 2, 83, 85, 87, 89, 95, 97, 99, 103, 153, 158, 202, 206, 211, 212, 220, 221, 244, 265, 267, 271, 305, 308, 312, 313, 316, 318, 322, 325, 326, 327, 328, 338, 339, 340, 341, 345, 347], "every_n_epoch": [305, 309, 312, 316], "everyon": [267, 274], "everyth": [24, 26, 86, 98, 202, 213, 267, 270, 274, 305, 311, 312, 317, 318, 320, 324, 331, 335, 338, 340], "evil": [267, 273, 274], "evolut": [8, 169, 170], "evolv": [318, 322, 325, 326], "ex": [305, 307], "exact": [1, 3, 10, 80, 92, 117, 122, 125, 129, 151, 152, 159, 165, 184, 190], "exactli": [17, 22, 305, 307, 318, 323, 338, 339, 340, 341, 347], "examin": [43, 50, 53, 62], "exampl": [3, 5, 7, 8, 9, 10, 11, 15, 16, 18, 20, 22, 26, 28, 30, 31, 35, 38, 43, 44, 45, 53, 54, 56, 66, 67, 69, 71, 81, 85, 86, 93, 97, 98, 108, 109, 112, 113, 121, 123, 126, 127, 131, 132, 133, 135, 136, 138, 142, 143, 144, 145, 159, 161, 164, 165, 166, 168, 169, 170, 176, 180, 181, 184, 186, 188, 189, 191, 196, 200, 201, 202, 204, 207, 212, 214, 216, 228, 229, 233, 243, 244, 253, 256, 261, 264, 265, 267, 269, 272, 275, 280, 291, 292, 303, 305, 306, 311, 312, 313, 318, 319, 324, 325, 326, 328, 329, 330, 331, 337, 338, 339, 340, 345, 347], "exce": [289, 290, 291, 292, 303], "excel": [117, 119, 125, 131], "except": [3, 6, 16, 159, 163, 202, 223, 235, 239, 244, 265, 305, 307, 338, 340], "excess": [338, 339], "excit": [244, 265], "exclus": [5, 108, 109, 113], "exdb": [13, 14], "execut": [3, 4, 5, 6, 7, 8, 14, 17, 18, 22, 43, 50, 53, 62, 66, 69, 81, 84, 85, 88, 93, 96, 97, 101, 103, 105, 108, 109, 114, 125, 127, 130, 133, 135, 142, 144, 147, 150, 154, 156, 158, 159, 161, 162, 164, 166, 167, 169, 170, 174, 175, 179, 183, 185, 186, 190, 191, 192, 202, 203, 204, 205, 206, 220, 229, 233, 235, 238, 239, 244, 256, 259, 265, 266, 275, 277, 280, 305, 307, 312, 314, 318, 320, 322, 325, 326, 327, 328, 330, 331, 332, 333, 335, 338, 339, 340, 343, 347], "execute_notebook": 0, "exercis": 91, "exhaust": [108, 109, 113, 202, 222, 325, 328], "exhibit": [267, 274, 331, 333], "exisitng": [24, 27], "exist": [4, 9, 24, 25, 26, 27, 28, 29, 30, 43, 44, 45, 52, 55, 56, 65, 81, 82, 83, 84, 86, 90, 93, 94, 95, 96, 98, 107, 125, 130, 142, 144, 145, 147, 150, 176, 181, 202, 227, 244, 250, 263, 267, 270, 275, 281, 285, 292, 301, 305, 309, 311, 312, 316, 317, 318, 320, 322, 324, 325, 330, 331, 333, 335, 337, 338, 339, 343, 345, 347, 348], "exist_ok": [5, 13, 305, 307, 309, 312, 316, 318, 320, 325, 327, 331, 333, 338, 340], "existing_vpc_id": [17, 22], "exit": [305, 310], "exogen": [331, 337], "exp": [331, 334], "expand": [28, 29, 43, 44, 53, 55, 63, 66, 68, 84, 96, 142, 144, 202, 215, 305, 308], "expect": [5, 7, 14, 16, 81, 93, 108, 109, 113, 202, 205, 212, 229, 233, 305, 309, 318, 320, 325, 327, 331, 337, 338, 340], "expens": [4, 7, 10, 11, 12, 14, 15, 117, 119, 125, 128, 147, 150, 184, 191, 196, 198, 229, 232, 244, 253, 264, 267, 273, 305, 307], "expensive_comput": [3, 159, 167], "expensive_squar": [2, 3, 153, 158, 159, 162, 166, 167], "experi": [2, 5, 7, 79, 80, 81, 82, 86, 91, 92, 93, 94, 98, 117, 124, 125, 128, 132, 133, 135, 137, 141, 142, 143, 153, 155, 202, 204, 206, 228, 229, 233, 267, 270, 292, 293, 295, 296, 304, 312, 317, 325, 330, 338, 339, 346, 347], "experiment": [11, 196, 201, 318, 323], "experiment_nam": [6, 202, 224, 235, 239], "experinc": [90, 106], "expert": [8, 169, 174, 267, 274, 305, 306, 312, 313, 338, 340], "expertli": [267, 274], "explain": [17, 21, 24, 26, 88, 100, 142, 145, 244, 265, 267, 273], "explan": [125, 128, 142, 144, 291, 292, 303], "explicit": [125, 131, 267, 270, 318, 319, 325, 328], "explicitli": [6, 7, 9, 10, 86, 90, 98, 107, 176, 180, 184, 189, 229, 233, 235, 238, 243, 244, 250, 261, 263], "explor": [8, 81, 83, 86, 91, 93, 95, 98, 108, 109, 110, 113, 117, 123, 124, 125, 126, 127, 132, 142, 145, 169, 170, 202, 228, 267, 270, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "explos": [137, 141], "export": [66, 69, 117, 120, 121, 125, 128, 133, 136, 146, 312, 317], "expos": [24, 27, 244, 265], "exposehead": [17, 22, 53, 63], "exposur": [17, 22], "expr": [267, 272], "express": [5, 84, 96, 202, 204, 267, 272, 305, 311], "expresswai": [244, 265], "extend": [3, 17, 22, 159, 168, 202, 216, 223, 224, 228, 305, 306, 311, 312, 317, 318, 324, 325, 328, 330, 331, 337, 338, 341, 347], "extens": [10, 81, 83, 93, 95, 125, 130, 184, 188], "extern": [8, 9, 17, 22, 27, 53, 63, 66, 73, 125, 127, 130, 132, 137, 139, 169, 173, 176, 178, 202, 228], "extra": [108, 109, 113, 244, 265, 318, 319, 320, 322, 325, 327, 331, 335, 337, 338, 345], "extra_st": [202, 223, 224], "extract": [6, 8, 10, 13, 14, 53, 57, 169, 170, 184, 190, 235, 238, 318, 320, 324, 338, 345], "extract_dir": [318, 320], "extractal": [318, 320], "extrem": [16, 83, 95, 108, 109, 112, 137, 141, 292, 293, 295, 296, 304, 325, 326], "f": [3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 28, 30, 82, 83, 84, 86, 94, 95, 96, 98, 108, 109, 115, 117, 122, 125, 128, 129, 130, 137, 141, 142, 144, 146, 147, 150, 159, 161, 164, 165, 167, 176, 179, 181, 184, 191, 194, 196, 200, 202, 206, 211, 215, 227, 229, 232, 235, 236, 238, 275, 280, 281, 305, 307, 309, 311, 312, 316, 317, 318, 320, 322, 324, 325, 327, 328, 329, 330, 331, 332, 333, 334, 335, 337, 338, 340, 343, 347], "f1": [244, 265, 338, 347], "f_": [305, 306, 312, 313, 325, 326, 331, 332, 338, 339], "face": [5, 6, 16, 17, 22, 91, 108, 109, 113, 117, 120, 121, 125, 128, 130, 202, 203, 216, 235, 237, 243, 244, 259, 261, 263, 265, 266, 267, 270, 274, 278, 279, 282, 288, 292, 299, 302, 304, 338, 339, 340, 347], "facial": [267, 274], "facilit": [8, 83, 95, 169, 170, 174], "fact": [244, 265, 267, 270, 273, 274], "factor": [322, 324], "fahrenheit": [3, 125, 130, 159, 168], "fail": [3, 5, 6, 10, 13, 14, 137, 141, 159, 163, 184, 186, 190, 202, 203, 222, 224, 225, 235, 237, 318, 322, 325, 328, 331, 336], "failur": [5, 6, 8, 10, 85, 86, 89, 90, 97, 98, 103, 106, 117, 122, 133, 135, 137, 139, 141, 160, 163, 169, 173, 184, 186, 190, 202, 203, 222, 224, 225, 226, 235, 237, 312, 316, 325, 326, 330, 331, 332, 335, 337, 338, 339, 343, 347], "failure_config": [202, 224, 226, 305, 309, 312, 316, 318, 322, 325, 328, 331, 335, 338, 344], "failureconfig": [222, 225, 228, 305, 306, 307, 309, 312, 313, 314, 316, 318, 319, 320, 322, 325, 326, 327, 328, 331, 332, 333, 335, 338, 340, 343, 344, 347], "fair": [244, 265], "fake": [267, 271], "fake_kei": [108, 117, 121, 125, 128, 129, 130], "fall": [318, 320], "fallback": [305, 311, 312, 317], "fals": [4, 5, 6, 7, 11, 12, 13, 14, 16, 142, 145, 147, 150, 196, 200, 202, 205, 208, 229, 232, 233, 235, 238, 239, 305, 308, 309, 311, 312, 315, 316, 317, 318, 320, 322, 325, 327, 328, 331, 333, 335, 337, 338, 343, 345, 347], "famili": [244, 265], "familiar": [79, 142, 143, 202, 218], "fan": [244, 265, 267, 270], "fanatic": [267, 274], "fanchant": [244, 265], "fantasi": [267, 273], "fantast": [267, 274], "far": [267, 270, 273, 325, 328], "fare": [267, 274], "fare_amount": [4, 12, 147, 150], "fashion": [9, 15, 176, 183], "fast": [5, 9, 10, 24, 26, 91, 108, 109, 113, 114, 125, 131, 176, 178, 184, 190, 202, 203, 204, 312, 313, 325, 326, 330, 338, 340, 343], "fastapi": [4, 8, 86, 90, 98, 107, 146, 147, 148, 150, 169, 175, 299, 302, 304], "fastapideploy": [86, 90, 98, 107], "faster": [7, 117, 123, 124, 202, 215, 229, 233, 267, 272, 275, 281, 305, 311], "fastest": [81, 93], "fate": [267, 274], "father": [267, 274], "fault": [5, 6, 8, 9, 10, 17, 21, 22, 90, 106, 117, 122, 169, 173, 176, 178, 183, 184, 186, 203, 223, 224, 226, 235, 237, 305, 306, 307, 309, 311, 312, 313, 316, 317, 318, 319, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 336, 337, 339, 343, 344, 347], "fc": 13, "feasibl": [312, 317], "featur": [4, 8, 17, 20, 24, 26, 81, 93, 108, 109, 113, 114, 117, 122, 123, 124, 132, 133, 136, 137, 138, 142, 145, 147, 150, 169, 170, 173, 174, 177, 179, 180, 267, 270, 285, 292, 301, 305, 307, 318, 324, 326, 327, 328, 330, 331, 337, 338, 340, 347], "feature_col": [325, 328, 329], "feature_column": [325, 327, 328, 329, 330], "feature_nam": [325, 328], "feb": [244, 265], "fed": [244, 265], "feder": [17, 22], "fee": [4, 12, 147, 150], "feed": [9, 176, 178, 180, 202, 216, 244, 265, 331, 332, 335, 338, 341], "feedback": [81, 93], "feel": [86, 90, 98, 107, 202, 218, 244, 265, 267, 270, 273, 274, 305, 311, 312, 317, 318, 324, 325, 330, 338, 347], "femal": [267, 270], "fenc": [244, 265], "fend": [267, 273], "ferrari": [244, 265], "ferri": [244, 265], "fetch": [5, 6, 160, 161, 162, 202, 203, 204, 215, 218, 235, 237, 275, 280, 325, 327, 331, 333], "fetch_covtyp": [325, 327], "few": [66, 73, 80, 81, 82, 92, 93, 94, 125, 128, 142, 143, 244, 266, 267, 269, 270, 273, 274, 305, 306, 309, 311, 312, 317, 318, 319, 320, 324, 325, 330, 331, 337, 338, 347], "fewer": [8, 35, 39, 169, 173], "ff": [244, 265], "fiat": [244, 265], "fid": [305, 311], "field": [9, 86, 90, 98, 107, 125, 129, 176, 179, 202, 220, 312, 314], "fifo": [12, 13, 14, 275, 282], "fifoschedul": [7, 14, 229, 233], "fig": [7, 13, 14, 229, 231, 305, 307, 311, 338, 340], "figsiz": [5, 7, 13, 14, 202, 204, 215, 229, 231, 305, 307, 309, 311, 312, 316, 318, 320, 322, 325, 327, 331, 333, 335, 337, 338, 340, 345], "figur": [5, 9, 15, 176, 179, 202, 204, 215, 305, 309, 312, 316, 318, 320, 322, 331, 333, 335, 337, 338, 345], "file": [0, 1, 4, 5, 6, 7, 8, 11, 12, 13, 17, 22, 24, 26, 28, 30, 34, 35, 38, 39, 43, 45, 46, 53, 56, 58, 66, 69, 70, 81, 82, 84, 85, 86, 91, 93, 94, 96, 97, 98, 125, 128, 137, 141, 142, 145, 147, 150, 151, 152, 169, 170, 179, 181, 183, 187, 188, 191, 195, 196, 201, 202, 204, 212, 219, 224, 227, 228, 229, 234, 235, 238, 240, 244, 260, 265, 268, 276, 300, 305, 307, 311, 312, 313, 317, 318, 319, 320, 324, 331, 333, 338, 339, 340, 341, 343, 347], "file_nam": [125, 128], "filenam": [81, 93, 202, 204, 305, 309, 312, 316], "filenotfounderror": [9, 176, 181, 305, 311, 338, 347], "filestor": [24, 26, 35, 36, 39, 66, 70], "filestore_capacity_gb": [35, 39], "filestore_instance_nam": [35, 39, 66, 70], "filestore_loc": [35, 39, 66, 70], "filestore_ti": [35, 39], "filesystem": [6, 13, 14, 142, 144, 202, 204, 235, 238, 275, 282, 331, 332], "fill": [244, 265, 267, 274], "film": [244, 265, 267, 270, 273, 274], "filmbr": [267, 274], "filmmak": [267, 270], "filter": [9, 28, 30, 35, 38, 43, 45, 53, 56, 66, 69, 71, 82, 84, 94, 96, 142, 145, 176, 183, 202, 205, 269, 273, 274, 318, 319, 321, 324], "filterwarn": [305, 309, 312, 316], "final": [2, 3, 7, 10, 14, 15, 66, 69, 84, 91, 96, 125, 130, 153, 158, 159, 166, 184, 194, 202, 205, 210, 214, 215, 221, 225, 226, 227, 228, 229, 233, 267, 273, 274, 275, 282, 305, 308, 312, 317, 318, 320, 324, 325, 328, 330, 331, 335, 337, 338, 343, 344, 346, 347], "find": [3, 4, 5, 6, 10, 17, 22, 28, 30, 35, 42, 43, 45, 53, 56, 57, 63, 66, 69, 78, 83, 84, 85, 86, 87, 91, 95, 96, 97, 98, 99, 142, 144, 147, 150, 159, 164, 184, 190, 202, 203, 228, 235, 237, 267, 270, 273, 274], "fine": [7, 24, 26, 85, 89, 97, 103, 125, 127, 128, 202, 228, 229, 232, 267, 273, 305, 309, 311, 325, 330, 338, 339, 343], "finer": [3, 159, 165], "finest": [244, 263, 265], "finetun": [5, 6, 7, 13, 14, 229, 233, 235, 240, 289, 290, 291, 292, 303], "finish": [3, 108, 109, 112, 159, 167, 202, 214, 312, 317, 325, 328, 338, 346], "fiorentina": [244, 265], "fiorina": [244, 265], "fir": [244, 265, 325, 326], "fire": [244, 265], "firewal": [17, 22, 35, 39, 42, 66, 70], "firewall_policy_nam": [35, 39, 66, 70], "first": [1, 2, 3, 4, 6, 7, 11, 12, 13, 14, 16, 28, 31, 35, 38, 40, 43, 47, 53, 57, 59, 66, 69, 75, 83, 84, 95, 96, 108, 109, 112, 113, 117, 120, 125, 128, 137, 141, 147, 149, 151, 152, 153, 155, 156, 158, 159, 162, 164, 196, 198, 200, 202, 205, 215, 229, 233, 235, 239, 244, 248, 249, 250, 263, 265, 267, 270, 273, 274, 305, 307, 309, 311, 318, 320, 324, 331, 333, 338, 343], "fit": [4, 5, 7, 10, 12, 13, 14, 15, 108, 109, 112, 147, 150, 184, 193, 214, 219, 221, 225, 226, 229, 232, 233, 238, 267, 269, 275, 281, 305, 309, 310, 312, 316, 318, 322, 323, 325, 326, 328, 330, 331, 335, 336, 337, 338, 339, 344, 346], "fit_model": [7, 229, 232], "five": [267, 273, 274, 305, 309, 312, 316, 338, 344], "fix": [202, 206, 275, 280, 305, 306, 318, 320, 331, 333], "flag": [84, 96], "flap": [267, 270], "flashi": [267, 274], "flatten": [305, 307, 338, 340], "flavor": [82, 94], "flawless": [244, 265], "fleet": [267, 270, 274, 331, 332], "flew": [244, 265], "flexibl": [3, 8, 9, 11, 15, 16, 88, 102, 108, 109, 114, 125, 126, 127, 159, 166, 169, 170, 172, 173, 176, 179, 196, 199, 243, 244, 261, 267, 269, 275, 281], "flexibli": 16, "flink": [8, 169, 173], "flip": [338, 339], "flip_sin_to_co": [6, 235, 238], "flippen": [267, 273], "float": [3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 147, 150, 159, 163, 176, 179, 184, 191, 196, 200, 202, 212, 224, 229, 232, 233, 235, 238, 244, 266, 305, 308, 312, 315, 318, 322, 325, 329, 331, 335], "float16": [6, 235, 238], "float32": [11, 12, 196, 200, 244, 265, 266, 305, 307, 312, 314, 317, 318, 322, 331, 333, 334, 337], "floral": [28, 30, 43, 45, 53, 56], "flore": [244, 265], "flow": [17, 22, 88, 102, 125, 130, 133, 135, 173, 312, 313], "flush": [3, 108, 109, 115, 117, 121, 122, 125, 128, 159, 163], "fly": [202, 216], "fmt": [325, 329], "fn_arg": [125, 130], "fn_call": [125, 130], "fn_callabl": [125, 130], "fn_constructor_arg": [325, 329, 330, 331, 337, 338, 347], "fn_constructor_kwarg": [10, 15, 16, 184, 191], "fn_kwarg": 15, "fname": [331, 333], "foam": [244, 265], "focu": [91, 125, 127, 137, 140, 202, 216, 267, 270, 331, 332], "focus": [0, 8, 24, 26, 108, 109, 111, 133, 136, 169, 170, 172, 244, 265, 331, 337], "folder": [4, 5, 9, 10, 11, 81, 83, 85, 93, 95, 97, 125, 128, 146, 147, 150, 176, 181, 184, 191, 196, 200, 260, 268, 276, 300, 325, 330, 331, 337, 338, 347], "follow": [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 22, 24, 26, 28, 29, 35, 37, 42, 43, 44, 46, 51, 53, 55, 58, 63, 64, 66, 68, 71, 78, 79, 80, 81, 82, 83, 84, 87, 91, 92, 93, 94, 95, 96, 99, 105, 108, 109, 115, 117, 119, 125, 128, 131, 133, 135, 136, 137, 141, 142, 145, 146, 151, 152, 153, 158, 159, 161, 169, 172, 175, 176, 179, 184, 186, 191, 196, 198, 202, 203, 205, 216, 217, 229, 233, 235, 237, 238, 239, 267, 274, 275, 281, 312, 317, 318, 319, 324, 325, 326, 331, 332, 337, 338, 339, 341], "followup": [267, 274], "fontsiz": [305, 307, 338, 340], "food": [306, 311, 347], "food101": [305, 307, 338, 339, 340, 347], "food101_diffusion_ft": [305, 309], "food101_diffusion_result": [305, 309], "food101_ft_resum": [338, 344, 347], "food101_ft_run": [338, 347], "food101_lit": [305, 307, 338, 340, 341, 343, 344, 347], "food101_single_run": [338, 347], "food101dataset": [339, 342, 347], "footag": [244, 265], "footbal": [244, 263, 265], "footer": 0, "forbidden": [13, 14], "forc": [202, 215, 332, 334, 337, 338, 347], "forcibli": [267, 273], "ford": [267, 270], "forecast": 337, "foreground": [267, 274], "foregroundbr": [267, 274], "forest": [327, 329], "forg": [1, 151, 152], "forget": [244, 265, 267, 274], "forgotten": [267, 274], "forgottenbr": [267, 274], "fork": 0, "form": [3, 12, 159, 161, 267, 274, 318, 320], "format": [7, 10, 14, 91, 125, 127, 128, 129, 130, 132, 146, 184, 188, 190, 202, 216, 219, 229, 231, 275, 280, 305, 307, 318, 320, 331, 333, 338, 339, 340], "fort": [267, 273], "forum": [117, 124, 125, 132], "forward": [6, 13, 202, 203, 206, 217, 223, 235, 238, 244, 265, 275, 280, 308, 309, 312, 313, 315, 316, 318, 319, 321, 325, 327, 331, 334, 337], "found": [7, 14, 17, 22, 202, 208, 223, 226, 227, 229, 233, 267, 274, 305, 311, 312, 317, 318, 319, 325, 328, 338, 347], "foundat": [7, 8, 14, 116, 125, 132, 169, 170, 229, 232, 318, 324], "four": [5, 6, 108, 109, 112, 202, 205, 235, 239, 318, 320], "fourth": [244, 265, 267, 273, 274], "fox": [244, 265], "foxx": [244, 265], "fp16": [108, 109, 113, 117, 119, 123], "fp8": [117, 123], "frac": [338, 341], "fraction": [11, 160, 168, 196, 198, 291, 292, 303], "fragrant": [244, 265], "frame": [325, 327, 328], "framework": [4, 5, 6, 9, 11, 12, 15, 16, 91, 132, 147, 149, 150, 170, 175, 176, 178, 196, 197, 199, 202, 203, 235, 237, 267, 269, 285, 292, 301], "franc": [108, 109, 115, 125, 128], "francisco": [125, 130], "frank": [244, 265], "fraud": [8, 169, 173], "freak": [267, 273], "free": [3, 84, 85, 86, 90, 91, 96, 97, 98, 107, 117, 123, 159, 164, 202, 215, 243, 244, 261, 265, 275, 277, 325, 330, 331, 337, 338, 347], "freed": [202, 215], "freeli": [9, 15, 176, 179], "freq_shift": [6, 235, 238], "frequenc": [6, 235, 238, 318, 320, 325, 327], "frequent": [8, 169, 170, 173], "fresh": [202, 215], "fri": [338, 339], "friction": [2, 153, 155], "fridai": [244, 265], "friend": [244, 265], "friendli": [85, 89, 97, 103, 117, 119, 137, 141, 305, 307, 318, 324, 338, 339, 340], "frighten": [267, 273, 274], "frill": [267, 274], "from": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 24, 27, 28, 29, 30, 32, 35, 36, 39, 41, 42, 43, 44, 45, 47, 48, 50, 53, 54, 57, 59, 60, 62, 63, 66, 67, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 93, 94, 95, 96, 97, 98, 99, 102, 105, 107, 108, 109, 111, 112, 113, 115, 117, 118, 120, 121, 122, 123, 125, 127, 128, 129, 130, 132, 133, 136, 137, 139, 141, 142, 144, 145, 147, 148, 149, 150, 151, 152, 159, 161, 164, 166, 168, 169, 173, 176, 178, 180, 181, 183, 184, 185, 186, 188, 190, 191, 196, 197, 198, 201, 203, 204, 205, 206, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 227, 228, 229, 230, 233, 235, 236, 237, 239, 243, 244, 247, 250, 259, 261, 262, 263, 265, 266, 267, 269, 270, 273, 274, 275, 278, 279, 280, 288, 292, 299, 302, 304, 306, 307, 309, 313, 314, 316, 319, 322, 324, 326, 327, 328, 332, 333, 335, 337, 338, 339, 340, 341, 343, 345, 346, 347], "from_directori": [5, 13, 202, 212, 224, 318, 322, 331, 335, 337, 338, 343, 347], "from_huggingfac": [244, 250, 263], "from_item": [16, 137, 141, 267, 271, 305, 307, 312, 314, 331, 337], "from_numpi": [331, 337, 338, 347], "from_pretrain": [6, 235, 238, 275, 280], "from_pydict": [338, 340], "from_pylist": [331, 333], "from_torch": [10, 184, 188], "fromarrai": [202, 220], "front": [325, 327, 338, 340], "frontal": [267, 270], "fr\u00e9chet": [305, 311], "fsdp": [5, 202, 204, 209, 228], "ft": [244, 263, 265], "fuck": [244, 265], "full": [4, 6, 9, 10, 15, 17, 20, 22, 24, 26, 53, 63, 85, 97, 103, 125, 128, 137, 141, 142, 145, 147, 150, 176, 179, 184, 186, 192, 207, 214, 222, 228, 235, 238, 267, 273, 274, 305, 306, 312, 313, 318, 320, 322, 324, 325, 328, 331, 335, 338, 340, 341, 345, 347], "full_path": [338, 341], "fulli": [3, 5, 6, 16, 17, 18, 80, 88, 91, 92, 101, 159, 161, 202, 203, 205, 216, 221, 235, 237, 305, 306, 309, 312, 313, 318, 319, 320, 322, 325, 326, 328, 338, 339, 343], "fullyshardeddataparallel": [5, 202, 209], "function": [3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 53, 63, 83, 87, 95, 99, 117, 120, 125, 127, 130, 132, 147, 150, 154, 158, 159, 163, 168, 170, 174, 176, 179, 180, 181, 182, 184, 188, 190, 193, 194, 196, 199, 202, 205, 206, 209, 211, 212, 220, 229, 233, 235, 236, 238, 239, 244, 256, 265, 267, 272, 274, 277, 279, 282, 291, 292, 303, 305, 307, 312, 313, 316, 317, 318, 319, 320, 322, 325, 326, 330, 331, 332, 338, 339, 343], "fundament": [3, 11, 16, 24, 26, 79, 108, 109, 110, 111, 115, 142, 143, 159, 160, 196, 199], "further": [10, 82, 94, 184, 188, 191, 202, 214, 267, 274, 291, 292, 303, 338, 341], "fuse": [10, 184, 190], "futur": [2, 9, 142, 145, 153, 157, 176, 182, 267, 270, 273, 274, 312, 313, 331, 332, 333, 334, 335], "future_tru": [331, 337], "g": [0, 2, 3, 7, 8, 9, 10, 11, 12, 14, 15, 17, 22, 24, 26, 27, 28, 29, 35, 36, 42, 43, 44, 53, 54, 66, 67, 78, 81, 83, 84, 86, 87, 90, 93, 95, 96, 98, 99, 106, 107, 117, 123, 133, 135, 137, 139, 142, 144, 153, 155, 159, 162, 163, 165, 169, 170, 173, 176, 181, 184, 186, 188, 190, 196, 198, 199, 202, 206, 207, 212, 213, 214, 216, 218, 222, 225, 228, 229, 233, 243, 244, 261, 263, 265, 318, 322, 325, 328, 331, 337], "g54aiirwj1": [83, 95], "g54aiirwj1s8t9ktgzikqur41k": [83, 95], "gain": [318, 324, 325, 329], "galaxi": [244, 265], "galleri": [244, 265], "gallo": [267, 270], "gambl": [267, 273], "game": [244, 265], "gan": [305, 306], "gandhi": [244, 265], "gang": [267, 273], "gannon": [267, 273], "gap": [8, 16, 125, 128, 169, 170, 331, 333], "gape": [267, 270], "garbag": [2, 153, 155, 202, 204, 215], "garden": [244, 265], "gate": [117, 120, 121, 125, 129, 130], "gatewai": [17, 22, 28, 30, 43, 45, 53, 56], "gather": [4, 84, 96, 147, 150, 338, 340], "gaussian": [305, 306, 311, 312, 314, 331, 337], "gb": [3, 83, 95, 117, 119, 123, 159, 161], "gc": [9, 24, 26, 35, 36, 42, 66, 78, 125, 128, 176, 181, 202, 204, 212, 215, 228, 318, 324, 338, 347], "gca": [325, 329], "gce": [17, 18, 24, 27, 40, 66, 69, 348], "gcloud": [35, 37, 38, 66, 68, 69, 71, 72, 78], "gcp": [17, 18, 22, 24, 26, 27, 37, 38, 39, 41, 42, 66, 68, 69, 70, 74, 79, 87, 88, 99, 101], "gcp_if_": [35, 39], "gcp_project_id": [35, 38, 39, 66, 69, 70, 72, 78], "gcp_region": [35, 38, 39, 66, 69, 70, 71, 72, 76, 78], "gcs_bucket_nam": [35, 39, 42, 66, 70, 78], "gear": [244, 265], "gee": [244, 265], "gener": [1, 2, 9, 10, 11, 13, 15, 16, 35, 39, 83, 84, 85, 89, 95, 96, 97, 103, 112, 116, 125, 127, 128, 129, 130, 131, 132, 137, 141, 142, 144, 145, 146, 151, 152, 153, 155, 176, 180, 182, 184, 189, 191, 193, 196, 200, 202, 204, 215, 228, 238, 243, 244, 253, 260, 261, 264, 267, 268, 270, 276, 300, 309, 313, 317, 318, 319, 324, 331, 332, 337], "generate_synthetic_imag": [137, 141], "generated_bi": [137, 141], "generative_cv": [305, 309, 311], "genit": [267, 270], "geniu": [267, 274], "genr": [318, 324], "geo": [325, 326], "german": [267, 274], "germani": [244, 265, 267, 274], "get": [1, 4, 5, 6, 8, 11, 12, 16, 17, 22, 28, 29, 30, 31, 35, 39, 40, 43, 44, 45, 47, 49, 50, 51, 53, 55, 57, 59, 61, 62, 63, 64, 66, 72, 73, 75, 81, 83, 84, 85, 89, 93, 95, 96, 97, 104, 110, 116, 117, 120, 122, 123, 126, 128, 130, 137, 140, 142, 144, 145, 147, 148, 150, 151, 152, 160, 161, 162, 163, 164, 165, 166, 168, 169, 173, 196, 200, 202, 205, 206, 215, 232, 235, 239, 244, 265, 267, 273, 274, 275, 280, 291, 292, 296, 303, 304, 305, 306, 309, 311, 312, 316, 317, 318, 320, 322, 325, 328, 331, 333, 335, 337, 338, 340, 343, 347], "get_best_result": [4, 7, 12, 14, 147, 150, 229, 233], "get_checkpoint": [202, 222, 223, 228, 305, 307, 309, 312, 314, 316, 318, 320, 322, 325, 327, 328, 330, 331, 333, 335, 338, 339, 340, 343], "get_config_dict": [6, 235, 238], "get_context": [4, 5, 13, 147, 150, 202, 206, 211, 212, 217, 223, 224, 305, 307, 309, 312, 314, 316, 318, 320, 322, 325, 327, 328, 331, 333, 335, 338, 340, 343], "get_current_temperatur": [125, 130], "get_dataset_shard": [6, 202, 218, 235, 239, 305, 309, 312, 316, 318, 320, 322, 325, 327, 328], "get_devic": [6, 235, 236, 238, 275, 280], "get_linear_schedule_with_warmup": [6, 235, 236, 238], "get_model": [325, 328, 329], "get_scor": [325, 329, 330], "get_temperature_d": [125, 130], "get_us": 146, "get_user_profil": 146, "get_world_rank": [4, 5, 13, 147, 150, 202, 211, 212, 224, 305, 309, 312, 316, 318, 322, 325, 328, 331, 335, 338, 343], "get_world_s": [5, 13, 202, 206, 217, 223], "getbucketloc": [17, 22], "getenv": [82, 83, 94, 95], "getlogg": [142, 145], "getobject": [17, 22], "getsizeof": [3, 159, 161], "gettempdir": [305, 309, 312, 316], "gettingstart": 91, "getvalu": [305, 307, 338, 340], "gh": 0, "ghetto": [244, 265], "giant": [267, 274], "gib": [12, 14], "gift": [244, 265], "girl": [244, 265], "git": [0, 86, 87, 90, 91, 98, 99, 107], "github": [0, 1, 17, 22, 43, 46, 48, 53, 58, 60, 66, 73, 76, 86, 91, 98, 151, 152, 331, 333], "githubusercont": [331, 333], "give": [7, 14, 53, 57, 80, 82, 86, 87, 92, 94, 98, 99, 108, 109, 114, 202, 205, 224, 229, 232, 267, 273, 318, 320, 324, 325, 327], "given": [2, 4, 5, 7, 8, 10, 13, 14, 15, 87, 99, 125, 130, 142, 145, 147, 150, 153, 156, 169, 173, 174, 175, 184, 193, 229, 233, 289, 290, 291, 292, 303, 305, 308, 312, 313, 315, 325, 326, 331, 332], "gke": [17, 18, 20, 24, 27, 69, 70, 72, 75, 348], "glanc": [80, 92], "glass": [244, 265], "glob": [305, 311, 312, 314, 317], "global": [28, 30, 35, 39, 43, 45, 53, 56, 66, 69, 70, 202, 206, 305, 308, 318, 320, 338, 341, 343], "global_batch_s": [5, 13, 202, 206, 207, 213, 217, 221, 223, 224, 226, 275, 281], "gloriou": [267, 273], "gloss": [267, 274], "gm": [244, 263, 265], "go": [3, 17, 22, 80, 92, 108, 109, 114, 117, 123, 125, 128, 159, 162, 243, 244, 261, 265, 267, 273, 274, 338, 340], "goal": [4, 90, 91, 106, 147, 150, 244, 265, 275, 277, 312, 313, 338, 339], "goaldotcom": [244, 265], "god": [244, 265], "goe": [84, 87, 96, 99, 244, 263, 265, 267, 270], "gold": [267, 273], "golions2012": [244, 265], "gone": [267, 274], "gonna": [244, 265], "good": [7, 8, 14, 28, 30, 43, 45, 53, 56, 117, 119, 137, 141, 169, 175, 202, 204, 215, 219, 229, 232, 244, 265, 267, 270, 273, 274, 325, 329, 331, 333], "googl": [8, 17, 19, 20, 24, 27, 37, 39, 68, 169, 170, 244, 265], "google_cloud": [66, 69], "google_project_id": [35, 39, 66, 70, 78], "google_region": [35, 39, 66, 70, 78], "googleapi": [35, 38, 66, 69], "gore": [267, 274], "got": [3, 159, 167, 244, 265, 267, 274], "gotrib": [244, 265], "gotten": [267, 274], "gpu": [1, 3, 4, 7, 8, 9, 10, 11, 12, 14, 15, 24, 27, 28, 34, 43, 46, 53, 58, 80, 82, 92, 94, 108, 109, 110, 111, 112, 113, 114, 117, 118, 119, 120, 121, 123, 124, 125, 126, 131, 132, 133, 135, 137, 139, 147, 148, 150, 151, 152, 159, 165, 168, 169, 174, 176, 183, 184, 186, 190, 191, 195, 196, 198, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 215, 216, 221, 228, 229, 230, 232, 233, 236, 237, 243, 244, 259, 261, 265, 266, 275, 277, 280, 281, 282, 291, 292, 303, 305, 306, 309, 311, 312, 313, 316, 317, 318, 319, 322, 324, 325, 328, 332, 338, 339, 342, 344, 347], "grab": [305, 307, 338, 347], "gracefulli": [86, 90, 98, 106, 292, 296, 304], "grad": [202, 206], "grade": [108, 109, 110, 114, 117, 124, 125, 132, 305, 311], "grader": [244, 265], "gradient": [4, 5, 6, 12, 13, 147, 150, 202, 203, 206, 209, 212, 235, 239, 275, 280, 325, 326, 327, 331, 333], "gradual": [90, 106, 137, 141, 305, 306, 325, 326], "grafana": [84, 86, 90, 96, 98, 106, 117, 123, 135, 142, 144, 145], "grai": [5, 7, 10, 13, 14, 16, 184, 189, 202, 204, 215, 229, 231], "grain": [3, 11, 24, 26, 159, 165, 196, 199], "grand": [244, 265], "grant": [17, 22, 24, 26, 267, 270], "granular": [87, 99, 142, 145], "granularli": 16, "graph": [2, 3, 133, 136, 137, 141, 153, 155, 159, 162], "grass": [267, 274], "grayscal": [5, 7, 13, 14, 202, 204, 205, 215, 229, 231], "great": [87, 99, 117, 122, 244, 265, 267, 274, 292, 293, 295, 296, 304], "greater": [12, 83, 84, 95, 96], "greec": [244, 265], "green": [305, 306, 338, 339], "grei": [244, 265], "grep": [28, 30, 43, 45, 49, 50, 51, 53, 56, 61, 62, 64], "grid": [7, 14, 202, 204, 215, 229, 233, 305, 309, 312, 316, 317, 318, 322, 331, 333, 335, 337, 338, 345], "grim": [267, 273, 274], "grimnoir": [267, 274], "ground": [10, 15, 184, 190, 193, 202, 204, 312, 314, 331, 332, 335, 337, 338, 347], "ground_truth_label": [10, 15, 184, 193], "group": [14, 21, 23, 24, 26, 28, 30, 34, 43, 45, 53, 56, 57, 63, 79, 80, 84, 87, 88, 92, 96, 99, 101, 177, 185, 202, 203, 213, 244, 265, 305, 306, 318, 324, 331, 332, 338, 339, 340, 341, 345], "groupbi": [8, 169, 174, 183, 318, 320, 322, 331, 335, 338, 345], "grouplen": [318, 320], "grow": [2, 4, 12, 91, 147, 149, 153, 155, 267, 270], "grpc": [8, 11, 16, 169, 175, 196, 198, 200], "gserviceaccount": [35, 39, 66, 70], "gsutil": [35, 42, 66, 78], "gt": [267, 274], "guarante": [8, 125, 129, 169, 174, 202, 212], "guard": [318, 322, 338, 345], "gucci": [244, 265], "gui": [244, 265], "guid": [1, 5, 6, 10, 13, 14, 24, 26, 28, 29, 43, 44, 53, 54, 66, 67, 79, 86, 91, 98, 107, 117, 124, 125, 127, 128, 129, 130, 132, 133, 134, 136, 142, 143, 145, 151, 152, 184, 188, 202, 212, 235, 239, 244, 265, 275, 282, 291, 292, 303], "gunman": [267, 273], "gym": [312, 313, 314, 317], "gymnasium": [312, 313, 314], "gz": [13, 14], "h": [5, 11, 13, 146, 196, 200, 202, 215, 305, 306, 307, 308, 331, 333, 338, 347], "ha": [1, 3, 5, 6, 8, 16, 17, 20, 35, 42, 53, 63, 66, 78, 80, 82, 83, 84, 87, 88, 90, 92, 94, 95, 96, 99, 101, 106, 117, 119, 125, 128, 132, 137, 141, 142, 145, 151, 152, 159, 161, 169, 173, 174, 175, 202, 203, 205, 219, 220, 235, 237, 244, 248, 249, 250, 256, 259, 263, 265, 266, 267, 270, 273, 274, 275, 280, 292, 293, 295, 296, 304, 318, 320], "had": [9, 176, 182, 244, 265, 267, 270], "hadoop": [8, 169, 173], "hahha": [244, 265], "hail": [267, 274, 331, 332], "half": [244, 265, 267, 273, 274, 331, 332, 333], "halfstarv": [267, 274], "halloween": [244, 263, 265], "halv": [7, 14, 229, 233], "ham": [244, 265], "hamburg": [338, 339], "hand": [1, 79, 91, 108, 109, 116, 125, 127, 151, 152, 244, 265, 267, 273, 318, 320, 338, 339], "handheld": [267, 274], "handl": [3, 4, 8, 10, 11, 12, 16, 17, 22, 24, 25, 26, 66, 73, 80, 85, 90, 91, 92, 97, 103, 106, 108, 109, 111, 114, 117, 120, 122, 125, 129, 133, 135, 137, 141, 146, 147, 150, 159, 163, 165, 168, 169, 170, 173, 174, 175, 184, 186, 196, 198, 199, 202, 203, 204, 205, 206, 208, 210, 215, 216, 217, 221, 222, 224, 227, 244, 259, 266, 267, 269, 275, 277, 280, 285, 292, 301, 305, 306, 309, 311, 312, 313, 314, 318, 319, 320, 322, 324, 325, 327, 328, 331, 332, 338, 339, 340, 342, 343, 345], "handwritten": [7, 11, 14, 196, 200, 202, 204, 229, 231, 232], "hang": [133, 135, 267, 273, 274, 312, 313], "happen": [2, 7, 8, 14, 137, 141, 153, 157, 169, 175, 202, 224, 229, 233, 267, 273, 274], "happi": [244, 265], "happybirthdayremuslupin": [244, 263, 265, 266], "hard": [267, 273, 274], "hardli": [267, 270], "hardwar": [5, 6, 10, 11, 82, 84, 91, 94, 96, 108, 109, 113, 114, 119, 120, 121, 132, 184, 186, 190, 191, 196, 198, 199, 202, 203, 216, 224, 235, 237, 275, 277, 280, 281, 282, 325, 326], "hark": [267, 273], "harm": 154, "harri": [244, 265], "hasattr": [338, 343], "hasek": [244, 265], "hash": [9, 176, 182], "hashicorp": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68], "hashtag": [244, 265], "hat": [318, 319, 338, 339], "hater": [244, 265], "have": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 28, 29, 30, 34, 35, 37, 43, 44, 45, 46, 52, 53, 55, 56, 58, 65, 66, 68, 81, 82, 83, 87, 91, 93, 94, 95, 99, 108, 109, 112, 117, 124, 137, 141, 142, 143, 147, 148, 151, 152, 159, 163, 166, 167, 169, 172, 173, 176, 181, 184, 186, 190, 191, 193, 196, 198, 200, 202, 212, 221, 229, 233, 235, 239, 244, 256, 265, 267, 270, 271, 272, 273, 274, 275, 280, 285, 292, 301, 318, 319, 325, 327, 328, 338, 339, 347], "hdf": [8, 9, 169, 173, 176, 181], "he": [244, 265, 267, 273, 274], "head": [3, 9, 14, 17, 21, 22, 24, 26, 43, 50, 53, 62, 80, 82, 83, 84, 85, 86, 87, 92, 94, 95, 96, 97, 98, 99, 106, 137, 140, 141, 159, 161, 165, 176, 179, 181, 267, 274, 305, 309, 312, 316, 325, 327, 331, 333, 337], "head_nod": [43, 50, 53, 62], "header": [0, 5, 13, 318, 324], "headlei": [244, 263, 265], "headless": [24, 26], "headnodeconfig": [43, 50, 53, 62], "health": [24, 25, 90, 106, 108, 109, 113, 146, 305, 309], "healthi": [82, 86, 94, 98, 331, 335], "heap": [3, 159, 167], "hear": [244, 265], "heard": [267, 270], "heart": [267, 273, 331, 335], "heat": [292, 293, 295, 296, 304], "heatmap": [325, 329], "heaven": [267, 273], "heavi": [5, 9, 10, 15, 80, 92, 176, 182, 184, 193, 202, 216], "heavili": [267, 274], "heavli": [202, 210], "hebdo": [244, 265], "hei": [17, 22, 244, 265], "height": [10, 15, 16, 137, 141, 184, 190, 305, 307, 325, 330], "held": [244, 265], "hell": [244, 265], "hello": [81, 83, 85, 89, 93, 95, 97, 104, 105, 108, 142, 145, 244, 265], "hello_world": [1, 81, 85, 93, 97, 151, 152], "helm": [24, 25, 43, 44, 45, 46, 48, 51, 53, 55, 56, 58, 60, 64, 66, 68, 73, 76, 78], "helm_upgrade_command": [43, 45, 53, 56], "help": [1, 7, 8, 10, 14, 16, 17, 20, 24, 26, 81, 93, 108, 109, 112, 133, 135, 142, 144, 151, 152, 169, 173, 184, 190, 202, 204, 229, 232, 267, 274, 325, 327, 329, 331, 332, 333], "helper": [125, 130, 202, 204, 209, 210, 211, 212, 318, 320, 325, 327, 333, 339, 347], "helper_tool_map": [125, 130], "her": [244, 265, 267, 270, 273, 274], "here": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 28, 30, 43, 45, 53, 63, 91, 108, 109, 110, 111, 114, 117, 118, 124, 125, 126, 128, 129, 131, 132, 137, 141, 142, 143, 144, 145, 147, 148, 150, 153, 154, 157, 158, 159, 160, 168, 169, 170, 171, 172, 173, 175, 176, 177, 180, 181, 182, 183, 184, 185, 188, 190, 193, 196, 197, 199, 200, 201, 202, 204, 206, 207, 214, 215, 217, 224, 228, 229, 230, 232, 233, 234, 235, 236, 238, 239, 240, 243, 244, 261, 265, 267, 269, 271, 273, 274, 275, 277, 285, 291, 292, 301, 303, 305, 306, 307, 312, 313, 318, 319], "herm": [125, 130], "hero": [244, 265, 267, 274], "herself": [267, 274], "heteregen": [10, 184, 186], "heterogen": [4, 8, 9, 10, 15, 147, 150, 169, 174, 176, 183, 184, 186, 195], "hf": [117, 120], "hf_d": [305, 307], "hf_dataset": [244, 250, 263], "hf_token": [117, 120, 121, 122, 123, 125, 128, 129, 130], "hi": [244, 263, 265, 267, 270, 273, 274], "hidden": [0, 267, 274, 312, 317], "hide": [267, 273], "hierarch": [142, 145], "hierarchi": [88, 102], "high": [3, 5, 6, 7, 8, 10, 11, 13, 14, 16, 17, 22, 86, 90, 98, 106, 108, 109, 111, 112, 113, 114, 125, 131, 159, 165, 169, 170, 173, 174, 184, 190, 196, 198, 199, 202, 205, 213, 229, 232, 235, 239, 244, 259, 265, 266, 267, 274, 285, 292, 299, 301, 304, 312, 317, 318, 319, 320, 322, 338, 339], "higher": [8, 15, 83, 95, 108, 109, 112, 117, 123, 124, 125, 131, 169, 174, 202, 216, 318, 319, 320], "highest": [267, 273, 305, 309, 318, 324], "highli": [17, 22, 24, 26, 137, 141, 325, 327], "highlight": [3, 6, 8, 159, 161, 169, 173, 202, 212, 235, 239, 267, 274, 318, 320, 325, 329], "hike": [244, 265], "him": [244, 265, 267, 273, 274], "himself": [244, 265, 267, 273], "hindu": [244, 265], "hint": [2, 3, 5, 6, 7, 10, 13, 14, 153, 158, 159, 168, 184, 190, 229, 233, 235, 239, 275, 278], "hire": [267, 273], "hist": [4, 147, 150, 318, 320, 325, 328], "histor": [8, 169, 170, 173, 318, 319, 331, 332], "histori": [125, 130, 202, 214, 318, 322, 331, 333, 335, 337, 338, 345, 347], "hit": [28, 30, 43, 45, 53, 56, 137, 141, 244, 265, 318, 324], "hitchhik": [244, 265], "hiya": [244, 265], "hmu": [244, 265], "hmw": [244, 265], "hoc": [312, 313, 325, 329], "hogan": [244, 265], "hogwart": [244, 263, 265, 266], "hold": [9, 10, 15, 176, 179, 184, 188, 202, 206, 275, 280], "hole": [244, 265], "holidai": [331, 337], "hollywood": [244, 265, 267, 273, 274], "home": [14, 83, 95, 244, 265], "homebrew": [28, 29, 43, 44, 53, 55, 66, 68, 133, 136], "homecom": [244, 265], "homepath": [133, 136], "hong": [267, 273], "hood": [3, 10, 159, 162, 184, 188, 202, 208], "hook": [267, 273, 274, 312, 317], "hop": [244, 265], "hope": [244, 265, 267, 273], "horizon": [331, 333, 334, 335, 337], "horizont": [117, 120, 123], "horribl": [267, 274], "horror": [267, 274], "hospit": [267, 274], "host": [5, 17, 20, 22, 24, 27, 80, 81, 88, 92, 93, 100, 101, 202, 210, 305, 306], "hostnam": 13, "hot": [11, 196, 201, 325, 329, 338, 339], "hotwif": [244, 265], "hour": [12, 108, 109, 113, 137, 141, 244, 265, 305, 307, 331, 332, 333, 337, 338, 340], "hourli": [332, 337], "hous": [267, 273], "housekeep": [202, 204], "hoverboard": [244, 265], "how": [3, 4, 5, 6, 7, 8, 13, 14, 15, 16, 18, 28, 30, 35, 36, 39, 43, 45, 53, 56, 63, 80, 83, 86, 88, 90, 91, 92, 95, 98, 100, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 122, 126, 127, 130, 132, 137, 141, 142, 143, 145, 146, 147, 150, 159, 163, 169, 171, 177, 179, 180, 181, 182, 185, 191, 205, 206, 207, 208, 212, 216, 217, 222, 224, 229, 233, 234, 235, 236, 238, 239, 243, 244, 256, 259, 260, 261, 263, 265, 266, 267, 268, 269, 272, 273, 274, 275, 276, 277, 282, 292, 296, 300, 304, 320, 322, 324, 328, 333, 347], "howev": [3, 8, 9, 10, 15, 24, 26, 159, 165, 169, 173, 176, 181, 182, 184, 193, 202, 212, 267, 273], "html": [0, 1, 14, 28, 29, 43, 44, 53, 55, 89, 90, 103, 106, 108, 109, 115, 151, 152, 244, 250, 263, 275, 282, 285, 291, 292, 301, 303], "http": [0, 1, 4, 8, 11, 12, 13, 14, 16, 17, 22, 24, 27, 28, 29, 35, 37, 43, 44, 46, 48, 53, 55, 58, 60, 63, 66, 68, 73, 76, 84, 86, 89, 90, 91, 96, 98, 103, 106, 107, 108, 109, 115, 117, 121, 122, 125, 128, 129, 130, 133, 136, 142, 145, 146, 147, 150, 151, 152, 169, 175, 196, 200, 244, 250, 263, 275, 282, 285, 291, 292, 296, 301, 303, 304, 318, 320, 331, 333], "huddleston": [244, 265], "hudi": [8, 169, 170], "hug": [117, 120, 121, 125, 128, 130, 243, 244, 259, 261, 263, 266, 267, 270, 278, 279, 282, 288, 292, 299, 302, 304, 338, 339, 340, 347], "huge": [267, 274], "huggingfac": [5, 6, 108, 109, 115, 117, 120, 121, 122, 125, 128, 129, 235, 237, 244, 247, 262], "huggingface_hub": [125, 128], "hugo": [267, 274], "hulk": [244, 265], "human": [9, 176, 179, 202, 212, 244, 265], "humbl": [267, 273], "humor": [267, 273, 274], "hundr": [325, 326], "hunt": [244, 265], "hurt": [244, 265], "hustl": [244, 265], "hvar": [244, 265], "hxwxc": [137, 141], "hybrid": [125, 131, 318, 324], "hydrologi": [325, 326], "hyperband": [7, 14, 229, 233], "hyperparam": [331, 337], "hyperparamet": [5, 6, 8, 12, 13, 148, 169, 173, 202, 206, 207, 213, 221, 224, 226, 230, 232, 234, 235, 239, 305, 311, 312, 317, 318, 324, 325, 328, 330, 331, 337, 338, 347], "hypnot": [267, 274], "i": [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 22, 24, 25, 27, 28, 32, 33, 35, 39, 41, 43, 44, 45, 49, 50, 51, 53, 54, 56, 61, 62, 63, 64, 66, 67, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 103, 106, 110, 112, 113, 114, 118, 121, 125, 126, 127, 128, 129, 130, 131, 133, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 170, 171, 173, 177, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 215, 216, 218, 219, 220, 222, 223, 224, 226, 229, 230, 231, 232, 233, 235, 236, 238, 239, 243, 244, 253, 256, 259, 261, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 277, 280, 281, 282, 291, 296, 299, 303, 304, 305, 306, 307, 309, 311, 312, 313, 317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 331, 332, 333, 334, 335, 336, 338, 339, 340, 341, 343, 344, 345, 347], "iam": [20, 21, 23, 24, 26, 27, 28, 29, 30, 34, 35, 36, 38, 39, 43, 44, 45, 55, 56, 63, 66, 69, 70], "ic": [244, 265], "iceberg": [8, 10, 169, 170, 184, 188], "ichiro": [244, 265], "icon": [81, 93, 125, 129, 267, 274], "id": [23, 28, 30, 35, 38, 39, 43, 45, 47, 48, 53, 56, 59, 60, 66, 69, 70, 75, 76, 80, 84, 92, 96, 125, 128, 130, 137, 141, 142, 145, 146, 154, 202, 213, 267, 271, 272, 273, 274, 275, 280, 319, 338, 340], "idea": [28, 30, 43, 45, 53, 56, 267, 274], "ideal": [4, 8, 81, 93, 117, 119, 124, 147, 150, 169, 170, 173, 267, 274, 325, 327, 331, 337], "ident": [17, 22, 24, 27, 35, 39, 43, 45, 53, 56, 66, 70, 87, 99, 202, 203, 212, 217], "identifi": [28, 30, 43, 45, 53, 56, 117, 120, 125, 128, 137, 141, 202, 211, 212], "idiot": [244, 265, 267, 274], "idl": [28, 30, 43, 45, 53, 56, 108, 109, 112, 113], "idx": [6, 202, 215, 235, 238, 318, 324, 331, 333, 338, 341, 347], "idx1": [13, 14], "idx2item": [318, 324], "idx3": [13, 14], "ig": [244, 265], "ignor": [305, 309, 312, 316], "iid": [318, 320, 324], "ill": [244, 265], "illustr": [10, 24, 26, 91, 184, 188, 244, 265, 305, 311], "iloc": [6, 235, 238, 318, 324, 331, 333, 337, 338, 341], "im": [244, 265, 267, 274], "imag": [3, 5, 6, 7, 8, 10, 13, 14, 15, 16, 24, 26, 80, 83, 86, 90, 92, 95, 98, 107, 108, 109, 115, 117, 122, 137, 141, 142, 145, 159, 164, 169, 170, 184, 188, 189, 190, 191, 197, 204, 205, 206, 210, 215, 217, 218, 219, 223, 229, 231, 232, 233, 235, 238, 239, 267, 274, 308, 311, 347], "image_arr": [202, 220], "image_arrai": [137, 141], "image_batch": 16, "image_byt": [305, 307, 338, 340, 341, 347], "image_bytes_raw": [305, 307], "image_classifi": 16, "image_classifier_ingress": 16, "image_height": [137, 141], "image_id": [10, 137, 141, 184, 190], "image_latents_256": [6, 235, 238], "image_uri": [108, 109, 115, 117, 122, 137, 141], "image_width": [137, 141], "imagebatchpredictor": [338, 347], "imagenet": [202, 205, 338, 340, 341], "imagenet_mean": [338, 341, 347], "imagenet_std": [338, 341, 347], "imageri": [267, 274], "imageserviceingress": 16, "imagin": [267, 273, 274], "imbalanc": [325, 327], "imdb": [267, 269, 270, 271, 273, 274], "img": [5, 10, 184, 189, 202, 204, 215, 305, 307, 311, 338, 340, 341, 347], "immatur": [8, 169, 174], "immedi": [2, 3, 8, 10, 86, 90, 98, 107, 108, 109, 112, 153, 157, 159, 165, 169, 173, 184, 189, 202, 226, 305, 310, 338, 341], "immut": [3, 159, 161], "impact": [8, 83, 95, 108, 109, 112, 113, 125, 131, 169, 174, 267, 274], "implement": [2, 5, 6, 7, 8, 9, 10, 14, 15, 17, 21, 86, 90, 98, 107, 142, 145, 146, 153, 158, 169, 170, 174, 175, 176, 182, 184, 190, 191, 197, 198, 202, 205, 206, 216, 229, 232, 235, 238, 239, 243, 244, 253, 261, 264, 338, 339], "implementaiton": 13, "impli": [267, 270], "implicit": [305, 311], "import": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 81, 82, 83, 84, 85, 86, 89, 90, 93, 94, 95, 96, 97, 98, 104, 105, 107, 108, 109, 115, 117, 120, 121, 122, 123, 125, 128, 129, 130, 137, 140, 141, 142, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 158, 159, 160, 165, 176, 177, 184, 185, 196, 197, 201, 211, 229, 230, 235, 236, 243, 261, 269, 274, 277, 285, 296, 301, 304, 309, 311, 316, 322, 326, 330, 335, 337, 343, 347], "import_path": [86, 90, 98, 107, 108, 109, 115, 117, 122, 125, 129], "importance_typ": [325, 329], "imposs": [267, 274], "impress": [267, 274], "improv": [2, 9, 10, 11, 125, 128, 129, 133, 135, 137, 140, 153, 158, 176, 182, 183, 184, 186, 195, 196, 198, 202, 216, 305, 311, 325, 326, 330, 331, 335, 337], "imshow": [5, 7, 10, 13, 14, 16, 184, 189, 202, 204, 215, 229, 231, 305, 307, 311, 338, 340, 347], "in_channel": [5, 6, 13, 202, 205, 235, 238], "in_count": [84, 96], "in_featur": 13, "in_proj": [331, 334], "inaccess": [137, 141], "incept": [305, 311], "includ": [0, 3, 4, 5, 8, 9, 10, 11, 12, 13, 17, 20, 24, 26, 28, 34, 43, 52, 53, 65, 81, 82, 83, 84, 85, 86, 89, 93, 94, 95, 96, 97, 98, 103, 108, 109, 113, 125, 128, 133, 135, 136, 142, 144, 147, 150, 159, 164, 165, 169, 170, 173, 174, 176, 179, 184, 193, 196, 199, 202, 214, 224, 227, 275, 277, 280, 281, 285, 292, 301, 318, 320, 322, 331, 335, 338, 345], "include_path": [10, 15, 16, 184, 188, 190], "incom": [16, 338, 347], "incomplet": [202, 210], "incorpor": [8, 169, 170, 338, 339], "incorrect_squar": [3, 159, 163], "increas": [108, 109, 112, 117, 123, 137, 141, 202, 228, 292, 299, 304, 305, 311, 318, 322], "increasingli": [1, 151, 152], "increment": [8, 86, 90, 98, 106, 169, 170, 338, 339], "incur": [8, 142, 145, 169, 174], "independ": [9, 10, 11, 83, 95, 117, 123, 176, 183, 184, 188, 196, 199, 202, 208, 216, 220, 331, 332, 335, 338, 341], "index": [0, 4, 8, 89, 90, 103, 106, 142, 144, 147, 150, 169, 170, 202, 204, 215, 285, 292, 301, 318, 320, 325, 327, 328, 331, 333, 338, 341, 347], "index_col": [325, 328], "indi": [267, 270], "indian": [244, 265], "indic": [1, 3, 151, 152, 159, 165, 202, 215, 318, 319, 320, 324, 325, 329, 338, 341], "individu": [2, 4, 12, 137, 140, 147, 149, 153, 158], "indonesiasayshbdforjustinbieb": [244, 265], "industri": 348, "ineffect": [267, 273], "ineffici": [108, 109, 112, 243, 244, 261], "infer": [3, 8, 9, 10, 11, 12, 15, 16, 82, 85, 89, 90, 94, 97, 103, 106, 113, 118, 124, 125, 128, 131, 148, 159, 165, 168, 169, 173, 174, 175, 176, 179, 183, 184, 186, 191, 195, 196, 200, 203, 204, 212, 214, 228, 260, 265, 268, 276, 291, 292, 300, 303, 305, 306, 311, 312, 313, 317, 320, 326, 328, 332, 333, 339], "inference_mod": [202, 215], "inference_row": [338, 347], "inferf": [318, 319], "infinit": [10, 184, 190], "influenc": [267, 274, 312, 313], "info": [1, 12, 14, 117, 123, 142, 145, 151, 152, 244, 265], "inform": [5, 13, 43, 44, 53, 54, 66, 67, 83, 95, 125, 130, 133, 136, 137, 141, 142, 144, 146, 202, 206, 244, 265, 285, 292, 301, 338, 345], "infrastructur": [1, 5, 6, 18, 19, 25, 26, 35, 36, 79, 81, 86, 88, 90, 91, 93, 98, 100, 101, 106, 113, 117, 122, 124, 125, 128, 132, 137, 140, 141, 151, 152, 202, 203, 225, 235, 237, 312, 313, 318, 324, 338, 339], "ingest": [4, 12, 15, 147, 150, 202, 216, 221, 228, 325, 326, 327, 330, 338, 339], "ingmar": [267, 270], "ingress": [4, 16, 24, 26, 27, 51, 52, 64, 65, 78, 147, 150, 289, 290, 291, 292, 303], "ingress_from_cidr_map": [17, 22], "ingress_with_self": [17, 22], "inher": 91, "inherit": [4, 12, 86, 90, 98, 107, 147, 149], "ini": [133, 136], "init": [1, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 84, 96, 151, 152, 202, 204, 244, 250, 263, 267, 270, 275, 281], "initi": [1, 3, 6, 8, 10, 13, 15, 24, 26, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 86, 98, 125, 128, 151, 152, 159, 162, 169, 174, 184, 191, 202, 203, 213, 215, 235, 238, 239, 244, 250, 253, 260, 263, 264, 268, 269, 275, 276, 277, 280, 281, 300, 318, 319, 331, 332], "initial_replica": 16, "inject": [142, 145, 202, 207, 210, 305, 306, 312, 313, 314, 315, 338, 342], "inlin": [291, 292, 303], "inner": [267, 273], "inning": [244, 263, 265], "innings": [244, 263, 265], "innoc": [267, 273, 274], "inplac": [13, 325, 327], "input": [6, 9, 10, 11, 12, 15, 16, 83, 95, 108, 109, 111, 142, 144, 176, 179, 181, 182, 184, 186, 190, 191, 192, 193, 194, 196, 200, 202, 204, 205, 206, 216, 228, 235, 238, 244, 253, 264, 267, 272, 275, 280, 291, 292, 303, 308, 325, 326, 331, 332, 333, 334, 335, 337], "input_window": [331, 333, 334, 335, 337], "inputdatabuff": [10, 184, 192], "inscrut": [8, 169, 173], "insert": [108, 109, 112], "insid": [3, 9, 83, 84, 88, 95, 96, 101, 142, 144, 145, 159, 166, 168, 176, 183, 202, 204, 207, 221, 267, 270, 305, 309, 312, 313, 325, 328, 330, 338, 339, 341, 347], "insight": [8, 137, 139, 141, 169, 173], "inspect": [3, 4, 5, 6, 13, 83, 86, 95, 98, 147, 150, 159, 165, 203, 204, 228, 235, 238, 243, 244, 261, 305, 307, 340], "inspir": [244, 265], "instal": [3, 4, 24, 25, 27, 28, 29, 30, 37, 44, 45, 51, 52, 54, 55, 56, 64, 65, 67, 68, 72, 74, 79, 81, 82, 83, 93, 94, 95, 117, 121, 147, 148, 159, 164, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "instanc": [1, 3, 4, 5, 9, 10, 12, 15, 21, 23, 24, 26, 30, 34, 39, 43, 50, 53, 62, 66, 70, 80, 82, 84, 92, 94, 96, 117, 123, 137, 141, 142, 145, 146, 147, 150, 151, 152, 159, 165, 168, 176, 182, 184, 186, 193, 202, 204, 210, 215, 243, 244, 261, 267, 274, 291, 296, 303], "instance_iam_role_arn": [17, 23], "instance_profile_arn": [28, 30], "instance_typ": [43, 50, 53, 62, 137, 141], "instanceid": [28, 30, 43, 45, 53, 56], "instant": [81, 93], "instanti": [3, 6, 16, 159, 168, 235, 238, 338, 339, 344], "instead": [3, 4, 5, 7, 9, 13, 14, 83, 84, 86, 90, 95, 96, 98, 107, 147, 150, 159, 162, 163, 167, 176, 179, 181, 182, 202, 203, 208, 209, 216, 217, 218, 223, 229, 233, 243, 244, 253, 261, 264, 267, 273, 274, 305, 306, 318, 319, 320, 325, 326, 328, 329, 331, 332, 338, 340, 343], "instruct": [24, 25, 82, 91, 94, 108, 109, 115, 117, 119, 120, 123, 125, 128, 129, 131], "instrument": [146, 244, 265], "insubordin": [267, 273], "int": [3, 4, 5, 6, 7, 10, 13, 14, 15, 137, 141, 147, 150, 159, 161, 163, 165, 184, 190, 193, 202, 210, 211, 215, 218, 224, 229, 231, 232, 233, 235, 238, 305, 307, 308, 312, 314, 315, 318, 321, 325, 329, 330, 331, 333, 335, 338, 340, 342, 343, 347], "int32": [325, 329, 331, 333], "int4": [117, 123], "int64": [244, 263, 266, 267, 271, 272, 312, 314, 318, 320], "intact": [202, 206], "integ": [12, 202, 220, 318, 319, 320, 338, 339], "integr": [4, 5, 6, 8, 9, 12, 13, 17, 21, 24, 27, 79, 80, 86, 90, 92, 98, 106, 108, 109, 114, 117, 123, 125, 127, 129, 130, 132, 133, 135, 136, 137, 141, 142, 145, 147, 149, 169, 170, 173, 174, 175, 176, 183, 203, 210, 217, 218, 221, 235, 237, 243, 244, 261, 267, 269, 275, 281, 285, 292, 301, 306, 312, 313, 316, 317, 318, 319, 325, 330, 331, 333, 338, 339, 347], "intellig": [8, 125, 130, 169, 170], "intend": [83, 86, 95, 98, 267, 270, 275, 281], "intens": [1, 3, 4, 7, 8, 14, 108, 109, 113, 117, 119, 147, 150, 151, 152, 159, 165, 169, 174, 202, 216, 229, 231, 267, 274], "intent": [267, 273, 274, 305, 306, 325, 326], "interact": [8, 16, 24, 26, 81, 88, 91, 93, 101, 108, 109, 113, 114, 125, 130, 132, 169, 173, 267, 274, 318, 319, 320, 325, 330], "interconnect": [117, 123], "interest": [244, 265, 267, 270], "interfac": [4, 5, 8, 133, 136, 147, 149, 169, 172, 202, 204, 216, 285, 292, 301, 318, 322], "intermedi": [3, 5, 10, 13, 108, 109, 111, 159, 162, 184, 186, 202, 212, 325, 330, 331, 337, 338, 347], "intern": [7, 17, 22, 137, 139, 202, 217, 229, 234, 318, 322, 324, 338, 342], "internet": [17, 22], "interoper": [8, 169, 170], "interpret": [318, 324, 325, 330, 331, 337], "interrupt": [82, 94, 202, 224, 226, 331, 336, 338, 339], "interv": [6, 8, 16, 169, 175, 235, 238, 331, 332, 333], "intervent": [202, 224, 318, 323, 331, 336, 338, 339], "interview": [9, 15, 176, 183, 244, 265], "intrigu": [267, 273, 274], "intro": [11, 12, 43, 44, 53, 55, 66, 68, 196, 201, 348], "introduc": [2, 8, 11, 16, 81, 93, 133, 134, 137, 140, 153, 155, 169, 173, 196, 197], "introduct": [15, 16, 216, 275, 277], "introductori": 91, "intuit": [8, 169, 175], "invari": [267, 270], "invent": [267, 273, 274, 331, 332], "invert_yaxi": [325, 329], "invest": [244, 263, 265], "invit": [88, 101], "invoc": [2, 153, 156, 158], "invok": [2, 3, 153, 157, 158, 159, 166, 325, 330], "involv": [9, 10, 15, 108, 109, 115, 176, 182, 184, 193, 244, 265, 267, 273, 274, 285, 292, 301], "io": [0, 1, 10, 14, 43, 44, 46, 48, 53, 55, 58, 60, 66, 68, 73, 76, 151, 152, 160, 184, 187, 202, 204, 244, 250, 263, 275, 282, 285, 291, 292, 301, 303, 305, 307, 331, 333, 338, 340, 341, 347], "iot": [8, 169, 173], "ip": [13, 14, 17, 20, 22, 28, 30, 43, 45, 53, 56, 66, 73, 202, 213, 338, 339], "ipykernel": [1, 151, 152], "ipynb": [81, 83, 91, 93, 95], "iran": [244, 265], "iron": [244, 265], "irrupt": [244, 265], "is_avail": [5, 7, 229, 232, 275, 280, 305, 311, 312, 317, 331, 337, 338, 347], "is_big_tip": 12, "isdir": [202, 227, 338, 347], "isfil": [338, 347], "isinst": [3, 159, 162], "islam": [244, 265], "isn": [5, 13, 53, 63, 108, 109, 113, 202, 210, 244, 265, 267, 270, 305, 311, 338, 339], "isol": [8, 11, 17, 19, 22, 169, 170, 196, 199], "issu": [2, 5, 6, 8, 53, 63, 66, 71, 84, 96, 125, 132, 133, 135, 137, 141, 153, 155, 169, 173, 202, 203, 215, 224, 235, 237, 267, 270, 338, 339], "itali": [244, 265], "itbr": [267, 274], "item": [2, 3, 5, 6, 7, 10, 13, 14, 125, 128, 133, 136, 137, 141, 153, 158, 159, 167, 184, 190, 202, 211, 229, 231, 233, 235, 238, 267, 274, 275, 280, 305, 307, 312, 314, 321, 322, 325, 329, 331, 335, 337, 338, 343, 347], "item2idx": [318, 320, 324], "item_col": [318, 320], "item_embed": [318, 321, 324], "item_id": [318, 320, 324], "item_idx": [318, 319, 320, 321, 322], "item_metadata": [318, 324], "item_vec": [318, 321], "item_vector": [318, 324], "iter": [9, 10, 11, 12, 14, 80, 81, 92, 93, 108, 109, 112, 125, 131, 176, 180, 184, 187, 196, 198, 202, 206, 218, 275, 280, 305, 306, 309, 311, 312, 316, 317, 318, 320, 322, 323, 325, 330, 331, 333], "iter_torch_batch": [202, 216, 218, 228, 305, 309, 312, 316, 318, 319, 322, 324], "iterations_since_restor": 13, "iterrow": [318, 324], "its": [2, 3, 5, 7, 8, 9, 10, 11, 14, 15, 53, 63, 83, 86, 88, 90, 95, 98, 101, 106, 108, 109, 112, 114, 117, 123, 137, 140, 141, 142, 144, 145, 153, 155, 159, 161, 165, 169, 170, 176, 181, 183, 184, 188, 196, 199, 202, 203, 204, 206, 213, 214, 215, 229, 233, 243, 244, 261, 265, 267, 274, 285, 292, 293, 295, 296, 301, 304, 305, 306, 307, 318, 319, 322, 325, 326, 327, 328, 329, 331, 332, 338, 341, 343], "itself": [9, 137, 141, 176, 179], "iv": [267, 274], "j": [0, 2, 153, 158, 244, 265, 318, 320, 324], "jack": [267, 273], "jackson": [244, 265], "jadwal": [244, 265], "jai": [244, 263, 265, 267, 273], "jail": [244, 265], "jame": [244, 265, 267, 273], "jami": [244, 265], "jan": [244, 265], "janet": [244, 265], "januari": [244, 263, 265], "java": [8, 169, 173], "jean": [267, 274], "jeanmarc": [267, 274], "jeff": [267, 273], "jgz99": [117, 122], "jit": [10, 11, 15, 16, 184, 191, 196, 200], "job": [4, 6, 7, 14, 17, 22, 24, 26, 27, 28, 32, 34, 35, 41, 43, 50, 53, 62, 63, 66, 77, 82, 83, 84, 87, 88, 94, 95, 96, 99, 101, 125, 128, 137, 139, 141, 142, 144, 147, 150, 202, 203, 205, 206, 208, 212, 213, 215, 221, 222, 224, 225, 228, 229, 233, 235, 237, 239, 244, 263, 265, 266, 267, 273, 274, 305, 311, 312, 317, 318, 324, 325, 326, 330, 331, 337, 338, 339, 344, 347, 348], "job_config": [85, 89, 97, 105], "job_descript": [125, 128], "job_id": [85, 89, 97, 105], "jobconfig": [85, 89, 97, 105], "joe": [244, 265], "john": [244, 263, 265, 267, 270, 273], "johnson": [244, 265, 267, 270], "joi": [244, 265], "join": [5, 6, 8, 9, 13, 17, 22, 83, 95, 125, 128, 132, 137, 141, 146, 169, 174, 176, 183, 202, 212, 215, 223, 224, 235, 239, 244, 265, 269, 271, 274, 305, 309, 311, 312, 316, 317, 320, 322, 325, 327, 328, 331, 333, 335, 337, 338, 340, 343, 347], "join_typ": [267, 273], "joint": [305, 306, 331, 332, 338, 340], "joke": [117, 121], "journei": [146, 267, 274], "jpeg": [8, 169, 170, 305, 306, 307, 311, 338, 340], "jq": [28, 30, 43, 45, 53, 56], "json": [4, 8, 10, 11, 12, 16, 109, 115, 126, 127, 128, 130, 132, 142, 145, 146, 147, 150, 169, 170, 184, 188, 196, 197, 200, 292, 296, 304, 305, 307, 312, 314, 318, 320, 325, 327, 338, 340, 343], "json_method1": [125, 129], "json_request": [11, 16, 142, 145, 196, 200], "json_schema": [125, 129], "juda": [244, 265], "judg": [267, 273, 325, 327], "juggl": [11, 196, 198], "jule": [244, 265], "jump": [11, 16, 196, 200, 267, 274, 312, 313], "jun": [244, 265], "june": [4, 12, 66, 69, 91, 147, 150], "jupyt": [82, 94], "jupyter_execute_notebook": 0, "jupyterlab": [81, 93], "just": [7, 9, 14, 15, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 81, 87, 93, 99, 108, 109, 113, 125, 130, 176, 179, 181, 202, 216, 224, 229, 232, 233, 244, 256, 265, 267, 270, 271, 273, 274, 305, 306, 307, 318, 319, 338, 340, 347], "justifi": [244, 265, 267, 273], "justin": [244, 265], "j\u00e4reg\u00e5rd": [267, 274], "k": [108, 109, 112, 244, 263, 265, 275, 280, 312, 313, 317, 318, 324, 325, 326, 331, 337, 338, 347], "k8": [17, 18, 20, 27, 43, 45, 46, 50, 53, 56, 58, 62, 66, 69, 77, 79, 133, 136, 348], "kafka": [8, 169, 173], "katharina": [267, 274], "ke": [244, 265], "keep": [6, 82, 87, 94, 99, 108, 109, 115, 202, 203, 205, 206, 212, 215, 216, 217, 224, 227, 235, 238, 244, 259, 265, 266, 267, 272, 305, 307, 309, 312, 313, 316, 317, 318, 319, 320, 322, 325, 327, 328, 331, 335, 337, 338, 340, 343, 344, 345, 347], "keeper": [267, 273], "kei": [7, 8, 9, 10, 11, 14, 15, 20, 22, 24, 26, 28, 30, 43, 45, 53, 56, 83, 91, 95, 110, 111, 115, 123, 142, 144, 169, 170, 176, 182, 184, 185, 193, 196, 199, 202, 204, 205, 206, 212, 217, 223, 229, 233, 275, 277, 318, 324, 325, 328, 329, 338, 343], "kenni": [244, 265], "kept": [244, 265, 318, 320], "kernel": [1, 24, 27, 108, 109, 114, 151, 152, 202, 205], "kernel_s": [5, 7, 13, 14, 202, 205, 229, 232, 233], "kerri": [244, 263, 265], "kessler": [267, 274], "keylogg": [125, 128], "kick": [318, 322, 338, 344], "kiddo": [244, 265], "kill": [7, 14, 202, 215, 229, 233, 267, 270, 273, 274, 305, 307], "kim": [244, 265], "kind": [1, 151, 152, 325, 327], "kingdom": [267, 274], "kitchen": [244, 265], "klaviyo": 16, "klondik": [267, 273], "km": [17, 22], "know": [3, 8, 125, 128, 137, 141, 159, 167, 169, 174, 202, 222, 224, 267, 273, 274, 331, 332], "knowledg": [79, 117, 124, 142, 143, 325, 329, 330], "known": [305, 306], "kong": [267, 273], "kpop": [244, 265], "kri": [244, 265], "kube": [43, 46, 51, 53, 58, 63, 64], "kubeconfig": [43, 46, 53, 58], "kubectl": [43, 44, 46, 49, 50, 51, 53, 55, 58, 61, 62, 63, 64, 68, 73, 78], "kuberai": [133, 136], "kubernet": [11, 17, 19, 20, 44, 45, 47, 52, 55, 56, 59, 65, 68, 69, 73, 75, 78, 79, 196, 198], "kucinich": [244, 265], "kueue": [24, 26], "kurt": [244, 265], "kv": [111, 113, 116, 117, 123, 325, 329], "kv_cache_util": [117, 123], "kvedzwag2qa8i5bj": [117, 122], "l": [4, 5, 9, 10, 13, 15, 53, 63, 147, 150, 176, 179, 181, 184, 188, 305, 306, 312, 313, 318, 319, 338, 339], "l4": [108, 109, 115, 125, 128, 129], "l40": [117, 120, 121, 123, 125, 130], "l6": [244, 253, 264], "lab": [7, 14, 229, 233], "label": [4, 5, 7, 10, 11, 12, 13, 14, 15, 142, 143, 147, 150, 184, 190, 191, 193, 196, 200, 202, 204, 206, 215, 217, 218, 219, 220, 223, 229, 231, 232, 233, 244, 248, 249, 250, 263, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 279, 280, 292, 296, 304, 305, 307, 309, 311, 312, 316, 318, 322, 325, 327, 328, 329, 330, 331, 335, 337, 340, 341, 345, 347], "label_col": [325, 328], "label_column": [4, 12, 147, 150, 325, 328], "label_nam": [305, 307, 338, 340, 347], "labeled_batch": [10, 184, 190], "labia": [267, 270], "labour": [244, 265], "lack": [8, 169, 170, 174], "lag": [331, 337], "lai": [244, 265], "lakehous": [10, 184, 188, 202, 216, 228], "lambda": [6, 83, 95, 137, 141, 235, 238, 267, 272, 325, 329, 330], "land": [0, 325, 326], "landscap": [173, 292, 293, 295, 296, 304, 348], "languag": [5, 6, 8, 13, 111, 169, 170, 172, 173, 235, 240, 318, 324], "laptop": [1, 151, 152, 267, 269], "lar": [267, 274], "larami": [267, 273], "larg": [0, 4, 5, 6, 8, 9, 10, 13, 15, 83, 95, 111, 112, 113, 117, 119, 124, 125, 131, 137, 141, 147, 150, 160, 161, 162, 168, 169, 170, 173, 176, 183, 184, 186, 202, 222, 235, 238, 239, 243, 244, 253, 259, 261, 264, 266, 267, 269, 272, 274, 275, 277, 280, 281, 282, 285, 292, 301, 305, 306, 307, 311, 318, 320, 324, 325, 326, 327, 330, 331, 332, 337, 338, 340], "large_mat_from_object_stor": [3, 159, 161], "large_matrix": [3, 159, 161], "larger": [8, 16, 83, 95, 169, 174, 202, 228, 267, 272, 305, 311, 312, 317, 318, 319, 338, 347], "largest": [125, 128], "last": [3, 5, 6, 8, 15, 53, 57, 86, 90, 98, 107, 108, 109, 112, 125, 130, 159, 167, 169, 173, 174, 203, 210, 214, 222, 223, 224, 235, 237, 244, 263, 265, 266, 275, 282, 318, 322, 323, 325, 328, 330, 331, 333, 335, 338, 341, 345], "last_login": 146, "lastmodifi": [83, 95], "latenc": [8, 11, 83, 95, 112, 116, 117, 123, 125, 131, 142, 145, 169, 173, 175, 196, 198, 312, 317, 338, 347], "latent": [6, 235, 238, 318, 319], "later": [7, 13, 14, 16, 81, 93, 108, 109, 114, 146, 202, 204, 208, 212, 214, 219, 220, 229, 232, 267, 274, 305, 307, 308, 312, 315, 318, 319, 320, 322, 325, 327, 338, 341], "latest": [1, 5, 13, 28, 29, 43, 44, 53, 55, 66, 74, 81, 93, 151, 152, 202, 214, 222, 223, 224, 225, 226, 244, 250, 263, 267, 273, 285, 291, 292, 301, 303, 309, 312, 313, 316, 317, 318, 319, 322, 323, 324, 331, 332, 335, 336, 337, 338, 339, 346], "latin": [318, 324], "laugh": [244, 265], "laughter": [244, 265], "launch": [2, 3, 4, 6, 7, 9, 10, 11, 17, 19, 82, 86, 87, 88, 90, 94, 98, 99, 101, 106, 108, 109, 110, 118, 125, 126, 143, 147, 148, 153, 154, 159, 160, 167, 176, 177, 184, 185, 196, 197, 206, 208, 215, 229, 230, 235, 236, 243, 244, 261, 267, 269, 272, 275, 277, 285, 292, 301, 306, 313, 325, 326, 328, 330, 332, 337, 339], "layer": [4, 17, 19, 117, 120, 123, 147, 149, 202, 205, 305, 311, 318, 320, 331, 332], "layer1": 13, "layer2": 13, "layer3": 13, "layer4": 13, "layers_per_block": [6, 235, 238], "layout": [318, 320], "lazi": [9, 15, 176, 179, 180, 185, 244, 256, 259, 265, 266, 318, 320, 325, 327], "lbc": [43, 46, 53, 58], "lbl": [305, 307], "le": [244, 265], "lead": [3, 8, 159, 165, 167, 169, 174, 244, 265, 267, 274], "leader": [244, 265], "leagu": [244, 265], "leakag": [331, 333], "lean": [338, 339], "learn": [3, 4, 5, 6, 7, 9, 10, 13, 14, 15, 28, 34, 108, 109, 110, 117, 124, 132, 137, 138, 139, 147, 149, 159, 163, 165, 170, 173, 176, 183, 184, 195, 204, 206, 214, 229, 233, 235, 238, 243, 244, 253, 259, 260, 261, 264, 266, 267, 268, 269, 270, 274, 275, 276, 277, 278, 280, 281, 282, 285, 292, 300, 301, 311, 317, 320, 321, 322, 324, 327, 333, 335, 337, 340, 347], "learning_r": [202, 207], "least": [3, 17, 22, 24, 26, 27, 108, 109, 115, 159, 163, 165], "leav": [5, 28, 30, 43, 45, 80, 82, 86, 92, 94, 98, 267, 273, 305, 309, 325, 327, 330, 331, 337, 338, 347], "lecun": [13, 14], "left": [2, 8, 81, 82, 93, 94, 108, 109, 112, 153, 158, 169, 175, 267, 274, 318, 323, 324, 325, 330, 331, 336], "leftarrow": [305, 306, 312, 313], "leftov": [202, 227], "legend": [305, 309, 312, 316, 318, 322, 331, 335, 337, 338, 345], "leigh": [244, 265], "lemieux": [244, 265], "len": [3, 5, 6, 84, 96, 159, 167, 202, 204, 215, 235, 238, 267, 271, 289, 290, 291, 292, 303, 318, 320, 322, 324, 325, 327, 328, 329, 330, 331, 333, 335, 338, 340, 341], "lena": [267, 270], "length": [10, 28, 30, 43, 45, 53, 56, 108, 109, 111, 112, 113, 117, 120, 125, 131, 184, 186, 289, 290, 291, 292, 303, 331, 333], "leo": [267, 274], "leopold": [267, 274], "lesnar": [244, 265], "less": [125, 128, 267, 270, 305, 311], "lesson": [12, 348], "let": [2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 35, 39, 43, 45, 46, 53, 58, 81, 82, 83, 86, 87, 93, 94, 95, 98, 99, 108, 109, 113, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 132, 142, 144, 147, 148, 150, 153, 158, 159, 161, 162, 163, 165, 167, 168, 176, 179, 180, 181, 182, 184, 188, 189, 191, 193, 194, 196, 200, 202, 204, 205, 206, 210, 212, 229, 231, 232, 233, 235, 238, 239, 305, 306, 307, 308, 309, 312, 313, 318, 319, 325, 326, 327, 331, 335, 338, 340, 342, 347], "level": [0, 5, 6, 7, 8, 11, 13, 14, 16, 17, 22, 24, 27, 84, 87, 96, 99, 108, 109, 112, 114, 133, 135, 137, 140, 142, 145, 160, 163, 169, 173, 196, 199, 202, 205, 213, 229, 232, 235, 239, 267, 270, 318, 320, 322, 331, 332, 337, 338, 339], "leverag": [8, 10, 11, 24, 25, 142, 145, 169, 170, 184, 186, 195, 196, 198, 243, 244, 259, 261, 266, 267, 269, 275, 277, 281, 305, 311, 312, 317, 325, 330], "lexu": [125, 129, 244, 265], "lgbm": [5, 6, 202, 203, 235, 237], "lh": [9, 176, 181], "li": [267, 274], "liar": [244, 265], "lib": [83, 95, 244, 265], "libomp": [4, 147, 148], "librari": [2, 5, 7, 8, 9, 10, 11, 14, 15, 16, 80, 83, 84, 92, 95, 96, 137, 141, 153, 155, 169, 173, 174, 176, 178, 179, 184, 188, 190, 191, 193, 196, 199, 200, 202, 204, 229, 232, 233, 243, 260, 261, 268, 269, 276, 277, 279, 281, 285, 296, 300, 301, 304, 305, 307, 312, 314, 318, 320, 331, 333, 338, 340], "licens": [1, 151, 152], "lie": [244, 265, 312, 314], "life": [244, 265, 267, 270, 274], "lifecycl": [8, 11, 13, 24, 25, 26, 83, 85, 89, 91, 95, 97, 103, 105, 169, 175, 196, 199, 203], "lift": [80, 92], "light": [0, 10, 184, 190, 244, 265], "lightli": 91, "lightn": [5, 237, 306, 307, 308, 311, 312, 313, 314, 316, 317], "lightning_training_loop": [6, 235, 238], "lightningmodul": [6, 235, 238, 306, 313], "lightweight": [11, 84, 96, 196, 198, 244, 253, 264, 318, 319, 320, 324, 325, 327, 331, 333, 337, 338, 339], "lik": [267, 274], "like": [3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 23, 24, 26, 27, 28, 30, 31, 35, 38, 39, 40, 41, 43, 45, 47, 53, 56, 59, 62, 66, 75, 81, 82, 93, 94, 108, 109, 114, 117, 120, 121, 125, 129, 147, 150, 159, 162, 163, 164, 165, 169, 170, 173, 174, 175, 176, 179, 181, 183, 184, 191, 202, 203, 206, 207, 211, 213, 218, 229, 233, 235, 239, 244, 265, 267, 269, 270, 273, 274, 275, 281, 285, 292, 296, 301, 304, 305, 311, 312, 317, 318, 319, 320, 324, 325, 326, 328], "likeeeeeeeee": [244, 265], "likelihood": [338, 339], "limit": [7, 8, 9, 24, 27, 28, 30, 43, 45, 53, 56, 82, 83, 94, 95, 108, 109, 111, 125, 128, 169, 170, 176, 181, 191, 229, 233], "limousin": [4, 9, 12, 147, 150, 176, 179], "line": [81, 86, 90, 93, 98, 107, 133, 136, 267, 274, 305, 306, 331, 332, 338, 339], "linear": [3, 7, 13, 14, 159, 168, 229, 233, 312, 315, 331, 334], "linearmodel": [3, 159, 168], "lineup": [244, 263, 265, 266], "link": [0, 24, 26, 86, 98, 125, 127, 132, 202, 208], "linux": [133, 136], "list": [3, 4, 9, 10, 15, 28, 30, 32, 33, 35, 38, 41, 43, 45, 49, 50, 51, 53, 56, 57, 61, 62, 64, 66, 69, 71, 77, 78, 81, 83, 85, 88, 93, 95, 97, 101, 108, 109, 113, 125, 128, 137, 141, 147, 150, 159, 162, 176, 179, 181, 184, 188, 194, 202, 214, 215, 219, 267, 273, 305, 307, 312, 314, 318, 320, 324, 325, 327, 328, 331, 337], "list_": [331, 333], "list_objects_v2": [83, 95, 125, 128], "listbucket": [17, 22], "listbucketmultipartupload": [17, 22], "listdir": [338, 347], "listfil": [10, 184, 192], "listmultipartuploadpart": [17, 22], "lit": [244, 263, 265], "lite": [305, 306, 307, 340], "liter": [244, 265], "littl": [244, 265, 267, 273, 274], "live": [244, 265, 267, 273, 292, 293, 295, 296, 304, 318, 324, 331, 332], "ll": [1, 5, 6, 7, 14, 17, 18, 35, 37, 43, 47, 50, 53, 59, 62, 66, 68, 70, 75, 80, 81, 84, 91, 92, 93, 96, 108, 109, 110, 116, 117, 118, 119, 120, 122, 126, 128, 137, 138, 151, 152, 204, 205, 207, 208, 213, 228, 229, 232, 235, 238, 244, 265, 267, 273, 331, 332], "llama": [108, 109, 115, 120, 121, 122, 123, 125, 128, 131], "llamafactoryai": [125, 128], "llm": [10, 112, 116, 119, 121, 122, 124, 127, 129, 130, 132, 184, 186], "llm_config": [108, 109, 115, 117, 120, 123, 125, 128, 129, 130], "llmconfig": [108, 109, 115, 117, 120, 123, 125, 128, 130], "lo": [1, 151, 152], "load": [3, 4, 8, 11, 12, 16, 17, 22, 24, 27, 28, 34, 51, 52, 64, 65, 81, 82, 86, 90, 93, 94, 98, 106, 107, 108, 109, 114, 115, 117, 120, 122, 125, 128, 130, 147, 150, 159, 165, 168, 169, 170, 175, 177, 183, 185, 186, 191, 196, 198, 200, 203, 204, 210, 212, 216, 222, 224, 226, 228, 230, 238, 243, 253, 259, 261, 264, 266, 269, 274, 275, 277, 278, 279, 280, 281, 282, 285, 291, 292, 301, 303, 306, 310, 311, 312, 317, 319, 322, 324, 326, 329, 330, 332, 335, 337, 339, 341, 343, 347], "load_data": [4, 147, 150], "load_dataset": [244, 247, 250, 262, 263, 267, 270, 275, 278, 280, 305, 307, 331, 333, 338, 340, 347], "load_from_checkpoint": [6, 235, 239], "load_model": [4, 12, 147, 150, 325, 328], "load_model_ray_train": [5, 13, 202, 206, 209, 217, 223], "load_model_torch": [5, 13], "load_state_dict": [5, 13, 202, 215, 223, 305, 311, 312, 317, 318, 322, 324, 331, 335, 337, 338, 343, 347], "loadbalanc": [24, 27], "loaded_df": [6, 235, 238], "loaded_model": [5, 13], "loaded_model_ray_train": [5, 6, 13, 235, 239], "loader": [5, 202, 217, 218, 220, 223, 275, 277, 305, 309, 318, 319, 331, 333, 338, 339, 341, 342, 347], "loan": [244, 265], "loc": [12, 14], "local": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 22, 28, 31, 35, 40, 43, 47, 51, 53, 59, 64, 66, 75, 86, 90, 91, 98, 107, 108, 109, 110, 115, 118, 124, 125, 126, 134, 135, 142, 143, 144, 145, 147, 148, 150, 153, 154, 159, 160, 166, 169, 173, 176, 177, 181, 184, 185, 191, 194, 196, 197, 198, 200, 201, 202, 203, 204, 212, 215, 216, 229, 230, 235, 236, 243, 244, 250, 261, 263, 265, 267, 269, 275, 277, 281, 282, 285, 292, 301, 305, 306, 307, 309, 312, 313, 316, 318, 319, 325, 326, 328, 331, 332, 338, 343, 347], "local_fil": [83, 95], "local_file_path": [125, 128], "local_idx": [338, 341], "local_path": [5, 10, 11, 13, 15, 16, 125, 128, 142, 145, 184, 191, 196, 200], "local_pred_fold": [10, 184, 194], "local_storag": [6, 13, 235, 238], "local_zip": [318, 320], "localhost": [0, 4, 11, 12, 16, 108, 109, 115, 117, 121, 125, 128, 129, 130, 142, 145, 147, 150, 196, 200, 292, 296, 304], "locat": [1, 5, 35, 39, 66, 70, 83, 87, 95, 99, 125, 130, 142, 145, 151, 152, 202, 204, 212, 214, 325, 326], "lock": [1, 84, 96, 151, 152, 260, 267, 268, 273, 276, 300], "lodg": [325, 326], "lofton": [244, 265], "log": [5, 6, 8, 10, 12, 13, 14, 17, 21, 22, 28, 31, 32, 35, 36, 40, 41, 43, 47, 50, 53, 59, 62, 63, 66, 75, 77, 81, 85, 86, 91, 93, 97, 98, 103, 108, 109, 114, 117, 123, 133, 135, 137, 139, 140, 141, 143, 146, 169, 170, 173, 184, 192, 202, 203, 204, 206, 211, 212, 213, 214, 217, 221, 226, 228, 235, 237, 238, 275, 282, 305, 308, 309, 311, 312, 313, 314, 315, 316, 317, 318, 319, 322, 325, 326, 328, 330, 331, 334, 335, 337, 338, 339, 343, 348], "log_engine_metr": [117, 120, 123], "log_every_n_step": [6, 235, 238], "log_level": [142, 145], "log_result": [84, 96], "logdir": [12, 13, 275, 282], "logger": [142, 145], "logging_config": [142, 145], "logic": [3, 5, 6, 8, 10, 11, 12, 13, 14, 16, 17, 19, 22, 125, 131, 159, 165, 169, 174, 175, 184, 190, 196, 199, 202, 205, 217, 223, 235, 239, 275, 282, 291, 292, 293, 295, 296, 303, 304, 305, 306, 309, 311, 312, 313, 318, 319, 322, 331, 333, 338, 339, 343], "login": [28, 31, 35, 38, 40, 43, 47, 53, 59, 66, 69, 75, 81, 93], "logist": 12, "logit": [10, 11, 15, 16, 184, 191, 196, 200, 202, 205, 215, 275, 279, 280, 338, 343, 347], "logloss": 12, "loguniform": [7, 14, 229, 233], "loki": [244, 265], "lol": [244, 265], "london": [244, 265], "long": [3, 4, 5, 6, 7, 12, 13, 14, 17, 21, 88, 101, 108, 109, 112, 113, 125, 131, 147, 150, 159, 165, 202, 203, 222, 229, 232, 235, 237, 239, 244, 265, 267, 274, 318, 320, 322, 331, 332], "longer": [5, 8, 169, 175, 202, 206, 215, 217, 227, 228, 305, 311], "longrightarrow": [312, 313, 331, 332], "look": [3, 4, 5, 7, 8, 9, 10, 13, 15, 16, 28, 30, 31, 35, 40, 43, 45, 47, 53, 56, 57, 59, 66, 75, 117, 124, 125, 128, 132, 142, 144, 147, 150, 159, 168, 169, 175, 176, 182, 184, 189, 193, 202, 204, 206, 218, 229, 233, 244, 263, 265, 267, 273, 274, 305, 311, 325, 327], "look_back_period_": 16, "lookup": [318, 319], "loop": [125, 130, 154, 203, 204, 205, 207, 208, 212, 213, 214, 216, 220, 221, 222, 224, 225, 226, 228, 237, 239, 275, 277, 280, 305, 306, 309, 311, 317, 319, 326, 330, 332, 338, 339, 343, 347], "lora": [108, 109, 112, 116, 126, 127, 132], "lora_checkpoint": [125, 128], "lora_config": [125, 128], "lose": [202, 222, 225], "loss": [5, 6, 7, 12, 13, 14, 202, 204, 205, 206, 211, 212, 214, 217, 223, 229, 232, 233, 235, 238, 267, 274, 275, 280, 281, 282, 306, 308, 313, 315, 317, 325, 326, 328, 330, 337, 339, 340, 343, 344], "loss_fn": [6, 235, 238, 305, 308, 312, 315, 331, 335], "loss_funct": 13, "loss_ms": [6, 235, 238], "lot": [5, 6, 202, 203, 235, 237], "louboutin": [244, 263, 265], "loung": [244, 265], "love": [244, 265, 267, 274, 292, 296, 304], "low": [8, 108, 109, 112, 114, 125, 128, 131, 169, 173, 175, 312, 315, 317, 331, 337, 338, 339, 347], "lower": [83, 84, 95, 96, 117, 119, 124, 125, 131, 244, 265, 289, 290, 291, 292, 303], "lowercas": [289, 290, 291, 292, 303], "lowest": [338, 344], "lr": [5, 6, 7, 13, 14, 202, 206, 217, 223, 229, 232, 233, 235, 238, 239, 275, 280, 281, 305, 308, 312, 315, 318, 322, 331, 335, 337, 338, 343, 344], "lr_schedul": [6, 235, 238], "lssf": [1, 151, 152], "lstm": [331, 337], "lstrip": [83, 95], "lt": [267, 274], "luck": [244, 265, 267, 273], "lucki": [244, 265], "lupin": [244, 263, 265, 266], "lustr": [8, 169, 170], "ly": [267, 274], "m": [0, 1, 3, 5, 13, 151, 152, 159, 167, 244, 265, 267, 273, 274, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "m1": [1, 151, 152], "m2": [1, 151, 152], "m3": [1, 151, 152], "m5": [84, 96, 137, 141], "mac": [244, 265, 275, 280], "machin": [1, 3, 4, 9, 16, 17, 20, 27, 80, 81, 82, 85, 89, 92, 93, 94, 97, 105, 133, 134, 137, 139, 147, 149, 151, 152, 159, 163, 170, 173, 176, 183, 243, 244, 253, 256, 260, 261, 264, 265, 266, 267, 268, 269, 274, 275, 276, 277, 281, 282, 285, 292, 300, 301, 325, 326], "maco": [1, 4, 133, 136, 147, 148, 151, 152], "macosx": [1, 151, 152], "maddon": [244, 265], "made": [6, 11, 196, 199, 235, 239, 267, 270, 273, 274, 318, 322], "madison": [244, 265], "magnitud": [331, 333], "mai": [3, 5, 6, 28, 30, 35, 39, 42, 43, 45, 53, 56, 63, 66, 70, 71, 73, 78, 82, 83, 84, 87, 89, 90, 94, 95, 96, 99, 103, 106, 133, 136, 159, 165, 167, 168, 202, 216, 235, 236, 244, 265, 267, 273, 274, 285, 292, 301, 325, 329, 338, 347], "maiden": [244, 265], "main": [0, 3, 5, 6, 9, 11, 15, 28, 30, 35, 39, 43, 45, 53, 56, 85, 86, 97, 98, 105, 106, 108, 109, 115, 117, 120, 137, 141, 142, 145, 146, 159, 166, 176, 182, 196, 201, 202, 206, 235, 238, 244, 265, 267, 274, 277, 318, 320, 331, 333], "maintain": [3, 8, 11, 17, 19, 83, 95, 108, 109, 112, 159, 168, 169, 170, 175, 196, 199, 267, 274, 312, 313], "mainten": [24, 25], "major": [267, 270], "make": [0, 1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 66, 78, 81, 83, 86, 87, 90, 91, 93, 95, 98, 99, 107, 108, 109, 111, 125, 127, 128, 137, 141, 147, 150, 151, 152, 159, 162, 166, 169, 175, 176, 179, 182, 183, 196, 201, 202, 204, 210, 214, 216, 219, 220, 221, 222, 224, 225, 229, 233, 234, 235, 238, 239, 243, 244, 261, 265, 267, 269, 270, 273, 274, 275, 277, 281, 285, 289, 290, 291, 292, 301, 303, 305, 306, 307, 312, 314, 318, 324, 325, 326, 327, 331, 332, 333, 336, 337, 338, 339, 341], "make_pendulum_dataset": [312, 314], "makedir": [305, 307, 309, 312, 316, 318, 320, 325, 327, 331, 333, 338, 340], "malaga": [244, 265], "male": [267, 270, 274], "mall": [244, 265], "malloc": [133, 136], "mamba": [260, 268, 276, 300], "man": [244, 265, 267, 270, 273, 274], "manag": [2, 5, 6, 8, 12, 17, 19, 20, 21, 22, 25, 27, 35, 37, 39, 43, 47, 51, 53, 59, 64, 66, 68, 75, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 105, 107, 114, 116, 117, 122, 133, 136, 137, 140, 142, 145, 153, 155, 160, 169, 170, 202, 203, 217, 235, 237, 244, 265, 275, 281, 285, 292, 301, 305, 306, 309, 311, 312, 313, 316, 319, 324, 325, 326, 331, 332, 338, 339, 342, 343, 347], "mani": [2, 4, 9, 10, 11, 28, 30, 43, 45, 53, 56, 84, 86, 90, 96, 98, 107, 108, 109, 112, 117, 124, 125, 129, 131, 147, 150, 153, 155, 160, 164, 165, 176, 179, 184, 190, 196, 198, 202, 205, 207, 208, 210, 244, 253, 256, 263, 264, 265, 267, 270, 274, 292, 299, 304, 318, 319, 320, 325, 328, 331, 337, 338, 339], "manipul": [267, 273], "mann": [267, 273, 274], "manner": [4, 9, 10, 15, 147, 150, 176, 177, 179, 181, 184, 185, 267, 269, 275, 281, 338, 339], "mansbridg": [244, 265], "manual": [5, 35, 42, 66, 78, 203, 204, 206, 208, 209, 212, 222, 224, 228, 244, 265, 275, 280, 305, 306, 311, 312, 313, 318, 319, 323, 325, 326, 328, 330, 331, 332, 336, 338, 339, 342, 343, 345], "manual_se": [331, 335], "map": [6, 12, 16, 125, 128, 130, 137, 141, 202, 220, 235, 238, 275, 280, 312, 313, 318, 320, 324, 325, 326, 327, 331, 337, 338, 339, 340, 341, 343, 347], "map_batch": [4, 9, 10, 12, 15, 16, 142, 144, 147, 150, 176, 180, 184, 190, 191, 193, 243, 244, 253, 256, 259, 261, 264, 265, 266, 305, 307, 312, 314, 318, 320, 325, 329, 330, 331, 337, 338, 347], "map_group": [9, 10, 15, 176, 182, 183, 184, 193], "map_loc": [5, 6, 13, 202, 215, 235, 239, 305, 311, 312, 317, 318, 322, 324, 331, 335, 337, 338, 343, 347], "mapbatch": [10, 184, 192, 244, 266], "mapreduc": [8, 169, 173], "mar": [292, 293, 295, 296, 304], "marathon": [244, 265], "marc": [267, 274], "march": [244, 265], "mario": [244, 265], "mark": [133, 136, 267, 274], "markdown": 0, "marker": [305, 309, 312, 316, 318, 322, 331, 335, 337, 338, 345], "market_typ": [137, 141], "marlei": [244, 265], "marri": [267, 270], "martial": [267, 273], "martin": [244, 265], "mask": [275, 280], "mass": [244, 265], "massiv": [10, 184, 186], "master": [1, 14, 125, 127, 151, 152, 267, 273, 275, 282, 331, 333], "mat1_ref": [3, 159, 161], "mat2_ref": [3, 159, 161], "match": [3, 10, 43, 51, 53, 64, 84, 86, 90, 96, 98, 107, 117, 120, 125, 128, 129, 131, 159, 165, 184, 191, 202, 226, 244, 265, 325, 327], "matching_analysi": [125, 128], "materi": [4, 9, 11, 12, 16, 137, 141, 142, 144, 147, 150, 176, 179, 180, 182, 185, 186, 188, 189, 190, 191, 193, 196, 200, 243, 244, 261, 266, 318, 320, 325, 327, 328, 330, 338, 347], "materialized_d": [244, 259, 266], "materializeddataset": [244, 266, 267, 271], "math": [2, 153, 158, 312, 314, 331, 333, 334], "mathbb": [305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339], "mathcal": [305, 306, 312, 313, 318, 319, 338, 339], "mathemat": [125, 131], "matmul": [3, 159, 161, 318, 324], "matplotlib": [5, 7, 10, 13, 14, 16, 184, 185, 202, 204, 229, 230, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "matric": [338, 347], "matrix": [322, 324], "matrixfactorizationmodel": [318, 321, 322, 324], "matt": [244, 265], "matter": [108, 109, 113, 244, 265, 267, 270, 338, 341], "matur": [8, 86, 90, 98, 106, 169, 174], "max": [9, 10, 15, 16, 83, 95, 117, 120, 176, 182, 184, 190, 193, 267, 274, 305, 309, 312, 316, 318, 322, 331, 333, 335, 338, 343], "max_": 15, "max_depth": [4, 12, 147, 150, 325, 328, 330], "max_epoch": [6, 235, 238, 239, 305, 309, 311, 312, 316], "max_failur": [202, 224, 228, 305, 309, 312, 316, 318, 322, 325, 328, 331, 335, 338, 339, 344], "max_len": [331, 334], "max_length": [275, 280], "max_lora": [125, 128], "max_lora_rank": [125, 128], "max_model_len": [108, 109, 115, 120, 125, 128, 129, 130], "max_nod": [43, 50, 53, 62, 137, 141], "max_num_adapters_per_replica": [125, 128], "max_ongoing_request": [8, 169, 175], "max_replica": [16, 108, 109, 115, 117, 120, 123, 125, 130], "max_retri": [3, 159, 163], "max_siz": [10, 184, 191], "max_step": [6, 235, 239], "max_t": [305, 308, 312, 315], "maxim": [8, 9, 108, 109, 111, 113, 114, 169, 170, 175, 176, 182, 243, 244, 261], "maximum": [16, 43, 50, 53, 62, 80, 82, 92, 94, 108, 109, 111, 112, 117, 119, 123, 124, 125, 128, 131, 202, 226], "maxpool": 13, "maxpool2d": 13, "maxpumperla": [0, 275, 282], "mayb": [267, 273], "mb": [331, 333], "mcintir": [267, 273], "md": [0, 142, 145, 260, 268, 276, 300], "mdmad": [244, 265], "me": [17, 22, 117, 121, 122, 244, 265, 267, 270, 273, 274], "mean": [1, 3, 7, 9, 10, 14, 15, 66, 69, 108, 109, 112, 151, 152, 159, 161, 165, 176, 179, 180, 182, 184, 189, 193, 202, 209, 215, 229, 233, 305, 306, 312, 313, 318, 319, 324, 331, 333, 337, 338, 341, 342, 347], "meant": [8, 169, 170, 172, 318, 324], "meantim": [267, 273], "measur": [318, 324], "meat": [267, 270], "mechan": [8, 10, 133, 135, 169, 170, 184, 186, 338, 339, 343], "medium": [108, 109, 113, 116, 121, 124, 125, 131], "meet": [1, 10, 11, 16, 108, 109, 112, 151, 152, 184, 186, 196, 198, 244, 265], "melodrama": [267, 274], "member": [87, 99], "memori": [2, 3, 5, 6, 9, 10, 12, 14, 15, 84, 96, 111, 112, 114, 116, 117, 119, 121, 123, 125, 127, 128, 131, 133, 135, 137, 139, 141, 153, 155, 159, 161, 165, 167, 173, 174, 176, 178, 179, 184, 186, 188, 189, 190, 192, 193, 202, 204, 210, 215, 220, 235, 236, 238, 243, 256, 261, 265, 267, 269, 274, 305, 307, 311, 318, 320, 327, 331, 337, 338, 340, 341, 347], "memory_usag": [9, 176, 179], "memorydb": 21, "men": [267, 270, 273], "mental": [267, 270], "mention": [267, 274, 331, 337], "merg": [9, 176, 183, 318, 324], "merlin": [244, 265], "messag": [1, 3, 8, 13, 14, 16, 108, 109, 115, 117, 121, 122, 125, 128, 129, 130, 137, 139, 146, 151, 152, 159, 168, 169, 173, 305, 309, 312, 316], "messages_cv": [125, 128], "messages_nemoguard": [125, 128], "messages_yara": [125, 128], "messi": [244, 263, 265], "meta": [108, 109, 115, 117, 119, 120, 123, 125, 128, 318, 322, 331, 335, 338, 343], "meta_path": [338, 343], "metadata": [8, 9, 84, 96, 137, 141, 169, 175, 176, 183, 202, 224, 244, 250, 263, 266, 267, 269, 271, 273, 312, 316, 318, 322, 324, 325, 330, 338, 341], "metadataprint": [248, 249], "method": [3, 5, 6, 7, 9, 10, 11, 14, 15, 16, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 133, 136, 159, 161, 168, 176, 180, 184, 189, 191, 192, 193, 196, 200, 202, 203, 229, 233, 235, 237, 238, 243, 244, 253, 261, 264, 267, 274, 275, 280, 305, 311, 318, 319, 324], "method_nam": [3, 159, 168], "metric": [4, 6, 7, 12, 14, 16, 17, 22, 81, 86, 89, 93, 98, 103, 117, 120, 123, 125, 132, 133, 135, 136, 137, 139, 140, 141, 143, 147, 150, 203, 204, 205, 206, 213, 217, 221, 223, 224, 226, 228, 229, 232, 233, 235, 237, 239, 277, 281, 282, 305, 309, 310, 311, 312, 313, 316, 317, 319, 320, 324, 325, 327, 328, 330, 331, 332, 335, 336, 337, 338, 339, 343, 344, 345, 346, 347, 348], "metrics_datafram": [5, 6, 13, 202, 214, 235, 239, 305, 309, 312, 316, 318, 322, 331, 335, 338, 345], "metrics_interval_": 16, "mf_ray_train": [318, 322], "miami": [244, 265], "mic": [244, 263, 265], "michael": [244, 265, 267, 274], "micro": [338, 343], "microservic": [88, 101, 108, 109, 113, 146], "mid": [202, 226, 325, 326, 338, 346], "mid_block_scale_factor": [6, 235, 238], "middl": [244, 265], "midwai": [244, 265], "might": [4, 5, 6, 7, 10, 12, 24, 26, 28, 30, 43, 45, 53, 56, 66, 69, 108, 109, 112, 147, 148, 150, 184, 190, 202, 203, 229, 233, 235, 237, 267, 270, 305, 311, 325, 328, 330, 338, 347], "migrat": [11, 16, 196, 200, 202, 205, 238, 312, 313], "milan": [244, 265], "mile": [4, 8, 9, 12, 15, 147, 150, 169, 173, 174, 176, 179], "million": [4, 12, 147, 150, 244, 263, 265, 267, 274], "min": [4, 7, 9, 10, 12, 14, 15, 16, 117, 120, 147, 150, 176, 182, 184, 189, 190, 193, 229, 233, 325, 328, 335, 337, 338, 344], "min_": 15, "min_nod": [43, 50, 53, 62], "min_replica": [16, 108, 109, 115, 117, 120, 123, 125, 130], "min_siz": [10, 184, 191], "mind": [244, 265, 267, 270, 274], "mine": [244, 265], "minecraft": [244, 263, 265, 266], "miner": [267, 273], "mini": [5, 6, 202, 203, 235, 239, 305, 306, 312, 313], "miniconda": [1, 151, 152], "miniforge3": [1, 151, 152], "minilm": [244, 253, 264], "minim": [5, 6, 7, 8, 12, 14, 17, 20, 22, 24, 26, 169, 170, 173, 202, 203, 229, 233, 235, 237, 238, 275, 281, 305, 308, 312, 313, 317, 318, 319, 325, 326, 338, 339, 347], "minimalist": 0, "minimum": [16, 17, 22, 43, 50, 53, 62, 80, 82, 92, 94], "minu": 12, "minut": [5, 9, 35, 39, 43, 45, 53, 56, 66, 70, 73, 80, 82, 92, 94, 176, 179, 244, 265, 331, 332, 333], "mirror": [331, 332, 338, 339], "mise": [267, 274], "miseenscen": [267, 274], "misfortun": [267, 273], "mismatch": [331, 335], "miss": [244, 265, 267, 273, 274, 305, 311, 312, 316], "missouri": [244, 265], "mistak": [267, 273], "mistral": [125, 131], "mitch": [244, 265], "mix": [6, 235, 238, 239, 305, 311, 312, 317, 338, 347], "mkdir": [5, 13], "ml": [1, 4, 8, 9, 11, 12, 15, 16, 82, 83, 85, 87, 89, 90, 91, 94, 95, 97, 99, 103, 106, 147, 149, 150, 151, 152, 169, 172, 174, 175, 176, 183, 196, 197, 198, 199, 200, 243, 244, 259, 261, 266, 285, 288, 291, 292, 301, 302, 303, 318, 320, 324, 338, 339], "mlbcentral": [244, 265], "mlflow": [202, 228, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "mlogloss": [325, 328], "mlop": [4, 12, 125, 128, 147, 149, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "mlp": [312, 315, 317, 318, 324], "mm": [3, 159, 165], "mmlu": [125, 131], "mnist": [5, 7, 10, 11, 14, 15, 16, 142, 145, 184, 188, 190, 191, 193, 196, 200, 203, 206, 209, 210, 212, 214, 215, 219, 227, 228, 229, 230, 231, 232, 233], "mnist_app": [11, 16, 142, 145, 196, 200, 201], "mnist_app_handl": [11, 196, 200], "mnist_classifi": [11, 16, 196, 200], "mnist_classifier_arg": [10, 184, 191], "mnist_deploy": 16, "mnist_deployment_handl": 16, "mnist_pr": [10, 15, 184, 194, 195], "mnist_preprocessor": 16, "mnistclassifi": [10, 11, 15, 16, 184, 191, 192, 196, 200], "mnt": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 83, 95, 142, 144, 147, 150, 176, 181, 184, 191, 196, 200, 202, 204, 205, 210, 212, 214, 219, 227, 235, 238, 239, 240, 305, 307, 309, 311, 312, 316, 317, 318, 320, 322, 324, 325, 327, 328, 330, 331, 332, 333, 338, 339, 340, 341, 343, 344, 347], "mock": [267, 273], "modal": [9, 10, 15, 176, 183, 184, 195], "modano": [244, 265], "mode": [0, 4, 7, 11, 12, 14, 125, 129, 147, 150, 185, 196, 201, 202, 206, 215, 229, 233, 275, 280], "model": [3, 8, 9, 10, 11, 12, 15, 16, 82, 83, 85, 89, 94, 95, 97, 103, 111, 113, 114, 115, 118, 121, 122, 124, 126, 127, 128, 129, 130, 132, 159, 165, 168, 169, 170, 171, 174, 175, 176, 180, 183, 184, 186, 191, 193, 195, 196, 198, 199, 200, 201, 203, 204, 206, 212, 213, 214, 215, 217, 222, 223, 224, 226, 228, 230, 232, 236, 240, 243, 253, 259, 260, 261, 264, 266, 267, 268, 274, 275, 276, 277, 278, 279, 280, 281, 282, 299, 300, 302, 304, 305, 306, 308, 309, 311, 312, 313, 314, 316, 317, 320, 322, 323, 324, 326, 327, 329, 330, 332, 333, 335, 337, 340, 343, 347], "model1": [4, 147, 150], "model1_predict": [4, 147, 150], "model2": [4, 147, 150], "model2_predict": [4, 147, 150], "model_config": [6, 235, 238], "model_dump": [4, 125, 130, 147, 150], "model_id": [108, 109, 115, 117, 120, 123, 125, 128, 129, 130], "model_json_schema": [125, 129], "model_kwarg": [331, 337], "model_loading_config": [108, 109, 115, 117, 120, 123, 125, 128, 129, 130], "model_nam": [6, 82, 94, 235, 238, 239, 244, 251, 252, 253, 264], "model_path": [4, 5, 11, 13, 147, 150, 196, 200, 202, 215, 325, 328], "model_predict": [4, 147, 150], "model_select": [4, 147, 148, 325, 327], "model_sourc": [108, 109, 115, 117, 120, 123, 125, 128, 129, 130], "model_state_dict": [202, 223], "modelcheckpoint": [305, 309, 311, 312, 316], "modelclass": [251, 252], "modelwork": [202, 215], "moder": [108, 109, 112, 125, 128, 131], "modern": [8, 169, 170, 173, 243, 244, 259, 261, 266, 267, 269, 275, 277, 282], "modif": [86, 90, 98, 107], "modifi": [3, 4, 5, 9, 10, 11, 28, 30, 35, 39, 43, 45, 53, 56, 66, 71, 83, 84, 86, 90, 95, 96, 98, 107, 142, 144, 145, 147, 150, 159, 167, 176, 181, 184, 191, 196, 200, 222, 305, 306, 338, 339], "modul": [5, 13, 17, 21, 22, 35, 39, 53, 57, 108, 109, 110, 115, 116, 117, 122, 124, 125, 127, 132, 142, 145, 209, 212, 216, 222, 223, 224, 318, 321, 324, 331, 334, 337, 338, 339, 347], "modular": [17, 21, 22, 318, 319], "mofo": [244, 265], "mom": [244, 265], "momentum": [13, 275, 280], "mondai": [244, 265], "monei": [267, 270, 273], "mongodb": [8, 169, 170], "monitor": [5, 6, 8, 13, 24, 25, 81, 85, 86, 91, 93, 97, 98, 108, 118, 120, 122, 124, 125, 131, 132, 133, 135, 136, 137, 139, 140, 141, 142, 143, 144, 169, 173, 202, 203, 211, 212, 235, 237, 325, 330, 331, 337], "monro": [244, 265], "month": [4, 9, 125, 130, 147, 150, 176, 179, 244, 265], "moon": [244, 265], "more": [2, 3, 7, 8, 9, 10, 11, 13, 14, 16, 17, 22, 26, 28, 30, 43, 44, 45, 53, 54, 56, 66, 67, 84, 88, 96, 101, 108, 109, 112, 114, 115, 119, 124, 127, 133, 136, 137, 139, 141, 142, 144, 153, 158, 159, 162, 163, 165, 166, 167, 169, 172, 174, 175, 176, 182, 183, 184, 186, 190, 193, 195, 196, 198, 199, 201, 202, 203, 204, 208, 212, 214, 228, 229, 232, 233, 237, 240, 260, 267, 268, 273, 274, 275, 276, 280, 282, 285, 291, 292, 299, 300, 301, 303, 304, 305, 311, 318, 324, 325, 329, 330, 331, 335, 338, 347], "morn": [244, 265], "moron": [267, 273], "morri": [267, 273], "morti": [244, 265], "mosh": [244, 265], "most": [3, 8, 9, 10, 15, 24, 26, 80, 82, 83, 88, 92, 94, 95, 101, 125, 128, 129, 159, 162, 168, 169, 172, 176, 180, 184, 187, 189, 202, 214, 226, 267, 273, 274, 292, 296, 304, 305, 309, 312, 316, 318, 320, 323, 325, 329, 338, 340, 343, 345], "most_rec": [83, 95], "mostli": [108, 109, 113], "motion": [312, 313], "motiv": [267, 270], "mount": [17, 22, 83, 95], "mountaincar": [312, 317], "mouth": [267, 273], "move": [5, 6, 7, 8, 13, 14, 91, 108, 109, 116, 169, 170, 202, 205, 206, 209, 210, 212, 213, 215, 217, 218, 229, 232, 233, 235, 238, 243, 244, 261, 267, 274, 275, 280, 305, 311, 312, 313, 317, 325, 327, 331, 333, 338, 339], "movement": [6, 235, 238], "movi": [244, 265, 267, 270, 273, 274, 319, 320], "movielen": [319, 324], "mp": [244, 251, 252, 253, 259, 264, 265, 266, 275, 277, 280, 281], "mse": [305, 306, 309, 312, 316, 318, 319, 322], "mse_loss": [6, 235, 238, 318, 322], "mseloss": [305, 308, 312, 315], "mta": [244, 265], "mtv": [267, 274], "mtvstar": [244, 265], "mtvstarsof2015": [244, 265], "much": [8, 9, 108, 109, 113, 125, 128, 137, 141, 169, 174, 176, 179, 181, 244, 265, 267, 270, 273, 274, 305, 311, 318, 319, 325, 327], "muck": [267, 274], "muddi": [267, 273], "multi": [9, 10, 11, 15, 16, 17, 19, 82, 94, 108, 109, 112, 113, 114, 117, 119, 123, 124, 125, 128, 131, 132, 176, 181, 183, 184, 195, 196, 198, 202, 203, 204, 213, 216, 221, 228, 305, 306, 309, 311, 312, 313, 318, 319, 320, 324, 325, 326, 328, 338, 339, 347], "multi_actor_tracing_ray_serve_exampl": 146, "multiclass": 13, "multiclassaccuraci": [338, 343], "multimod": [305, 306], "multipl": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 22, 24, 26, 79, 82, 88, 94, 101, 108, 109, 112, 113, 114, 117, 118, 119, 120, 123, 124, 125, 127, 128, 131, 132, 137, 139, 142, 145, 146, 147, 149, 153, 158, 159, 161, 165, 166, 169, 170, 173, 176, 181, 183, 184, 190, 191, 196, 198, 199, 202, 203, 210, 212, 216, 219, 229, 233, 235, 237, 239, 243, 244, 253, 261, 264, 267, 269, 272, 275, 277, 280, 281, 282, 285, 292, 301, 305, 306, 307, 311, 312, 313, 317, 318, 320, 322, 324, 325, 326, 330, 331, 335, 337, 338, 340, 345, 347, 348], "multiplex": [10, 11, 16, 184, 186, 196, 198], "multipli": [3, 159, 168, 331, 337], "multiprocess": [3, 159, 165, 338, 341], "multithread": [3, 10, 159, 165, 184, 190], "multivari": [331, 337], "mum": [244, 263, 265], "muslim": [244, 265], "must": [125, 128, 202, 226, 305, 306, 312, 313], "mutat": [3, 159, 168, 244, 266], "mutual": [108, 109, 113], "my": [0, 3, 17, 22, 35, 38, 39, 81, 93, 108, 109, 115, 117, 120, 121, 122, 123, 125, 128, 129, 130, 146, 159, 164, 244, 263, 265, 267, 270, 274], "my_custom_env": [3, 159, 164], "my_simple_model": [7, 14, 229, 233], "my_xgboost_func": [4, 147, 150], "myself": [244, 265, 267, 270, 274], "mysentimentmodel": [291, 292, 296, 303], "mysql": [8, 169, 170], "n": [1, 3, 5, 24, 26, 53, 63, 83, 95, 117, 122, 125, 128, 130, 151, 152, 159, 165, 167, 202, 206, 244, 259, 260, 263, 265, 266, 267, 268, 274, 276, 300, 305, 306, 308, 309, 312, 313, 317, 319, 325, 329, 330, 338, 347], "n_step": [312, 314, 317], "nab": [331, 333], "naiv": [10, 11, 184, 186, 196, 198, 305, 311], "naiveti": [267, 273, 274], "nake": [267, 273], "nam": [53, 57], "name": [1, 5, 6, 7, 8, 11, 12, 13, 14, 16, 17, 21, 22, 23, 28, 30, 31, 32, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 53, 56, 57, 58, 59, 62, 66, 69, 70, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 92, 93, 94, 95, 96, 97, 98, 99, 105, 108, 109, 115, 117, 122, 125, 128, 129, 130, 137, 141, 142, 144, 145, 146, 151, 152, 169, 175, 196, 200, 201, 202, 206, 212, 213, 215, 221, 224, 226, 229, 233, 235, 239, 267, 270, 273, 305, 307, 309, 311, 312, 316, 317, 318, 320, 322, 324, 325, 328, 331, 335, 338, 340, 344, 347], "namespac": [43, 45, 46, 48, 49, 50, 51, 53, 56, 58, 60, 61, 62, 64, 66, 73, 76, 78, 202, 212], "nandito": [244, 265], "narrat": [267, 274], "naruto": [244, 265], "nash": [244, 265], "nashnewvideo": [244, 265], "nat": [17, 22, 24, 26, 28, 30, 43, 45, 53, 56], "natgatewai": [28, 30, 43, 45, 53, 56], "nation": [244, 265, 267, 273, 292, 293, 295, 296, 304], "nativ": [2, 4, 8, 24, 25, 27, 91, 108, 109, 113, 114, 137, 140, 141, 147, 149, 153, 157, 169, 170, 172, 173, 174, 267, 273, 274, 305, 309, 311, 312, 313, 316, 317, 331, 332, 338, 339], "nativesbr": [267, 274], "natur": [125, 130, 292, 293, 295, 296, 304], "navig": [81, 85, 86, 87, 90, 93, 97, 98, 99, 107, 133, 136, 142, 144, 146], "nbsp": [80, 83, 92, 95], "nc": [267, 270], "nccl": [275, 281, 338, 339], "ndarrai": [7, 10, 11, 14, 15, 16, 184, 190, 191, 193, 196, 200, 229, 233, 244, 251, 252, 253, 264, 266, 331, 337], "ndcg": [318, 324], "ndim": [202, 215], "necessari": [5, 9, 13, 16, 28, 29, 30, 34, 35, 38, 39, 43, 44, 45, 52, 53, 54, 56, 63, 65, 66, 67, 69, 176, 180, 202, 210, 244, 250, 263, 275, 277, 278, 289, 290, 291, 292, 303, 325, 327, 338, 339], "need": [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 27, 28, 29, 30, 31, 35, 36, 37, 39, 40, 42, 43, 44, 45, 47, 53, 55, 56, 57, 59, 63, 66, 68, 70, 71, 75, 78, 79, 80, 81, 82, 86, 92, 93, 94, 98, 108, 109, 113, 117, 118, 119, 120, 122, 124, 125, 126, 128, 129, 130, 131, 133, 136, 137, 141, 151, 152, 153, 158, 159, 161, 162, 164, 169, 170, 173, 176, 179, 181, 183, 184, 186, 191, 194, 196, 198, 200, 202, 203, 204, 205, 208, 210, 212, 216, 217, 220, 227, 228, 229, 233, 235, 237, 239, 244, 256, 265, 267, 273, 274, 275, 281, 285, 292, 301, 305, 307, 311, 312, 313, 318, 320, 322, 324, 325, 327, 328, 329, 338, 339, 340, 341, 342, 343, 345], "neg": [8, 169, 174, 267, 270, 292, 293, 295, 296, 304], "nemoguard": [125, 128], "nephew": [244, 265], "nest": [160, 162], "net": [6, 235, 238, 305, 308, 312, 315, 338, 343], "netflix": [9, 10, 15, 176, 183, 184, 195], "network": [5, 6, 7, 9, 10, 14, 15, 17, 20, 22, 24, 26, 27, 53, 63, 66, 78, 79, 83, 95, 108, 109, 113, 133, 135, 137, 139, 141, 176, 182, 184, 186, 193, 202, 203, 229, 232, 235, 237, 305, 306, 308, 338, 339], "networkinterfaceid": [28, 30, 43, 45, 53, 56], "neural": [7, 14, 229, 232, 318, 324, 331, 334, 338, 339], "never": [0, 244, 265, 267, 273, 274], "new": [3, 4, 8, 9, 13, 16, 24, 27, 28, 30, 46, 50, 53, 58, 62, 80, 81, 82, 83, 84, 85, 87, 90, 92, 93, 94, 95, 96, 97, 99, 105, 106, 108, 109, 112, 117, 122, 125, 130, 147, 150, 159, 167, 168, 169, 171, 176, 179, 202, 226, 244, 260, 265, 267, 268, 270, 273, 274, 276, 300, 309, 312, 317, 325, 326, 330, 331, 332, 338, 346, 348], "newaxi": [325, 329], "newli": [66, 72], "newsha": [244, 265], "next": [3, 7, 9, 13, 14, 16, 17, 20, 35, 37, 43, 47, 53, 59, 66, 68, 75, 80, 82, 85, 86, 90, 92, 94, 97, 98, 107, 111, 126, 128, 131, 135, 142, 144, 159, 167, 176, 181, 204, 209, 229, 233, 244, 265, 307, 320, 328, 332, 333, 340, 343], "nf": [83, 95], "nfl": [244, 263, 265], "nginx": [24, 27, 51, 52, 64, 65, 78], "nhead": [331, 334, 335, 337], "nhl": [244, 263, 265, 266], "nia": [244, 265], "niall": [244, 265], "nice": [202, 228], "nicer": [8, 169, 174], "nick": [244, 265], "nicki": [244, 265], "nigga": [244, 265], "night": [244, 263, 265, 266, 267, 273, 274], "nightli": [312, 317, 331, 337], "nightmar": [267, 273, 274], "nightmarish": [267, 274], "nine": [305, 307, 338, 340], "nirvana": [244, 265], "nlb": [24, 27], "nlp": [202, 228], "nn": [5, 6, 7, 13, 14, 202, 204, 205, 209, 212, 224, 229, 230, 232, 233, 235, 236, 275, 278, 305, 307, 308, 312, 314, 315, 318, 320, 321, 331, 333, 334, 335, 338, 340, 343], "no_grad": [5, 10, 11, 13, 15, 16, 184, 191, 196, 200, 305, 311, 312, 317, 318, 322, 324, 331, 335, 338, 343], "no_restart": [202, 215], "node": [1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 14, 16, 17, 20, 21, 22, 24, 26, 27, 28, 30, 35, 36, 39, 43, 45, 46, 50, 56, 58, 62, 63, 66, 70, 72, 80, 83, 84, 85, 86, 89, 92, 95, 96, 97, 98, 103, 106, 108, 109, 112, 113, 117, 119, 120, 122, 123, 124, 125, 131, 135, 137, 139, 140, 141, 151, 152, 153, 155, 159, 161, 163, 165, 176, 179, 181, 184, 190, 196, 198, 202, 203, 204, 206, 212, 213, 216, 220, 224, 225, 228, 229, 232, 235, 237, 244, 256, 265, 267, 269, 271, 305, 306, 311, 312, 313, 318, 319, 324, 325, 326, 327, 328, 333, 336, 337, 338, 339, 343, 347], "node_ip": 13, "nodegroup": [53, 57], "noderol": [53, 57], "nofril": [267, 274], "noir": [267, 273, 274], "noirlik": [267, 274], "nois": [6, 235, 238, 308, 311, 312, 313, 314, 315, 317, 331, 333], "noise_schedul": [6, 235, 238], "noised_lat": [6, 235, 238], "noiser": [305, 311], "noisi": [305, 308, 312, 313, 315], "noisy_act": [312, 314, 315], "noisy_img": [305, 308], "non": [5, 11, 108, 109, 113, 114, 115, 117, 121, 123, 125, 128, 129, 130, 142, 145, 196, 201, 202, 203, 204, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340, 343], "non_block": [202, 215], "none": [5, 6, 7, 10, 13, 14, 184, 191, 202, 211, 212, 215, 224, 229, 233, 235, 238, 275, 282, 305, 309, 311, 312, 316, 317, 318, 322, 324, 325, 328, 331, 334, 335, 338, 341, 347], "norm": [202, 215, 331, 333, 337], "norm_ep": [6, 235, 238], "norm_num_group": [6, 235, 238], "normal": [5, 7, 10, 13, 14, 15, 16, 184, 185, 190, 192, 202, 204, 210, 215, 216, 220, 229, 230, 231, 233, 305, 306, 311, 313, 315, 317, 318, 319, 324, 325, 327, 329, 332, 337, 338, 339, 341, 347], "normalci": [267, 274], "normalis": [331, 333, 338, 341], "normalize_cpu": [202, 215], "normalized_batch": [10, 15, 16, 184, 190], "normalized_img": 5, "north": [244, 265, 267, 273], "not_ready_ref": [3, 159, 167], "note": [1, 7, 9, 11, 13, 14, 15, 16, 17, 20, 24, 27, 28, 30, 35, 39, 43, 45, 47, 53, 56, 59, 66, 70, 75, 108, 109, 114, 133, 135, 136, 142, 145, 151, 152, 154, 160, 162, 163, 176, 180, 181, 182, 189, 190, 193, 196, 200, 201, 204, 216, 218, 220, 229, 233, 238, 244, 265, 266, 267, 272, 273, 274, 292, 296, 304, 318, 324, 325, 327, 338, 339, 343], "notebook": [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 28, 29, 35, 36, 43, 44, 53, 54, 80, 82, 83, 88, 91, 92, 94, 95, 100, 108, 109, 110, 117, 118, 125, 126, 133, 134, 137, 138, 142, 143, 147, 148, 153, 154, 159, 160, 162, 176, 177, 184, 185, 196, 197, 202, 203, 204, 229, 230, 235, 236, 243, 244, 260, 261, 266, 268, 274, 275, 276, 277, 282, 285, 292, 296, 299, 300, 301, 304, 305, 306, 312, 313, 317, 318, 319, 320, 325, 330, 331, 332, 338, 339, 340, 347, 348], "noth": [202, 215, 244, 265, 267, 274], "notic": [5, 9, 176, 181, 202, 206, 244, 265, 267, 274], "notif": [142, 145, 146], "notificationservic": 146, "nov": [13, 244, 265], "novelti": [318, 324], "now": [1, 3, 4, 5, 6, 10, 11, 13, 15, 16, 17, 22, 28, 30, 34, 35, 39, 43, 45, 51, 52, 53, 63, 64, 65, 66, 70, 81, 84, 85, 87, 93, 96, 97, 99, 108, 109, 115, 117, 121, 122, 123, 124, 125, 127, 128, 129, 130, 132, 142, 144, 145, 147, 150, 151, 152, 159, 162, 184, 191, 196, 200, 202, 205, 210, 212, 213, 215, 217, 218, 219, 220, 221, 223, 224, 235, 239, 244, 265, 267, 273, 274, 305, 306, 307, 309, 311, 312, 317, 318, 320, 322, 324, 325, 327, 330, 338, 347], "nowher": [267, 270], "np": [2, 3, 5, 6, 7, 10, 11, 14, 15, 16, 137, 141, 142, 145, 153, 154, 159, 160, 161, 165, 167, 184, 185, 190, 191, 193, 196, 197, 200, 202, 204, 215, 220, 229, 230, 233, 235, 236, 238, 244, 247, 251, 252, 253, 262, 264, 265, 275, 278, 279, 305, 307, 312, 314, 317, 318, 320, 325, 327, 328, 329, 331, 333, 337, 338, 340, 347], "nthread": [325, 328], "ntop": [318, 324], "nude": [267, 270], "nuditi": [267, 270], "nuge": [244, 265], "nugent": [244, 265], "null": [28, 30, 43, 45, 53, 56], "num": [13, 133, 136], "num_actor": [338, 347], "num_block": [244, 266, 267, 271], "num_boost_round": [4, 147, 150, 325, 328], "num_class": [5, 13, 202, 205, 325, 328, 338, 343, 347], "num_cpu": [3, 9, 10, 159, 165, 176, 182, 184, 190, 305, 307, 325, 329, 330], "num_decoder_lay": [331, 334], "num_encoder_lay": [331, 334], "num_epoch": [5, 7, 13, 14, 202, 206, 207, 213, 217, 221, 223, 224, 226, 229, 232, 233], "num_gpu": [3, 7, 10, 15, 16, 159, 165, 184, 190, 191, 202, 215, 229, 232, 244, 256, 265, 331, 337, 338, 347], "num_imag": [137, 141], "num_item": [318, 320, 321, 322, 324], "num_label": [275, 280], "num_lay": [331, 334, 335, 337], "num_parquet_shard": [318, 320], "num_partit": [267, 273], "num_replica": [16, 289, 290, 291, 292, 299, 303, 304], "num_return": [3, 159, 167], "num_row": [244, 263, 266, 267, 270, 271, 272, 338, 341], "num_row_group": [338, 341], "num_sampl": [4, 7, 12, 14, 147, 150, 229, 233], "num_to_keep": [305, 309, 312, 316, 318, 322, 325, 328, 331, 335, 338, 344], "num_training_step": [6, 235, 238], "num_us": [318, 320, 321, 322, 324], "num_warmup_step": [6, 235, 238, 239], "num_work": [4, 5, 6, 12, 13, 147, 150, 202, 203, 208, 228, 235, 238, 239, 275, 281, 282, 305, 309, 312, 316, 318, 322, 325, 328, 331, 333, 335, 338, 339, 341, 342, 344, 347], "number": [3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 24, 27, 80, 82, 84, 92, 94, 96, 117, 120, 123, 125, 129, 137, 139, 142, 144, 145, 147, 150, 159, 165, 167, 176, 179, 181, 184, 188, 190, 191, 202, 205, 206, 207, 213, 221, 226, 229, 231, 233, 235, 239, 243, 244, 253, 261, 264, 267, 270, 272, 275, 277, 280, 281, 282, 291, 292, 299, 303, 304, 312, 314, 318, 320, 322, 338, 347], "numenta": [331, 333], "numer": [9, 125, 128, 176, 179, 202, 204, 305, 311, 325, 326, 338, 340], "numpi": [2, 3, 5, 6, 7, 9, 10, 11, 14, 15, 16, 137, 141, 142, 145, 153, 154, 159, 160, 176, 180, 184, 185, 188, 191, 196, 197, 200, 202, 204, 215, 220, 229, 230, 235, 236, 244, 247, 262, 266, 275, 278, 305, 307, 311, 312, 314, 318, 320, 325, 327, 328, 331, 333, 337, 338, 340, 347], "nuremburg": [244, 265], "nutshel": 12, "nvdp": [43, 46, 51, 53, 58, 64], "nvidia": [24, 27, 51, 52, 64, 65, 125, 128, 244, 256, 265], "nvlink": [117, 123], "nvme": [5, 202, 203, 204], "nyc": [4, 9, 142, 144, 147, 150, 176, 179, 182, 337], "nyc_taxi": [331, 333], "nyc_taxi_2021": 12, "nyc_taxi_t": [331, 333], "nyc_taxi_transform": [331, 335], "o": [2, 3, 5, 6, 11, 13, 82, 83, 94, 95, 108, 117, 120, 123, 125, 128, 130, 133, 135, 137, 141, 153, 154, 159, 160, 164, 165, 196, 201, 202, 204, 212, 215, 223, 224, 227, 235, 236, 238, 239, 244, 260, 263, 265, 268, 275, 276, 278, 300, 305, 307, 309, 311, 312, 314, 316, 317, 318, 320, 322, 324, 325, 327, 328, 330, 331, 333, 335, 337, 338, 340, 343, 345, 347], "ob": [312, 314, 315, 317], "obj": [83, 95, 125, 128], "obj_ref": [3, 159, 161], "object": [2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 35, 42, 66, 78, 84, 86, 90, 96, 98, 107, 108, 109, 112, 117, 120, 125, 128, 130, 147, 150, 153, 157, 158, 162, 168, 169, 174, 175, 176, 179, 184, 192, 193, 196, 198, 200, 202, 212, 213, 214, 220, 226, 228, 229, 232, 233, 243, 244, 259, 261, 265, 266, 320, 324, 325, 328, 330, 338, 340, 345], "object_ref": [3, 159, 167], "objectref": [2, 3, 153, 157, 159, 161, 162, 167], "oblig": [267, 274], "oblivi": [267, 274], "obs_dim": [312, 315, 317], "obs_sampl": [312, 317], "observ": [5, 6, 14, 86, 90, 98, 106, 108, 125, 132, 141, 202, 203, 235, 237, 275, 282, 312, 313, 314, 315, 325, 327, 331, 334, 338, 341], "observed_data": [142, 144], "obtain": [83, 95, 275, 280], "obtus": [267, 270], "obviou": [267, 270], "occupi": [202, 227], "occur": [8, 137, 141, 169, 173, 202, 220, 225, 318, 320], "ocean": [244, 265], "oct": [244, 263, 265], "octob": [244, 265], "off": [0, 5, 7, 10, 13, 14, 117, 123, 125, 128, 131, 184, 189, 202, 204, 215, 229, 231, 244, 263, 265, 267, 274, 305, 307, 311, 318, 322, 323, 325, 329, 330, 331, 336, 338, 340, 341, 344, 347], "offenc": [244, 265], "offend": [244, 265], "offer": [4, 8, 11, 24, 25, 26, 85, 88, 89, 97, 102, 103, 117, 119, 123, 147, 149, 169, 170, 172, 173, 174, 196, 198, 199, 267, 270, 274, 305, 307], "offici": [17, 22, 35, 39, 81, 82, 93, 94, 133, 136, 142, 145], "offlin": [12, 84, 96, 312, 313, 314, 318, 320, 325, 326], "offlinemnistclassifi": 16, "offlinepredictor": [4, 12, 147, 150], "offload": [83, 95, 338, 339], "often": [8, 91, 169, 170, 173, 174, 202, 215, 318, 320, 325, 329], "oh": [244, 265], "olap": [8, 169, 170], "old": [90, 106, 244, 265, 267, 270, 274, 325, 330, 331, 337, 338, 347], "older": [142, 145, 305, 311], "oltp": [8, 169, 170], "olympics2012": [244, 265], "omp_num_thread": [3, 159, 165], "on_demand": [137, 141], "on_epoch": [305, 308, 312, 315], "on_fit_start": [6, 235, 238], "on_step": [6, 235, 238], "onc": [2, 10, 12, 15, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 81, 82, 83, 85, 93, 94, 95, 97, 108, 109, 112, 115, 117, 121, 122, 125, 128, 153, 158, 160, 162, 166, 168, 184, 190, 191, 202, 203, 204, 212, 215, 243, 244, 253, 261, 264, 275, 280, 305, 309, 318, 319, 325, 329, 331, 332, 337, 338, 339, 341, 347], "one": [2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 16, 17, 22, 24, 26, 27, 28, 30, 43, 45, 53, 56, 82, 83, 84, 88, 90, 94, 95, 96, 101, 106, 108, 109, 111, 112, 113, 114, 125, 127, 128, 131, 133, 136, 142, 144, 145, 147, 150, 153, 158, 159, 163, 165, 166, 167, 169, 171, 175, 176, 181, 183, 184, 186, 196, 198, 199, 202, 203, 205, 208, 210, 214, 216, 228, 235, 237, 243, 244, 261, 265, 267, 270, 273, 274, 305, 309, 325, 327, 328, 332, 335, 336, 338, 339, 341, 343, 344], "onehellofanighttour": [244, 265], "ones": [3, 8, 28, 30, 43, 45, 53, 56, 81, 93, 108, 109, 112, 117, 119, 159, 167, 169, 172, 244, 265, 285, 292, 301], "ongo": [16, 24, 25, 79], "onli": [2, 3, 5, 9, 10, 13, 15, 16, 17, 20, 24, 26, 28, 31, 35, 40, 43, 47, 53, 59, 66, 75, 82, 83, 84, 87, 88, 91, 94, 95, 96, 99, 101, 108, 109, 112, 114, 125, 128, 133, 136, 137, 140, 141, 144, 153, 158, 159, 168, 176, 179, 181, 184, 189, 190, 192, 193, 203, 205, 206, 211, 224, 227, 228, 244, 265, 267, 269, 270, 272, 273, 274, 275, 280, 281, 305, 311, 312, 313, 317, 318, 320, 322, 324, 325, 326, 328, 331, 333, 335, 337, 338, 339, 341, 343, 345], "onlin": [8, 11, 12, 16, 84, 90, 96, 106, 169, 170, 196, 200, 202, 216, 244, 260, 265, 268, 276, 291, 300, 303, 331, 337, 338, 347], "onlinemnistclassifi": [11, 16, 196, 200], "onlinemnistpreprocessor": 16, "onlinepredictor": 12, "onto": [3, 7, 10, 14, 159, 165, 184, 189, 202, 215, 229, 232], "onu": [8, 169, 173], "oom": [5, 6, 10, 137, 141, 184, 190, 235, 236], "op": [338, 340], "open": [0, 1, 2, 5, 8, 13, 35, 38, 81, 82, 83, 84, 85, 91, 93, 94, 95, 96, 97, 125, 130, 131, 133, 136, 137, 140, 142, 144, 146, 151, 152, 153, 155, 169, 170, 244, 265, 267, 273, 305, 307, 338, 340, 341, 347], "openai": [108, 109, 114, 115, 117, 120, 121, 122, 125, 128, 129, 130], "openapi": 16, "opentelemetri": [133, 135, 146], "oper": [8, 12, 17, 19, 26, 35, 39, 44, 45, 46, 49, 51, 52, 54, 56, 58, 61, 63, 64, 65, 67, 70, 78, 79, 88, 101, 108, 109, 111, 113, 133, 135, 136, 142, 143, 144, 146, 169, 170, 171, 174, 177, 183, 185, 186, 189, 190, 244, 259, 266, 267, 269, 271, 272, 273, 275, 278, 305, 306, 307, 338, 339], "opinion": [17, 21, 267, 270], "oppos": [267, 274], "opt": [133, 136], "opt_path": [338, 343], "opt_state_path": [331, 335], "optim": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 22, 24, 26, 110, 114, 116, 118, 124, 125, 131, 132, 169, 170, 174, 176, 180, 182, 183, 184, 190, 196, 198, 202, 204, 206, 217, 222, 223, 224, 226, 228, 229, 230, 232, 233, 235, 238, 243, 244, 253, 261, 264, 275, 280, 305, 308, 312, 315, 318, 322, 324, 331, 333, 335, 338, 339, 340, 343, 347], "optimizerlrschedul": [6, 235, 236, 238], "option": [3, 4, 5, 9, 10, 11, 12, 14, 15, 16, 20, 21, 26, 28, 30, 34, 35, 36, 39, 42, 45, 51, 52, 56, 64, 65, 78, 82, 94, 108, 109, 112, 115, 125, 128, 129, 137, 141, 142, 145, 147, 148, 149, 159, 163, 165, 176, 182, 184, 188, 191, 193, 196, 199, 201, 202, 204, 215, 218, 243, 244, 256, 261, 265, 275, 282, 291, 292, 299, 303, 304, 305, 307, 309, 312, 316, 318, 322, 328, 330, 331, 334, 335, 337, 348], "optuna": [7, 14, 229, 230, 233], "optunasearch": [7, 14, 229, 233], "orang": [3, 159, 161], "orc": [8, 169, 170], "orchestr": [24, 25, 26, 113, 117, 120, 146, 170, 202, 203, 204, 208, 213, 305, 306, 311, 312, 313, 318, 319, 322, 324, 325, 330, 331, 332, 333, 335, 337, 338, 339, 340, 343], "order": [2, 7, 14, 80, 92, 137, 139, 153, 155, 229, 233, 244, 265, 267, 273, 325, 327, 328], "ordinari": [267, 270], "oregon": [244, 265], "org": [88, 101, 102, 318, 320, 348], "org_967t9ah1lbk1yqf1zau6a1v247": [83, 95], "org_xxxxxxx": [35, 39], "organ": [8, 9, 17, 19, 22, 35, 39, 79, 84, 87, 91, 96, 99, 102, 169, 170, 176, 179, 202, 212], "organiz": [24, 26, 91], "orient": 12, "origin": [5, 13, 53, 63, 202, 205, 244, 263, 265, 266, 267, 274, 305, 307, 318, 323, 324, 331, 333, 338, 339], "original_user_id": [318, 324], "oscar": [244, 265], "oss": [137, 140, 141], "ossci": [13, 14], "other": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 17, 22, 24, 25, 66, 78, 81, 83, 84, 93, 95, 96, 125, 127, 133, 135, 147, 149, 150, 151, 152, 153, 155, 159, 166, 168, 169, 170, 172, 174, 176, 179, 196, 199, 202, 203, 204, 207, 215, 216, 229, 233, 235, 237, 239, 244, 265, 266, 267, 273, 274, 275, 277, 285, 292, 301, 305, 309, 311, 312, 317, 318, 320, 322, 331, 335, 338, 339], "otherwis": [5, 8, 125, 128, 169, 175, 318, 320], "otlp": 146, "our": [3, 5, 6, 7, 9, 10, 11, 14, 15, 16, 84, 85, 96, 97, 120, 121, 122, 125, 128, 129, 130, 142, 144, 146, 159, 166, 176, 179, 184, 188, 194, 196, 200, 202, 212, 229, 231, 232, 233, 235, 238, 239, 244, 265, 267, 274, 275, 279], "out": [3, 4, 5, 6, 8, 9, 10, 16, 82, 83, 84, 85, 86, 90, 94, 95, 96, 97, 98, 106, 107, 133, 135, 137, 141, 147, 150, 159, 163, 167, 169, 174, 176, 183, 184, 187, 202, 203, 230, 233, 235, 236, 243, 261, 263, 265, 267, 270, 273, 274, 305, 306, 307, 312, 314, 318, 320, 324, 325, 330, 331, 337, 338, 341, 347], "out_channel": [5, 6, 13, 202, 205, 235, 238], "out_featur": 13, "out_img_byt": [305, 307], "out_label": [305, 307], "out_proj": [331, 334], "out_ref": [3, 159, 162], "outbound": [17, 22], "outbr": [267, 274], "outdoor": [244, 265], "outhous": [244, 265], "outlier": [11, 196, 198], "outlook": 118, "output": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 28, 30, 31, 35, 39, 40, 42, 43, 45, 47, 53, 56, 57, 59, 66, 69, 70, 73, 75, 78, 84, 86, 89, 96, 98, 103, 108, 109, 111, 113, 116, 117, 120, 126, 127, 128, 130, 132, 142, 144, 145, 147, 150, 151, 152, 153, 158, 159, 163, 176, 178, 179, 181, 184, 188, 190, 192, 194, 196, 200, 202, 203, 204, 205, 206, 212, 214, 217, 223, 227, 229, 232, 233, 235, 238, 243, 244, 261, 275, 280, 305, 308, 311, 318, 320, 324, 331, 332, 334, 337], "output_column": [267, 274], "output_csv": [318, 320], "output_dir": [305, 307, 338, 340], "output_path": [137, 141], "output_s": 13, "outsid": [5, 6, 13, 24, 27, 235, 239], "outstand": [8, 169, 175], "over": [3, 4, 7, 9, 10, 12, 14, 24, 26, 80, 82, 83, 84, 85, 86, 92, 94, 95, 96, 97, 98, 147, 150, 159, 168, 176, 180, 183, 184, 186, 187, 202, 206, 217, 222, 229, 233, 244, 265, 267, 273, 274, 275, 280, 305, 311, 318, 322, 324, 325, 326, 330, 335, 337, 338, 339, 347], "overal": [15, 275, 277, 325, 328], "overcom": [244, 265], "overfit": [338, 345], "overhead": [2, 5, 6, 8, 81, 91, 93, 108, 109, 112, 117, 123, 153, 155, 169, 173, 202, 203, 235, 237, 312, 317], "overlap": [137, 140, 202, 203, 331, 332], "overload": [137, 141], "overr": [267, 274], "overrid": [28, 30, 35, 39, 43, 45, 53, 56, 86, 90, 98, 107, 202, 205], "overriden": [85, 89, 97, 103], "overse": 13, "oversubscrib": [3, 159, 165], "overview": [7, 8, 9, 10, 14, 15, 83, 95, 118, 124, 126, 134, 136, 137, 138, 142, 144, 148, 154, 169, 170, 176, 177, 184, 185, 197, 202, 203, 229, 232, 239, 348], "overwhelm": [137, 141], "overwrit": [81, 93], "own": [3, 5, 9, 11, 12, 24, 26, 35, 39, 80, 83, 86, 88, 90, 92, 95, 98, 101, 106, 117, 122, 125, 132, 159, 161, 176, 181, 196, 199, 202, 204, 244, 263, 265, 267, 274, 285, 292, 293, 295, 296, 301, 304, 305, 306, 307, 309, 312, 313, 316, 318, 319, 322, 325, 326, 327, 328, 331, 332, 338, 341, 343, 347], "owner": [35, 37, 66, 68, 88, 101], "ownership": [267, 273], "ox": [244, 265], "p": [267, 274, 305, 306, 331, 337], "p50": [142, 145], "p90": [142, 145], "p99": [142, 145], "pa": [305, 307, 325, 327, 328, 331, 333, 338, 340], "pack": [244, 265, 305, 308], "packag": [1, 82, 83, 94, 95, 151, 152, 202, 224, 260, 268, 276, 300, 305, 311, 312, 314, 318, 324, 325, 330], "pad": [5, 7, 13, 14, 202, 205, 229, 232, 233, 275, 280, 305, 308], "page": [0, 9, 10, 15, 108, 109, 116, 117, 123, 142, 144, 176, 182, 184, 193], "pagedattent": [108, 109, 114], "pagerduti": [142, 145], "pai": [108, 109, 114, 267, 274], "paid": [4, 9, 147, 150, 176, 179], "pain": [8, 169, 173], "pair": [5, 318, 319, 338, 341], "pal": [244, 265], "pale": [267, 274], "pan": [267, 273], "pancak": [338, 339], "panda": [4, 5, 6, 9, 12, 13, 147, 148, 176, 177, 179, 180, 181, 182, 202, 204, 214, 219, 235, 236, 269, 270, 305, 307, 312, 314, 318, 320, 325, 327, 328, 329, 330, 331, 333, 337, 338, 340, 347], "panel": [2, 3, 153, 158, 159, 167], "pant": [244, 265], "paper": [244, 265], "par": [267, 274], "parallel": [1, 7, 8, 9, 10, 14, 15, 111, 113, 120, 124, 151, 152, 154, 169, 173, 176, 178, 180, 182, 184, 186, 188, 206, 208, 216, 219, 220, 221, 229, 233, 238, 243, 244, 250, 256, 259, 261, 263, 265, 266, 267, 269, 271, 272, 275, 280, 285, 292, 301, 305, 306, 307, 311, 312, 313, 314, 316, 318, 319, 320, 324, 325, 326, 327, 329, 331, 332, 337, 338, 339, 340, 347], "parallel_strategi": [5, 202, 209, 228], "parallel_strategy_kwarg": [5, 202, 209], "param": [4, 12, 147, 150, 292, 296, 304, 318, 322, 325, 328], "param_nam": [202, 207], "param_spac": [4, 7, 12, 14, 147, 150, 229, 233], "paramet": [2, 4, 5, 6, 7, 9, 10, 13, 14, 15, 28, 31, 35, 40, 43, 47, 53, 59, 66, 75, 82, 86, 90, 94, 98, 107, 108, 109, 113, 117, 119, 124, 125, 128, 130, 132, 147, 150, 153, 158, 176, 180, 182, 184, 190, 191, 193, 202, 206, 207, 209, 216, 217, 221, 223, 228, 229, 232, 233, 235, 238, 244, 256, 265, 267, 272, 275, 277, 280, 291, 292, 303, 305, 308, 312, 315, 318, 322, 325, 328, 331, 335, 338, 339, 343], "parameter": [11, 196, 201, 202, 223], "paramor": [244, 265], "parcel": [267, 274], "parent": [5, 13], "parish": [244, 265], "pariti": [24, 26], "park": [244, 265, 292, 293, 295, 296, 304], "parquet": [4, 6, 8, 9, 10, 12, 15, 83, 95, 137, 141, 142, 144, 147, 150, 169, 170, 176, 179, 181, 182, 183, 184, 188, 194, 202, 216, 219, 227, 228, 235, 238, 306, 311, 312, 317, 319, 326, 328, 330, 332, 335, 339, 343, 347], "parquet_256": [305, 307, 338, 340, 341], "parquet_dir": [318, 320, 325, 327, 331, 333, 335], "parquet_fil": [338, 341], "parquet_path": [305, 307, 331, 333, 338, 341, 342, 347], "parquetdataset": [6, 235, 238], "parquetfil": [338, 341], "pars": [8, 11, 16, 84, 96, 109, 115, 117, 121, 125, 129, 169, 175, 196, 200, 331, 333], "parseabl": [125, 127, 129], "part": [0, 7, 9, 13, 14, 15, 16, 53, 57, 81, 93, 117, 120, 176, 177, 179, 229, 230, 267, 274, 275, 277, 281, 318, 320], "parti": [133, 135, 244, 265], "particular": [10, 184, 190, 267, 270], "particularli": [3, 159, 166, 244, 265], "partit": [10, 184, 186, 243, 244, 250, 261, 263, 305, 306, 312, 313, 325, 327, 338, 341], "partner": [244, 265, 267, 273], "pass": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 86, 98, 147, 150, 160, 165, 168, 169, 174, 176, 179, 181, 184, 186, 191, 196, 200, 202, 203, 206, 207, 208, 209, 213, 216, 217, 219, 220, 221, 223, 224, 226, 228, 229, 233, 235, 238, 239, 244, 265, 275, 280, 318, 319, 325, 328, 338, 347], "passeng": [4, 9, 12, 147, 150, 176, 179, 182, 267, 274, 337], "passenger_count": [4, 9, 12, 147, 150, 176, 179], "passrol": [17, 22], "past": [81, 82, 84, 85, 93, 94, 96, 97, 105, 244, 265, 331, 332, 333, 334, 335, 337], "past_list": [331, 337], "past_norm": [331, 337], "patch": [8, 24, 26, 169, 174], "path": [4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 24, 27, 66, 69, 83, 95, 117, 120, 125, 128, 137, 141, 142, 144, 145, 147, 150, 169, 175, 176, 181, 184, 189, 190, 191, 193, 194, 196, 200, 201, 202, 204, 212, 213, 214, 215, 221, 223, 224, 226, 227, 235, 238, 239, 275, 282, 305, 307, 309, 310, 311, 312, 316, 317, 318, 320, 322, 324, 325, 327, 328, 330, 331, 333, 335, 337, 338, 340, 343, 347], "pathlib": [4, 5, 13, 147, 150, 202, 204, 331, 333], "paths_to_delet": [202, 227, 338, 347], "patient": [244, 265], "pattern": [8, 11, 154, 160, 162, 165, 169, 170, 196, 201, 202, 212, 217, 228, 311, 320, 324, 330, 347], "payload": [4, 12, 147, 150], "payment_typ": [9, 176, 179, 182], "pb": [137, 141], "pc": [244, 263, 265, 266], "pd": [4, 5, 6, 9, 12, 13, 147, 148, 150, 176, 177, 179, 180, 202, 204, 219, 235, 236, 238, 267, 270, 305, 307, 312, 314, 318, 320, 324, 325, 327, 329, 330, 331, 333, 337, 338, 340], "pdf": [318, 320], "pe": [331, 334], "peac": [244, 265], "peak": [108, 109, 113], "peer": [24, 26], "penalti": [142, 145], "pend": [12, 13, 275, 282], "pendulum": [316, 317], "pendulum_diffus": [312, 316, 317], "pendulum_diffusion_ft": [312, 316], "pendulum_diffusion_result": [312, 316], "peopl": [244, 265, 267, 270, 274], "per": [9, 10, 15, 16, 83, 90, 95, 106, 108, 109, 113, 114, 117, 120, 123, 125, 128, 133, 135, 142, 144, 145, 176, 179, 182, 184, 188, 189, 205, 207, 208, 212, 213, 221, 277, 281, 305, 308, 309, 312, 315, 316, 320, 322, 325, 326, 328, 329, 330, 331, 332, 335, 337, 338, 339, 343, 345, 347], "per_worker_batch": [202, 206], "percentag": [9, 176, 180], "percentil": [331, 337], "perceptu": [305, 311], "perfect": [117, 119, 244, 265, 267, 274, 305, 307], "perform": [2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 82, 83, 84, 86, 90, 94, 95, 96, 98, 106, 108, 109, 110, 111, 117, 119, 123, 124, 125, 129, 130, 131, 132, 133, 135, 137, 140, 142, 145, 147, 148, 149, 150, 153, 158, 159, 165, 169, 170, 173, 174, 175, 176, 177, 180, 182, 183, 184, 185, 186, 190, 191, 193, 196, 197, 198, 200, 202, 212, 214, 216, 218, 222, 228, 229, 230, 233, 235, 238, 239, 244, 253, 256, 259, 264, 265, 266, 267, 272, 273, 274, 275, 277, 282, 305, 307, 311, 312, 313, 317, 318, 324, 325, 327, 329, 330, 331, 332, 335, 337, 338, 339, 343, 347], "performantli": [1, 151, 152], "perhap": [7, 8, 14, 169, 174, 229, 233, 267, 273, 274], "period": [16, 80, 82, 92, 94, 108, 109, 113, 331, 337], "permiss": [17, 22, 24, 26, 28, 29, 35, 36, 43, 44, 53, 55, 63, 83, 87, 95, 99], "permut": [305, 311], "persi": [244, 265], "persis": [202, 204], "persist": [5, 6, 17, 21, 22, 24, 26, 83, 84, 89, 95, 96, 103, 133, 135, 137, 140, 185, 186, 203, 204, 205, 206, 210, 219, 222, 224, 227, 235, 239, 312, 316, 318, 320, 323, 325, 326, 327, 339, 343], "person": [267, 274, 318, 319, 324], "perspect": [267, 270, 274], "pertain": [84, 96], "phase": [116, 275, 280], "philip": [244, 265], "philosop": [267, 274], "philosophi": [267, 273], "photo": [244, 265, 338, 339], "photograph": [267, 274, 305, 306, 338, 340], "physic": [3, 10, 137, 139, 159, 165, 184, 190], "pi": [84, 96, 312, 313, 314, 317], "pi4_sampl": [84, 96], "pi_": [312, 313], "pic": [244, 265], "pick": [53, 57, 108, 109, 115, 202, 226, 228, 244, 265, 267, 274, 318, 323, 331, 336, 338, 339, 346, 347], "pickup": [331, 332], "pid": [13, 14, 275, 282], "piec": [142, 144, 244, 265], "pil": [5, 202, 204, 210, 215, 220, 305, 307, 338, 340], "pile": [267, 270], "pin": [202, 215, 305, 307, 338, 339, 342], "pine": [325, 326], "pinecon": [8, 169, 170], "pink": [267, 270], "pinterest": [10, 184, 195], "pioneer": [8, 169, 173], "pip": [0, 1, 66, 74, 81, 82, 83, 93, 94, 95, 117, 121, 133, 136, 146, 151, 152, 160, 260, 268, 276, 300, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "pipelin": [4, 6, 8, 9, 10, 15, 82, 94, 108, 109, 112, 113, 124, 125, 132, 137, 141, 143, 147, 150, 160, 169, 170, 173, 176, 181, 183, 184, 186, 190, 191, 195, 202, 204, 216, 218, 219, 221, 228, 235, 238, 267, 269, 274, 288, 289, 290, 291, 292, 302, 303, 305, 306, 307, 312, 313, 317, 318, 319, 325, 326, 330, 331, 332, 337, 338, 339, 341, 347], "pipeline_parallel_s": [117, 123], "pitch": [244, 265], "pivot": [312, 313], "pixel": [7, 10, 14, 184, 190, 202, 204, 210, 219, 220, 229, 231, 306, 307, 309, 312, 314, 338, 340], "pixeldiffus": [305, 308, 309, 311], "pizza": [338, 339], "pl": [6, 235, 236, 238, 239, 305, 307, 308, 309, 312, 314, 315, 316], "pl_ckpt": [305, 311], "place": [0, 8, 108, 109, 112, 169, 175, 202, 204, 223, 225, 244, 265, 267, 274, 292, 293, 295, 296, 304], "placehold": [28, 29, 35, 36, 43, 44, 51, 53, 54, 64, 66, 67, 81, 93, 108, 109, 115, 117, 121], "placement": [202, 206, 209, 210, 213, 312, 313, 331, 337, 338, 339, 342, 343, 347], "plai": [81, 93, 244, 265, 267, 273, 338, 347], "plain": [305, 307, 338, 339], "plan": [10, 17, 22, 24, 26, 27, 28, 30, 35, 39, 43, 45, 51, 53, 56, 64, 66, 70, 79, 125, 128, 142, 145, 184, 189, 190, 192, 244, 263, 265], "plane": [19, 21], "planner": [331, 332], "plate": [244, 265], "plateau": [331, 337], "platform": [8, 16, 17, 20, 24, 26, 79, 84, 89, 90, 91, 96, 103, 106, 108, 109, 113, 133, 135, 136, 137, 140, 142, 143, 144, 145, 169, 170, 172, 173, 202, 204, 244, 250, 263, 275, 277, 281], "plausibl": [312, 317], "pleas": [17, 22, 28, 31, 35, 40, 43, 44, 47, 51, 53, 54, 59, 64, 66, 67, 75, 78, 91, 125, 128, 142, 144, 145, 260, 268, 276, 300, 318, 324, 338, 339], "plot": [16, 202, 204, 214, 215, 267, 270, 273, 274, 307, 308, 314, 315, 320, 325, 327, 330, 333, 337, 340, 347], "plotlin": [267, 274], "plt": [5, 7, 10, 13, 14, 16, 184, 185, 189, 202, 204, 215, 229, 230, 231, 305, 307, 309, 311, 312, 314, 316, 318, 320, 322, 325, 327, 329, 331, 333, 335, 337, 338, 340, 345, 347], "plu": [17, 22, 202, 204, 312, 314, 338, 340], "plugin": [6, 24, 27, 51, 52, 64, 65, 66, 72, 235, 239, 305, 309, 312, 316], "pm": [312, 313], "pndm": [305, 311], "png": [10, 184, 190], "poc": [24, 26], "pod": [24, 26, 27, 43, 49, 50, 51, 53, 61, 62, 64, 66, 73, 108, 109, 113], "point": [5, 10, 11, 17, 20, 24, 26, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 83, 95, 108, 109, 115, 117, 122, 146, 184, 190, 196, 201, 202, 206, 212, 226, 243, 244, 261, 265, 267, 273, 274, 325, 327], "pointless": [267, 270], "pole": [325, 326], "polici": [17, 21, 22, 24, 26, 27, 35, 39, 42, 63, 66, 70, 308, 311, 314], "polish": [267, 274], "polit": [125, 128, 267, 270, 274], "politician": [267, 270], "poll": [244, 265], "pont": [244, 265], "pool": [10, 184, 191], "poor": [11, 196, 198], "poorli": [7, 14, 229, 233], "popul": [9, 28, 30, 43, 45, 176, 179], "popular": [4, 8, 125, 128, 147, 149, 150, 169, 172, 173, 244, 251, 252, 253, 264, 318, 320], "porn": [267, 270], "porno": [267, 270], "pornograph": [267, 270], "port": [133, 136, 325, 326], "portion": [275, 280], "pos_enc": [331, 334], "posit": [8, 169, 174, 244, 263, 265, 267, 269, 270, 272, 274, 292, 296, 304, 331, 334], "posix": [17, 22], "possibl": [117, 124, 125, 130, 132, 267, 269], "possibli": [8, 169, 175], "post": [4, 5, 6, 7, 11, 12, 13, 16, 17, 22, 142, 145, 146, 147, 150, 196, 200, 229, 234, 235, 240, 291, 292, 303, 305, 306, 318, 319, 324], "poster": [267, 274], "postgresql": [8, 169, 170], "postwar": [267, 273, 274], "potato": [267, 270], "potemkin": [267, 274], "potenti": [3, 137, 139, 159, 163], "potter": [244, 265], "power": [3, 125, 126, 127, 130, 159, 166, 243, 244, 261, 267, 269, 275, 277, 281, 325, 330], "powershel": [125, 128], "pq": [305, 307, 331, 333, 338, 340, 341], "practic": [1, 5, 17, 20, 35, 39, 79, 88, 100, 108, 126, 127, 128, 132, 133, 135, 151, 152, 202, 203, 212, 215, 228, 244, 259, 266, 267, 271, 275, 277, 331, 333, 338, 339], "practition": [4, 12, 147, 149], "prayer": [244, 265], "pre": [10, 15, 35, 39, 81, 82, 83, 93, 94, 95, 184, 191, 275, 280, 289, 290, 291, 292, 302, 303, 305, 311, 318, 324, 331, 332, 338, 339], "preced": [267, 274], "precis": [6, 108, 109, 113, 117, 119, 235, 238, 239, 305, 311, 312, 317, 338, 347], "precomput": [108, 109, 111, 338, 341], "preconfigur": [202, 205], "pred": [5, 202, 215, 312, 315, 318, 322, 325, 329, 330, 331, 334, 335, 337, 338, 343, 347], "pred_d": [325, 329, 330, 331, 337, 338, 347], "pred_label": [325, 328, 329], "pred_nois": [305, 308, 311, 312, 317], "pred_norm": [331, 337], "pred_prob": [325, 328], "pred_row": [331, 337, 338, 347], "predefin": [82, 94], "predic": [9, 176, 183], "predict": [4, 7, 8, 10, 11, 13, 14, 15, 16, 125, 131, 147, 150, 169, 171, 184, 191, 194, 196, 200, 229, 233, 243, 244, 261, 275, 279, 280, 285, 291, 292, 296, 301, 303, 304, 305, 306, 308, 311, 312, 313, 315, 318, 319, 321, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 337, 338, 340, 347], "predicted_label": [10, 11, 15, 16, 142, 145, 184, 191, 193, 196, 200, 338, 347], "predicted_prob": 12, "prediction_pipelin": [4, 147, 150], "predictor": [4, 12, 108, 109, 111, 147, 150, 338, 347], "preemption": [10, 84, 96, 184, 186, 202, 225], "prefer": [24, 26, 28, 31, 35, 40, 43, 47, 53, 59, 66, 73, 75, 81, 93, 125, 131, 146, 267, 274, 305, 311], "prefer_spot": [137, 141], "prefetch": [338, 347], "prefetch_batch": [6, 202, 218, 235, 239], "prefil": 116, "prefix": [5, 9, 35, 39, 83, 95, 176, 181, 318, 324, 331, 337, 338, 347], "prefix_for_the_resources_ad": [35, 39], "preinstal": [80, 92], "prem": [80, 92], "premier": [244, 265], "premis": [17, 20, 87, 99], "prepar": [3, 6, 79, 86, 90, 98, 107, 125, 128, 159, 164, 206, 209, 213, 216, 228, 235, 239, 267, 269, 273, 274, 275, 277, 280, 312, 316, 335, 337, 339, 347], "prepare_data_load": [5, 13, 203, 205, 206, 212, 228, 331, 333, 338, 339, 340, 342, 347], "prepare_model": [5, 13, 203, 205, 206, 212, 228, 318, 319, 320, 322, 331, 333, 335, 338, 339, 340, 343, 347], "prepare_train": [305, 309, 312, 316], "preprocess": [5, 6, 8, 9, 10, 12, 15, 16, 82, 94, 169, 170, 176, 179, 183, 184, 190, 202, 204, 210, 215, 216, 220, 221, 228, 235, 238, 260, 268, 269, 275, 276, 277, 300, 305, 306, 307, 312, 313, 317, 318, 320, 324, 331, 332, 338, 339, 340, 341], "preprocess_imag": [305, 307], "preprocessed_df": [267, 274], "preprocessor": [9, 16, 176, 183, 267, 270], "preprocessor_app": 16, "preprocessor_handl": 16, "prerequisit": [36, 54, 67], "presenc": [325, 330], "present": [5, 8, 108, 109, 113, 125, 129, 142, 143, 169, 173, 202, 204, 267, 273, 274, 318, 320, 325, 326, 328, 331, 333, 337, 338, 347], "preserv": [202, 203, 205, 325, 327, 331, 332], "press": [1, 151, 152], "pressur": [108, 109, 112, 137, 141, 292, 293, 295, 296, 304], "pretend": [267, 271], "pretenti": [267, 270], "pretrain": [6, 235, 238, 239, 240, 244, 251, 252, 253, 264], "pretrainedconfig": [6, 235, 238], "pretti": [267, 273, 338, 347], "prevent": [10, 184, 190, 331, 333], "preview": [0, 28, 30, 35, 39, 43, 45, 53, 56, 126], "previou": [43, 48, 53, 60, 66, 76, 81, 93, 108, 109, 111, 112, 125, 130, 142, 143, 202, 216, 217, 223, 226, 292, 296, 304, 318, 324, 325, 326, 338, 339], "previous": [5, 202, 223], "price": [4, 12, 147, 150, 244, 263, 265, 331, 332], "priest": [244, 265], "primari": [8, 133, 136, 169, 173], "primarili": [6, 10, 15, 16, 24, 26, 184, 192, 235, 238, 331, 335], "prime": [244, 265], "primit": [338, 340], "princip": [17, 22], "print": [1, 3, 4, 5, 6, 7, 10, 13, 14, 81, 82, 83, 84, 85, 89, 93, 94, 95, 96, 97, 104, 108, 109, 115, 117, 121, 122, 125, 128, 129, 130, 142, 144, 147, 150, 151, 152, 159, 161, 163, 165, 167, 184, 190, 202, 206, 211, 227, 229, 232, 233, 235, 238, 244, 250, 259, 263, 266, 267, 270, 271, 272, 273, 274, 275, 280, 281, 292, 296, 304, 305, 307, 309, 310, 311, 312, 314, 316, 317, 318, 320, 322, 324, 325, 327, 328, 329, 330, 331, 333, 335, 336, 337, 338, 340, 341, 343, 344, 346, 347], "print_metrics_ray_train": [5, 13, 202, 206, 211, 217, 223], "printout": [338, 347], "prior": [8, 81, 93, 169, 170, 325, 328], "priorit": [7, 14, 229, 233], "privat": [17, 20, 22, 24, 26, 28, 30, 31, 35, 40, 43, 45, 47, 53, 56, 59, 66, 69, 75, 83, 87, 95, 99], "private_subnet": [17, 22], "privileg": [17, 22, 24, 26], "pro": [125, 131], "prob": [3, 159, 163, 325, 329], "probabilist": [331, 337], "probabl": [244, 265, 338, 339], "problem": [9, 84, 96, 108, 109, 114, 125, 131, 133, 135, 176, 179], "proce": [6, 13, 16, 235, 238], "process": [2, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 20, 22, 66, 67, 82, 84, 86, 90, 94, 96, 98, 106, 112, 114, 115, 116, 117, 118, 129, 130, 137, 139, 142, 144, 147, 150, 153, 156, 157, 158, 160, 162, 164, 166, 170, 175, 176, 178, 181, 183, 184, 186, 187, 188, 190, 191, 196, 199, 202, 203, 205, 206, 208, 210, 212, 213, 215, 216, 219, 229, 232, 233, 235, 238, 239, 243, 244, 250, 253, 256, 259, 261, 263, 264, 265, 266, 274, 275, 277, 280, 281, 282, 285, 292, 301, 307, 312, 313, 317, 318, 319, 320, 325, 326, 329, 331, 332, 333, 337, 338, 341], "processed_d": [305, 307], "prod": [3, 159, 164], "produc": [4, 5, 6, 8, 9, 10, 13, 15, 147, 150, 169, 171, 176, 181, 184, 188, 202, 212, 235, 239, 267, 273, 305, 306, 318, 324, 331, 337], "product": [1, 3, 8, 17, 20, 79, 82, 83, 94, 95, 108, 109, 110, 111, 113, 114, 117, 118, 119, 122, 123, 124, 125, 126, 127, 129, 131, 132, 142, 145, 151, 152, 159, 165, 169, 174, 177, 185, 202, 212, 228, 230, 236, 243, 244, 261, 267, 274, 285, 292, 301, 305, 311, 318, 319, 320, 321, 324, 325, 326, 328, 338, 339], "production": [85, 89, 90, 91, 97, 103, 106, 202, 228], "profession": 79, "profil": [24, 26, 28, 30, 34, 133, 135, 312, 317, 338, 347], "profile_data": 146, "prog_bar": [6, 235, 238, 305, 308, 312, 315], "program": [3, 125, 131, 142, 145, 159, 167], "programm": 16, "programmat": [16, 85, 89, 97, 103, 105, 142, 145], "progress": [5, 13, 84, 96, 202, 203, 211, 214, 222, 224, 225, 318, 320, 322, 338, 339, 340], "project": [0, 10, 11, 17, 19, 22, 35, 37, 38, 39, 66, 68, 69, 70, 72, 78, 83, 95, 100, 102, 146, 184, 186, 196, 198, 244, 265, 267, 274], "project_numb": [35, 39], "prometheu": 135, "promot": [331, 337], "promote_opt": [325, 328], "prompt": [1, 80, 81, 92, 93, 108, 109, 111, 112, 125, 128, 151, 152], "promptli": [267, 273], "proof": [24, 26, 244, 265], "proper": [28, 29, 35, 36, 43, 44, 53, 55, 202, 206], "properli": [1, 8, 125, 130, 133, 136, 151, 152, 169, 173, 260, 268, 275, 276, 277, 292, 296, 300, 304, 338, 340], "properti": [8, 125, 130, 169, 170], "proport": [9, 10, 15, 176, 181, 184, 188, 191, 318, 320], "proprietari": [8, 169, 170], "prosper": [244, 265], "protect": [325, 326], "protocol": [8, 11, 146, 169, 170, 175, 196, 198], "prototyp": [81, 93, 117, 119, 124], "prove": [16, 305, 310, 338, 346], "provid": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 24, 27, 28, 30, 35, 39, 41, 45, 50, 53, 56, 66, 70, 79, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 97, 98, 99, 102, 105, 106, 108, 109, 110, 112, 114, 117, 119, 120, 122, 123, 124, 125, 127, 128, 129, 130, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 149, 151, 152, 153, 154, 155, 159, 160, 169, 170, 173, 175, 176, 177, 178, 184, 185, 193, 196, 198, 199, 201, 202, 203, 204, 207, 216, 221, 226, 235, 237, 267, 269, 275, 277, 281, 285, 292, 301, 305, 309, 318, 320, 322, 325, 329, 331, 333, 335, 338, 339], "provis": [5, 6, 10, 12, 24, 25, 26, 27, 79, 88, 101, 184, 186, 235, 237, 305, 306, 325, 326, 338, 339], "proxi": [8, 142, 145, 146, 169, 175], "proxim": [325, 326], "proxy_http_request": [142, 145, 146], "proxy_route_to_replica": [142, 145, 146], "prune": [305, 311, 312, 317, 325, 330], "pseudo": [305, 311], "pt": [5, 10, 11, 13, 15, 16, 184, 191, 195, 196, 200, 201, 202, 212, 215, 223, 224, 305, 311, 318, 322, 324, 331, 335, 337, 338, 343, 347], "public": [4, 9, 10, 11, 15, 16, 17, 20, 22, 117, 120, 142, 144, 147, 150, 176, 179, 182, 184, 188, 190, 191, 193, 196, 200, 243, 244, 259, 261, 265, 266, 267, 269, 270, 273, 274], "public_subnet": [17, 22], "publicli": [87, 99, 125, 128], "publish": [86, 98], "pull": [81, 93, 244, 265, 305, 307, 318, 322, 325, 328, 331, 335, 338, 339, 340, 341, 345], "pulocationid": [9, 176, 179], "pumpkin": [244, 265], "pun": [267, 270], "punchestown": [244, 265], "punctuat": [267, 274], "pure": [305, 306, 318, 319], "purpl": [244, 265], "purpos": [1, 2, 15, 22, 151, 152, 153, 155, 170, 267, 270, 271, 338, 339], "push": [81, 93, 244, 265, 267, 274, 338, 339], "pushdown": [9, 176, 183], "put": [3, 17, 22, 154, 159, 161, 244, 265, 267, 273, 274], "putobject": [17, 22], "pwd": 5, "py": [0, 1, 10, 11, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 81, 83, 84, 85, 86, 93, 95, 96, 97, 98, 105, 106, 108, 109, 115, 117, 120, 123, 125, 128, 129, 130, 133, 136, 137, 141, 142, 144, 151, 152, 184, 189, 193, 196, 201], "py311": [108, 109, 115, 117, 122], "py312": [137, 141], "pyarrow": [8, 10, 142, 144, 169, 170, 184, 188, 305, 307, 312, 314, 318, 320, 325, 327, 328, 331, 333, 338, 339, 340], "pydant": [4, 16, 125, 129, 147, 148], "pydata": [331, 333], "pyflink": [8, 169, 173], "pypi": [83, 95], "pyplot": [5, 7, 10, 13, 14, 16, 184, 185, 202, 204, 229, 230, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "pyproj": [82, 94], "pyspark": [8, 169, 173], "python": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 16, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 81, 82, 83, 84, 85, 86, 91, 93, 94, 95, 96, 97, 98, 103, 108, 109, 113, 115, 117, 120, 122, 125, 128, 130, 133, 136, 142, 144, 145, 146, 147, 149, 151, 152, 153, 155, 156, 157, 159, 162, 163, 168, 169, 170, 172, 173, 174, 176, 178, 196, 198, 202, 205, 215, 229, 233, 235, 239, 260, 268, 275, 276, 277, 282, 285, 292, 300, 301, 305, 307, 312, 314, 318, 319, 320, 325, 327, 331, 333, 338, 340], "python3": [0, 83, 95], "pythonmalloc": [133, 136], "pytorch": [10, 184, 188, 202, 203, 204, 205, 206, 210, 212, 216, 217, 218, 220, 228, 230, 237, 278, 281, 282, 285, 292, 301, 305, 306, 307, 309, 311, 312, 313, 314, 316, 318, 319, 320, 321, 322, 324, 332, 334, 338, 339, 340, 341, 347], "pyyaml": 0, "q": [108, 109, 112, 125, 131, 318, 320], "q2": [244, 265], "q_q": [244, 265], "qp": [142, 145], "qt": [244, 263, 265, 266], "qtr": [244, 265], "quad": [305, 306, 312, 313, 318, 319], "qualif": [125, 128], "qualit": [338, 347], "qualiti": [8, 117, 119, 169, 170, 305, 311, 318, 324], "quantiz": [108, 109, 112, 124], "queri": [8, 28, 30, 43, 45, 53, 56, 57, 84, 86, 96, 98, 117, 121, 125, 128, 137, 140, 141, 142, 145, 146, 169, 170, 292, 304], "question": [244, 265], "queu": [24, 26], "queue": [2, 8, 11, 16, 24, 26, 142, 145, 153, 155, 169, 175, 196, 199], "quick": [2, 5, 12, 17, 20, 24, 26, 84, 96, 125, 128, 148, 153, 154, 202, 204, 212, 213, 305, 307, 309, 318, 320, 325, 327, 328, 338, 340, 347], "quickli": [1, 9, 81, 87, 93, 99, 108, 109, 113, 151, 152, 176, 179, 267, 274, 305, 307, 325, 327, 331, 335, 338, 340, 345], "quickstart": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68], "quit": [244, 265, 267, 274], "quot": [244, 265], "quota": [17, 19, 108, 109, 113], "qwen": [125, 129, 130, 131], "qwen2": [125, 129], "qwen3": [125, 130], "r": [0, 1, 8, 13, 35, 42, 66, 78, 146, 151, 152, 169, 170, 244, 260, 265, 267, 268, 270, 276, 300, 312, 313, 318, 319, 320, 325, 326, 328, 331, 332, 338, 339, 340], "r1": [125, 131], "r2": [8, 169, 170], "race": [244, 265, 267, 270, 273], "radio": [244, 265], "rafe": [244, 265], "ragnarok": [244, 265], "rahul": [244, 265], "rai": [17, 19, 20, 22, 25, 27, 34, 42, 49, 50, 51, 52, 61, 62, 64, 65, 79, 80, 81, 82, 83, 85, 86, 88, 90, 92, 93, 94, 95, 97, 98, 101, 102, 103, 106, 107, 113, 116, 122, 123, 124, 127, 129, 130, 132, 134, 135, 140, 141, 155, 156, 157, 161, 162, 163, 164, 165, 166, 173, 179, 180, 181, 182, 188, 189, 190, 192, 193, 194, 200, 201, 204, 205, 207, 208, 209, 210, 211, 213, 214, 223, 224, 225, 226, 227, 232, 238, 247, 251, 252, 253, 262, 263, 264, 265, 278, 280, 281, 291, 296, 302, 303, 308, 311, 314, 317, 321, 323, 324, 330, 336, 340, 343, 344, 345, 346], "railwai": [267, 274], "rais": [3, 159, 163, 305, 309, 311, 312, 316, 338, 347], "ram": [10, 184, 190], "ramen": [338, 339], "rammstein": [244, 265], "rand": [3, 11, 16, 142, 145, 159, 161, 165, 196, 200], "randint": [3, 5, 6, 7, 12, 14, 125, 130, 137, 141, 159, 167, 202, 204, 215, 229, 233, 235, 238, 305, 308, 312, 314], "randn": [305, 311, 312, 314, 317], "randn_lik": [6, 235, 238, 305, 308], "random": [2, 3, 5, 7, 9, 10, 11, 12, 14, 15, 16, 35, 41, 53, 62, 84, 96, 125, 130, 137, 141, 142, 145, 153, 154, 159, 160, 161, 163, 165, 167, 176, 182, 184, 193, 196, 200, 202, 204, 215, 229, 233, 305, 307, 311, 312, 313, 314, 317, 318, 320, 324, 325, 327, 338, 339, 340], "random_shuffl": [9, 10, 15, 176, 182, 184, 193, 305, 307, 312, 314, 325, 327], "random_st": [4, 147, 150, 325, 327, 338, 341], "randomize_block_ord": [9, 10, 15, 176, 182, 184, 193, 318, 320], "randomli": [5, 9, 10, 15, 176, 182, 184, 193, 202, 204, 318, 324], "rang": [2, 3, 5, 7, 10, 12, 13, 14, 16, 17, 22, 84, 96, 137, 141, 153, 158, 159, 161, 163, 167, 184, 190, 202, 204, 206, 215, 217, 220, 223, 229, 232, 233, 267, 271, 275, 280, 285, 292, 301, 305, 311, 312, 314, 317, 318, 322, 325, 329, 331, 332, 333, 335, 338, 341, 343], "rank": [4, 5, 6, 13, 125, 128, 147, 150, 203, 205, 206, 211, 213, 224, 228, 235, 239, 305, 309, 322, 324, 325, 328, 329, 330, 331, 335, 338, 342, 343], "rap": [244, 265], "rapid": [81, 93], "rapidli": [80, 92], "rate": [6, 7, 14, 24, 27, 108, 109, 112, 142, 145, 202, 206, 229, 233, 235, 238, 244, 265, 267, 270, 275, 280, 281, 305, 311, 312, 317, 321, 322, 324, 331, 337, 338, 347], "rather": [10, 108, 109, 111, 184, 189, 267, 270, 274, 318, 324, 325, 327, 331, 332], "ratings_d": [318, 320], "ratings_parquet": [318, 320], "ratings_parquet_uri": [318, 320], "ratio": [146, 325, 327, 329], "rattl": [267, 274], "rattler": [244, 265], "ravenstein": [267, 274], "raw": [8, 12, 13, 14, 53, 57, 137, 140, 141, 169, 170, 202, 219, 220, 244, 265, 275, 280, 305, 307, 311, 318, 320, 324, 325, 326, 327, 329, 331, 333, 338, 339, 340, 347], "raw_path": [318, 320], "ray_actor_opt": 16, "ray_data_synthet": [137, 141], "ray_dedup_log": [14, 275, 282], "ray_enable_windows_or_osx_clust": [133, 136], "ray_pl_ckpt": [305, 309, 312, 316], "ray_result": [14, 275, 282], "ray_scheduler_ev": [13, 14, 16], "ray_train_v2_en": [305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "rayddp": [305, 309], "rayddpstrategi": [6, 235, 236, 239, 305, 309, 312, 316], "raylightningenviron": [6, 235, 236, 239, 305, 307, 309, 312, 314, 316], "rayproject": [82, 94], "rayserv": [291, 292, 303], "raytaskerror": [3, 159, 163], "raytrainreportcallback": [6, 235, 236, 239, 305, 306, 309, 312, 313, 316, 317, 325, 326, 327, 328, 329, 330], "raytrainwork": [13, 275, 282], "raytrainxgboosttrain": [4, 147, 148, 150], "rayturbo": [10, 11, 184, 186, 196, 198], "rbac": [88, 101, 102], "rbi": [244, 263, 265], "rd": [17, 22, 244, 265, 325, 327], "rdata": [331, 333, 337, 338, 347], "re": [1, 3, 7, 10, 11, 15, 28, 31, 33, 35, 40, 42, 43, 46, 47, 53, 58, 59, 66, 75, 78, 79, 81, 84, 85, 86, 93, 96, 97, 98, 117, 121, 122, 125, 132, 151, 152, 159, 163, 184, 193, 196, 201, 202, 209, 210, 215, 227, 228, 229, 232, 244, 265, 267, 270, 305, 306, 307, 311, 312, 313, 317, 318, 319, 324, 325, 326, 327, 330, 331, 337, 338, 339, 340], "reach": [24, 26, 108, 109, 111, 202, 226, 267, 273], "read": [2, 3, 5, 6, 8, 11, 12, 13, 17, 22, 88, 101, 153, 158, 159, 162, 166, 167, 169, 170, 179, 181, 185, 188, 196, 201, 202, 204, 215, 216, 219, 235, 240, 267, 273, 274, 305, 307, 318, 320, 325, 327, 331, 333, 338, 339, 340, 341, 347], "read_csv": [5, 9, 13, 176, 179, 318, 320, 324, 331, 333], "read_databricks_t": [10, 184, 188], "read_imag": [10, 15, 16, 184, 188, 190, 193], "read_json": [9, 176, 179], "read_parquet": [4, 6, 9, 10, 12, 142, 144, 147, 150, 176, 179, 182, 184, 188, 202, 219, 235, 238, 305, 307, 318, 320, 325, 327, 338, 347], "read_row_group": [338, 341], "read_tabl": [331, 333, 338, 341], "readabl": [9, 176, 179, 202, 212], "readfil": [10, 184, 192], "readi": [1, 3, 28, 31, 35, 40, 43, 46, 47, 53, 58, 59, 66, 75, 79, 80, 81, 86, 90, 92, 93, 98, 106, 108, 109, 116, 117, 119, 124, 125, 128, 129, 132, 151, 152, 159, 167, 202, 210, 220, 228, 244, 265, 275, 280, 312, 314, 317, 318, 319, 320, 331, 333, 338, 339, 341], "readm": [142, 145, 260, 268, 276, 300, 348], "ready_ref": [3, 159, 167], "real": [8, 108, 109, 111, 125, 130, 132, 169, 173, 244, 265, 267, 271, 275, 280, 313, 317, 318, 319, 324, 325, 330, 331, 337, 338, 339], "realist": [267, 274, 305, 306, 318, 320, 338, 347], "realiti": [267, 270], "realknowncaus": [331, 333], "realli": [244, 265, 267, 270, 273, 274], "reason": [83, 95, 108, 109, 112, 114, 117, 119, 125, 131, 137, 139, 312, 317], "reasoning_pars": [125, 130], "reassur": [338, 341], "rebuild": [0, 202, 215, 318, 324], "rec": [305, 307, 331, 333, 338, 340], "rec_sys_tutori": [318, 320, 322, 324], "recal": 16, "recalcul": [108, 109, 112], "recap": [7, 14, 229, 233], "receiv": [5, 6, 8, 24, 26, 125, 128, 130, 169, 175, 202, 203, 235, 239, 285, 292, 301, 318, 319, 320, 322, 331, 333], "recent": [83, 95, 202, 214, 226, 267, 274, 305, 309, 312, 316, 318, 323, 331, 337, 338, 343, 345], "recent_kei": [83, 95], "recent_nam": [83, 95], "recip": [5, 244, 265], "recipi": 146, "reclaim": [305, 311, 318, 324], "recommend": [0, 4, 5, 6, 7, 8, 9, 10, 11, 17, 22, 24, 26, 82, 94, 108, 109, 110, 117, 118, 126, 128, 129, 142, 143, 147, 148, 169, 173, 176, 177, 180, 184, 185, 195, 196, 197, 201, 202, 204, 212, 228, 229, 230, 235, 236, 238, 321], "recomput": [318, 324], "record": [4, 9, 12, 133, 135, 147, 150, 176, 179, 305, 307, 331, 333, 338, 340, 345], "recov": [10, 184, 186, 202, 222, 305, 306, 325, 330, 331, 332, 337], "recoveri": [202, 212, 222, 224, 228, 305, 306, 312, 317, 318, 319, 322, 331, 335, 338, 339, 347], "recreat": [16, 331, 337], "recurr": [331, 332], "recurs": [28, 33, 43, 51, 53, 64, 202, 227], "red": [108, 109, 112, 244, 265, 267, 274, 305, 306, 338, 339], "redefin": [4, 147, 150], "redeploi": [312, 313], "redi": [17, 21], "redshift": [8, 169, 170], "reduc": [4, 7, 8, 9, 10, 11, 82, 94, 108, 109, 110, 125, 127, 129, 147, 148, 169, 170, 176, 177, 184, 185, 189, 190, 196, 197, 229, 230, 244, 259, 266, 275, 281, 331, 337], "reduct": [117, 123], "redund": [108, 109, 112, 202, 212, 338, 347], "ref": [2, 3, 153, 157, 158, 159, 161, 162, 165, 167, 168], "refer": [2, 3, 6, 9, 10, 13, 15, 17, 22, 28, 29, 35, 37, 43, 44, 53, 54, 55, 66, 67, 68, 81, 83, 88, 93, 95, 101, 108, 109, 111, 133, 135, 136, 142, 145, 153, 155, 157, 159, 161, 162, 163, 165, 176, 179, 184, 190, 194, 202, 214, 215, 235, 238, 275, 279], "reflect": [8, 169, 170, 338, 345], "refresh": [267, 273], "reg": [4, 147, 150], "regard": [244, 265], "regardless": [5, 6, 235, 238, 331, 335], "region": [12, 13, 14, 17, 23, 28, 30, 35, 38, 39, 43, 45, 46, 48, 53, 56, 58, 60, 66, 69, 70, 72, 76, 83, 95, 125, 128], "regist": [20, 22, 24, 26, 29, 30, 33, 34, 36, 39, 41, 44, 45, 49, 52, 54, 56, 61, 65, 67, 70, 82, 88, 91, 94, 101, 133, 135, 142, 143, 289, 290, 291, 292, 303, 305, 311, 318, 324, 338, 347], "register_buff": [331, 334], "register_us": 146, "registr": [35, 39, 42, 66, 78, 79], "registration_complet": 146, "registri": [338, 347], "regress": [7, 14, 229, 233, 325, 330], "regular": [2, 84, 96, 153, 156, 202, 205, 318, 324, 338, 341, 347], "reimplement": [202, 217], "reinforc": [8, 169, 173], "rel": [0, 5, 331, 337], "rel_path": [125, 128], "relat": [43, 51, 53, 57, 64, 84, 96, 146], "relationship": [7, 14, 17, 19, 22, 229, 233], "releas": [1, 28, 30, 43, 45, 51, 53, 56, 64, 151, 152, 202, 215, 244, 265, 267, 270, 312, 317], "relev": [5, 9, 10, 15, 125, 128, 176, 182, 183, 184, 193, 318, 324, 331, 335, 338, 341], "reli": [8, 11, 169, 170, 173, 175, 196, 198, 202, 216, 325, 327, 338, 340], "reliabl": [5, 6, 8, 10, 85, 86, 89, 90, 97, 98, 103, 106, 125, 127, 129, 132, 169, 173, 184, 186, 202, 203, 222, 228, 235, 237, 338, 343], "religi": [244, 265], "religion": [244, 265], "reload": [11, 125, 128, 196, 201, 202, 215, 224, 225, 318, 324], "relpath": [125, 128], "relu": [13, 305, 308, 312, 315], "remain": [117, 119, 137, 141, 202, 205, 212, 244, 263, 265, 266, 305, 307, 318, 320, 331, 335], "remaind": [318, 320], "remark": [244, 265], "remast": [244, 265], "remateri": [325, 327], "rememb": [84, 85, 86, 96, 97, 98, 117, 123, 202, 212, 267, 274, 305, 311], "remind": [244, 265, 267, 274], "remot": [1, 3, 4, 7, 10, 11, 15, 16, 81, 84, 85, 89, 93, 96, 97, 104, 146, 147, 150, 151, 152, 154, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 184, 188, 196, 200, 202, 215, 229, 232, 325, 326, 329, 330, 338, 347], "remote_add": [2, 3, 153, 156, 157, 159, 162, 165, 166], "remote_funct": [2, 153, 157], "remote_path": [10, 11, 184, 191, 196, 200], "remov": [1, 3, 6, 9, 43, 51, 53, 64, 108, 109, 114, 151, 152, 159, 168, 176, 181, 202, 204, 212, 227, 235, 238, 318, 324, 338, 347], "remove_code_output": 0, "remu": [244, 263, 265, 266], "renam": [81, 93, 325, 327, 331, 333], "renew": [24, 27], "rent": [267, 270], "repackag": [312, 313], "repartit": [9, 137, 141, 176, 182, 243, 244, 250, 261, 263, 266], "repeat": [13, 14, 275, 282, 325, 329, 331, 334], "repeatedli": [244, 253, 264], "replac": [28, 29, 30, 31, 35, 36, 38, 40, 41, 43, 44, 45, 47, 48, 53, 54, 56, 57, 59, 60, 62, 66, 67, 69, 72, 75, 76, 81, 83, 86, 93, 95, 98, 108, 109, 112, 114, 125, 128, 137, 141, 202, 216, 217, 220, 244, 265, 267, 274, 305, 311, 312, 313, 317, 318, 324, 331, 337, 338, 347], "replic": [3, 5, 6, 159, 161, 202, 203, 212, 235, 239], "replica": [8, 16, 86, 98, 108, 109, 114, 115, 120, 125, 128, 142, 145, 146, 169, 175, 198, 202, 203, 289, 290, 291, 292, 299, 303, 304, 338, 347], "replica_handle_request": [142, 145, 146], "repo": [0, 43, 46, 48, 53, 58, 60, 66, 73, 76, 108, 109, 115, 125, 128], "repo_id": [125, 128], "report": [6, 7, 8, 12, 14, 16, 169, 171, 203, 204, 205, 206, 214, 217, 221, 223, 224, 228, 229, 232, 233, 235, 239, 244, 265, 305, 306, 308, 309, 311, 312, 313, 314, 318, 319, 320, 322, 325, 328, 331, 332, 335, 337, 338, 339, 343, 345, 347], "report_metrics_torch": [5, 13], "reportedli": [244, 265], "repositori": [0, 66, 73, 86, 98], "repres": [4, 7, 9, 12, 108, 109, 113, 147, 150, 176, 179, 229, 231, 267, 274, 318, 319, 321], "represent": [108, 109, 111, 113], "reproduc": [1, 151, 152, 260, 268, 276, 300, 305, 307, 318, 320, 325, 326, 327, 338, 341], "republican": [244, 265], "req": [81, 93], "request": [2, 4, 8, 10, 11, 12, 16, 24, 26, 27, 108, 109, 111, 112, 114, 120, 123, 125, 128, 129, 130, 142, 145, 147, 148, 150, 153, 158, 160, 166, 169, 175, 184, 191, 196, 197, 198, 199, 200, 202, 215, 285, 291, 301, 303, 331, 333, 337, 338, 347], "request_data": 12, "requir": [0, 3, 5, 6, 8, 9, 10, 11, 15, 17, 19, 20, 21, 22, 24, 26, 27, 37, 39, 46, 52, 58, 65, 68, 74, 78, 79, 82, 83, 85, 88, 89, 91, 94, 95, 97, 101, 103, 112, 114, 117, 119, 120, 121, 124, 129, 130, 146, 159, 165, 166, 169, 170, 173, 176, 182, 184, 192, 193, 196, 198, 199, 202, 203, 209, 216, 224, 235, 236, 237, 244, 256, 260, 265, 268, 275, 276, 280, 281, 300, 305, 306, 312, 313, 318, 320, 331, 333, 338, 339, 343, 345], "rerun": [325, 330], "res18": [202, 221], "resampl": 332, "rescal": [305, 311], "research": [117, 119, 124, 125, 129, 243, 244, 261, 275, 281], "reserv": [2, 3, 4, 5, 6, 7, 8, 9, 10, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 108, 109, 110, 117, 118, 125, 126, 133, 134, 137, 138, 142, 143, 147, 148, 153, 154, 159, 160, 165, 169, 170, 176, 177, 184, 185, 229, 230, 235, 236, 243, 244, 261, 267, 269, 275, 277, 285, 292, 301], "reset": [13, 202, 206, 312, 314], "reshap": [6, 10, 184, 186, 235, 238], "resid": [331, 337], "residu": [331, 332], "resili": [86, 90, 98, 106, 117, 122, 202, 222, 224, 228, 312, 313, 325, 326, 331, 332], "resiz": 339, "resnet": [13, 203, 204, 209, 215, 305, 311, 338, 339], "resnet18": [5, 7, 13, 14, 202, 204, 205, 212, 214, 229, 230, 232, 233, 338, 340, 343, 347], "resolut": [6, 235, 238, 239], "resolv": [5, 53, 63], "resourc": [5, 6, 7, 8, 11, 12, 13, 14, 16, 18, 19, 22, 23, 24, 25, 26, 27, 33, 34, 36, 37, 42, 50, 51, 52, 57, 62, 63, 64, 65, 68, 78, 79, 80, 82, 84, 85, 86, 88, 90, 92, 94, 96, 97, 98, 101, 102, 107, 111, 112, 119, 131, 133, 135, 142, 144, 160, 166, 168, 169, 174, 175, 186, 196, 198, 199, 202, 205, 208, 213, 215, 221, 224, 226, 229, 233, 235, 239, 243, 244, 261, 267, 272, 275, 277, 281, 282, 312, 313, 317, 331, 332, 338, 339], "resources_per_work": [275, 281, 325, 328], "resp": [142, 145], "respect": [267, 273, 274], "respond": [125, 128], "respons": [11, 16, 24, 27, 82, 83, 94, 95, 108, 109, 111, 113, 115, 117, 121, 122, 125, 127, 128, 129, 130, 142, 145, 146, 196, 200, 285, 292, 296, 301, 304], "response_format": [125, 129], "rest": [43, 51, 53, 64, 202, 205, 217, 223, 267, 274, 285, 292, 301, 318, 320, 338, 340], "restart": [1, 82, 83, 94, 95, 125, 128, 151, 152, 202, 204, 222, 224, 225, 292, 296, 304, 312, 316, 317, 318, 322, 325, 326], "restor": [214, 222, 223, 224, 228, 267, 274, 305, 306, 311, 318, 323, 324], "restored_train": [202, 226], "restrict": [87, 99], "result": [1, 4, 7, 10, 12, 14, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 81, 84, 85, 89, 93, 96, 97, 104, 125, 130, 147, 150, 151, 152, 154, 158, 160, 161, 162, 168, 184, 186, 203, 204, 205, 212, 213, 215, 226, 228, 229, 233, 243, 244, 261, 267, 273, 274, 275, 280, 281, 282, 289, 290, 291, 292, 296, 303, 304, 305, 309, 310, 312, 313, 316, 318, 320, 322, 323, 324, 325, 328, 330, 335, 336, 338, 340, 344, 345, 346, 347], "resum": [5, 6, 203, 212, 222, 223, 224, 225, 228, 235, 237, 306, 309, 311, 312, 313, 316, 319, 322, 324, 325, 326, 328, 330, 335, 337, 338, 339, 343, 346], "resume_from_checkpoint": [318, 323], "retain": [84, 96, 137, 140, 325, 328, 331, 335], "retent": [267, 274, 338, 339], "rethink": [267, 274], "retrain": [312, 317, 318, 319, 325, 330, 331, 337, 338, 347], "retri": [5, 6, 10, 89, 103, 160, 184, 190, 203, 222, 225, 226, 228, 235, 237, 305, 309, 312, 313, 318, 319, 322, 325, 328, 338, 339, 343, 344], "retriev": [2, 3, 153, 157, 159, 168, 202, 215, 218, 318, 322, 325, 328], "retry_except": [3, 159, 163], "return": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 81, 84, 85, 89, 93, 96, 97, 104, 125, 129, 130, 137, 141, 142, 144, 147, 150, 151, 152, 153, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 176, 180, 184, 190, 191, 193, 196, 200, 201, 202, 205, 206, 209, 210, 211, 214, 215, 218, 220, 226, 229, 231, 233, 235, 238, 244, 251, 252, 253, 256, 264, 265, 267, 274, 275, 279, 280, 289, 290, 291, 292, 303, 305, 307, 308, 311, 312, 314, 315, 317, 318, 319, 320, 321, 325, 327, 328, 329, 330, 331, 333, 334, 337, 338, 341, 342, 347], "reus": [3, 10, 15, 17, 22, 108, 109, 112, 159, 161, 184, 191, 202, 204, 215, 244, 253, 264, 325, 327, 329, 330, 331, 337], "reusabl": [331, 332], "reveal": [318, 320], "reveng": [267, 273], "revers": [325, 329, 331, 333, 338, 347], "review": [10, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 83, 95, 184, 188, 267, 269, 272, 273, 274, 275, 277, 282], "rewrit": [5, 6, 202, 203, 235, 237, 318, 324, 338, 339], "rf": [4, 5, 6, 7, 9, 10, 12, 13, 15, 16, 147, 150, 176, 181, 183, 184, 195, 229, 234, 235, 240], "rg_idx": [338, 341], "rg_meta": [338, 341], "rgb": [202, 205, 215, 305, 306, 307, 338, 339, 341, 347], "rice": [338, 339], "rich": [8, 81, 93, 169, 173], "richer": [312, 317], "rick": [244, 265], "ricki": [244, 265], "ride": [4, 12, 147, 150, 267, 273, 331, 332, 337], "ridicul": [267, 274], "ridlei": [267, 274], "rifl": [267, 273], "riget": [267, 274], "right": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 16, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 81, 93, 108, 109, 110, 112, 117, 118, 125, 126, 131, 132, 133, 134, 137, 138, 142, 143, 147, 148, 153, 154, 158, 159, 160, 169, 170, 176, 177, 184, 185, 189, 196, 200, 202, 205, 212, 218, 229, 230, 235, 236, 243, 244, 261, 265, 267, 269, 273, 274, 275, 277, 285, 292, 301, 312, 313], "rightarrow": [325, 326], "rigid": [8, 169, 172], "rip": [267, 273], "rise": [338, 345], "risibl": [267, 270], "risk": [267, 274], "riskbr": [267, 274], "river": [244, 265, 267, 274], "riverboat": [267, 273], "rkn": [137, 141], "rllib": [8, 169, 173], "rm": [4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 16, 28, 33, 35, 42, 43, 51, 53, 64, 66, 78, 147, 150, 176, 181, 183, 184, 195, 196, 201, 229, 234, 235, 240], "rmse": [4, 7, 14, 147, 150, 229, 233, 318, 324], "rmtree": [202, 227, 305, 311, 312, 317, 318, 320, 324, 325, 330, 331, 337, 338, 347], "road": [325, 326], "roadmap": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 108, 109, 110, 117, 118, 125, 126, 142, 143, 147, 148, 153, 154, 159, 160, 169, 170, 176, 177, 184, 185, 196, 197, 202, 205, 229, 230, 235, 236], "roar": [267, 273], "robert": [267, 273], "robin": [24, 27, 244, 265], "robot": [312, 317], "robust": [0, 8, 125, 129, 169, 170, 202, 225, 228, 312, 313, 331, 337, 338, 339], "rock": [244, 263, 265], "role": [21, 23, 24, 26, 27, 28, 30, 34, 35, 37, 43, 45, 56, 63, 66, 68, 83, 95, 108, 109, 115, 117, 121, 122, 125, 128, 129, 130, 137, 140, 267, 274], "roll": [86, 90, 98, 106, 117, 122, 142, 145, 244, 265, 312, 314], "rollin": [244, 265], "rollout": [11, 142, 145, 196, 198, 312, 313], "roma": [244, 265], "roman": [267, 273], "ronda": [267, 273], "roof": [244, 265], "root": [2, 5, 6, 7, 13, 14, 83, 95, 125, 128, 133, 136, 142, 145, 146, 153, 158, 202, 203, 204, 210, 229, 231, 233, 235, 237, 318, 324], "roughli": [305, 307, 312, 314, 338, 340], "round": [24, 27, 244, 265, 267, 274, 318, 320, 325, 326, 328, 330], "rout": [3, 11, 16, 17, 22, 24, 26, 27, 108, 109, 113, 146, 159, 168, 196, 198, 291, 292, 303], "route_prefix": [4, 16, 86, 90, 98, 107, 125, 129, 147, 150], "row": [5, 6, 142, 144, 179, 188, 190, 202, 204, 214, 215, 219, 220, 235, 238, 243, 244, 248, 249, 250, 261, 263, 265, 266, 267, 269, 270, 271, 272, 273, 274, 305, 307, 309, 318, 319, 320, 322, 324, 325, 326, 327, 329, 331, 333, 337, 338, 339, 341, 345, 347], "row_group": [338, 341], "row_group_idx": [338, 341], "row_group_map": [338, 341], "royal": [244, 265], "rpc": [3, 8, 159, 168, 169, 170], "rstrip": [83, 95], "rubbish": [267, 274], "rubbl": [267, 273, 274], "rube": [267, 273], "ruin": [267, 274], "rule": [8, 17, 22, 82, 94, 125, 128, 169, 175, 244, 265], "rumor": [244, 265], "run": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 19, 21, 25, 27, 28, 30, 31, 32, 33, 34, 40, 41, 42, 43, 45, 46, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 61, 62, 64, 65, 66, 69, 75, 77, 78, 80, 81, 82, 83, 84, 86, 88, 91, 92, 93, 94, 95, 96, 98, 101, 105, 106, 108, 109, 110, 113, 115, 118, 119, 121, 123, 124, 125, 126, 127, 128, 129, 130, 133, 135, 136, 137, 139, 140, 141, 143, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 159, 160, 162, 164, 168, 169, 174, 176, 177, 178, 179, 181, 183, 184, 185, 186, 189, 191, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 210, 212, 213, 214, 216, 220, 221, 222, 225, 226, 227, 228, 229, 230, 232, 233, 234, 236, 238, 240, 243, 253, 261, 264, 265, 267, 269, 273, 274, 275, 277, 280, 281, 282, 285, 292, 296, 299, 301, 303, 304, 305, 306, 307, 309, 310, 311, 312, 313, 316, 317, 318, 319, 322, 323, 324, 325, 326, 328, 330, 332, 335, 336, 339, 341, 343, 344, 345, 346], "run_command": [81, 93], "run_config": [4, 5, 6, 12, 13, 147, 150, 202, 212, 213, 221, 224, 226, 235, 239, 305, 309, 312, 316, 318, 322, 325, 328, 331, 335, 338, 344], "runawai": [244, 265], "runconfig": [4, 5, 6, 12, 13, 147, 148, 150, 203, 204, 221, 224, 226, 228, 235, 239, 305, 307, 309, 312, 314, 316, 318, 320, 322, 325, 326, 327, 328, 331, 332, 333, 335, 338, 339, 340, 343, 344, 345, 347], "runnabl": [142, 143], "runnng": [84, 96], "runtim": [11, 80, 82, 92, 94, 125, 127, 128, 142, 145, 160, 196, 199, 275, 278, 292, 296, 303, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340, 348], "runtime_env": [3, 117, 120, 123, 125, 128, 129, 130, 159, 164, 165, 275, 278], "runtimeenv": [275, 278], "runwai": [9, 15, 176, 183], "ruse": [267, 273], "rust": [8, 169, 170], "ruth": [267, 273], "rw": 13, "ryan": [244, 265], "s3": [4, 8, 9, 10, 11, 12, 13, 14, 15, 16, 21, 23, 24, 26, 28, 30, 33, 34, 43, 45, 51, 53, 56, 57, 63, 64, 83, 95, 125, 128, 142, 144, 147, 150, 169, 170, 176, 179, 181, 182, 184, 188, 190, 191, 193, 196, 200, 202, 212, 228, 305, 311, 318, 324, 338, 347], "s3_bucket_id": [17, 23, 28, 30], "s3_f": [142, 144], "s3_kei": [83, 95, 125, 128], "s3_path": [6, 235, 238], "s3f": [6, 235, 236, 238], "s3filesystem": [6, 142, 144, 235, 238], "s5": [108, 109, 112], "s6": [108, 109, 112, 244, 265], "s7": [108, 109, 112], "s_": [312, 313], "s_k": [312, 313], "saatchi": [244, 265], "sacrif": [312, 313], "safe": [28, 30, 43, 45, 53, 56, 90, 106, 202, 204, 222, 244, 265, 305, 307, 331, 335, 338, 343], "safetensor": [125, 128], "safeti": [125, 129], "sai": [3, 17, 22, 85, 89, 97, 105, 159, 162, 244, 265, 267, 273, 274], "said": [244, 265, 267, 270], "sake": [9, 176, 180], "salad": [338, 339], "sam": [244, 265], "same": [1, 3, 4, 5, 6, 7, 9, 11, 12, 13, 14, 16, 24, 26, 28, 30, 43, 45, 53, 56, 83, 85, 95, 97, 103, 108, 109, 115, 117, 122, 124, 125, 132, 147, 150, 151, 152, 159, 164, 176, 180, 182, 196, 199, 200, 202, 205, 208, 209, 210, 212, 217, 220, 221, 223, 226, 228, 229, 233, 235, 238, 239, 244, 265, 266, 267, 270, 274, 305, 306, 307, 309, 312, 317, 318, 324, 325, 328, 330, 331, 337, 338, 341, 345, 347], "sampl": [4, 5, 6, 7, 12, 14, 15, 16, 17, 22, 28, 30, 34, 35, 39, 43, 45, 46, 53, 56, 58, 66, 70, 73, 83, 95, 133, 135, 147, 150, 203, 206, 207, 215, 229, 233, 235, 238, 267, 270, 274, 291, 292, 303, 307, 314, 318, 324, 325, 327, 331, 333, 338, 339, 340, 341, 347], "sample_act": [312, 317], "sample_batch": [12, 325, 329], "sample_count": [84, 96], "sample_idx": [5, 202, 204], "sample_imag": [305, 311], "sample_s": [6, 235, 238], "sampler": [5, 6, 13, 202, 203, 206, 217, 235, 239, 338, 343], "samsara": 16, "samsung": [244, 265], "san": [125, 130, 244, 265], "sander": [244, 265], "saniti": [202, 204, 213, 309, 312, 316, 318, 320, 325, 327], "sat": [244, 265], "satisfi": [3, 159, 161], "satur": [137, 141], "saturdai": [244, 265], "save": [3, 5, 6, 53, 63, 82, 83, 84, 94, 95, 96, 108, 109, 113, 125, 128, 159, 165, 203, 205, 206, 214, 217, 221, 222, 223, 226, 228, 235, 239, 267, 273, 305, 306, 307, 308, 311, 312, 313, 316, 317, 318, 322, 323, 324, 325, 326, 328, 330, 331, 332, 335, 338, 339, 340, 343, 347], "save_checkpoint_and_metrics_ray_train": [5, 13, 202, 206, 212, 217], "save_checkpoint_and_metrics_ray_train_with_extra_st": [202, 223, 224], "save_checkpoint_and_metrics_torch": [5, 13], "save_hyperparamet": [6, 235, 238], "save_last": [305, 309, 312, 316], "save_model": [4, 147, 150], "save_top_k": [305, 309, 312, 316], "saw": [244, 265], "sayhellodebuglog": [142, 145], "sayhellodefaultlog": [142, 145], "scaffold": [11, 196, 201], "scala": [8, 169, 173], "scalabl": [4, 8, 9, 11, 12, 16, 17, 22, 83, 85, 86, 88, 89, 90, 91, 95, 97, 98, 102, 103, 106, 111, 116, 117, 124, 125, 132, 147, 149, 169, 170, 173, 174, 176, 178, 196, 197, 199, 202, 216, 228, 243, 244, 259, 261, 266, 267, 269, 274, 275, 281, 282, 285, 292, 301, 305, 306, 312, 317, 318, 319, 320, 325, 326, 329, 330, 331, 332, 337, 338, 339, 341], "scalar": [3, 159, 168, 305, 308, 312, 313, 315], "scale": [1, 2, 3, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 22, 24, 25, 26, 27, 43, 50, 53, 62, 63, 79, 82, 84, 86, 88, 90, 91, 94, 96, 98, 102, 106, 108, 109, 110, 113, 114, 119, 120, 122, 125, 131, 137, 141, 151, 152, 153, 155, 159, 165, 169, 170, 173, 175, 176, 179, 183, 184, 190, 191, 196, 199, 203, 204, 205, 210, 213, 216, 220, 222, 224, 226, 228, 229, 233, 237, 238, 243, 259, 261, 266, 267, 269, 275, 277, 281, 285, 299, 301, 304, 305, 306, 307, 311, 314, 318, 319, 320, 321, 324, 325, 326, 327, 330, 331, 332, 333, 337, 338, 339, 340, 341, 347], "scaling_config": [4, 5, 6, 12, 13, 147, 150, 202, 208, 213, 221, 224, 226, 235, 239, 275, 281, 305, 309, 312, 316, 318, 322, 325, 328, 331, 335, 338, 344], "scalingconfig": [4, 5, 6, 12, 13, 147, 150, 203, 204, 228, 235, 239, 275, 278, 281, 305, 306, 307, 309, 312, 313, 314, 316, 318, 319, 320, 322, 325, 326, 327, 328, 331, 332, 333, 335, 338, 340, 344], "scan": [267, 273], "scari": [267, 274], "scenario": [3, 8, 16, 17, 22, 28, 30, 43, 45, 53, 56, 117, 119, 124, 159, 165, 167, 169, 170, 275, 280, 338, 339], "scene": [267, 270, 274, 318, 319], "schedul": [2, 3, 4, 5, 6, 7, 9, 10, 12, 14, 24, 26, 84, 96, 108, 109, 114, 147, 150, 153, 155, 157, 158, 159, 165, 166, 168, 176, 181, 184, 190, 191, 229, 233, 235, 238, 244, 265, 275, 282, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 339, 347], "schema": [8, 9, 125, 129, 130, 169, 170, 176, 179, 244, 263, 266, 267, 271, 272, 305, 307, 318, 320, 331, 333], "schemat": [5, 6, 202, 203, 235, 239], "schlong": [267, 270], "school": [244, 265], "schumer": [244, 265], "scienc": [8, 169, 173], "scientif": [8, 125, 131, 169, 173, 312, 314, 325, 327], "scikit": [285, 292, 301, 325, 327], "scipt": [142, 145], "scope": [17, 19, 83, 88, 95, 101, 137, 140], "score": [125, 128, 244, 265, 292, 296, 304, 305, 309, 311, 318, 319, 320, 324, 325, 328, 330, 331, 333, 335], "scoreless": [244, 263, 265], "scott": [267, 274], "scotu": [244, 265], "scratch": [7, 14, 28, 30, 43, 45, 53, 56, 229, 233, 305, 309, 312, 316, 338, 339, 347], "screen": [80, 81, 87, 92, 93, 99, 267, 274], "script": [0, 81, 84, 86, 88, 93, 96, 98, 101, 125, 128, 137, 141, 142, 145, 305, 311, 312, 317, 325, 326, 338, 339], "scroll": [53, 63], "scrumptiou": [28, 30, 43, 45, 53, 56], "sdk": [35, 37, 66, 68, 69, 85, 97], "sea": [244, 265], "seaborn": [325, 327], "seal": [267, 274], "seamless": [8, 83, 95, 108, 109, 114, 125, 130, 169, 170, 173, 174, 267, 269, 312, 313, 317, 331, 336, 337], "seamlessli": [1, 8, 9, 91, 151, 152, 169, 174, 176, 183, 202, 217, 224, 243, 244, 261, 305, 306, 312, 313, 318, 324, 325, 326, 330, 331, 332, 333, 338, 339], "search": [4, 7, 12, 14, 17, 22, 84, 96, 147, 150, 229, 230, 233, 305, 311, 312, 317, 325, 330, 331, 337, 338, 347], "search_alg": [7, 14, 229, 233], "season": [331, 333], "seattl": [267, 273], "second": [0, 2, 3, 7, 12, 14, 53, 57, 66, 69, 117, 120, 142, 144, 145, 153, 158, 159, 162, 229, 233, 244, 256, 265, 269, 325, 330, 338, 339, 346], "secondarili": [10, 15, 184, 192], "secret": [17, 22, 244, 265, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "section": [0, 7, 14, 17, 20, 21, 24, 26, 81, 82, 84, 93, 94, 96, 133, 135, 136, 142, 144, 145, 202, 204, 229, 232], "secur": [19, 21, 23, 24, 26, 28, 30, 34, 35, 39, 53, 63, 79, 88, 102, 108, 109, 113, 114, 117, 122, 125, 132], "security_group_descript": [17, 22], "security_group_id": [17, 23, 28, 30], "security_group_nam": [17, 22], "securitygroup": [17, 21, 22], "sedan": [125, 129], "see": [1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 22, 24, 25, 28, 30, 32, 35, 39, 41, 43, 45, 50, 53, 56, 62, 63, 66, 69, 77, 83, 86, 87, 88, 95, 98, 99, 101, 108, 109, 113, 115, 117, 120, 122, 125, 128, 129, 130, 133, 136, 137, 141, 142, 144, 145, 151, 152, 153, 158, 159, 162, 168, 176, 181, 182, 183, 184, 193, 195, 196, 201, 202, 206, 208, 213, 214, 222, 229, 232, 234, 235, 238, 239, 244, 260, 263, 265, 267, 268, 270, 273, 274, 275, 276, 282, 291, 292, 296, 300, 303, 304, 305, 306, 311, 312, 313, 331, 335, 337, 338, 339], "seed": [305, 311, 312, 314, 318, 320, 338, 341], "seek": [305, 307], "seem": [267, 274], "seen": [117, 124, 244, 265, 267, 270, 274], "segment": [15, 17, 22, 331, 337], "seiz": [267, 270], "select": [1, 5, 28, 30, 43, 45, 53, 56, 80, 81, 86, 87, 92, 93, 98, 99, 117, 123, 130, 132, 142, 144, 151, 152, 202, 204, 275, 277, 280, 318, 324, 325, 328, 331, 337, 338, 345], "select_column": [4, 147, 150], "selector": [87, 99], "self": [3, 4, 6, 10, 11, 12, 15, 16, 142, 145, 147, 150, 159, 168, 184, 191, 196, 200, 202, 215, 235, 238, 244, 251, 252, 253, 264, 289, 290, 291, 292, 303, 305, 308, 309, 312, 315, 316, 318, 321, 325, 329, 331, 332, 333, 334, 337, 338, 339, 341, 347], "sell": [267, 273], "semant": [3, 159, 166, 168], "semi": [8, 169, 170], "send": [3, 12, 16, 125, 129, 130, 137, 141, 142, 145, 146, 159, 168, 285, 291, 301, 303], "send_welcome_email": 146, "sens": [4, 7, 12, 14, 147, 148, 229, 233, 267, 273, 274], "sent": [8, 142, 145, 146, 169, 175, 202, 215, 267, 273], "sentenc": [125, 128, 131, 202, 208, 243, 244, 247, 253, 256, 261, 262, 264, 265], "sentence_transform": [244, 247, 262], "sentencetransform": [243, 244, 247, 251, 252, 253, 256, 259, 261, 262, 264, 265, 266], "sentiment": [244, 250, 263, 285, 289, 290, 291, 292, 296, 299, 301, 303, 304], "sep": [318, 320, 324], "separ": [2, 3, 5, 7, 8, 11, 14, 83, 88, 95, 102, 133, 136, 153, 157, 159, 166, 169, 170, 196, 199, 202, 210, 229, 233, 318, 319, 325, 327], "sept": [244, 265], "sequenc": [9, 108, 109, 112, 176, 179, 267, 274, 275, 277, 278, 280, 282, 325, 326, 333, 334], "sequenti": [7, 13, 14, 108, 109, 111, 229, 232, 305, 308, 312, 315, 331, 332], "sequoia": [133, 136], "seri": [79, 333, 337], "serial": [2, 8, 153, 155, 169, 170, 173, 325, 328, 338, 339], "serializ": [305, 307], "series_id": [331, 333], "serious": [267, 270, 274], "serv": [12, 17, 19, 20, 28, 29, 43, 44, 53, 54, 86, 87, 90, 98, 99, 106, 107, 112, 116, 119, 122, 123, 124, 127, 129, 130, 132, 133, 136, 137, 140, 143, 173, 174, 200, 201, 202, 215, 228, 260, 268, 276, 291, 296, 300, 302, 303, 305, 311, 312, 317, 318, 320, 321, 324, 325, 330, 331, 337, 338, 347], "serve_llama": [108, 109, 115], "serve_llama_3_1_70b": [117, 120, 121, 122, 123], "serve_my_lora_app": [125, 128], "serve_my_qwen": [125, 129], "serve_my_qwen3": [125, 130], "serveclass": [289, 290], "server": [0, 108, 133, 136], "serverless": [17, 20], "servic": [7, 8, 17, 21, 22, 24, 26, 27, 28, 30, 35, 36, 38, 39, 43, 45, 53, 56, 63, 66, 69, 70, 73, 78, 82, 83, 84, 87, 88, 94, 95, 96, 99, 101, 108, 109, 112, 115, 118, 121, 123, 124, 142, 145, 146, 169, 175, 197, 198, 229, 234, 260, 268, 276, 285, 292, 300, 301, 348], "session": [83, 88, 95, 101, 146], "session_2024": [12, 13], "session_2025": [275, 282], "session_latest": [133, 136, 142, 145, 146], "set": [0, 2, 3, 5, 6, 7, 10, 11, 12, 13, 16, 17, 18, 21, 24, 27, 28, 30, 34, 35, 38, 39, 43, 45, 46, 48, 52, 53, 56, 58, 60, 65, 66, 69, 70, 76, 80, 82, 86, 88, 90, 92, 94, 98, 100, 107, 118, 123, 124, 126, 128, 129, 134, 142, 145, 153, 158, 159, 164, 165, 184, 190, 196, 200, 201, 202, 203, 204, 205, 208, 210, 213, 215, 222, 229, 231, 233, 235, 237, 238, 239, 244, 265, 267, 272, 273, 274, 275, 277, 280, 281, 282, 291, 292, 303, 305, 307, 318, 320, 322, 325, 328, 331, 332, 335, 338, 339, 341], "set_epoch": [5, 13, 202, 206, 217, 338, 343], "set_float32_matmul_precis": [312, 317], "set_grad_en": [275, 280, 331, 337, 338, 347], "set_index": [331, 333], "set_titl": [7, 13, 14, 229, 231, 305, 307, 338, 340], "seth": [244, 265], "setup": [6, 8, 12, 17, 18, 20, 24, 25, 26, 43, 51, 53, 64, 79, 82, 87, 91, 94, 99, 101, 102, 117, 119, 123, 124, 125, 130, 135, 142, 143, 145, 169, 173, 202, 205, 213, 221, 223, 224, 226, 235, 238, 239, 260, 268, 276, 277, 282, 300, 309, 313, 320, 322, 327, 330, 333, 340, 348], "seven": [108, 109, 112], "sever": [8, 17, 21, 84, 96, 108, 109, 112, 113, 114, 169, 173, 267, 273, 285, 292, 301], "sevigni": [267, 270], "sex": [244, 265, 267, 270], "sexist": [244, 265], "sg": [17, 22, 28, 30], "sgd": [275, 280], "sh": [1, 43, 44, 53, 55, 66, 68, 151, 152, 305, 307], "shallow": [325, 326], "shame": [267, 274], "shape": [3, 6, 9, 10, 15, 16, 24, 26, 82, 83, 94, 95, 137, 141, 159, 165, 176, 181, 184, 190, 202, 215, 235, 238, 244, 265, 266, 305, 308, 312, 314, 317, 325, 327, 331, 333, 334, 337, 338, 341], "shard": [4, 5, 6, 13, 108, 109, 113, 147, 150, 202, 203, 205, 206, 208, 210, 212, 216, 217, 218, 219, 221, 224, 235, 239, 305, 306, 307, 309, 311, 312, 313, 314, 316, 317, 318, 319, 320, 322, 324, 325, 326, 328, 330, 331, 332, 333, 338, 347], "shard_0": [338, 340, 341], "share": [0, 3, 5, 8, 9, 11, 17, 21, 22, 24, 26, 28, 34, 35, 36, 53, 63, 87, 91, 99, 125, 128, 132, 133, 136, 159, 161, 169, 170, 176, 181, 196, 198, 202, 204, 212, 227, 244, 265, 308, 312, 315, 325, 326, 327, 331, 332, 333, 337, 338, 339], "shared_path": [83, 95], "shared_storag": [83, 95], "sharetea": [244, 265], "she": [244, 265, 267, 270, 273, 274], "sheeran": [244, 265], "shell": [10, 11, 66, 69, 184, 191, 196, 200], "sheriff": [267, 273], "shift": [81, 85, 86, 89, 90, 93, 97, 98, 105, 106, 325, 327, 331, 332, 335], "shine": [244, 265, 267, 273, 274], "shippuden": [244, 265], "shit": [244, 265], "shock": [267, 270], "shoe": [267, 274], "shoot": [244, 265, 267, 273], "shootout": [267, 273], "short": [267, 270, 325, 328, 331, 332], "shorter": [117, 123], "shot": [267, 270, 274], "should": [1, 3, 5, 6, 9, 10, 11, 13, 24, 26, 53, 63, 86, 90, 98, 107, 125, 128, 142, 144, 151, 152, 159, 167, 168, 176, 178, 182, 184, 192, 196, 200, 202, 205, 208, 235, 239, 244, 265, 267, 270, 273, 292, 296, 304, 305, 311, 312, 317, 318, 324, 325, 330, 338, 347], "should_checkpoint": 13, "show": [3, 4, 6, 7, 10, 14, 16, 28, 33, 35, 36, 43, 51, 53, 64, 84, 96, 108, 109, 112, 114, 125, 127, 142, 143, 146, 147, 150, 159, 165, 184, 189, 191, 192, 202, 203, 204, 212, 213, 214, 215, 217, 229, 233, 235, 238, 239, 243, 244, 248, 249, 250, 256, 260, 261, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 300, 305, 307, 309, 311, 312, 313, 316, 318, 320, 322, 325, 327, 329, 331, 333, 335, 337, 338, 339, 340, 345, 347], "showcas": [125, 126, 244, 256, 260, 265, 267, 268, 270, 274, 276, 285, 292, 300, 301], "shown": [142, 145, 202, 204, 267, 270], "shuffl": [5, 6, 7, 13, 14, 177, 185, 202, 206, 210, 216, 217, 218, 228, 229, 231, 233, 235, 238, 239, 275, 278, 280, 312, 314, 318, 320, 325, 330, 331, 333, 335, 338, 341, 342, 343], "shut": [123, 202, 212, 243, 244, 261, 267, 269, 275, 277], "shutdown": [12, 16, 117, 121, 123, 125, 128, 129, 130, 277, 285, 296, 301], "shutil": [202, 204, 227, 305, 307, 311, 312, 314, 317, 318, 320, 324, 325, 327, 330, 331, 333, 337, 338, 340, 347], "sick": [244, 265], "sid": [17, 22], "side": [8, 169, 174, 267, 273, 274, 305, 311, 318, 324, 338, 347], "sidebar": 0, "sidecar": [108, 109, 113], "sidestep": [305, 306], "sidewalk": [267, 273], "sight": [267, 270], "sign": [10, 11, 133, 135, 142, 143, 184, 191, 196, 200, 244, 265], "signal": [137, 139, 331, 333], "signatur": [6, 7, 14, 229, 233, 235, 239], "signifi": [9, 176, 179], "signific": [5, 6, 8, 108, 109, 113, 169, 174, 202, 203, 235, 237, 243, 244, 261], "significantli": [82, 94, 137, 140], "signup": 91, "silicon": [1, 151, 152, 243, 244, 256, 261, 265, 275, 277, 280], "silu": [6, 235, 238], "sim": [305, 306, 312, 313], "similar": [4, 12, 24, 27, 117, 120, 121, 133, 136, 142, 144, 147, 150, 305, 311], "similarli": [3, 8, 159, 168, 169, 171, 291, 292, 303, 318, 320], "simpl": [2, 4, 5, 6, 7, 8, 9, 11, 12, 14, 16, 81, 83, 86, 87, 93, 95, 98, 99, 108, 109, 112, 115, 116, 117, 119, 125, 131, 143, 147, 150, 153, 155, 156, 169, 175, 176, 178, 180, 196, 200, 202, 204, 207, 213, 215, 229, 232, 233, 235, 238, 260, 267, 268, 273, 274, 276, 285, 289, 290, 291, 292, 300, 301, 303, 305, 306, 311, 318, 319, 320, 321, 324, 338, 339, 347], "simple_pipelin": [142, 144], "simpler": [117, 124, 125, 128], "simpli": [5, 285, 292, 301, 325, 330, 338, 346], "simplifi": [15, 35, 39, 80, 91, 92, 338, 339], "simul": [3, 146, 159, 163, 285, 301, 312, 313, 317, 318, 320, 338, 339], "simultan": [108, 109, 111, 137, 141], "sin": [312, 313, 314, 317, 331, 334], "sinc": [3, 9, 83, 95, 108, 109, 112, 137, 141, 142, 145, 159, 165, 176, 179, 202, 205, 212, 244, 265, 267, 274, 305, 307, 318, 323], "sing": [244, 265], "singl": [1, 3, 8, 9, 10, 12, 16, 108, 109, 111, 112, 113, 117, 119, 124, 125, 127, 128, 132, 142, 145, 151, 152, 159, 165, 169, 173, 176, 179, 184, 189, 202, 203, 204, 206, 215, 227, 236, 239, 244, 265, 267, 269, 274, 305, 306, 307, 312, 316, 318, 319, 325, 326, 327, 330, 331, 332, 338, 339, 340, 341], "single_gpu_mnist": 13, "sink": [4, 10, 147, 150, 184, 187], "sinusoid": [331, 334], "sisterlif": [244, 265], "sit": [244, 263, 265, 267, 270], "site": [0, 17, 22, 83, 95, 267, 270], "situat": [267, 274], "six": [108, 109, 112], "size": [5, 7, 9, 10, 13, 14, 80, 82, 84, 92, 94, 96, 108, 109, 112, 113, 116, 121, 123, 125, 128, 137, 141, 142, 145, 176, 180, 184, 186, 190, 202, 204, 205, 206, 207, 229, 231, 244, 250, 256, 263, 265, 267, 273, 275, 280, 281, 305, 308, 309, 311, 312, 314, 317, 318, 324, 328, 331, 333, 334, 337, 338, 340], "size_in_byt": [3, 159, 161], "sj": [244, 263, 265, 266], "skagwai": [267, 273], "skew": [10, 184, 186, 318, 320, 325, 327], "skill": [125, 128], "skip": [305, 307, 325, 326, 327, 338, 347], "sklearn": [4, 147, 148, 325, 327], "skylynn": [244, 265], "slack": [142, 145], "sleazi": [267, 273], "sleep": [2, 3, 142, 144, 153, 158, 159, 162, 167, 267, 274], "slice": [4, 147, 150, 305, 306, 318, 320, 325, 326, 328, 338, 339, 341], "slick": [267, 274], "slide": [244, 265, 332], "slim": [82, 94], "slip": [267, 273], "slo": [108, 109, 112], "slope": [325, 326], "slot": [202, 204, 331, 332], "slow": [11, 117, 119, 196, 198, 243, 244, 261], "slow_adjust_total_amount": [142, 144], "slower": [267, 272], "slowest": [9, 10, 15, 176, 182, 184, 193], "slowli": [267, 274], "slowyourrol": [244, 265], "sm": [244, 265], "small": [8, 9, 10, 15, 16, 84, 96, 108, 109, 113, 117, 119, 124, 125, 128, 131, 169, 170, 176, 179, 180, 184, 189, 193, 305, 306, 312, 317, 318, 320, 324, 331, 337, 338, 339], "small_siz": [275, 280], "small_unet_model_config": [6, 235, 238], "smaller": [0, 3, 17, 22, 125, 128, 131, 137, 141, 159, 165], "smallest": [80, 92], "smart": [108, 109, 114, 267, 273], "smith": [244, 263, 265, 266], "smoke": [202, 228], "smoothl1": [331, 335], "smoothl1loss": [331, 335], "smoothli": [267, 269], "sn": [325, 327, 329], "snake": [244, 265], "snap": [244, 263, 265], "snapshot": [83, 95, 305, 310, 325, 330], "snapshot_download": [125, 128], "snicker": [267, 274], "snippet": [5, 6, 13, 81, 83, 93, 95, 235, 239], "snowflak": [8, 10, 169, 170, 184, 188], "so": [0, 6, 7, 9, 13, 14, 80, 81, 82, 84, 86, 90, 92, 93, 94, 96, 98, 107, 125, 131, 133, 135, 137, 141, 176, 180, 181, 202, 203, 204, 205, 210, 211, 212, 213, 215, 216, 217, 219, 220, 222, 223, 224, 226, 229, 233, 235, 238, 244, 263, 265, 267, 270, 273, 274, 285, 292, 301, 305, 308, 312, 315, 318, 320, 325, 327, 328, 331, 332, 333, 335, 338, 339, 341, 347], "socket": [8, 169, 173], "softbal": [244, 265], "softmax": [325, 326], "softprob": [325, 328], "softwar": [117, 121, 125, 128, 133, 135], "soil": [325, 326, 329], "sole": [17, 22, 267, 274], "solid": [318, 324], "solut": [2, 3, 5, 6, 7, 8, 10, 11, 13, 14, 15, 17, 21, 108, 109, 110, 114, 116, 125, 132, 153, 158, 159, 168, 169, 174, 184, 186, 190, 196, 198, 202, 203, 216, 229, 233, 235, 237, 239, 275, 281, 331, 337], "solv": [108, 109, 113, 125, 131], "some": [4, 5, 6, 7, 8, 9, 10, 13, 15, 53, 63, 83, 84, 95, 96, 108, 109, 112, 117, 121, 125, 127, 129, 137, 140, 141, 142, 143, 147, 150, 169, 172, 173, 176, 182, 183, 184, 188, 189, 193, 229, 234, 235, 240, 244, 265, 267, 270, 271, 272, 273, 274], "someth": [3, 43, 47, 53, 59, 66, 75, 159, 162, 244, 265, 267, 273, 338, 339], "sometim": [3, 28, 30, 43, 45, 53, 56, 159, 165, 267, 274], "somewher": [3, 159, 168], "song": [244, 265], "sonnet": [125, 131], "soon": [3, 159, 167, 244, 265], "sophist": [7, 8, 14, 125, 127, 130, 132, 169, 174, 229, 233], "sorri": [244, 263, 265, 266], "sort": [8, 9, 83, 95, 169, 170, 176, 182, 267, 270, 318, 319, 320, 324, 325, 329, 338, 347], "sort_index": [318, 322, 331, 335, 338, 345], "soul": [244, 263, 265], "sound": [244, 265, 267, 274], "soup": [244, 265], "sourc": [0, 1, 2, 10, 17, 22, 66, 69, 91, 133, 136, 137, 140, 151, 152, 153, 155, 184, 188, 202, 216, 228, 267, 271, 273, 274], "south": [244, 265], "sox": [244, 265], "space": [3, 4, 7, 8, 14, 17, 22, 147, 150, 159, 167, 169, 171, 202, 227, 229, 233, 267, 273, 305, 311, 312, 317, 318, 319, 324, 325, 330, 331, 337, 338, 347], "span": [146, 244, 265, 331, 333], "spark": [9, 173, 176, 183], "spatial": [325, 326], "spawn": [202, 213, 215], "speak": [267, 274], "speci": [325, 330], "special": [8, 9, 117, 124, 125, 127, 128, 131, 132, 169, 170, 176, 181, 244, 263, 265, 267, 273], "specif": [3, 6, 9, 15, 24, 27, 35, 36, 39, 66, 70, 82, 83, 84, 87, 94, 95, 96, 99, 108, 109, 114, 117, 120, 125, 131, 133, 135, 136, 137, 139, 142, 145, 154, 155, 159, 165, 168, 176, 182, 193, 202, 213, 215, 235, 238, 239, 305, 306, 311, 325, 329, 330, 338, 341], "specifi": [1, 3, 5, 6, 7, 9, 10, 11, 13, 14, 16, 24, 26, 80, 81, 82, 86, 87, 90, 92, 93, 94, 98, 99, 107, 125, 128, 129, 146, 151, 152, 159, 163, 165, 176, 179, 184, 190, 191, 196, 199, 201, 202, 209, 221, 226, 229, 233, 235, 239, 275, 277, 280, 281, 282, 289, 290, 291, 292, 303, 318, 322], "specific": [202, 205], "speed": [4, 5, 6, 7, 8, 11, 14, 16, 83, 95, 125, 131, 147, 150, 169, 170, 196, 198, 202, 203, 229, 232, 235, 237, 267, 269, 274, 305, 307, 338, 340, 347], "speedup": [243, 244, 261], "speific": [84, 96], "spend": [267, 270], "spike": [108, 109, 113, 114, 117, 122], "spiki": [80, 82, 92, 94], "spill": [10, 15, 184, 192], "spillov": [84, 96], "spin": [10, 15, 84, 86, 96, 98, 106, 184, 191, 243, 244, 253, 261, 264, 325, 326], "split": [4, 10, 12, 15, 83, 95, 108, 109, 113, 117, 120, 147, 150, 184, 190, 193, 202, 203, 204, 206, 207, 217, 244, 250, 263, 265, 267, 270, 274, 306, 328, 329, 331, 333, 340, 347], "split_at_indic": [305, 307, 312, 314], "split_idx": [312, 314], "split_notebook": 0, "split_proportion": [318, 320], "spoil": [267, 273, 274], "spoiler": [267, 274], "spoken": [267, 273], "spot": [10, 82, 84, 94, 96, 184, 186, 244, 265, 325, 329], "spotifi": [7, 9, 176, 183, 229, 234], "sprai": [267, 274], "sprang": [267, 274], "spruce": [325, 326], "spur": [267, 273], "spy": [133, 136], "sql": [8, 169, 170, 174], "sqrt": [2, 7, 14, 153, 158, 229, 233, 331, 334], "sqrt_add": [2, 153, 158], "squad": [244, 265], "squar": [2, 3, 153, 158, 159, 162, 202, 204, 244, 265, 305, 306, 318, 319, 324], "square_ref": [3, 159, 162], "square_ref_1": [3, 159, 166], "square_ref_2": [3, 159, 166], "square_valu": [3, 159, 162], "squarederror": [4, 147, 150], "squeez": [7, 14, 229, 231, 305, 311, 312, 317, 318, 324, 331, 334, 335], "src": [331, 334], "ssh": [17, 20, 22, 338, 339], "ssl": [24, 27], "sso": [108, 109, 114], "sst": [289, 290, 291, 292, 303], "st": [17, 22, 244, 265], "stabil": [305, 311, 318, 322, 331, 333], "stabilityai": [6, 235, 238, 239], "stabl": [5, 13, 108, 109, 115, 236, 239, 240], "stablediffus": [6, 235, 238, 239], "stack": [6, 28, 32, 35, 39, 41, 43, 45, 50, 53, 56, 62, 66, 70, 77, 91, 137, 140, 141, 235, 238, 312, 314, 325, 327, 331, 332, 337, 338, 347], "stadium": [244, 265], "stage": [3, 5, 6, 8, 10, 142, 144, 159, 165, 169, 172, 184, 186, 190, 191, 202, 204, 212, 235, 238, 244, 265], "stagnant": [331, 335], "stai": [202, 203, 217, 244, 265, 305, 306], "stakehold": [331, 337], "standalon": [202, 205], "standard": [6, 8, 35, 39, 82, 85, 89, 94, 97, 103, 142, 145, 169, 170, 202, 204, 206, 210, 212, 220, 235, 238, 239, 267, 270, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 334, 338, 339, 340, 341], "stander": [267, 274], "stapl": [267, 270], "star": [244, 265, 267, 274, 312, 313, 318, 320], "stare": [267, 270], "starlett": [11, 12, 16, 196, 197], "start": [1, 3, 4, 5, 6, 9, 10, 11, 12, 13, 16, 17, 20, 28, 29, 32, 35, 36, 41, 43, 44, 50, 53, 54, 55, 62, 66, 67, 69, 77, 81, 82, 83, 84, 85, 89, 93, 94, 95, 96, 97, 103, 110, 112, 116, 117, 118, 121, 125, 126, 128, 131, 135, 137, 141, 142, 143, 144, 145, 147, 148, 150, 151, 152, 159, 160, 164, 165, 176, 177, 184, 185, 188, 190, 191, 196, 197, 202, 203, 204, 205, 208, 209, 213, 221, 222, 223, 225, 230, 235, 236, 238, 239, 243, 244, 261, 267, 269, 270, 273, 277, 280, 281, 285, 292, 296, 301, 304, 305, 306, 311, 312, 313, 317, 318, 320, 331, 333, 335, 338, 339, 340, 346], "start_epoch": [202, 223, 318, 322, 331, 335, 338, 343], "start_token": [331, 335], "starter": [86, 98], "startswith": [318, 324, 338, 347], "startup": [3, 24, 26, 117, 124, 133, 136, 159, 164], "starv": [267, 274], "state": [3, 4, 8, 11, 12, 17, 22, 28, 30, 43, 45, 53, 56, 81, 93, 108, 109, 114, 125, 130, 137, 139, 142, 144, 147, 150, 159, 168, 169, 173, 174, 185, 196, 198, 199, 222, 223, 226, 228, 244, 253, 264, 265, 267, 270, 274, 305, 311, 314, 315, 317, 318, 323, 325, 329, 331, 332, 337, 338, 347], "state_dict": [5, 13, 202, 212, 215, 224, 305, 311, 312, 317, 318, 322, 324, 331, 335, 337, 338, 343, 347], "stateless": [2, 10, 153, 156, 184, 191, 305, 307], "statement": [5, 17, 22], "static": [8, 10, 108, 109, 112, 169, 175, 184, 186], "station": [267, 274], "statist": [9, 137, 139, 176, 183], "stats_d": [325, 329, 330], "statu": [12, 13, 14, 16, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 85, 86, 97, 98, 105, 137, 141, 142, 144, 146, 244, 265, 275, 282], "std": [9, 10, 15, 176, 182, 184, 193, 202, 215, 331, 333, 337, 338, 341, 347], "stderr": [142, 145], "steadi": [305, 309], "steadili": [202, 214, 331, 335], "steak": [338, 339], "steal": [267, 273], "steam": [267, 270], "step": [2, 3, 5, 6, 7, 8, 10, 13, 14, 28, 29, 30, 35, 37, 39, 42, 43, 44, 45, 47, 48, 53, 54, 55, 56, 59, 60, 66, 67, 68, 71, 75, 76, 78, 86, 90, 91, 98, 107, 112, 126, 131, 137, 141, 142, 143, 153, 154, 156, 158, 159, 160, 169, 175, 184, 187, 190, 203, 205, 206, 207, 210, 214, 215, 217, 219, 220, 223, 229, 232, 233, 235, 238, 244, 265, 267, 269, 275, 280, 306, 313, 314, 319, 320, 322, 326, 327, 332, 333, 335, 339, 343], "step_size_hour": [331, 337], "sterl": [244, 265], "steven": [244, 265], "stewart": [267, 273], "still": [3, 5, 8, 13, 81, 93, 159, 162, 169, 174, 202, 212, 217, 218, 224, 226, 244, 265, 267, 273, 292, 299, 304, 305, 307, 331, 335, 338, 340], "stillkidrauhl": [244, 265], "stockholm": [267, 270], "stop": [1, 7, 14, 108, 109, 111, 151, 152, 202, 215, 229, 233, 267, 274, 292, 304, 305, 311, 325, 330, 331, 337, 338, 347], "storag": [6, 8, 9, 10, 15, 16, 17, 21, 22, 28, 34, 35, 36, 38, 39, 66, 69, 70, 80, 91, 92, 125, 128, 137, 140, 141, 169, 170, 173, 176, 178, 180, 181, 184, 188, 194, 203, 204, 205, 210, 213, 214, 221, 222, 224, 226, 228, 235, 238, 239, 312, 317, 320, 331, 332, 333, 338, 339, 340, 347, 348], "storage_fold": [4, 5, 9, 10, 11, 147, 150, 176, 181, 183, 184, 191, 194, 195, 196, 200, 201], "storage_path": [4, 5, 6, 12, 13, 147, 150, 202, 203, 212, 221, 224, 226, 235, 238, 239, 305, 309, 312, 316, 318, 322, 323, 325, 328, 331, 335, 338, 343, 344], "store": [6, 8, 10, 11, 12, 15, 17, 21, 84, 96, 133, 136, 142, 144, 167, 168, 169, 170, 174, 175, 184, 192, 193, 196, 198, 202, 204, 205, 212, 213, 214, 219, 220, 221, 224, 228, 235, 238, 239, 243, 244, 259, 261, 265, 266, 267, 270, 274, 305, 306, 309, 311, 312, 317, 318, 320, 322, 324, 325, 326, 327, 330, 331, 333, 335, 338, 339, 340, 343, 345], "stori": [267, 270, 274], "storylin": [267, 273, 274], "str": [4, 5, 6, 7, 10, 11, 13, 14, 15, 16, 125, 129, 130, 137, 141, 147, 150, 184, 190, 191, 193, 196, 200, 202, 212, 224, 229, 233, 235, 238, 244, 251, 252, 253, 264, 289, 290, 291, 292, 303, 325, 328, 331, 337, 338, 341, 342, 347], "strang": [267, 274], "stranger": [267, 273], "strategi": [6, 86, 90, 98, 106, 124, 235, 239, 305, 307, 309, 312, 316], "stratifi": [325, 327], "streak": [244, 263, 265], "stream": [4, 9, 10, 11, 15, 16, 108, 109, 113, 115, 117, 121, 122, 125, 128, 142, 145, 147, 150, 175, 176, 183, 184, 186, 196, 198, 202, 216, 217, 218, 220, 221, 228, 244, 263, 265, 266, 305, 306, 312, 313, 317, 318, 319, 320, 322, 324, 325, 326, 327, 328, 331, 333, 337], "streaming_split": [10, 184, 189], "streamlin": [17, 21, 24, 25, 85, 89, 97, 105], "street": [267, 273], "strength": [125, 128], "stretch": [244, 265], "strftime": [5, 13], "strict": [305, 311, 312, 317], "stride": [5, 7, 13, 14, 202, 205, 229, 232, 233, 331, 333], "strike": [267, 273, 274], "string": [35, 39, 43, 45, 48, 53, 56, 60, 66, 76, 84, 96, 125, 129, 130, 244, 263, 266, 267, 271, 272, 274, 338, 340], "strip": [318, 324, 331, 337, 338, 347], "strong": [108, 109, 113, 117, 119, 137, 141, 318, 321, 331, 333], "stronger": [117, 119], "structur": [10, 12, 100, 102, 109, 116, 126, 127, 128, 130, 132, 142, 145, 174, 183, 184, 188, 267, 270, 305, 306, 312, 313, 317, 318, 319, 331, 334, 335, 348], "stuck": [11, 137, 141, 196, 198], "student": [267, 270], "studi": [5, 6, 13, 235, 240, 267, 270], "studio": [81, 93, 244, 265, 267, 273], "stuff": [244, 265], "stun": [267, 273, 274], "stupid": [267, 274], "style": [0, 6, 202, 218, 228, 235, 238, 267, 274, 305, 311, 318, 320, 322, 331, 332, 338, 339], "sub": [3, 159, 166], "subdirectori": 0, "subfold": [6, 235, 238], "subject": [125, 131], "submiss": [7, 14, 85, 89, 97, 103, 229, 233], "submit": [7, 14, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 81, 84, 93, 96, 137, 141, 229, 232, 244, 265], "subnet": [20, 23, 28, 30, 34, 35, 36, 39, 66, 70, 79], "subnet_id": [17, 23, 28, 30], "suboptim": [10, 184, 186], "subplot": [7, 13, 14, 202, 204, 215, 229, 231, 305, 307, 311, 318, 320, 338, 340], "subprocess": [10, 11, 184, 185, 191, 196, 197, 200, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "subraman": [244, 265], "subsampl": [325, 330], "subsequ": [108, 109, 113, 133, 135, 136, 142, 145, 267, 274], "subset": [5, 7, 9, 10, 15, 87, 99, 176, 180, 184, 188, 189, 229, 233, 275, 280, 305, 306, 307, 318, 320, 325, 327, 331, 333, 338, 339, 340], "substanc": [267, 274], "substanti": [331, 335], "subtract": [305, 311], "subword": [108, 109, 111], "success": [7, 14, 146, 229, 233], "successfulli": [28, 31, 34, 35, 40, 43, 47, 52, 53, 59, 65, 66, 75, 82, 85, 94, 97, 117, 124, 125, 128, 312, 317], "suck": [244, 265], "sudo": [133, 136], "suffer": [11, 196, 198], "suffici": [117, 121, 305, 307], "suffix": [2, 153, 157], "suggest": [10, 125, 128, 184, 191, 325, 329], "suit": [8, 169, 170, 202, 203, 318, 321], "suitabl": [0, 267, 274, 275, 277, 305, 311, 318, 320], "sum": [3, 9, 10, 15, 84, 96, 159, 161, 176, 179, 182, 184, 189, 193, 267, 270, 318, 321, 322, 325, 329, 330], "sum_ref": [3, 159, 162], "sum_valu": [3, 159, 162], "summar": [108, 109, 112, 117, 124, 125, 128, 131, 132, 244, 259, 266], "summari": [118, 125, 128, 132, 243, 261, 277], "summer": [244, 263, 265, 267, 274], "summerslam": [244, 263, 265], "summit": [9, 10, 13, 15, 176, 183, 184, 195], "sun": [244, 265, 267, 273, 274], "sunbeam": [28, 30], "sunda": [244, 265], "sundai": [244, 263, 265], "super": [6, 235, 238, 244, 260, 265, 268, 276, 300, 305, 308, 312, 315, 318, 321, 331, 334], "superior": [267, 274], "supervis": [312, 314, 331, 333, 338, 339], "suppli": [267, 273, 312, 317], "support": [1, 3, 5, 6, 8, 9, 10, 11, 15, 22, 24, 26, 27, 91, 108, 117, 123, 125, 128, 129, 132, 133, 135, 137, 140, 142, 143, 145, 151, 152, 159, 165, 168, 169, 170, 173, 174, 175, 176, 183, 184, 186, 188, 193, 194, 196, 198, 201, 202, 203, 204, 218, 223, 224, 226, 235, 237, 267, 269, 275, 281, 285, 292, 301, 312, 313, 318, 323, 324, 325, 327, 330, 338, 343], "suppos": [244, 265], "suptitl": [305, 307, 311, 338, 340], "sur": [244, 265], "sure": [0, 5, 66, 78, 81, 93, 125, 128, 202, 227, 244, 263, 265, 266, 305, 311, 318, 324, 338, 341], "surfac": [142, 145], "surg": [331, 337], "surpris": [267, 274], "surprisingli": [3, 159, 168, 244, 265], "surround": [267, 270], "surviv": [244, 263, 265, 266, 312, 317], "sushi": [338, 339], "suspens": [267, 274], "suv": [125, 129], "swai": [267, 274], "swap": [82, 94, 305, 311, 312, 317, 338, 347], "swede": [267, 270], "swedish": [267, 270], "sweep": [305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "swing": [244, 265, 267, 270, 312, 313], "switch": [125, 127, 128, 202, 228, 318, 324], "switcher": 0, "sy": [2, 3, 153, 154, 159, 160, 161, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "sydow": [267, 274], "symbol": [1, 151, 152, 312, 313], "sync": [81, 93, 202, 203, 244, 265, 338, 339, 343], "sync_dist": [6, 235, 238, 305, 308, 312, 315], "sync_on_comput": [338, 343], "synchron": [3, 5, 13, 159, 161, 202, 203, 208, 209, 212, 224, 305, 309, 338, 343], "synthet": [137, 141, 312, 317], "synthetic_image_output": [137, 141], "system": [2, 3, 6, 8, 9, 10, 12, 14, 17, 22, 24, 26, 43, 46, 51, 53, 58, 63, 64, 66, 69, 83, 86, 90, 91, 95, 98, 106, 125, 127, 128, 129, 130, 133, 135, 136, 137, 139, 140, 142, 145, 153, 155, 159, 163, 169, 170, 173, 176, 181, 184, 190, 195, 202, 228, 235, 238, 324, 331, 336, 338, 339], "t": [1, 2, 3, 5, 8, 9, 10, 13, 15, 24, 26, 53, 63, 83, 91, 95, 108, 109, 113, 142, 144, 151, 152, 153, 158, 159, 162, 163, 164, 169, 174, 176, 180, 182, 184, 190, 193, 202, 203, 204, 208, 210, 215, 244, 263, 265, 267, 270, 273, 274, 305, 306, 307, 308, 311, 312, 313, 315, 317, 318, 320, 324, 331, 332, 333, 334, 335, 337, 338, 339, 340, 341, 342, 347], "t10k": [13, 14], "t4": [10, 12, 13, 14, 66, 71, 82, 94, 184, 191], "t_": [312, 313], "t_futur": [331, 337], "t_img": [305, 308], "t_past": [331, 337], "t_scale": [305, 308], "tab": [80, 81, 82, 83, 85, 86, 92, 93, 94, 95, 97, 98, 142, 144], "tabl": [7, 8, 10, 28, 30, 43, 45, 53, 56, 169, 170, 184, 188, 229, 233, 325, 328, 331, 333, 338, 340, 341], "tabular": [9, 15, 176, 179, 202, 219, 318, 319, 324, 327, 330, 338, 340], "tackl": [331, 332], "tag": [8, 28, 30, 43, 45, 53, 56, 169, 174, 318, 324, 325, 330], "tail": [318, 320], "tailor": [202, 205], "take": [3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 23, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 73, 125, 127, 142, 144, 147, 150, 159, 168, 176, 179, 180, 184, 189, 190, 196, 200, 204, 205, 212, 229, 232, 233, 235, 237, 239, 244, 256, 265, 267, 270, 273, 274, 275, 279, 307, 321, 327, 333, 340], "take_al": [318, 320], "take_batch": [9, 10, 12, 15, 16, 176, 180, 184, 189, 190, 191, 244, 256, 265, 325, 327], "takeawai": 110, "taken": [267, 270, 338, 347], "talent": [244, 265, 267, 274], "talk": [9, 10, 15, 125, 128, 176, 183, 184, 195, 244, 265, 267, 274], "taman": [244, 265], "tank": [244, 265], "target": [4, 5, 6, 10, 16, 17, 22, 125, 128, 147, 150, 184, 190, 202, 212, 219, 235, 238, 260, 268, 276, 300, 312, 314, 318, 319, 320, 325, 327, 330, 331, 333, 335], "target_num_rows_per_block": [137, 141], "target_ongoing_request": 16, "target_path": [305, 311, 312, 317, 318, 324], "task": [2, 4, 7, 8, 9, 10, 11, 13, 14, 15, 43, 44, 53, 55, 66, 68, 81, 82, 84, 93, 94, 96, 108, 109, 113, 117, 119, 128, 142, 144, 147, 150, 153, 155, 156, 157, 158, 161, 167, 168, 169, 172, 173, 174, 176, 179, 181, 184, 186, 188, 190, 191, 196, 198, 229, 232, 267, 269, 274, 275, 278, 281, 305, 307, 311, 312, 317, 318, 321, 324, 325, 329, 330, 338, 339], "task_id": [84, 96], "taskpoolmapoper": [10, 184, 192], "tatum": [267, 273], "tavakolian": [244, 265], "tax": [4, 12, 147, 150, 244, 265], "taxi": [4, 9, 142, 144, 147, 150, 176, 179, 182, 337], "taximet": [9, 176, 179], "taxiwindowdataset": [331, 333], "tb": [10, 184, 186], "tbh": [244, 265], "tbl": [325, 328], "tc": [125, 130], "tcm": [267, 273], "tcp": [17, 22], "td3": [312, 317], "tea": [244, 265], "teach": [305, 306, 312, 313], "teacher": [267, 270, 332, 334], "team": [8, 87, 88, 99, 100, 102, 169, 173, 244, 265, 338, 339], "tear": [85, 89, 97, 103], "teardown": [24, 26], "tech": [244, 265], "technic": [2, 153, 155, 267, 274], "techniqu": [7, 14, 229, 232, 267, 274], "technologi": [8, 169, 170], "ted": [244, 265], "teen": [244, 265], "telemetri": [133, 135, 137, 141], "tell": [4, 6, 117, 121, 122, 147, 150, 202, 208, 212, 235, 239, 244, 265, 267, 270, 273, 338, 344], "temp": [28, 30, 43, 45, 53, 56, 202, 204, 212, 331, 335, 338, 340, 343], "temp_checkpoint_dir": [5, 13, 202, 212, 224], "tempdir": [338, 343], "temperatur": [3, 125, 130, 159, 168], "tempfil": [5, 13, 202, 204, 212, 224, 305, 307, 309, 312, 316, 318, 320, 322, 325, 327, 331, 335, 338, 340, 343], "templat": [24, 27, 86, 98, 108, 109, 116, 119, 202, 228], "tempor": [312, 317], "temporari": [108, 109, 113, 202, 212, 312, 317, 318, 322, 325, 330, 331, 337, 338, 343, 347], "temporarydirectori": [5, 13, 202, 212, 224, 318, 322, 331, 335, 338, 343], "ten": [338, 339], "tenant": [125, 132, 318, 324], "tenni": [244, 265], "tensor": [5, 6, 10, 11, 13, 15, 16, 108, 109, 113, 117, 120, 124, 184, 191, 196, 200, 202, 210, 211, 215, 218, 220, 235, 238, 275, 278, 307, 308, 311, 312, 317, 318, 324, 331, 333], "tensor_parallel_s": [117, 120, 123, 125, 130], "tensorflow": [285, 292, 301], "term": [3, 8, 17, 21, 159, 162, 169, 175, 267, 270, 331, 332], "termin": [1, 5, 12, 13, 24, 27, 28, 31, 32, 33, 35, 40, 42, 43, 47, 50, 51, 53, 59, 62, 64, 66, 69, 75, 78, 81, 83, 84, 85, 86, 93, 95, 96, 97, 98, 106, 109, 115, 117, 120, 122, 142, 144, 146, 151, 152, 202, 204, 226, 275, 282, 312, 314], "terminologi": [88, 100], "terraform": [17, 21, 22, 23, 29, 31, 33, 34, 37, 40, 42, 44, 47, 51, 52, 55, 57, 59, 64, 65, 68, 71, 75, 78, 79], "terrain": [325, 330], "terribl": [267, 274], "test": [4, 7, 11, 12, 14, 16, 30, 39, 45, 56, 70, 79, 81, 86, 93, 98, 117, 121, 124, 125, 129, 130, 131, 132, 133, 136, 147, 150, 196, 198, 200, 201, 202, 228, 229, 231, 244, 265, 275, 280, 281, 285, 301, 338, 341], "test_job": [28, 32, 35, 41, 43, 50, 53, 62, 66, 77], "test_siz": [4, 12, 147, 150, 325, 327], "texan": [244, 265], "text": [6, 8, 53, 57, 83, 95, 112, 116, 125, 130, 137, 140, 169, 170, 235, 238, 243, 244, 248, 249, 250, 253, 256, 259, 261, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 277, 280, 289, 290, 291, 292, 296, 303, 304, 312, 313, 318, 319, 331, 332], "text_token": [267, 274], "textembedd": [244, 251, 252, 253, 256, 259, 264, 265, 266], "tf": [28, 30, 35, 39, 43, 45, 53, 56], "tfenv": [28, 29, 43, 44, 53, 55, 66, 68], "tfi": [244, 265], "tfvar": [28, 30, 43, 45, 53, 56], "tgt": [331, 334], "than": [5, 8, 10, 12, 81, 93, 108, 109, 111, 115, 117, 119, 125, 128, 137, 141, 169, 172, 174, 175, 184, 189, 190, 267, 270, 272, 273, 274, 318, 324, 325, 327, 329, 331, 332, 335], "thank": [125, 132, 146, 244, 265, 331, 335], "thats": [267, 274], "theater": [267, 270, 274], "thei": [1, 3, 8, 9, 10, 11, 15, 28, 30, 43, 45, 53, 56, 80, 82, 92, 94, 108, 109, 111, 113, 137, 141, 151, 152, 159, 164, 165, 167, 169, 173, 174, 176, 180, 184, 189, 195, 196, 199, 202, 207, 216, 222, 244, 265, 267, 270, 273, 274, 285, 292, 301, 325, 327], "them": [0, 2, 3, 5, 6, 8, 10, 17, 20, 23, 24, 27, 53, 63, 81, 83, 84, 87, 93, 95, 96, 99, 117, 122, 125, 128, 137, 141, 153, 155, 157, 159, 164, 166, 167, 168, 169, 175, 184, 191, 202, 204, 215, 235, 238, 244, 256, 265, 267, 274, 305, 307, 308, 311, 312, 313, 316, 317, 318, 319, 322, 324, 338, 339, 340, 341, 345], "theme": [0, 146], "themselv": [267, 273], "theoret": [267, 274], "therefor": [85, 89, 97, 103, 267, 270], "theta": [305, 306, 312, 313, 317, 325, 326, 331, 332, 338, 339], "theta_": [312, 313], "theta_dot": [312, 317], "thfc": [244, 265], "thi": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 34, 35, 36, 37, 38, 39, 43, 44, 45, 47, 50, 52, 53, 54, 56, 57, 59, 62, 63, 65, 66, 67, 68, 69, 70, 75, 79, 80, 81, 82, 83, 84, 87, 88, 89, 91, 92, 93, 94, 95, 96, 99, 100, 101, 103, 108, 109, 110, 111, 112, 114, 116, 117, 118, 119, 123, 125, 126, 127, 128, 130, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 153, 154, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 227, 229, 230, 232, 233, 234, 235, 236, 238, 239, 240, 243, 244, 253, 256, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 279, 280, 281, 282, 285, 291, 292, 299, 300, 301, 303, 304, 307, 309, 316, 320, 321, 322, 323, 327, 328, 333, 334, 335, 336, 340, 341, 342, 343, 344, 345, 348], "thine": [244, 265], "thing": [3, 125, 130, 159, 164, 244, 265, 267, 273, 274], "think": [2, 3, 125, 131, 153, 154, 159, 160, 243, 244, 261, 265, 267, 269, 273, 274, 275, 277, 285, 292, 301], "third": [7, 14, 53, 57, 133, 135, 229, 233, 244, 265, 267, 273, 274], "tho": [244, 265], "thoma": [244, 265], "thor": [244, 265], "those": [8, 43, 50, 53, 62, 84, 96, 169, 174, 267, 270, 274, 325, 329], "though": [82, 94, 108, 109, 112, 244, 265], "thought": [267, 270], "three": [2, 6, 9, 10, 15, 108, 109, 114, 115, 116, 137, 140, 153, 158, 176, 179, 184, 187, 188, 202, 228, 235, 238, 267, 274, 305, 311, 338, 344], "thriller": [267, 274], "throb": [267, 270], "through": [3, 4, 5, 6, 7, 8, 13, 14, 17, 22, 23, 24, 27, 28, 29, 43, 44, 53, 54, 66, 67, 80, 81, 91, 92, 93, 105, 108, 109, 111, 114, 116, 117, 118, 125, 127, 130, 137, 139, 140, 142, 145, 146, 147, 150, 159, 162, 169, 171, 174, 202, 204, 207, 228, 229, 230, 235, 238, 243, 244, 259, 261, 265, 266, 267, 273, 274, 305, 306, 312, 317, 325, 326, 328, 331, 332, 338, 339], "throughout": [1, 151, 152, 338, 340], "throughput": [8, 10, 108, 109, 112, 142, 144, 145, 169, 175, 184, 195, 202, 216, 228, 243, 244, 259, 261, 266, 325, 329, 338, 347], "throw": [43, 51, 53, 64, 244, 259, 266], "thru": [267, 270], "thu": [5, 24, 27], "thumb": [8, 169, 175], "thursdai": [244, 263, 265, 266], "ti": [5, 6, 202, 205, 235, 239], "ticket": [244, 263, 265], "tidi": [325, 330, 331, 333, 337, 338, 347], "tie": [244, 265], "tiger": [244, 265, 267, 274], "tight": [244, 265], "tight_layout": [202, 215, 305, 307, 309, 311, 312, 316, 318, 320, 322, 331, 333, 335, 337, 338, 340, 345], "tightli": [17, 22], "tild": [312, 313], "tilt": [8, 169, 172], "timber": [244, 265], "time": [2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 24, 26, 82, 84, 88, 94, 96, 101, 108, 109, 111, 112, 113, 117, 120, 124, 125, 130, 137, 141, 142, 144, 145, 147, 150, 153, 154, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 173, 175, 184, 190, 196, 198, 202, 203, 204, 216, 217, 224, 229, 232, 235, 237, 239, 243, 244, 261, 265, 267, 270, 273, 274, 275, 282, 305, 306, 312, 313, 316, 317, 318, 319, 322, 324, 325, 328, 330, 333, 335, 337, 338, 339, 346], "time_since_restor": 13, "time_this_iter_": 13, "time_total_": 13, "timedelta": [331, 333], "timelin": [3, 159, 167], "timeseri": [142, 144], "timeseriesbatchpredictor": [331, 337], "timeseriestransform": [331, 334, 335, 337], "timestamp": [5, 13, 146, 202, 204, 318, 320, 324, 331, 333], "timestep": [6, 235, 238, 305, 306, 308, 311, 312, 313, 314, 315, 317], "tini": [305, 308, 311, 312, 315, 325, 327, 331, 337, 338, 341], "tint": [267, 274], "tip": [4, 9, 13, 14, 16, 147, 150, 176, 179, 180], "tip_amount": [4, 9, 142, 144, 147, 150, 176, 179, 180], "tip_percentag": [9, 176, 180], "titan": [244, 265], "titl": [5, 10, 66, 69, 184, 189, 202, 204, 215, 244, 263, 265, 305, 309, 312, 316, 320, 322, 325, 327, 329, 331, 333, 335, 337, 338, 345, 347], "tl": [24, 27], "tlc": [9, 176, 179], "tloss": [275, 280], "tmp": [12, 13, 84, 96, 133, 136, 142, 145, 146, 275, 282], "tmp_checkpoint": [338, 347], "tmpdir": [318, 322, 331, 335, 338, 343], "to_arrow_ref": [325, 328], "to_csv": [9, 176, 181, 318, 320, 331, 333], "to_datetim": [331, 333], "to_json": 12, "to_numpi": [325, 328, 329, 331, 333, 337], "to_panda": [9, 10, 15, 176, 182, 184, 193, 267, 274, 325, 328, 338, 341], "to_parquet": [9, 176, 181, 202, 219, 318, 320, 325, 327, 338, 341], "to_pylist": [331, 333], "to_tensor": [202, 215], "todai": [1, 151, 152, 244, 265], "todo": [142, 145], "togeth": [4, 5, 6, 10, 11, 16, 147, 149, 154, 184, 190, 196, 199, 202, 205, 212, 213, 235, 239, 244, 265], "toke": [117, 120], "token": [86, 98, 108, 109, 111, 112, 113, 114, 115, 117, 120, 121, 122, 123, 125, 128, 129, 130, 131, 146, 269, 270, 277, 278, 282], "tokenization_fn": [267, 274], "tokenize_funct": [275, 280], "toler": [5, 6, 8, 9, 10, 17, 21, 22, 86, 90, 98, 106, 117, 122, 169, 173, 176, 178, 183, 184, 186, 203, 223, 224, 226, 235, 237, 305, 306, 307, 309, 311, 312, 313, 316, 317, 318, 319, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 336, 337, 339, 343, 344, 347], "tolist": [11, 16, 142, 145, 196, 200, 202, 215, 219, 318, 324, 331, 333], "toll": [4, 9, 147, 150, 176, 179], "tolls_amount": [4, 9, 147, 150, 176, 179], "tomorrow": [125, 130, 244, 263, 265, 266], "ton": [267, 274], "tone": [125, 128], "tonight": [244, 263, 265, 266], "tonit": [244, 265], "too": [4, 9, 10, 28, 30, 43, 45, 53, 56, 117, 119, 147, 150, 160, 176, 181, 184, 190, 244, 259, 266, 267, 269, 273, 274, 331, 333], "took": [3, 159, 165], "tool": [4, 8, 12, 24, 25, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 81, 84, 91, 93, 96, 109, 116, 126, 127, 132, 133, 135, 136, 137, 139, 140, 141, 142, 144, 147, 149, 169, 170, 174, 175, 243, 244, 261, 267, 269, 275, 277, 281, 282, 312, 313, 318, 320, 325, 327, 331, 333], "tool_cal": [125, 130], "tool_call_cli": [125, 130], "tool_call_id": [125, 130], "tool_call_pars": [125, 130], "tool_choic": [125, 130], "top": [2, 4, 9, 12, 80, 87, 92, 99, 125, 128, 147, 149, 153, 155, 160, 164, 167, 176, 178, 244, 265, 285, 292, 301, 312, 317, 319, 325, 329, 331, 335, 338, 347], "top20": [244, 265], "top_item_id": [318, 324], "top_items_df": [318, 324], "top_scor": [318, 324], "topic": [118, 124, 128, 244, 265], "topic_safety_output_restrict": [125, 128], "topk": [318, 324], "torch": [5, 7, 10, 11, 13, 14, 15, 16, 184, 185, 191, 196, 197, 200, 202, 204, 205, 206, 209, 210, 211, 212, 215, 223, 224, 229, 230, 231, 232, 233, 236, 237, 239, 244, 247, 262, 275, 278, 280, 305, 307, 308, 309, 311, 312, 314, 315, 316, 317, 318, 319, 320, 322, 324, 331, 333, 334, 335, 337, 338, 339, 340, 343, 347], "torch_": [5, 13], "torch_config": [275, 281], "torch_d": [10, 184, 188], "torchconfig": [275, 278, 281], "torchmetr": [13, 338, 343], "torchrec": [318, 319], "torchscript": [312, 317], "torchtrain": [5, 204, 206, 207, 208, 216, 219, 224, 226, 228, 236, 275, 277, 278, 281, 306, 307, 311, 313, 314, 318, 319, 320, 322, 323, 324, 331, 332, 333, 335, 337, 339, 340, 347], "torchtrainer_2025": [275, 282], "torchtrainer_4dd7a_00000": [275, 282], "torchtrainer_4dd7a_00000_0_2025": [275, 282], "torchtrainer_d89d0_00000_0_2024": 13, "torchvis": [5, 7, 10, 13, 14, 15, 16, 184, 185, 202, 204, 205, 220, 229, 230, 305, 307, 338, 339, 340], "torqu": [312, 313, 317], "torranc": [244, 265], "total": [3, 4, 5, 9, 12, 13, 14, 108, 109, 113, 117, 123, 147, 150, 159, 168, 176, 179, 202, 206, 207, 275, 281, 305, 307, 312, 314], "total_amount": [9, 142, 144, 176, 179, 180], "total_amt": [7, 14, 229, 233], "totensor": [5, 7, 10, 13, 14, 15, 16, 184, 185, 190, 202, 204, 210, 215, 220, 229, 230, 231, 233, 338, 341, 347], "touch": [267, 270, 318, 324, 325, 327], "tough": [292, 293, 295, 296, 304], "tougher": [292, 293, 295, 296, 304], "tour": [2, 153, 154, 244, 265], "tourism": [244, 265], "tourist": [125, 128], "toward": [8, 169, 172, 260, 268, 276, 292, 296, 300, 304, 312, 313, 318, 320], "tower": [318, 324], "town": [244, 265, 267, 273], "tpot": [108, 109, 113], "tpu": [3, 5, 6, 159, 165, 202, 205, 235, 239], "tqdm": [5, 275, 278, 305, 307, 318, 320, 338, 340], "tr_model": [331, 334], "trace": [5, 6, 8, 133, 135, 143, 169, 173, 202, 203, 235, 237, 267, 270], "traceback": [8, 169, 173], "track": [1, 5, 84, 85, 96, 97, 137, 140, 151, 152, 202, 203, 214, 228, 260, 267, 268, 274, 276, 300, 305, 309, 312, 313, 317, 318, 319, 322, 324, 325, 330], "track_running_stat": 13, "tractabl": [305, 306], "trade": [117, 123, 125, 131], "tradit": [108, 109, 113, 318, 319, 325, 326], "traffic": [17, 22, 53, 63, 66, 73, 86, 90, 98, 106, 108, 109, 113, 114, 117, 122, 142, 145, 285, 292, 299, 301, 304, 331, 332], "trail": [244, 265, 267, 273], "train": [7, 8, 9, 10, 12, 14, 15, 82, 83, 85, 89, 90, 94, 95, 97, 103, 106, 108, 109, 111, 169, 171, 173, 174, 176, 178, 180, 183, 184, 187, 189, 191, 195, 204, 205, 207, 208, 209, 210, 218, 219, 220, 224, 227, 229, 231, 232, 233, 234, 244, 250, 263, 265, 267, 270, 274, 278, 289, 290, 291, 292, 302, 303, 308, 310, 311, 314, 315, 321, 324, 329, 333, 334, 340, 343, 347], "train_arrow": [325, 328], "train_batch": [338, 343], "train_bert": [275, 277, 281, 282], "train_config": [275, 281, 318, 322, 324], "train_count": [305, 307], "train_ctx": [4, 147, 150], "train_d": [202, 219, 220, 221, 305, 307, 309, 312, 314, 316, 318, 320, 322, 325, 327, 328], "train_data": [5, 7, 13, 14, 202, 210, 229, 231, 233], "train_dataload": [6, 235, 238, 239, 305, 309, 312, 316], "train_dataset": [12, 275, 280], "train_df": [325, 327], "train_frac": [318, 320], "train_func": [325, 326, 328, 330], "train_func_per_work": [275, 280, 281], "train_label": 13, "train_linear_model": [7, 14, 229, 233], "train_load": [5, 6, 13, 202, 210, 235, 238, 305, 309, 312, 316, 318, 322, 331, 335, 338, 343], "train_loop": [306, 311, 312, 313, 316], "train_loop_config": [4, 5, 6, 13, 147, 150, 213, 221, 224, 226, 235, 239, 275, 281, 318, 322, 325, 328, 331, 335, 338, 344], "train_loop_per_work": [6, 202, 224, 226, 235, 239, 275, 281, 305, 309, 318, 319, 322, 331, 332, 335, 339, 344], "train_loop_ray_train": [5, 6, 13, 202, 205, 206, 207, 208, 213, 235, 239], "train_loop_ray_train_ray_data": [202, 217, 221], "train_loop_ray_train_with_checkpoint_load": [202, 223, 224, 226], "train_loop_torch": [5, 7, 13, 14, 229, 232], "train_loss": [305, 308, 309, 312, 315, 316, 318, 319, 322, 331, 335, 338, 343, 345], "train_loss_sum": [331, 335], "train_loss_tot": [338, 343], "train_my_simple_model": [7, 14, 229, 233], "train_my_simple_model_2024": 14, "train_my_simple_model_3207e_00000_0_a": 14, "train_my_simple_model_3207e_00000terminated10": 14, "train_my_simple_model_3207e_00001terminated10": 14, "train_my_simple_model_3207e_00002terminated10": 14, "train_my_simple_model_3207e_00003terminated10": 14, "train_my_simple_model_3207e_00004terminated10": 14, "train_parquet": [325, 327], "train_pytorch": [7, 14, 229, 233], "train_pytorch_7cf0c_00000terminated10": 14, "train_pytorch_7cf0c_00001terminated10": 14, "train_record": [331, 333], "train_test_split": [4, 12, 147, 148, 150, 318, 320, 325, 327], "trainabl": [4, 7, 14, 147, 150, 229, 233], "trainbr": [267, 274], "traincontext": [5, 202, 206], "trainer": [4, 5, 6, 12, 13, 147, 150, 204, 205, 214, 221, 224, 225, 226, 228, 235, 238, 239, 275, 278, 281, 305, 309, 310, 312, 316, 318, 319, 322, 323, 330, 331, 335, 336, 338, 339, 344, 346, 347], "training_iter": 13, "training_step": [6, 235, 238, 305, 308, 312, 315], "trainingargu": [275, 278], "trajectori": [312, 317], "transact": [8, 169, 170], "transfer": [2, 3, 8, 9, 10, 15, 137, 141, 153, 155, 159, 162, 169, 170, 176, 182, 184, 190, 193, 338, 347], "transform": [5, 6, 7, 8, 11, 12, 13, 14, 16, 24, 27, 169, 170, 171, 173, 177, 178, 179, 182, 183, 185, 187, 189, 193, 196, 199, 200, 204, 210, 216, 221, 229, 230, 231, 233, 235, 236, 244, 247, 253, 259, 262, 264, 266, 267, 269, 270, 274, 275, 277, 278, 280, 282, 288, 291, 292, 302, 303, 305, 306, 307, 312, 314, 317, 318, 320, 325, 327, 337, 339, 340, 342, 347], "transform_imag": [202, 220], "transient": [202, 224, 225, 338, 343], "transit": [83, 95, 305, 306, 312, 313, 318, 319, 325, 326, 331, 332], "transpar": [305, 309], "transpos": [305, 307], "travel": [125, 128, 244, 265], "treat": [267, 270, 273, 338, 339], "tree": [4, 5, 6, 12, 147, 150, 202, 203, 235, 237, 238, 244, 263, 265, 267, 270, 325, 326, 327], "tree_method": [4, 147, 150, 325, 328], "tremend": [267, 274], "trend": [244, 265], "tri": [267, 270, 325, 326], "trial": [4, 7, 12, 13, 14, 147, 150, 229, 233, 267, 274, 275, 282], "trial_id": 13, "tribul": [267, 274], "trier": [267, 274], "trigger": [10, 184, 189, 192, 244, 259, 266, 312, 314, 325, 327, 330], "trim": [305, 307, 338, 340], "trip": [4, 9, 12, 147, 150, 176, 179, 182, 267, 273, 331, 333, 337], "trip_amount": [4, 147, 150], "trip_dist": [4, 9, 12, 147, 150, 176, 179, 182], "trip_dur": 12, "trivial": [108, 109, 114], "trndnl": [244, 265], "troubleshoot": [81, 84, 93, 96, 125, 132, 133, 136], "truck": [125, 129], "true": [0, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 35, 39, 108, 109, 115, 117, 120, 121, 122, 123, 125, 128, 130, 142, 144, 145, 146, 159, 163, 176, 179, 184, 188, 190, 191, 196, 200, 202, 203, 204, 208, 210, 215, 229, 231, 233, 235, 238, 239, 267, 270, 275, 280, 305, 307, 308, 309, 312, 315, 316, 318, 320, 322, 323, 325, 327, 329, 331, 333, 334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347], "truli": [244, 265, 267, 274], "trump": [244, 265], "truncat": [275, 280, 289, 290, 291, 292, 293, 295, 296, 303, 304, 305, 307, 312, 314], "trust": [17, 19, 22, 244, 265, 331, 337], "truth": [10, 15, 184, 190, 193, 202, 204, 312, 314, 331, 332, 335, 337, 338, 347], "try": [2, 3, 7, 8, 13, 14, 28, 29, 43, 44, 51, 53, 55, 64, 66, 68, 117, 124, 125, 132, 153, 158, 159, 163, 169, 174, 202, 228, 229, 233, 267, 273, 274, 305, 307, 311, 325, 330, 338, 340], "tryna": [244, 265], "tsui": [267, 273], "ttft": [108, 109, 113], "ttm": [267, 273, 274], "tuesdai": [244, 265], "tune": [8, 10, 12, 85, 89, 97, 103, 108, 109, 114, 117, 124, 125, 127, 128, 131, 132, 169, 173, 174, 184, 190, 202, 218, 228, 232, 267, 274, 305, 311, 312, 317, 318, 324, 325, 329, 330, 331, 337, 338, 339, 347], "tune_config": [4, 7, 12, 14, 147, 150, 229, 233], "tuneconfig": [4, 7, 12, 14, 147, 150, 229, 233], "tuner": [4, 7, 12, 14, 147, 150, 229, 233], "tupl": [3, 6, 10, 159, 162, 184, 191, 202, 216, 217, 235, 238], "turn": [2, 7, 9, 153, 156, 176, 181, 229, 233, 244, 265, 267, 274, 325, 326], "tutori": [17, 20, 81, 83, 93, 95, 117, 119, 204, 227, 305, 306, 311, 312, 313, 318, 319, 320, 324, 325, 326, 331, 332, 333, 335, 337, 338, 339, 340], "tv": [244, 265, 267, 273, 274], "tweet": [244, 265], "tweet_ev": [244, 250, 263], "twilight": [244, 265], "twitter": [244, 265], "two": [2, 3, 17, 22, 24, 26, 82, 85, 89, 94, 97, 105, 112, 133, 136, 142, 144, 146, 153, 158, 159, 161, 162, 163, 202, 205, 214, 219, 244, 256, 265, 271, 274, 318, 320, 322, 324, 325, 327, 331, 333, 338, 339, 345], "txt": [0, 1, 83, 95, 146, 151, 152, 260, 268, 276, 300], "type": [3, 5, 6, 7, 8, 9, 10, 11, 14, 16, 18, 22, 24, 26, 28, 32, 35, 41, 43, 50, 53, 62, 66, 71, 77, 80, 82, 88, 92, 94, 101, 108, 109, 113, 117, 120, 130, 131, 137, 141, 142, 145, 146, 159, 163, 169, 170, 173, 176, 183, 184, 188, 190, 191, 196, 197, 229, 230, 235, 236, 238, 244, 247, 262, 267, 270, 275, 278, 326, 329, 330], "typic": [9, 24, 27, 91, 117, 119, 125, 128, 176, 178, 243, 244, 261, 267, 274, 305, 307, 318, 320, 338, 339], "u": [6, 7, 12, 13, 14, 15, 16, 28, 30, 35, 38, 39, 43, 45, 53, 56, 66, 69, 70, 71, 83, 95, 125, 128, 132, 133, 136, 142, 144, 202, 205, 229, 232, 235, 238, 239, 244, 265, 267, 270, 274, 312, 313, 318, 319, 320, 324], "u002c": [244, 265], "u002c000": [244, 265], "u2019": [244, 265], "u2019ll": [244, 265], "u2019m": [244, 265], "u2019r": [244, 265], "u2019t": [244, 265], "u2019v": [244, 265], "u_": [312, 313, 318, 319], "u_k": [312, 313], "uber": [7, 229, 234], "ubj": [4, 12, 147, 150], "ubyt": [13, 14], "udf": [8, 169, 173], "ui": [0, 16, 17, 22, 83, 85, 95, 97], "uid": [318, 320, 324], "uint8": [16, 137, 141, 202, 220], "un": [267, 274], "unabl": [267, 274], "unassoci": [28, 30, 43, 45, 53, 56], "unattach": [28, 30, 43, 45, 53, 56], "unavail": [3, 159, 165], "unavoid": [267, 270], "unbound": [10, 184, 190], "uncas": [289, 290, 291, 292, 303], "uncertainti": [331, 337], "unchang": [202, 220], "uncl": [267, 274], "uncom": [35, 39, 43, 46, 53, 58, 125, 129, 130, 275, 281], "uncondit": [305, 311], "unconnect": [267, 274], "unconnectedbr": [267, 274], "under": [3, 10, 66, 69, 82, 86, 90, 94, 98, 107, 142, 144, 159, 162, 184, 188, 202, 204, 205, 208, 212, 219, 267, 273, 305, 307, 309, 311, 312, 316, 317, 318, 320, 324, 331, 332, 333, 335, 338, 340, 343], "underbrac": [331, 332], "underli": [3, 4, 5, 6, 10, 87, 99, 117, 120, 147, 150, 159, 161, 184, 189, 202, 203, 235, 237], "undersid": [267, 274], "understand": [7, 10, 14, 17, 18, 20, 79, 84, 86, 96, 98, 108, 109, 110, 111, 113, 115, 116, 117, 119, 120, 133, 135, 137, 141, 142, 145, 184, 190, 229, 232, 267, 273, 274, 275, 280, 318, 324, 325, 326, 338, 339], "understat": [267, 274], "understood": [117, 124], "underutil": [108, 109, 111, 112], "uneasy": [244, 265], "unet": [6, 235, 238], "unet2dconditionmodel": [6, 235, 236, 238], "unexpect": [5, 6, 133, 135, 202, 203, 235, 237, 244, 265, 331, 333], "ungat": [117, 120, 125, 128], "unifi": [1, 4, 8, 17, 23, 91, 108, 109, 114, 147, 149, 151, 152, 169, 170, 173], "uniform": [3, 4, 147, 150, 159, 166, 275, 280, 312, 313], "uniformli": [10, 184, 186], "uniniti": [10, 15, 184, 191], "uninstal": [43, 51, 53, 64, 66, 78], "uniqu": [9, 15, 83, 95, 108, 109, 113, 117, 120, 176, 179, 202, 210, 212, 267, 271, 274, 318, 324, 338, 340], "unique_item": [318, 324], "unique_us": [318, 324], "unit": [10, 11, 15, 108, 109, 113, 125, 130, 184, 193, 196, 199, 201, 267, 270, 338, 340], "univari": [331, 334], "univers": [267, 273], "unless": [9, 85, 87, 89, 97, 99, 103, 176, 180, 202, 215, 223, 267, 270], "unlik": [85, 89, 97, 103, 202, 204, 220, 244, 265], "unnecessari": [10, 11, 84, 85, 86, 96, 97, 98, 184, 190, 196, 198, 202, 212, 215, 244, 265, 312, 317, 325, 328], "unnot": [267, 274], "unpredict": [108, 109, 113], "unread": [305, 307], "unregist": [28, 33, 43, 51, 53, 64], "unreleas": [244, 265], "unrelentingli": [267, 273, 274], "unrival": [267, 274], "uns4": [244, 265], "unshuffl": [325, 327], "unsloth": [108, 109, 115, 117, 120, 125, 128], "unsqueez": [5, 13, 202, 215, 312, 317, 331, 333, 334, 335, 337], "unstabl": [305, 306], "unstructur": [8, 169, 170, 186, 348], "until": [3, 5, 8, 9, 13, 15, 108, 109, 112, 113, 137, 141, 159, 161, 169, 175, 176, 179, 180, 202, 213, 244, 256, 265, 267, 274, 325, 328], "untitl": [81, 93], "unus": [28, 30, 43, 45, 53, 56], "unveil": [244, 265], "unwrap": [5, 13, 202, 212, 224], "up": [0, 3, 5, 6, 7, 8, 9, 10, 11, 15, 17, 18, 21, 24, 27, 28, 30, 33, 34, 35, 38, 39, 42, 45, 46, 50, 52, 56, 58, 62, 65, 66, 69, 70, 78, 82, 88, 94, 100, 106, 108, 109, 113, 115, 118, 123, 124, 126, 128, 131, 134, 135, 142, 143, 159, 163, 164, 169, 175, 176, 179, 184, 189, 191, 196, 199, 200, 203, 213, 224, 226, 229, 232, 233, 235, 237, 238, 243, 244, 253, 261, 264, 265, 267, 269, 270, 273, 274, 275, 277, 281, 307, 309, 313, 316, 322, 323, 326, 327, 328, 336, 339, 340, 344, 346], "up_block_typ": [6, 235, 238], "upblock2d": [6, 235, 238], "upcom": 177, "updat": [0, 1, 4, 5, 6, 7, 8, 11, 14, 16, 43, 46, 51, 53, 58, 64, 66, 73, 82, 84, 86, 94, 96, 98, 107, 117, 122, 147, 150, 151, 152, 169, 170, 196, 201, 202, 203, 206, 212, 223, 229, 233, 235, 239, 260, 268, 275, 276, 280, 285, 292, 300, 301, 305, 311, 331, 337, 338, 343], "upgrad": [11, 43, 45, 46, 48, 53, 56, 58, 60, 73, 76, 81, 86, 90, 93, 98, 106, 196, 199, 305, 311, 312, 313, 317], "upload": [5, 13, 83, 95, 125, 128, 325, 330], "upload_fil": [83, 95, 125, 128], "upon": [84, 96], "upper": [81, 93], "upright": [312, 313], "upscal": 16, "upscale_delay_": 16, "upset": [244, 265], "ur": [244, 265], "uri": [83, 95], "url": [8, 85, 97, 169, 175, 331, 333], "urljoin": [109, 115, 117, 121], "urllib": [109, 115, 117, 121], "urmitz": [267, 274], "us": [2, 3, 11, 13, 20, 21, 23, 25, 27, 28, 29, 31, 34, 35, 36, 39, 40, 43, 44, 47, 52, 53, 54, 55, 59, 63, 65, 66, 67, 68, 69, 70, 71, 75, 81, 82, 83, 84, 85, 86, 87, 88, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 108, 109, 111, 112, 113, 114, 115, 118, 119, 121, 122, 124, 127, 132, 133, 135, 136, 137, 140, 141, 143, 144, 146, 149, 150, 153, 155, 156, 157, 159, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173, 175, 177, 179, 180, 181, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 198, 200, 201, 204, 205, 206, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 226, 227, 228, 232, 234, 236, 238, 239, 240, 243, 244, 250, 253, 256, 259, 260, 261, 263, 264, 265, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 279, 280, 281, 282, 291, 296, 299, 300, 303, 304, 307, 309, 311, 314, 316, 317, 321, 322, 324, 328, 329, 330, 333, 335, 337, 340, 343, 345, 347], "usabl": [125, 129, 202, 220], "usag": [11, 12, 13, 14, 16, 84, 96, 108, 109, 112, 114, 125, 127, 128, 131, 133, 135, 137, 139, 141, 196, 198, 275, 282, 318, 322], "use_gpu": [4, 5, 6, 12, 13, 147, 150, 202, 203, 208, 235, 239, 305, 309, 312, 316, 318, 322, 325, 328, 331, 335, 338, 339, 344], "use_gpu_actor": [338, 347], "usecol": [318, 324], "user": [1, 3, 5, 8, 10, 13, 14, 17, 19, 24, 26, 66, 69, 80, 83, 84, 85, 87, 89, 90, 92, 95, 96, 97, 99, 100, 103, 106, 108, 109, 111, 112, 113, 115, 117, 121, 122, 125, 128, 129, 130, 131, 137, 139, 140, 141, 142, 145, 151, 152, 159, 161, 165, 169, 173, 174, 184, 190, 202, 206, 209, 210, 244, 263, 265, 266, 275, 282, 285, 292, 301, 321, 322], "user2idx": [318, 320, 324], "user_col": [318, 320], "user_embed": [318, 321, 324], "user_id": [146, 318, 320, 324], "user_idx": [318, 319, 320, 321, 322, 324], "user_nam": [66, 69], "user_storag": [83, 95], "user_vec": [318, 321], "user_vector": [318, 324], "userguid": [28, 29, 43, 44, 53, 55], "userservic": [142, 145, 146], "usual": [8, 9, 169, 172, 176, 180, 202, 215, 267, 273], "utc": 13, "util": [3, 5, 6, 7, 10, 11, 13, 14, 16, 84, 86, 90, 96, 98, 106, 108, 109, 111, 112, 113, 114, 142, 145, 159, 165, 184, 186, 190, 195, 196, 198, 199, 202, 203, 204, 210, 212, 216, 228, 229, 230, 231, 233, 235, 236, 238, 275, 277, 278, 280, 305, 307, 312, 314, 317, 318, 320, 331, 333, 338, 339, 340], "uuid": [312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "uv": [260, 268, 276, 300], "ux": [8, 169, 173], "v": [17, 20, 27, 43, 49, 50, 53, 61, 62, 81, 85, 93, 97, 108, 109, 112, 125, 131, 172, 173, 202, 203, 215, 227, 244, 265, 275, 280, 318, 319, 324, 331, 335, 337, 338, 347, 348], "v1": [24, 27, 109, 115, 117, 121, 122, 125, 128, 129, 130, 312, 313, 314, 317], "v2": [202, 203, 244, 253, 264, 305, 306, 309, 312, 313, 318, 319, 325, 326, 331, 332], "v_": [318, 319], "val": [309, 314, 318, 320, 322, 325, 327, 328, 331, 333, 335, 337, 338, 341, 343, 345, 347], "val_batch": [338, 343], "val_d": [305, 307, 309, 312, 314, 316, 318, 320, 322, 325, 327, 328, 329, 330], "val_dataload": [305, 309, 312, 316], "val_df": [325, 327], "val_load": [305, 309, 312, 316, 318, 322, 331, 335, 338, 343], "val_loss": [305, 308, 309, 312, 315, 316, 318, 319, 322, 331, 335, 338, 343, 344, 345], "val_loss_sum": [331, 335], "val_loss_tot": [338, 343], "val_parquet": [325, 327], "val_pd": [325, 328, 329], "val_record": [331, 333], "val_xb": [338, 343], "val_yb": [338, 343], "valid": [4, 12, 16, 79, 86, 98, 125, 129, 147, 150, 275, 280, 305, 308, 309, 312, 313, 314, 315, 316, 319, 328, 329, 330, 339, 343, 344, 347], "valid_dataset": 12, "valid_dataset_featur": 12, "validation_step": [305, 308, 312, 315], "valu": [3, 5, 7, 10, 13, 14, 28, 29, 30, 35, 36, 39, 43, 44, 45, 46, 51, 53, 54, 56, 58, 64, 66, 67, 72, 73, 86, 90, 98, 107, 111, 142, 144, 145, 159, 161, 162, 166, 184, 189, 190, 202, 207, 210, 211, 214, 220, 229, 231, 267, 272, 305, 306, 309, 312, 313, 314, 318, 320, 324, 331, 332, 333, 334, 338, 343, 345], "valuabl": [267, 273], "value_count": [318, 320, 325, 327], "valueerror": [3, 159, 163, 305, 309, 312, 316], "values_nginx": [43, 46, 53, 58], "values_nginx_gke_priv": [66, 73], "values_nginx_gke_publ": [66, 73], "values_nvdp": [43, 46, 53, 58], "vamp": [244, 265], "vampett": [244, 265], "vampir": [244, 265], "van": [244, 265], "vanilla": [6, 108, 109, 112, 148, 230, 235, 239, 331, 332, 333], "var": [17, 22, 66, 78, 202, 204, 305, 307, 312, 314, 318, 320, 325, 327, 331, 333, 338, 340], "varepsilon": [305, 306], "varepsilon_": [312, 313], "varepsilon_k": [312, 313], "vari": [8, 9, 10, 15, 108, 109, 113, 169, 170, 176, 182, 184, 186, 191, 193], "variabl": [3, 28, 30, 35, 39, 43, 45, 53, 56, 66, 69, 82, 83, 84, 94, 95, 96, 159, 164, 165], "variat": [7, 14, 229, 233], "varieti": [10, 15, 184, 188, 193, 267, 269], "variou": [8, 82, 83, 94, 95, 117, 124, 169, 171, 202, 216, 275, 281], "vast": [8, 169, 170], "ve": [28, 34, 108, 109, 116, 117, 124, 125, 127, 132, 244, 265, 267, 270, 274, 338, 347], "vector": [8, 10, 108, 109, 111, 169, 170, 184, 190, 243, 244, 261, 312, 314, 317, 318, 319, 321, 325, 330], "veget": [325, 326], "veloc": [312, 313], "venu": [292, 293, 295, 296, 304], "venv": 0, "verbos": [142, 145], "veri": [5, 6, 7, 8, 9, 13, 14, 15, 117, 119, 169, 174, 176, 180, 202, 204, 229, 233, 235, 238, 239, 244, 265, 267, 274, 331, 337], "verif": 79, "verifi": [6, 10, 28, 33, 35, 38, 51, 63, 64, 66, 69, 72, 73, 74, 82, 84, 85, 94, 96, 97, 184, 190, 235, 238, 327, 331, 333], "vermaelen": [244, 265], "version": [1, 8, 15, 17, 22, 28, 29, 35, 37, 43, 44, 46, 53, 55, 58, 66, 68, 73, 74, 80, 90, 92, 106, 125, 128, 133, 136, 137, 140, 142, 145, 151, 152, 169, 170, 202, 217, 244, 260, 265, 268, 276, 285, 292, 300, 301, 305, 307, 309, 331, 333], "versu": [305, 311, 325, 330, 338, 347], "via": [0, 2, 3, 5, 6, 8, 10, 17, 20, 24, 26, 27, 28, 30, 35, 39, 43, 45, 53, 56, 81, 87, 88, 89, 93, 99, 101, 103, 133, 135, 142, 143, 145, 146, 153, 155, 159, 165, 169, 173, 174, 184, 186, 202, 203, 206, 207, 208, 215, 220, 228, 235, 237, 244, 265, 267, 273, 305, 308, 312, 316, 318, 320, 322, 338, 342, 343], "vicki": [244, 263, 265], "vid": [244, 265], "video": [8, 10, 169, 170, 184, 186, 244, 265, 267, 270, 274], "vietnam": [267, 270], "view": [1, 7, 9, 10, 11, 13, 14, 16, 17, 22, 28, 30, 32, 35, 39, 41, 43, 45, 50, 53, 56, 62, 63, 66, 77, 82, 84, 94, 96, 117, 123, 133, 135, 137, 141, 142, 144, 145, 151, 152, 176, 179, 180, 184, 190, 196, 201, 229, 233, 267, 270, 274, 305, 308, 312, 315], "viewer": [267, 270], "vincent": [267, 270], "violat": [125, 128], "viridi": [325, 329], "virtual": [0, 17, 20, 22, 27, 82, 94, 108, 109, 112, 133, 136], "virtuou": [267, 270], "visibl": [117, 123, 202, 211, 267, 270, 331, 337], "vision": [202, 228, 311, 347], "visit": [7, 14, 16, 133, 136, 229, 233], "visual": [7, 8, 9, 10, 15, 81, 84, 93, 96, 117, 123, 133, 135, 136, 137, 141, 169, 171, 176, 179, 184, 189, 229, 231, 267, 273, 274, 309, 311, 312, 313, 316, 322, 326, 335, 345], "visualis": [331, 333], "vit": [202, 224, 226], "vit_b_16": [338, 347], "vit_l_32": [338, 347], "vllm": [115, 116, 117, 123, 124, 125, 129], "vm": [17, 18, 20, 27, 28, 32, 35, 39, 41, 66, 70, 79, 348], "vocal": [244, 265], "voic": [244, 265, 267, 274], "volatil": [5, 202, 204], "volleybal": [244, 265], "volum": [5, 8, 83, 95, 137, 141, 169, 170, 173, 202, 204, 325, 327, 338, 339, 347], "von": [267, 274], "vpc": [20, 21, 23, 24, 26, 28, 30, 34, 35, 36, 39, 43, 44, 45, 53, 55, 56, 66, 70, 78, 79, 108, 109, 114], "vpc_cidr": [17, 22], "vpc_id": [17, 22, 23, 28, 30], "vpcid": [28, 30, 43, 45, 53, 56], "vram": [117, 120, 244, 265], "vscode": [86, 98], "vtripl": [125, 128], "vulva": [267, 270], "w": [7, 11, 14, 83, 95, 196, 200, 202, 215, 229, 233, 244, 263, 265, 267, 274, 305, 306, 307, 308, 338, 347], "w0": [3, 159, 168], "w1": [3, 159, 168], "wa": [6, 9, 12, 82, 85, 94, 97, 137, 141, 176, 179, 202, 226, 235, 238, 244, 265, 267, 270, 273, 274, 318, 323], "wai": [1, 3, 8, 9, 10, 16, 17, 20, 81, 85, 88, 89, 93, 97, 102, 105, 117, 123, 151, 152, 159, 162, 169, 171, 172, 176, 178, 184, 189, 190, 202, 212, 244, 265, 267, 269, 273, 274, 285, 292, 293, 295, 296, 301, 304, 312, 317, 338, 339, 341, 347], "wait": [2, 66, 73, 82, 94, 108, 109, 112, 153, 157, 158, 160, 166, 244, 265], "wake": [244, 265, 267, 274], "walk": [5, 7, 13, 14, 28, 29, 43, 44, 53, 54, 66, 67, 81, 83, 93, 95, 117, 118, 125, 128, 229, 230, 244, 259, 265, 266, 305, 306, 325, 326, 331, 332], "walter": [267, 273], "wander": [267, 270], "wanna": [244, 265], "want": [0, 2, 3, 5, 6, 8, 9, 10, 11, 12, 15, 16, 17, 22, 35, 39, 42, 53, 56, 66, 69, 78, 81, 84, 93, 96, 108, 109, 113, 153, 157, 159, 161, 162, 167, 168, 169, 171, 176, 179, 181, 182, 184, 190, 191, 193, 196, 200, 201, 202, 204, 210, 211, 215, 235, 238, 244, 253, 263, 264, 265, 267, 270, 273, 274, 275, 282, 331, 332, 335], "war": [244, 265, 267, 270], "warehous": [10, 184, 188], "warm": [6, 235, 238, 244, 265], "warmth": [244, 265], "warn": [244, 265, 267, 273, 274, 305, 309, 312, 316], "warner": [244, 263, 265], "wasn": [267, 273, 331, 333], "wast": [267, 274], "watch": [66, 73, 244, 265, 267, 274], "water": [267, 274], "wave": [267, 274], "wc": [9, 176, 179], "we": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 28, 29, 35, 37, 43, 44, 46, 50, 53, 55, 58, 62, 66, 68, 70, 80, 81, 82, 83, 84, 85, 86, 91, 92, 93, 94, 95, 96, 97, 98, 108, 109, 110, 112, 115, 116, 118, 119, 120, 122, 126, 128, 129, 142, 144, 147, 150, 153, 157, 158, 159, 162, 163, 164, 166, 168, 169, 171, 172, 176, 179, 180, 181, 182, 184, 188, 190, 191, 192, 193, 196, 200, 202, 204, 205, 206, 207, 208, 209, 212, 213, 215, 217, 218, 219, 220, 221, 223, 224, 229, 232, 233, 235, 238, 239, 243, 244, 261, 265, 267, 270, 271, 273, 274, 275, 279, 281, 282, 285, 288, 292, 296, 299, 301, 302, 304, 318, 319, 322, 338, 345], "wealth": [267, 274], "weather": [331, 337], "weaviat": [8, 169, 170], "web": [81, 84, 93, 96, 133, 135, 143, 285, 292, 301, 312, 317], "webservic": [288, 302], "websit": [0, 133, 136], "webster": [267, 273], "wed": [244, 265], "wednesdai": [244, 263, 265], "week": [244, 265, 331, 332, 333], "weight": [3, 13, 108, 109, 113, 117, 120, 159, 168, 202, 203, 206, 212, 215, 224, 228, 305, 311, 312, 317, 318, 322, 324, 325, 327, 330, 331, 337, 338, 347], "weight_decai": [6, 235, 238, 239], "weights_onli": [5, 6, 13, 202, 215, 235, 239], "welbeck": [244, 265], "welcom": [12, 146], "well": [3, 8, 17, 22, 24, 26, 82, 88, 94, 102, 159, 168, 169, 174, 244, 265, 267, 273, 274, 285, 292, 301, 318, 320, 321], "wellcraft": [267, 274], "welllll": [244, 265], "went": [267, 274], "were": [6, 202, 226, 235, 238, 244, 265, 267, 273, 274, 325, 327], "werewolf": [267, 274], "west": [12, 13, 14, 28, 30, 43, 45, 53, 56, 83, 95, 125, 128], "west2": [35, 38, 39, 66, 69, 70, 71], "western": [267, 273, 274], "wget": [318, 320], "what": [2, 4, 6, 7, 10, 12, 14, 18, 35, 36, 66, 70, 80, 92, 110, 115, 128, 130, 134, 137, 138, 140, 141, 147, 148, 150, 153, 157, 177, 184, 190, 208, 228, 229, 233, 235, 238, 244, 265, 267, 270, 274, 320], "whatev": [325, 330, 331, 337, 338, 347], "when": [1, 2, 3, 7, 14, 28, 30, 33, 35, 39, 42, 43, 45, 46, 50, 53, 56, 58, 62, 66, 70, 78, 81, 82, 83, 84, 85, 86, 90, 93, 94, 95, 96, 97, 98, 107, 108, 109, 111, 112, 117, 121, 122, 123, 125, 130, 137, 140, 141, 142, 143, 145, 151, 152, 153, 157, 158, 159, 161, 163, 164, 165, 167, 168, 175, 177, 180, 181, 185, 187, 188, 189, 190, 191, 192, 193, 197, 210, 213, 214, 216, 224, 227, 229, 233, 236, 243, 244, 253, 261, 264, 265, 267, 270, 274, 292, 299, 304, 305, 307, 311, 312, 317, 318, 322, 324, 325, 327, 331, 335, 337, 338, 340, 343, 345], "where": [2, 5, 6, 7, 8, 9, 12, 13, 14, 53, 56, 66, 71, 83, 87, 88, 91, 95, 99, 101, 108, 109, 111, 117, 119, 133, 136, 137, 140, 153, 155, 169, 174, 175, 176, 179, 212, 213, 214, 221, 226, 229, 231, 235, 239, 244, 265, 267, 270, 273, 274, 275, 280, 285, 292, 301, 313, 319, 320, 323, 326, 332, 336, 339, 341], "wherea": [8, 169, 172, 175], "wherev": [244, 265], "whether": [1, 5, 6, 7, 12, 13, 14, 79, 83, 87, 95, 99, 151, 152, 202, 205, 208, 209, 210, 229, 233, 235, 239, 275, 277, 318, 322, 325, 327, 331, 335], "which": [2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 20, 27, 28, 30, 43, 45, 53, 56, 57, 63, 66, 69, 82, 86, 94, 98, 117, 119, 123, 125, 128, 130, 133, 136, 137, 141, 142, 145, 147, 150, 153, 158, 159, 160, 161, 162, 166, 167, 168, 169, 171, 174, 176, 179, 180, 181, 182, 184, 189, 190, 191, 192, 202, 204, 206, 210, 211, 212, 214, 217, 229, 233, 235, 238, 243, 244, 261, 267, 270, 273, 274, 275, 277, 280, 281, 285, 292, 301, 305, 306, 307, 325, 326, 329, 331, 335, 337, 338, 339, 340, 343, 347], "while": [3, 4, 8, 11, 81, 82, 84, 91, 93, 94, 96, 117, 119, 125, 128, 137, 140, 141, 142, 145, 147, 149, 159, 166, 167, 169, 170, 174, 196, 199, 202, 203, 205, 206, 216, 217, 218, 267, 270, 273, 274, 305, 307, 309, 318, 319, 320, 325, 327, 331, 332, 333, 338, 339, 340, 345], "whilst": [267, 274], "white": [7, 14, 229, 231, 244, 265, 267, 274], "who": [17, 22, 79, 87, 99, 108, 109, 113, 244, 265, 267, 270, 273, 274, 275, 282], "whole": [4, 12, 147, 150, 325, 328], "whose": [267, 273, 274], "why": [10, 118, 124, 184, 185, 202, 203, 222, 244, 265, 267, 273], "whyyyyyyi": [244, 265], "wichita": [244, 265], "wide": [8, 66, 73, 169, 173, 244, 265, 267, 274, 285, 292, 301], "widescreen": [267, 273], "width": [10, 15, 16, 137, 141, 184, 190, 305, 307], "wife": [267, 274], "wildlif": [292, 293, 295, 296, 304], "wilki": [267, 273], "willam": [244, 263, 265], "william": [244, 265, 267, 273], "wilmer": [244, 265], "win": [8, 169, 174, 244, 263, 265], "wind": [267, 273], "window": [1, 35, 38, 81, 93, 117, 123, 133, 136, 151, 152, 202, 228, 267, 270, 332, 334, 337], "wire": [202, 228, 305, 309], "wise": [202, 215], "wish": [244, 265], "with_resourc": [7, 14, 229, 233], "within": [3, 8, 17, 19, 22, 24, 26, 81, 83, 85, 87, 88, 89, 93, 95, 97, 99, 101, 105, 133, 136, 159, 165, 169, 170, 312, 317, 318, 320], "without": [3, 8, 11, 17, 22, 81, 82, 91, 93, 94, 108, 109, 112, 125, 128, 129, 133, 136, 159, 161, 162, 165, 167, 169, 170, 196, 199, 202, 203, 222, 224, 225, 267, 270, 274, 305, 306, 311, 312, 313, 318, 319, 320, 322, 323, 324, 325, 328, 329, 330, 331, 332, 333, 337, 338, 339, 346], "woman": [267, 273, 274], "women": [267, 270], "won": [3, 8, 159, 163, 169, 174, 267, 270], "wonder": [7, 24, 26, 229, 233, 244, 263, 265], "wood": [244, 265], "wooden": [267, 273], "word": [108, 109, 111, 267, 274], "work": [1, 2, 3, 5, 7, 9, 11, 13, 24, 26, 27, 81, 83, 84, 86, 87, 91, 93, 95, 96, 98, 99, 108, 109, 111, 117, 120, 124, 125, 127, 151, 152, 153, 158, 159, 161, 176, 181, 182, 185, 196, 199, 209, 219, 228, 229, 232, 243, 244, 253, 261, 264, 265, 267, 273, 274, 275, 281, 310, 312, 313, 318, 319, 320, 322, 325, 326, 327, 338, 339, 341, 346, 347], "worker": [1, 3, 4, 9, 10, 11, 13, 15, 17, 22, 24, 26, 43, 50, 53, 62, 80, 86, 92, 98, 137, 139, 141, 147, 150, 151, 152, 159, 161, 163, 164, 166, 168, 176, 178, 182, 184, 191, 193, 196, 199, 203, 205, 207, 208, 209, 210, 211, 212, 213, 215, 218, 219, 220, 221, 222, 224, 225, 244, 265, 267, 272, 277, 281, 282, 305, 306, 307, 308, 309, 311, 312, 313, 316, 317, 318, 319, 320, 322, 326, 327, 330, 331, 332, 333, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347], "worker_devic": [244, 256, 265], "worker_nod": [43, 50, 53, 62, 137, 141], "worker_rank": [4, 147, 150], "workernodegroupconfig": [43, 50, 53, 62], "workflow": [1, 8, 24, 26, 80, 81, 84, 85, 91, 92, 93, 96, 97, 125, 130, 151, 152, 169, 170, 172, 173, 174, 197, 202, 203, 212, 215, 228, 243, 244, 259, 260, 261, 266, 267, 268, 269, 274, 275, 276, 281, 282, 285, 292, 300, 301, 312, 313, 325, 326, 327, 331, 332, 337, 338, 339], "working_dir": [85, 89, 97, 105, 108, 109, 115, 117, 122], "workload": [1, 3, 4, 5, 6, 8, 9, 10, 12, 15, 17, 22, 25, 27, 28, 34, 43, 52, 53, 65, 80, 82, 83, 84, 85, 89, 90, 91, 92, 94, 95, 96, 97, 102, 103, 106, 108, 109, 113, 117, 119, 124, 133, 136, 137, 140, 141, 143, 147, 149, 150, 151, 152, 159, 165, 169, 170, 173, 174, 176, 183, 184, 186, 202, 210, 216, 228, 235, 238, 285, 292, 301, 311, 317, 324, 337, 347], "workload_identity_pool_provid": [35, 39, 66, 70], "workloadidentitypool": [35, 39, 66, 70], "workloadserviceaccountnam": [43, 45, 48, 53, 56, 60, 66, 76], "workshop": [80, 89, 90, 92, 103, 106], "workspac": [17, 21, 22, 24, 26, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 101, 103, 117, 119, 123, 124, 312, 313, 325, 330, 331, 337, 338, 339, 347, 348], "workspace_v2": [81, 93], "world": [5, 81, 85, 89, 93, 97, 104, 105, 125, 132, 142, 145, 202, 206, 244, 265, 267, 274, 275, 280, 305, 309, 318, 319, 324, 338, 339], "world_rank": [5, 202, 211, 213], "world_siz": [5, 202, 206, 217], "worri": [338, 339], "worth": [244, 265], "would": [5, 6, 9, 13, 117, 119, 124, 137, 141, 176, 180, 181, 182, 202, 208, 235, 238, 239, 244, 265, 267, 269, 271, 273, 274, 275, 280], "wound": [267, 273], "wrangl": [202, 204], "wrap": [3, 5, 13, 86, 90, 91, 98, 107, 159, 162, 203, 204, 205, 206, 210, 212, 213, 221, 306, 313, 314, 320, 328, 332, 333, 339, 342], "wright": [244, 265], "write": [2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15, 17, 22, 81, 83, 93, 95, 137, 141, 146, 147, 150, 153, 155, 158, 159, 168, 169, 170, 177, 178, 180, 184, 185, 187, 189, 190, 194, 202, 212, 219, 224, 229, 233, 235, 239, 305, 307, 318, 319, 331, 333, 338, 339, 340, 341, 343], "write_csv": [9, 176, 181], "write_parquet": [4, 9, 10, 15, 137, 141, 142, 144, 147, 150, 176, 181, 184, 189, 194, 305, 307, 331, 333], "write_t": [331, 333, 338, 340], "writefil": [142, 144], "writer": [5, 13], "writerow": [5, 13], "written": [1, 8, 133, 136, 142, 145, 151, 152, 169, 172, 202, 204, 212, 267, 273, 305, 307, 325, 327, 331, 333, 338, 343], "wrong": [244, 265], "wrote": [9, 176, 181, 244, 265, 305, 307, 318, 320, 325, 327, 338, 340], "wt": [244, 263, 265], "ww2": [267, 274], "wwe": [244, 263, 265], "wyom": [267, 273], "x": [2, 3, 6, 7, 8, 14, 84, 96, 137, 141, 146, 153, 158, 159, 161, 162, 163, 167, 169, 171, 202, 215, 229, 233, 235, 238, 244, 265, 305, 306, 308, 312, 315, 317, 325, 328, 331, 334, 337, 338, 339, 347], "x_": [305, 306, 312, 313], "x_0": [305, 306, 308, 312, 313], "x_t": [305, 306, 308, 312, 313], "x_test": [4, 147, 150], "x_train": [4, 147, 150], "xb": [331, 333, 338, 343], "xgb": [5, 6, 202, 203, 235, 237, 325, 326, 327, 328, 329, 330], "xgb_model": [325, 328], "xgb_param": [325, 328], "xgboost": [7, 12, 229, 233, 327, 329, 330], "xgboost_predict": [4, 147, 150], "xgboosterror": [4, 147, 148], "xgboosttrain": [4, 12, 147, 148, 150, 325, 327, 328], "xgboosttrainer_2024": 12, "xgboosttrainer_81312_00000terminated10": 12, "xgboosttrainer_81312_00001terminated10": 12, "xgboosttrainer_81312_00002terminated10": 12, "xgbpredictor": [325, 329, 330], "xing": [35, 39, 66, 70], "xlabel": [305, 309, 312, 316, 318, 320, 322, 325, 329, 331, 335, 337, 338, 345], "xxx": [35, 39, 53, 56, 66, 69, 70], "xxxx": [35, 39, 66, 70], "xxxxx": [28, 30, 35, 39, 40, 43, 47, 53, 59, 66, 70, 75], "xxxxxx": [28, 30, 43, 45, 53, 56], "xxxxxxx": [43, 45, 53, 56], "xxxxxxxx": [28, 30, 43, 45, 53, 56], "xxxxxxxxx": [28, 30], "xxxxxxxxxx": [28, 30], "xxxxxxxxxxxx": [28, 30, 43, 45, 53, 56], "y": [1, 3, 5, 7, 13, 14, 84, 96, 109, 115, 117, 121, 123, 125, 128, 129, 130, 151, 152, 159, 161, 229, 233, 244, 265, 325, 328, 338, 339], "y_test": [4, 147, 150], "y_train": [4, 147, 150], "ya": [244, 265], "yaml": [11, 43, 46, 53, 58, 66, 73, 86, 98, 108, 109, 115, 117, 122, 125, 129, 137, 141, 146, 196, 201], "yanke": [244, 263, 265], "yann": [13, 14], "yara": [125, 128], "yard": [244, 265], "yb": [331, 333, 338, 343], "ye": [17, 22, 244, 265], "year": [4, 125, 128, 130, 147, 150, 244, 265, 267, 270, 274], "yellow": [4, 12, 108, 109, 112, 147, 150, 267, 270], "yellow_tripdata_": [4, 147, 150], "yellow_tripdata_2011": [9, 142, 144, 176, 179, 182], "yellow_tripdata_2021": [4, 147, 150], "yelp": [275, 277, 282], "yelp_review_ful": [275, 280], "yepo": [244, 265], "yesterdai": [244, 265], "yet": [267, 274, 318, 319, 320, 338, 339], "yield": [3, 159, 166], "ylabel": [305, 309, 312, 316, 318, 320, 322, 325, 327, 329, 331, 333, 335, 337, 338, 345], "yml": 0, "york": [4, 9, 147, 150, 176, 179, 331, 332], "you": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 77, 78, 80, 81, 82, 83, 84, 86, 87, 91, 92, 93, 94, 95, 96, 98, 99, 105, 108, 109, 110, 112, 113, 114, 115, 117, 118, 121, 122, 124, 125, 126, 127, 128, 129, 130, 132, 133, 136, 137, 138, 139, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 157, 159, 160, 162, 163, 164, 165, 166, 167, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 189, 190, 191, 192, 193, 194, 196, 197, 201, 204, 207, 208, 209, 210, 211, 212, 213, 214, 215, 221, 226, 227, 228, 229, 230, 233, 235, 236, 237, 239, 243, 244, 250, 253, 256, 261, 263, 264, 265, 267, 269, 270, 271, 272, 273, 274, 275, 277, 280, 281, 282, 285, 292, 296, 301, 304, 307, 309, 314, 315, 316, 320, 322, 327, 328, 333, 334, 335, 340, 341, 342, 345], "young": [267, 270, 274], "your": [0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 22, 25, 26, 28, 29, 30, 31, 32, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 54, 56, 58, 59, 60, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 92, 93, 94, 95, 99, 101, 106, 108, 109, 112, 113, 114, 115, 117, 120, 121, 122, 123, 125, 128, 129, 130, 131, 132, 142, 144, 145, 146, 147, 150, 153, 156, 158, 159, 164, 166, 168, 176, 181, 184, 186, 187, 190, 191, 193, 194, 196, 198, 200, 202, 203, 204, 205, 206, 207, 210, 212, 213, 216, 222, 225, 227, 229, 233, 235, 239, 243, 244, 253, 261, 264, 265, 267, 269, 273, 274, 275, 277, 281, 282, 305, 306, 312, 313, 318, 319, 320, 322, 324, 325, 326, 328, 330, 331, 332, 333, 335, 337, 338, 339, 344, 345, 347], "your_anyscale_org_id": [35, 39], "your_gcp_project_nam": [66, 78], "your_project_id": [35, 38], "yourself": [3, 159, 164, 305, 306], "yr": [244, 265], "ytick": [325, 329], "yunikorn": [24, 26], "z": [244, 265, 331, 333], "zentropa": [267, 273, 274], "zero": [8, 86, 90, 98, 106, 108, 109, 113, 114, 117, 124, 142, 145, 169, 170, 275, 280, 292, 299, 304, 325, 327, 331, 334, 335, 337, 338, 343, 347], "zero_copy_onli": [325, 328], "zero_grad": [5, 7, 13, 14, 202, 206, 217, 223, 229, 232, 233, 275, 280, 318, 322, 331, 335, 338, 343], "zeros_lik": [331, 335], "zilliz": [8, 169, 170], "zip": [7, 14, 229, 231, 305, 307, 311, 318, 320, 324, 325, 329, 338, 340], "zip_ref": [318, 320], "zipfil": [318, 320], "zone": [9, 17, 22, 43, 45, 53, 56, 66, 71, 176, 179], "zprofil": [1, 151, 152], "zsh": [66, 69], "zshrc": [66, 69], "zuoma": [244, 265], "\u03b8": [312, 314, 317], "\u03c0": [84, 96, 312, 314], "\u03f5": [305, 308, 312, 315]}, "titles": ["Ray Enablement Content: Jupyter Book Publishing", "Introduction to Ray: Developer", "Introduction to Ray Core: Getting Started", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "Introduction to Ray Train + PyTorch", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "Introduction to Ray Tune", "Introduction to Ray Data: Industry Landscape", "Introduction to Ray Data: Ray Data + Structured Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "Introduction to Ray Serve with PyTorch", "Introduction to the Ray AI Libraries", "Introduction to Ray Train", "Intro to Ray Tune", "Intro to Ray Data", "Intro to Ray Serve", "Anyscale Administrator Overview", "Anyscale Administrator Overview", "1. What is an Anyscale Cloud?", "2. Cloud Deployment Types", "3. A Demonstrative Example of Resource Creation with AWS EC2", "3.1 IAM Role Definition", "4. Register Anyscale Cloud to Your Cloud Provider", "Deployment Options: Virtual Machines vs. Kubernetes", "Deployment Options: Virtual Machines vs. Kubernetes", "2. Virtual Machines (VM) vs. Kubernetes (K8s)", "3. (Optional) More Kubernetes Deployments Components", "Introduction: Deploy Anyscale Ray on AWS EC2 Instances", "Introduction: Deploy Anyscale Ray on AWS EC2 Instances", "1. Create Anyscale Resources with Terraform", "2. Register the Anyscale Cloud", "3. Test", "4. Cleanup", "5. Conclusion", "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)", "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)", "Prerequisites", "1. Installation", "2. Create Anyscale Resources with Terraform", "3. Register the Anyscale Cloud", "4. Test", "5. Cleanup", "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster", "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster", "1. Create Anyscale Resources with Terraform", "2. Install Kubernetes Components", "3. Register the Anyscale Cloud", "4. Install the Anyscale Operator", "5. Verify the Installation", "6. Test", "7. Clean up", "8. Conclusion", "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster", "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster", "Prerequisites", "1. Create Anyscale Resources with Terraform", "2. Attach Required IAM Policies to Your existing EKS\u2019s Node Role", "3. Install Kubernetes Components", "4. Register the Anyscale Cloud", "5. Install the Anyscale Operator", "6. Verify the Installation", "7. Test", "8. Troubleshooting", "9. Clean up", "10. Conclusion", "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster", "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster", "Prerequisites", "1. Installation", "2. Create Anyscale Resources with Terraform", "3. Troubleshooting GPU Availability", "4. kubectl Configuration", "5. Install NGINX Ingress Controller", "6. (Optional) Upgrade Anyscale Dependencies", "7. Register the Anyscale Cloud", "8. Install the Anyscale Operator", "8. Test", "9. Cleanup", "Welcome to Anyscale Administration", "101 \u2014 Introduction to Anyscale Workspaces", "101 \u2013 Developing Application with Anyscale", "101 \u2013 Compute Configs and Execution Environments in Anyscale", "101 \u2013 Storage Options in the Anyscale Platform", "101 \u2013 Debug and Monitor Your Anyscale Application", "101 \u2013 Introduction to Anyscale Jobs", "101 \u2013  Introduction to Anyscale Services", "101 \u2013 Collaboration on Anyscale", "101 - Anyscale Organization and Cloud Setup", "Content Used", "Sources", "Last Updated 6/19", "101 \u2014 Introduction to Anyscale Workspaces", "101 \u2013 Developing Application with Anyscale", "101 \u2013 Compute Configs and Execution Environments in Anyscale", "101 \u2013 Storage Options in the Anyscale Platform", "101 \u2013 Debug and Monitor Your Anyscale Application", "101 \u2013 Introduction to Anyscale Jobs", "101 \u2013  Introduction to Anyscale Services", "101 \u2013 Collaboration on Anyscale", "101 - Anyscale Organization and Cloud Setup", "\ud83d\udccc Overview of Structure", "\ud83e\udde0 Summary", "Content Used", "Part 1. Creating and Submitting your first job", "Part 2. Automation and Scheduling", "Sources", "Part 1: Starting your first Anyscale Service", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "What is LLM Serving?", "Key Concepts and Optimizations", "Challenges in LLM Serving", "Ray Serve LLM + Anyscale Architecture", "Getting Started with Ray Serve LLM", "Key Takeaways", "Deploy a Medium-Sized LLM with Ray Serve LLM", "Deploy a Medium-Sized LLM with Ray Serve LLM", "Overview: Why Medium-Sized Models?", "Setting up Ray Serve LLM", "Local Deployment &amp; Inference", "Deploying to Anyscale Services", "Advanced Topics: Monitoring &amp; Optimization", "Summary &amp; Outlook", "Advanced LLM Features with Ray Serve LLM", "Advanced LLM Features with Ray Serve LLM", "Overview: Advanced Features Preview", "Example: Deploying LoRA Adapters", "Example: Getting Structured JSON Output", "Example: Setting up Tool Calling", "How to Choose an LLM?", "Conclusion: Next Steps", "Observability Introduction", "Observability Introduction", "Observability Overview", "Setting Up Local Ray Observability", "Ray and Anyscale Observability Introduction", "Ray and Anyscale Observability Introduction", "Ray Observability", "Anyscale Observability", "Example", "Ray and Anyscale Observability in Detail", "Ray and Anyscale Observability in Detail", "Data Pipeline Observability (Ray Data)", "Web Application Observability (Ray Serve)", "Multi-Actor Ray Serve Tracing Example", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "1. Overview of the Ray AI Libraries", "2. Quick end-to-end example", "Introduction to Ray: Developer", "Introduction to Ray: Developer", "Introduction to Ray Core: Getting Started", "Introduction to Ray Core: Getting Started", "0. Overview", "1. Creating Remote Functions", "2. Executing Remote Functions", "4. Putting It All Together", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "1. Object store", "2. Chaining Tasks and Passing Data", "3. Task retries", "4. Task Runtime Environments", "5. Resource allocation and management", "6. Nested Tasks", "7. Pattern: Pipeline data processing and waiting for results", "8. Ray Actors", "Introduction to Ray Data: Industry Landscape", "Introduction to Ray Data: Industry Landscape", "The Compute Layer", "The Orchestration Layer", "Distributed Computing Frameworks", "Data Processing with Ray Data", "Ray Serve", "Introduction to Ray Data: Ray Data + Structured Data", "Introduction to Ray Data: Ray Data + Structured Data", "0. What is Ray Data?", "2. Loading Data", "3. Transforming Data", "4. Writing Data", "5. Data Operations: Shuffling, Grouping and Aggregation", "6. When to use Ray Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "1. When to Consider Ray Data", "2. How to work with Ray Data", "3. Loading data", "3. Lazy execution mode", "4. Transforming data", "5. Stateful transformations with Ray Actors", "6. Materializing data", "7. Data Operations: grouping, aggregation, and shuffling", "8. Persisting data", "9. Ray Data in production", "Introduction to Ray Serve with PyTorch", "Introduction to Ray Serve with PyTorch", "1. When to Consider Ray Serve", "2. Overview of Ray Serve", "3. Implement an image classification service", "4. Development workflow", "\ud83d\udcda 01 \u00b7 Introduction to Ray Train", "\ud83d\udcda 01 \u00b7 Introduction to Ray Train", "01 \u00b7 Imports", "04 \u00b7 Define ResNet-18 Model for MNIST", "05 \u00b7 Define the Ray Train Loop (DDP per-worker)", "06 \u00b7 Define <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop_config</span></code>", "07 \u00b7 Configure Scaling with <code class=\"docutils literal notranslate\"><span class=\"pre\">ScalingConfig</span></code>", "08 \u00b7 Wrap the Model with <code class=\"docutils literal notranslate\"><span class=\"pre\">prepare_model()</span></code>", "09 \u00b7 Build the DataLoader with <code class=\"docutils literal notranslate\"><span class=\"pre\">prepare_data_loader()</span></code>", "10 \u00b7 Report Training Metrics", "11 \u00b7 Save Checkpoints and Report Metrics", "14 \u00b7 Create the <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code>", "16 \u00b7 Inspect the Training Results", "18 \u00b7 Load a Checkpoint for Inference", "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data", "01 \u00b7 Define Training Loop with Ray Data", "02 \u00b7 Build DataLoader from Ray Data", "03 \u00b7 Prepare Dataset for Ray Data", "05 \u00b7 Define Image Transformation", "07 \u00b7 Configure <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code> with Ray Data", "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train", "01 \u00b7 Modify Training Loop to Enable Checkpoint Loading", "02 \u00b7 Save Full Checkpoint with Extra State", "04 \u00b7 Launch Fault-Tolerant Training", "05 \u00b7 Manual Restoration from Checkpoints", "07 \u00b7 Clean Up Cluster Storage", "\ud83c\udf89 Wrapping Up &amp; Next Steps", "Introduction to Ray Tune", "Introduction to Ray Tune", "1. Loading the data", "2. Starting out with vanilla PyTorch", "3. Hyperparameter tuning with Ray Tune", "4. Ray Tune in Production", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "1. When to use Ray Train", "2. Single GPU Training with PyTorch Lightning", "3. Distributed Training with Ray Train and PyTorch Lightning", "4. Ray Train in Production", "Batch Inference with Ray Data\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb <strong>Launch Locally</strong>: You can run this notebook locally.\ud83d\ude80 <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)This example shows how to do batch inference with Ray Data.Batch inference with Ray Data enables you to efficiently generate predictions from machine learning models on large datasets by processing multiple data points at once. Instead of running inference on one row at a time, which can be slow and resource-inefficient, batch inference leverages vectorized computation and parallelism to maximize throughput. This is especially useful when working with modern deep learning models, which are optimized for batch processing on CPUs, GPUs, or Apple Silicon devices.The typical workflow begins by loading your dataset\u2014such as a public dataset from Hugging Face\u2014into a Ray Dataset. Ray Data can automatically partition the data for parallel processing, or you can repartition it explicitly to control the number of data blocks. Once the data is loaded, you define a callable class (such as a text embedding model) that loads the machine learning model in its constructor and implements a <code class=\"docutils literal notranslate\"><span class=\"pre\">__call__</span></code> method to process each batch. Ray Data\u2019s <code class=\"docutils literal notranslate\"><span class=\"pre\">map_batches</span></code> API is then used to apply this callable to each batch of data, with options to control concurrency and resource allocation (e.g., number of GPUs).This approach allows you to spin up multiple concurrent model instances, each processing different batches of data in parallel. The result is a significant speedup in inference time, especially for large datasets. After inference, you can materialize the results, inspect the output, and shut down the Ray cluster to free up resources. Batch inference with Ray Data is scalable, flexible, and integrates seamlessly with modern ML workflows, making it a powerful tool for production and research environments alike.", "Batch Inference with Ray Data\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb <strong>Launch Locally</strong>: You can run this notebook locally.\ud83d\ude80 <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)This example shows how to do batch inference with Ray Data.Batch inference with Ray Data enables you to efficiently generate predictions from machine learning models on large datasets by processing multiple data points at once. Instead of running inference on one row at a time, which can be slow and resource-inefficient, batch inference leverages vectorized computation and parallelism to maximize throughput. This is especially useful when working with modern deep learning models, which are optimized for batch processing on CPUs, GPUs, or Apple Silicon devices.The typical workflow begins by loading your dataset\u2014such as a public dataset from Hugging Face\u2014into a Ray Dataset. Ray Data can automatically partition the data for parallel processing, or you can repartition it explicitly to control the number of data blocks. Once the data is loaded, you define a callable class (such as a text embedding model) that loads the machine learning model in its constructor and implements a <code class=\"docutils literal notranslate\"><span class=\"pre\">__call__</span></code> method to process each batch. Ray Data\u2019s <code class=\"docutils literal notranslate\"><span class=\"pre\">map_batches</span></code> API is then used to apply this callable to each batch of data, with options to control concurrency and resource allocation (e.g., number of GPUs).This approach allows you to spin up multiple concurrent model instances, each processing different batches of data in parallel. The result is a significant speedup in inference time, especially for large datasets. After inference, you can materialize the results, inspect the output, and shut down the Ray cluster to free up resources. Batch inference with Ray Data is scalable, flexible, and integrates seamlessly with modern ML workflows, making it a powerful tool for production and research environments alike.", "Batch Inference with Ray Data", "Batch Inference with Ray Data", "Architecture", "Architecture", "Architecture Overview", "Load a datasetLoad a dataset from hugging face or local and convert into Ray Dataset. A Ray cluster automatically initialized on local or on Anyscale platform. You can also use <strong>ray.init()</strong> To explicitly create or connect to an existing Ray cluster.https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html#ray.init# load a Hugging Face datasethf_dataset = load_dataset(\u201ccardiffnlp/tweet_eval\u201d, \u201csentiment\u201d, split=\u201dtrain\u201d)# Convert the Hugging Face dataset to a Ray Datasetds = ray.data.from_huggingface(hf_dataset).repartition(2) # repartition to 2 blocks for parallel processing. Not necessary if already partitioned due to the size of the dataset.", "Load a datasetLoad a dataset from hugging face or local and convert into Ray Dataset. A Ray cluster automatically initialized on local or on Anyscale platform. You can also use <strong>ray.init()</strong> To explicitly create or connect to an existing Ray cluster.https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html#ray.init# load a Hugging Face datasethf_dataset = load_dataset(\u201ccardiffnlp/tweet_eval\u201d, \u201csentiment\u201d, split=\u201dtrain\u201d)# Convert the Hugging Face dataset to a Ray Datasetds = ray.data.from_huggingface(hf_dataset).repartition(2) # repartition to 2 blocks for parallel processing. Not necessary if already partitioned due to the size of the dataset.", "Loading a Dataset", "Batch Inference ClassMany machine learning models are optimized for processing a batch of inputs at once. When working with a large dataset, there could be many batches of data. Instead of loading machine learning models repeatedly to run each batch of data, you want to spin up a number of actor processes that are <strong>initialized once</strong> with your model <strong>and reused</strong> to process multiple batches. To implement this, you can use the <code class=\"docutils literal notranslate\"><span class=\"pre\">map_batches</span></code> API with a \u201cCallable\u201d class method that implements:- <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code>: Initialize any expensive state.- <code class=\"docutils literal notranslate\"><span class=\"pre\">__call__</span></code>: Perform the stateful transformation.In this example, a lightweight sentence transformer model, <strong>all-MiniLM-L6-v2</strong> is used to generate embeddings of text data.", "Batch Inference ClassMany machine learning models are optimized for processing a batch of inputs at once. When working with a large dataset, there could be many batches of data. Instead of loading machine learning models repeatedly to run each batch of data, you want to spin up a number of actor processes that are <strong>initialized once</strong> with your model <strong>and reused</strong> to process multiple batches. To implement this, you can use the <code class=\"docutils literal notranslate\"><span class=\"pre\">map_batches</span></code> API with a \u201cCallable\u201d class method that implements:- <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code>: Initialize any expensive state.- <code class=\"docutils literal notranslate\"><span class=\"pre\">__call__</span></code>: Perform the stateful transformation.In this example, a lightweight sentence transformer model, <strong>all-MiniLM-L6-v2</strong> is used to generate embeddings of text data.", "Defining the Batch Inference Class", "Create a batch data and call the modelDefine a Ray Data map_batches function to embed text using the SentenceTransformer model. This function will be applied to each batch of data in the Ray Data dataset. It will take a batch of sentences, encode them into embeddings, and return the batch with the embeddings added.Showcasing two options of to do batch inference based on if the ray cluster has have GPU nodes or if it has just CPU nodes. The second option also works on a local ray cluster on an Apple Silicon Mac with MPS.# setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPSworker_device = \u201ccpu\u201d # or \u201ccuda\u201d if you have a nvidia gpu on worker nodes# batch_size should be set based on VRAM if worker_device == \u201ccuda\u201d: # if you have a nvidia gpu on worker nodes    # adjust batch_size based on the VRAM available on the GPU    ds = ds.map_batches(TextEmbedder, num_gpus=1, concurrency=2, batch_size=64) # 2 nodes with 1 GPU eachelse:    ds = ds.map_batches(TextEmbedder, concurrency=2, batch_size=64) # either cpu or mps (on a mac)", "Create a batch data and call the modelDefine a Ray Data map_batches function to embed text using the SentenceTransformer model. This function will be applied to each batch of data in the Ray Data dataset. It will take a batch of sentences, encode them into embeddings, and return the batch with the embeddings added.Showcasing two options of to do batch inference based on if the ray cluster has have GPU nodes or if it has just CPU nodes. The second option also works on a local ray cluster on an Apple Silicon Mac with MPS.# setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPSworker_device = \u201ccpu\u201d # or \u201ccuda\u201d if you have a nvidia gpu on worker nodes# batch_size should be set based on VRAM if worker_device == \u201ccuda\u201d: # if you have a nvidia gpu on worker nodes    # adjust batch_size based on the VRAM available on the GPU    ds = ds.map_batches(TextEmbedder, num_gpus=1, concurrency=2, batch_size=64) # 2 nodes with 1 GPU eachelse:    ds = ds.map_batches(TextEmbedder, concurrency=2, batch_size=64) # either cpu or mps (on a mac)", "Creating a Data Batch and Calling the Model", "Run inference on the entire datasetExecute and materialize this dataset into object store memory. This operation will trigger execution of the lazy transformations performed on this dataset. The embedding model \u2018TextEmbedder\u2019 in map_batches() is called on the entire dataset.# Run inference on the entire dataset# Note that this does not mutate the original Dataset.materialized_ds = ds.materialize()# metadata after inferenceprint(\u2018** Original dataset:\u2019, ds)print(\u2018\\n** Materialized dataset:\u2019, materialized_ds)# Show a few rows of the materialized dataset with embeddingsmaterialized_ds.show(3)", "Run inference on the entire datasetExecute and materialize this dataset into object store memory. This operation will trigger execution of the lazy transformations performed on this dataset. The embedding model \u2018TextEmbedder\u2019 in map_batches() is called on the entire dataset.# Run inference on the entire dataset# Note that this does not mutate the original Dataset.materialized_ds = ds.materialize()# metadata after inferenceprint(\u2018** Original dataset:\u2019, ds)print(\u2018\\n** Materialized dataset:\u2019, materialized_ds)# Show a few rows of the materialized dataset with embeddingsmaterialized_ds.show(3)", "Running inference on the entire dataset", "Data Processing and ML examples with Ray", "Batch Inference with Ray Data", "Architecture", "Load a dataset", "Batch Inference Class", "Create a batch data and call the model", "Run inference on the entire dataset", "Data Processing with Ray Data", "Data Processing and ML examples with Ray", "Data Processing with Ray Data", "Library Imports", "Convert to Ray Dataset", "Filter Ray Dataset", "Join Two Ray Datasets", "Preprocessing with a Tokenizer", "Distributed training with Ray Train, PyTorch and Hugging Face", "Data Processing and ML examples with Ray", "Distributed training with Ray Train, PyTorch and Hugging Face", "1. Architecture", "3. Metrics Setup", "4. Training function per worker", "5. Main Training Function", "6. Start Training", "Online Model Serving with Ray Serve\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb <strong>Launch Locally</strong>: You can run this notebook locally.\ud83d\ude80 <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)Model serving is the process of deploying machine learning models to production so that they can be accessed and used by applications or users. It involves creating an API or interface that allows users to send requests to the modeland receive predictions in response. There are several libraries and frameworks available for model serving, each with its own features and capabilities. In this notebook, we showcase Ray Serve and FastAPI to deploy a sentiment analysismachine learning (ML) model.", "Online Model Serving with Ray Serve\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb <strong>Launch Locally</strong>: You can run this notebook locally.\ud83d\ude80 <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)Model serving is the process of deploying machine learning models to production so that they can be accessed and used by applications or users. It involves creating an API or interface that allows users to send requests to the modeland receive predictions in response. There are several libraries and frameworks available for model serving, each with its own features and capabilities. In this notebook, we showcase Ray Serve and FastAPI to deploy a sentiment analysismachine learning (ML) model.", "Online Model Serving with Ray Serve", "Architecture### Import librariesIn addition to ray and serve, we also import FastAPI to create webservice and Hugging Face transformers to download ML models.# Import ray serve and FastAPI librariesimport rayfrom ray import servefrom fastapi import FastAPI# library for pre-trained modelsfrom transformers import pipeline", "Architecture### Import librariesIn addition to ray and serve, we also import FastAPI to create webservice and Hugging Face transformers to download ML models.# Import ray serve and FastAPI librariesimport rayfrom ray import servefrom fastapi import FastAPI# library for pre-trained modelsfrom transformers import pipeline", "Architecture Overview", "FastAPI webservice and deploy a modelFastAPI is used to create a webservice \u2018app\u2019 to accept HTTP requests.MySentimentModel class loads the ML model and defines <em>predict</em> function for online inference. &#64;serve.deployment decorator defines the Ray Serve deployment.<em>&#64;app.get()</em> is used to create a GET \u2018/predict\u2019 route. Similarly, &#64;app.post() can be used POST requests. See https://docs.ray.io/en/latest/serve/http-guide.html for more details.In this example, <em>application_logic()</em> function is used to define a sample transformation or business logic that can be applied before sending the input to the ML model for inference. See inline comments for further explanation.### Scaling deployment<em>num_replicas</em> parameter sets the number of instances of the deployment. FastAPI and RayServe automatically load balances to send requests to each instance. There are more options to set the <em>accelerator_type</em> to GPU and even use fractional GPUs. See configuration options here: https://docs.ray.io/en/latest/serve/configure-serve-deployment.html .", "FastAPI webservice and deploy a modelFastAPI is used to create a webservice \u2018app\u2019 to accept HTTP requests.MySentimentModel class loads the ML model and defines <em>predict</em> function for online inference. &#64;serve.deployment decorator defines the Ray Serve deployment.<em>&#64;app.get()</em> is used to create a GET \u2018/predict\u2019 route. Similarly, &#64;app.post() can be used POST requests. See https://docs.ray.io/en/latest/serve/http-guide.html for more details.In this example, <em>application_logic()</em> function is used to define a sample transformation or business logic that can be applied before sending the input to the ML model for inference. See inline comments for further explanation.### Scaling deployment<em>num_replicas</em> parameter sets the number of instances of the deployment. FastAPI and RayServe automatically load balances to send requests to each instance. There are more options to set the <em>accelerator_type</em> to GPU and even use fractional GPUs. See configuration options here: https://docs.ray.io/en/latest/serve/configure-serve-deployment.html .", "Building a FastAPI Web Service and Deploying a Model", "Online Model Serving with Ray Serve", "Deploy the modelserve.run(MySentimentModel.bind()) # Bind the deployment to the Ray Serve runtimeDeploymentHandle(deployment=\u2019MySentimentModel\u2019)", "Deploy the modelserve.run(MySentimentModel.bind()) # Bind the deployment to the Ray Serve runtimeDeploymentHandle(deployment=\u2019MySentimentModel\u2019)", "Simulate Client: Send test requestsWe use <em>requests</em> library to send HTTP requests to the deployed model.Note: if you encounter any errors with serve not able to start, most likely it is due to previous instance of serve not being shutdown properly. Restart the notebook or see towards the end of notebook to see how to gracefully shutdown ray serve and the ray cluster.import requests # used to send HTTP requests to the deployed model# Query the deployed modelresponse = requests.get(\u201dhttp://localhost:8000/predict\u201d, params={\u201ctext\u201d: \u201cI love Ray Serve!\u201d})print(response.json())  # Should print the sentiment analysis result{\u2018text\u2019: \u2018i love ray serve!\u2019, \u2018sentiment\u2019: [{\u2018label\u2019: \u2018POSITIVE\u2019, \u2018score\u2019: 0.9998507499694824}]}", "Deploying Our Model and Testing it", "Shutdown the Ray Serve instances and Ray Cluster# stop ray serveserve.shutdown()  # Shutdown Ray Serve when done, ray cluster will still be runningray.shutdown()  # Shutdown Ray cluster", "Shutdown the Ray Serve instances and Ray Cluster# stop ray serveserve.shutdown()  # Shutdown Ray Serve when done, ray cluster will still be runningray.shutdown()  # Shutdown Ray cluster", "Shutdown and Summary", "Data Processing and ML examples with Ray", "Online Model Serving with Ray Serve", "Architecture", "FastAPI webservice and deploy a model", "Simulate Client: Send test requests", "04-d1 Generative computer-vision pattern with Ray Train", "04-d1 Generative computer-vision pattern with Ray Train", "1. Imports and setup", "8. Pixel diffusion LightningModule", "9. Ray Train <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop</span></code> (Lightning + Ray integration)", "12. Resume from latest checkpoint", "13. Reverse diffusion sampler", "04-d2 Diffusion-Policy Pattern with Ray Train", "04-d2 Diffusion-Policy Pattern with Ray Train", "1. Imports and setup", "4. DiffusionPolicy LightningModule", "5. Distributed Train loop with checkpointing", "8. Reverse diffusion helper", "04e Recommendation system pattern with Ray Train", "04e Recommendation system pattern with Ray Train", "1. Imports", "7. Define matrix factorization model", "8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)", "11. Resume training from checkpoint", "12. Inference: recommend top-N items for a user", "04b Tabular workload pattern with Ray Train", "04b Tabular workload pattern with Ray Train", "1. Imports", "8. Define the Ray Train worker loop (Arrow-based, memory-efficient)", "12. Confusion matrix visualization", "15. Continue training from the latest checkpoint", "04c Time-Series workload pattern with Ray Train", "04c Time-Series workload pattern with Ray Train", "1. Imports", "9. PositionalEncoding and Transformer model", "10. Ray Train training loop (with teacher forcing)", "13. Resume training from checkpoint", "14. Inference helper \u2014 Ray Data batch predictor on GPU", "04a Computer-vision pattern with Ray Train", "04a Computer-vision pattern with Ray Train", "1. Imports", "6. Custom <code class=\"docutils literal notranslate\"><span class=\"pre\">Food101Dataset</span></code> for Parquet", "10. Helper: Ray-prepared DataLoaders", "11. <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop_per_worker</span></code>", "12. Launch distributed training with <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code>", "13. Plot training and validation loss curves", "14. Demonstrate fault-tolerant resumption", "15. Batch inference with Ray Data", "Ray Enablement Content"], "titleterms": {"": [53, 57, 82, 94, 133, 136, 241, 242, 254, 255, 257, 258, 312, 313, 325, 326, 331, 332], "0": [2, 9, 17, 18, 153, 155, 176, 178, 202, 212, 293, 295], "01": [202, 203, 204, 215, 217, 223, 228], "02": [202, 204, 216, 218, 224, 228], "02_service_hello_world": [90, 107], "03": [202, 204, 219, 222, 224, 228], "04": [202, 205, 219, 225, 305, 306, 312, 313], "04a": [338, 339], "04b": [325, 326], "04c": [331, 332], "04e": [318, 319], "05": [202, 206, 220, 226], "06": [202, 207, 220, 226], "07": [202, 208, 221, 227], "08": [202, 209, 221], "09": [202, 210], "1": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 22, 24, 25, 26, 28, 30, 35, 38, 39, 43, 45, 46, 53, 56, 58, 66, 69, 70, 83, 85, 86, 89, 90, 91, 95, 97, 98, 104, 105, 107, 108, 109, 112, 113, 114, 115, 117, 119, 123, 125, 131, 133, 136, 146, 147, 149, 150, 151, 152, 153, 156, 158, 159, 161, 164, 165, 167, 176, 178, 184, 186, 190, 191, 193, 196, 198, 229, 231, 235, 237, 238, 239, 254, 255, 275, 278, 305, 307, 312, 313, 314, 318, 320, 325, 327, 331, 333, 338, 340], "10": [5, 53, 65, 202, 211, 305, 307, 309, 312, 317, 318, 322, 325, 328, 331, 335, 338, 340, 342], "100k": [318, 320], "101": [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 103, 106, 305, 307, 338, 339, 340], "11": [202, 212, 305, 309, 318, 323, 325, 328, 331, 335, 338, 343], "12": [202, 212, 305, 310, 318, 324, 325, 329, 331, 335, 338, 344], "128": [254, 255, 256], "13": [202, 212, 305, 311, 318, 324, 325, 329, 331, 336, 338, 345], "14": [202, 213, 305, 311, 318, 324, 325, 329, 331, 337, 338, 346], "15": [202, 213, 305, 311, 325, 330, 331, 337, 338, 347], "16": [202, 214, 325, 330, 331, 337, 338, 347], "17": [202, 214, 325, 330, 338, 347], "18": [202, 205, 215], "19": [91, 202, 215], "2": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 22, 24, 26, 28, 31, 35, 38, 39, 43, 46, 53, 57, 58, 66, 69, 70, 83, 85, 89, 90, 91, 95, 97, 104, 105, 107, 108, 109, 112, 113, 114, 115, 117, 123, 125, 131, 133, 136, 146, 147, 150, 151, 152, 153, 157, 158, 159, 162, 165, 167, 176, 179, 184, 187, 188, 190, 191, 193, 196, 199, 229, 232, 235, 238, 239, 248, 249, 254, 255, 275, 278, 305, 307, 312, 313, 314, 318, 320, 325, 327, 331, 333, 338, 340], "20": [202, 215], "2025": [241, 242, 283, 284], "3": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 24, 26, 27, 28, 32, 35, 40, 43, 46, 47, 53, 58, 66, 71, 83, 89, 90, 91, 95, 104, 105, 107, 108, 109, 112, 113, 114, 115, 117, 119, 123, 125, 131, 133, 136, 147, 150, 151, 152, 153, 157, 159, 163, 165, 176, 180, 184, 188, 189, 193, 196, 200, 229, 233, 235, 238, 239, 257, 258, 275, 279, 305, 307, 312, 313, 314, 318, 320, 325, 327, 331, 333, 338, 340], "30": [331, 333], "4": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 22, 23, 24, 26, 27, 28, 33, 35, 41, 43, 46, 48, 53, 58, 59, 66, 72, 83, 89, 90, 95, 104, 107, 108, 109, 112, 113, 115, 117, 123, 125, 131, 133, 136, 147, 150, 151, 152, 153, 158, 159, 164, 176, 181, 184, 190, 196, 201, 229, 234, 235, 238, 239, 240, 275, 280, 305, 307, 312, 315, 318, 320, 325, 327, 331, 333, 338, 340], "5": [1, 3, 4, 5, 6, 9, 10, 13, 14, 15, 17, 22, 28, 34, 35, 42, 43, 49, 53, 60, 66, 73, 89, 90, 104, 107, 117, 123, 147, 150, 151, 152, 159, 165, 176, 182, 184, 191, 235, 239, 275, 281, 305, 307, 312, 316, 318, 320, 325, 327, 331, 333, 338, 340], "6": [1, 3, 4, 5, 6, 9, 10, 13, 17, 22, 43, 50, 53, 61, 66, 74, 89, 91, 104, 147, 150, 151, 152, 159, 166, 176, 183, 184, 192, 235, 239, 275, 282, 305, 307, 312, 316, 318, 320, 325, 327, 331, 333, 338, 341], "64": [254, 255], "7": [1, 3, 5, 6, 9, 10, 13, 17, 22, 43, 51, 53, 62, 66, 75, 89, 104, 151, 152, 159, 167, 176, 183, 184, 193, 235, 239, 275, 282, 305, 307, 312, 316, 318, 321, 325, 327, 331, 333, 338, 341], "70b": [117, 119], "8": [1, 3, 5, 9, 10, 13, 17, 22, 43, 52, 53, 63, 66, 76, 77, 151, 152, 159, 168, 176, 183, 184, 194, 275, 282, 305, 308, 312, 317, 318, 322, 325, 328, 331, 333, 335, 338, 341], "8000": [293, 295], "9": [5, 10, 53, 64, 66, 78, 184, 195, 305, 309, 312, 317, 318, 322, 325, 328, 331, 334, 338, 341], "9998507499694824": [293, 295], "A": [17, 21, 43, 44, 66, 67, 248, 249], "For": [89, 105, 348], "If": [254, 255, 257, 258], "In": [1, 8, 89, 90, 104, 107, 151, 152, 169, 170, 241, 242, 251, 252, 257, 258, 283, 284, 289, 290], "It": [1, 2, 151, 152, 153, 158, 254, 255, 257, 258, 283, 284], "Not": [248, 249], "On": [8, 10, 169, 174, 184, 190], "The": [8, 85, 89, 97, 103, 104, 108, 109, 111, 169, 170, 171, 172, 241, 242, 254, 255, 257, 258], "There": [283, 284, 289, 290, 297, 298], "These": [125, 127], "To": [24, 26, 248, 249, 251, 252], "With": [283, 284], "__call__": [241, 242, 251, 252], "__init__": [251, 252], "abl": [293, 295], "about": [2, 3, 15, 153, 158, 159, 164, 241, 242, 283, 284], "accelerator_typ": [289, 290], "accept": [283, 284, 289, 290], "access": [5, 6, 13, 24, 26, 235, 239, 283, 284], "accomplish": [117, 124, 125, 132], "across": [283, 284], "action": [312, 313, 317], "activ": [1, 5, 6, 13, 151, 152, 235, 239], "actor": [3, 10, 15, 146, 159, 160, 168, 184, 191, 202, 215, 241, 242, 251, 252], "ad": [0, 254, 255, 305, 306], "adapt": [125, 128], "add": [1, 151, 152], "addit": [133, 136, 283, 284, 286, 287], "adjust": [254, 255], "admin": 348, "administr": [17, 18, 79], "advanc": [3, 16, 117, 123, 125, 126, 127, 132, 159, 160], "after": [89, 104, 241, 242, 257, 258, 259], "again": [89, 105], "aggreg": [9, 10, 15, 176, 182, 184, 193], "ai": [4, 8, 12, 147, 148, 149, 169, 171, 348], "alert": [142, 145], "align": [125, 131], "alik": [241, 242], "all": [2, 9, 10, 89, 105, 153, 158, 176, 182, 184, 193, 241, 242, 251, 252, 283, 284, 331, 337], "alloc": [3, 159, 165, 241, 242], "allow": [241, 242, 283, 284], "alreadi": [248, 249], "also": [89, 104, 248, 249, 254, 255, 257, 258, 283, 284, 286, 287], "altern": [108, 109, 112], "an": [3, 4, 11, 12, 16, 17, 19, 53, 54, 82, 89, 94, 104, 117, 122, 125, 131, 147, 148, 150, 159, 161, 196, 200, 248, 249, 254, 255, 283, 284, 312, 317], "analysi": [293, 295, 297, 298], "analysismachin": [283, 284], "ani": [251, 252, 293, 295], "annot": 14, "anti": [2, 153, 158], "anyscal": [17, 18, 19, 22, 23, 24, 25, 26, 28, 29, 30, 31, 35, 36, 39, 40, 43, 44, 45, 47, 48, 53, 54, 56, 59, 60, 66, 67, 70, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 114, 117, 122, 137, 138, 140, 142, 143, 145, 241, 242, 248, 249, 283, 284, 305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339, 348], "apach": [8, 169, 170], "api": [35, 38, 66, 69, 146, 241, 242, 248, 249, 251, 252, 257, 258, 283, 284], "app": [283, 284, 289, 290], "appl": [241, 242, 254, 255], "appli": [202, 220, 241, 242, 254, 255, 257, 258, 289, 290], "applic": [8, 11, 81, 84, 91, 93, 96, 142, 145, 169, 173, 196, 199, 283, 284], "application_log": [289, 290], "approach": [125, 127, 241, 242], "ar": [17, 20, 241, 242, 251, 252, 254, 255, 283, 284, 289, 290, 297, 298, 305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339], "architectur": [17, 22, 108, 109, 114, 146, 241, 242, 244, 247, 262, 275, 278, 283, 284, 286, 287, 288, 292, 302], "architecturearchitectur": [245, 246], "area": 13, "argument": [3, 89, 105, 159, 161], "arm": [1, 151, 152], "arrow": [8, 169, 170, 325, 328], "artifact": [331, 337], "assist": [125, 128, 130], "assumpt": [133, 136], "attach": [53, 57], "auroc": 13, "authent": [35, 38, 66, 69], "auto": [90, 107], "autom": [85, 89, 97, 105], "automat": [202, 224, 241, 242, 248, 249, 289, 290], "autosc": [10, 16, 184, 191, 283, 284], "autoscal": [43, 46, 53, 58, 297, 298], "avail": [3, 66, 71, 89, 105, 146, 159, 165, 254, 255, 257, 258, 283, 284], "avoid": [257, 258, 259], "aw": [17, 21, 28, 29, 43, 44, 46, 53, 54, 58], "awai": [202, 203, 216, 222, 305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339], "b": [241, 242], "backend": [24, 26], "balanc": [43, 46, 53, 58, 283, 284, 289, 290, 325, 327], "base": [9, 10, 15, 176, 182, 184, 193, 254, 255, 256, 318, 319, 325, 328], "basic": [89, 104], "batch": [3, 4, 8, 9, 10, 15, 108, 109, 112, 147, 150, 159, 167, 169, 173, 176, 182, 184, 193, 241, 242, 243, 244, 251, 252, 253, 254, 255, 256, 257, 258, 261, 264, 265, 325, 327, 329, 331, 333, 337, 338, 341, 347, 348], "batch_siz": [254, 255, 256, 257, 258], "befor": [289, 290], "begin": [241, 242], "being": [293, 295], "below": [89, 90, 104, 107], "benchmark": [125, 131], "benefit": [90, 106, 125, 128, 129, 130], "best": [257, 258, 305, 311], "better": [283, 284], "bind": [293, 294], "block": [9, 10, 15, 176, 182, 184, 188, 193, 241, 242, 248, 249], "book": 0, "both": [254, 255, 256], "bound": [3, 159, 165], "breakdown": [117, 120], "build": [0, 5, 13, 202, 210, 218, 291, 325, 328], "built": [283, 284], "busi": [289, 290], "cach": [108, 109, 112], "california": [325, 327], "call": [2, 125, 130, 153, 158, 244, 254, 255, 256, 257, 258, 265], "callabl": [241, 242, 251, 252, 257, 258], "caller": [254, 255], "can": [89, 104, 241, 242, 248, 249, 251, 252, 254, 255, 257, 258, 283, 284, 289, 290, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "capabl": [283, 284], "car": [125, 129], "cardiffnlp": [248, 249], "case": [1, 125, 131, 151, 152, 257, 258], "caus": [3, 159, 167], "chain": [3, 159, 162], "challeng": [8, 108, 109, 113, 169, 173], "characterist": [86, 98], "check": [89, 90, 104, 105, 107, 305, 307, 331, 333, 338, 340], "checkout": [90, 107], "checkpoint": [5, 6, 13, 202, 212, 215, 223, 224, 226, 235, 239, 305, 310, 311, 312, 316, 318, 322, 323, 325, 330, 331, 336], "choic": [283, 284], "choos": [117, 119, 125, 131], "class": [241, 242, 244, 251, 252, 253, 257, 258, 264, 289, 290, 325, 327], "classif": [11, 196, 200, 325, 326, 338, 339], "classifi": 16, "classmani": [251, 252], "clean": [1, 4, 16, 43, 51, 53, 64, 84, 85, 86, 96, 97, 98, 147, 150, 151, 152, 202, 215, 227, 305, 311, 312, 317, 318, 324, 325, 330, 338, 347], "cleanup": [28, 33, 35, 42, 66, 78, 89, 104, 331, 337], "cli": [89, 91, 105], "click": [241, 242, 283, 284], "client": [283, 284, 292, 293, 295, 296, 304], "clone": [87, 90, 91, 99, 107], "cloud": [17, 19, 20, 23, 28, 31, 35, 38, 40, 43, 47, 53, 59, 66, 69, 75, 83, 88, 95, 100, 101, 241, 242, 283, 284], "cluster": [3, 43, 44, 46, 53, 54, 58, 66, 67, 83, 95, 133, 136, 159, 165, 202, 227, 241, 242, 244, 248, 249, 254, 255, 256, 257, 258, 259, 266, 275, 282, 283, 284, 292, 293, 295, 297, 298, 299, 304], "code": [4, 89, 90, 104, 107, 125, 128, 147, 150, 254, 255, 256, 283, 284], "collabor": [87, 91, 99], "collison": [257, 258, 259], "command": [35, 39, 89, 90, 104, 107], "comment": [289, 290], "comparison": [117, 119], "compon": [24, 27, 43, 46, 53, 58, 117, 120], "compos": 16, "comput": [1, 8, 13, 35, 36, 82, 94, 151, 152, 169, 171, 173, 241, 242, 283, 284, 305, 306, 338, 339], "concept": [5, 6, 7, 14, 108, 109, 112, 229, 233, 235, 239], "conclus": [28, 34, 43, 52, 53, 65, 125, 132], "concurr": [10, 117, 123, 184, 190, 241, 242, 254, 255], "conda": [1, 151, 152], "config": [82, 94], "configur": [3, 5, 6, 13, 35, 38, 66, 69, 72, 82, 90, 94, 107, 108, 109, 115, 117, 120, 122, 125, 128, 142, 145, 146, 159, 165, 202, 208, 212, 221, 224, 235, 239, 289, 290, 325, 328], "confus": [325, 329], "connect": [248, 249], "consid": [10, 11, 184, 186, 196, 198], "consider": [108, 109, 112, 125, 131], "consol": [89, 104], "constraint": [257, 258], "constructor": [241, 242], "contain": [82, 90, 94, 107], "content": [0, 89, 103, 348], "context": [108, 109, 112, 125, 131], "continu": [108, 109, 112, 325, 330], "control": [17, 22, 24, 26, 43, 46, 53, 58, 66, 73, 241, 242, 254, 255], "convert": [248, 249, 250, 257, 258, 267, 271, 274], "core": [2, 3, 8, 153, 154, 159, 160, 169, 174, 248, 249, 348], "correct": [254, 255], "cost": [108, 109, 113, 125, 131], "could": [251, 252, 283, 284], "count": [331, 333], "cours": [0, 1, 151, 152, 202, 228], "cover": [85, 86, 97, 98, 125, 127, 257, 258, 325, 326, 327], "cpu": [241, 242, 254, 255, 256, 257, 258, 325, 329], "creat": [1, 2, 5, 6, 17, 22, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 82, 85, 89, 94, 97, 104, 105, 151, 152, 153, 156, 202, 213, 235, 238, 239, 241, 242, 244, 248, 249, 254, 255, 256, 265, 267, 271, 283, 284, 286, 287, 289, 290, 318, 320], "creation": [17, 21], "cuda": [254, 255], "cursor": [81, 93], "curv": [13, 305, 309, 318, 322, 338, 345], "custom": [0, 9, 10, 15, 16, 17, 22, 176, 182, 184, 193, 338, 341], "cv": 348, "d": [254, 255, 257, 258], "d1": [305, 306], "d2": [312, 313], "dashboard": [84, 96, 142, 144], "data": [3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 24, 26, 142, 144, 147, 148, 150, 159, 162, 167, 169, 170, 171, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 192, 193, 194, 195, 202, 203, 212, 216, 217, 218, 219, 220, 221, 228, 229, 231, 235, 239, 241, 242, 243, 244, 248, 249, 251, 252, 254, 255, 256, 257, 258, 260, 261, 265, 267, 268, 269, 276, 300, 305, 307, 318, 320, 325, 329, 331, 337, 338, 347, 348], "databas": [8, 169, 170], "datafram": [202, 214, 267, 274], "dataload": [5, 6, 202, 210, 218, 235, 238, 275, 280, 331, 333, 338, 341, 342], "dataset": [5, 9, 12, 13, 15, 176, 179, 202, 204, 219, 241, 242, 244, 245, 246, 248, 249, 250, 251, 252, 254, 255, 257, 258, 259, 263, 266, 267, 270, 271, 272, 273, 312, 313, 314, 318, 320, 325, 327, 331, 333], "datasetd": [248, 249], "datasetexecut": [257, 258], "datasethf_dataset": [248, 249], "datasetload": [248, 249], "ddp": [202, 203, 206], "de": [305, 306], "debug": [84, 96], "decod": [108, 109, 111, 305, 307], "decor": [289, 290], "deep": [241, 242], "deeper": [7, 14, 229, 233], "default": [14, 90, 107], "defin": [6, 13, 17, 20, 202, 205, 206, 207, 217, 220, 235, 238, 241, 242, 253, 257, 258, 289, 290, 318, 321, 322, 325, 328], "definit": [17, 22], "demand": [108, 109, 113, 331, 332], "demonstr": [17, 21, 257, 258, 338, 346], "depend": [1, 3, 24, 26, 66, 74, 151, 152, 159, 164], "deploi": [24, 25, 27, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 70, 90, 91, 106, 107, 117, 118, 122, 125, 128, 241, 242, 244, 254, 255, 256, 265, 283, 284, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 303], "deploy": [11, 16, 17, 20, 24, 25, 27, 79, 108, 109, 115, 117, 121, 146, 196, 199, 283, 284, 289, 290, 291, 292, 293, 294, 303], "deploymentnum_replica": [289, 290], "descript": [125, 129], "design": [283, 284], "detail": [142, 143, 289, 290], "develop": [1, 11, 81, 90, 91, 93, 107, 146, 151, 152, 196, 201], "devic": [43, 46, 53, 58, 241, 242, 254, 255], "diagnost": [325, 329], "diagram": [245, 246], "dictimport": [245, 246], "differ": [117, 124, 241, 242], "diffus": [6, 235, 238, 305, 306, 308, 311, 312, 313, 317], "diffusionpolici": [312, 315], "digit": [202, 204], "directori": 13, "disabl": 0, "displai": [89, 104, 305, 311], "distribut": [4, 5, 6, 8, 13, 147, 150, 169, 173, 202, 203, 212, 235, 239, 257, 258, 260, 268, 275, 276, 277, 283, 284, 300, 305, 306, 309, 312, 316, 318, 319, 322, 325, 326, 328, 331, 332, 337, 338, 339, 344, 348], "div": [241, 242, 283, 284], "dive": [7, 14, 229, 233], "do": [241, 242, 254, 255], "doc": [248, 249, 283, 284, 289, 290], "document": [89, 105], "doe": [257, 258, 259, 338, 339], "domain": [125, 131], "done": [297, 298], "down": [1, 117, 121, 122, 151, 152, 241, 242], "download": [90, 107, 202, 204, 286, 287], "downscal": [297, 298], "dual": [17, 22], "due": [248, 249, 293, 295], "duplic": [87, 99], "dure": [257, 258], "e": [241, 242], "each": [24, 26, 241, 242, 251, 252, 254, 255, 283, 284, 289, 290], "eachels": [254, 255], "easi": [283, 284], "easili": [241, 242, 283, 284], "easilydeploi": [283, 284], "ec2": [17, 21, 28, 29], "editor": [89, 104], "ef": [17, 22], "effici": [241, 242, 257, 258, 325, 328], "either": [254, 255], "ek": [43, 44, 53, 54, 57], "els": [90, 107], "emb": [254, 255], "embed": [241, 242, 251, 252, 254, 255, 257, 258, 259, 318, 319], "embeddingsmaterialized_d": [257, 258], "en": [248, 249, 283, 284, 289, 290], "enabl": [0, 35, 38, 66, 69, 117, 123, 202, 223, 241, 242, 257, 258, 348], "encod": [254, 255, 305, 307, 318, 320, 338, 340], "encount": [293, 295], "end": [4, 12, 147, 150, 202, 215, 293, 295], "endpoint": [90, 107, 146], "engin": [8, 35, 36, 66, 67, 108, 109, 114, 169, 171], "ensembl": [4, 147, 150], "ensur": [254, 255], "enter": [90, 107], "entir": [241, 242, 244, 257, 258, 259, 266], "environ": [1, 3, 82, 90, 94, 107, 151, 152, 159, 164, 241, 242, 283, 284, 312, 313], "error": [241, 242, 244, 257, 258, 259, 266, 293, 295], "errorsgpu": [257, 258], "especi": [241, 242], "evalu": [325, 328], "even": [289, 290], "everyth": [90, 107], "exampl": [0, 1, 4, 12, 13, 17, 21, 24, 27, 79, 89, 90, 104, 107, 117, 119, 124, 125, 128, 129, 130, 137, 141, 146, 147, 148, 150, 151, 152, 241, 242, 251, 252, 254, 255, 260, 268, 276, 289, 290, 300], "execut": [0, 2, 9, 10, 15, 24, 26, 82, 89, 94, 104, 153, 157, 176, 180, 184, 189, 254, 255, 257, 258], "exercis": [7, 14, 229, 233], "exist": [17, 22, 53, 54, 57, 248, 249, 283, 284], "expect": [125, 129], "expens": [251, 252], "experi": [12, 14], "explan": [289, 290], "explicitli": [241, 242, 248, 249], "explor": [84, 96], "extern": [24, 26], "extra": [202, 224], "face": [241, 242, 248, 249, 250, 257, 258, 275, 277, 286, 287, 297, 298], "factor": [318, 319, 321], "failur": [3, 159, 167], "failureconfig": [202, 224], "fastapi": [16, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 297, 298, 301, 303], "fault": [202, 222, 225, 228, 338, 346], "featur": [0, 9, 12, 16, 89, 103, 125, 126, 127, 176, 183, 283, 284, 325, 329], "fetch": [3, 159, 167], "few": [257, 258, 259], "file": [9, 10, 15, 83, 89, 90, 95, 104, 107, 117, 122, 176, 182, 184, 193, 325, 327], "filter": [267, 272], "find": [89, 104], "first": [85, 86, 89, 90, 97, 98, 103, 104, 107], "fit": [6, 202, 213, 235, 239], "flask": [283, 284, 285, 292, 301], "flexibl": [241, 242], "flow": [8, 146, 169, 175], "folder": [89, 105], "follow": [85, 86, 89, 90, 97, 98, 103, 104, 107], "food": [305, 307, 338, 339, 340], "food101dataset": [338, 341], "forc": [331, 335], "forecast": [331, 332], "forest": [325, 326], "format": [8, 169, 170], "forward": [305, 306], "foundat": [108, 109, 110, 348], "fraction": [3, 16, 159, 165, 289, 290], "framework": [8, 125, 131, 169, 173, 283, 284], "frameworkthat": [283, 284], "free": [241, 242], "from": [89, 104, 202, 218, 226, 241, 242, 248, 249, 257, 258, 297, 298, 305, 310, 311, 312, 317, 318, 320, 323, 325, 330, 331, 336], "from_huggingfac": [248, 249], "full": [89, 105, 202, 224], "function": [2, 8, 17, 19, 153, 156, 157, 169, 171, 254, 255, 275, 280, 281, 289, 290], "further": [289, 290], "g": [241, 242], "gce": [35, 36], "gcp": [35, 36], "gener": [0, 5, 6, 8, 108, 109, 111, 169, 173, 235, 239, 241, 242, 251, 252, 305, 306, 311, 312, 314, 348], "get": [2, 3, 7, 14, 91, 108, 109, 115, 125, 129, 146, 153, 154, 157, 158, 159, 167, 229, 233, 289, 290, 293, 295, 348], "github": [90, 107], "give": [90, 107], "gke": [66, 67], "global": [9, 10, 15, 176, 182, 184, 193], "go": [202, 228, 241, 242], "googl": [35, 38, 66, 67, 69], "gpu": [5, 6, 13, 16, 66, 71, 235, 238, 239, 241, 242, 254, 255, 256, 257, 258, 289, 290, 331, 335, 337], "gracefulli": [293, 295], "grafana": [133, 136], "group": [9, 10, 15, 17, 22, 176, 182, 184, 193], "groupbi": [9, 10, 15, 176, 182, 184, 193], "guid": [90, 106, 289, 290], "ha": [254, 255, 257, 258], "handl": [89, 104, 257, 258, 283, 284], "hardwar": [117, 123, 125, 131], "harm": [2, 153, 158], "have": [254, 255, 283, 284], "head": [89, 90, 104, 107, 133, 136], "hello_world": [89, 104], "helper": [312, 317, 331, 337, 338, 342], "here": [241, 242, 283, 284, 289, 290], "hf_dataset": [248, 249], "high": [257, 258, 283, 284, 297, 298], "hourli": [331, 333], "how": [0, 9, 10, 12, 17, 20, 24, 26, 117, 124, 125, 131, 176, 178, 184, 187, 202, 203, 241, 242, 254, 255, 257, 258, 293, 295, 305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339], "html": [248, 249, 283, 284, 289, 290], "http": [248, 249, 283, 284, 289, 290, 293, 295], "hug": [241, 242, 248, 249, 250, 257, 258, 275, 277, 286, 287, 297, 298], "huggingfac": [245, 246, 260, 268, 276, 300], "hyperparamet": [4, 7, 14, 147, 150, 229, 233], "i": [8, 9, 17, 19, 89, 104, 108, 109, 111, 117, 122, 169, 174, 175, 176, 178, 241, 242, 251, 252, 254, 255, 257, 258, 283, 284, 285, 289, 290, 292, 293, 295, 297, 298, 301], "iam": [17, 22, 53, 57], "id": [2, 17, 22, 81, 93, 153, 158, 318, 320, 324], "imag": [11, 82, 94, 196, 200, 202, 220, 305, 306, 307, 338, 339, 340, 341], "implement": [4, 11, 16, 147, 148, 196, 200, 241, 242, 251, 252], "import": [202, 204, 241, 242, 244, 245, 246, 247, 262, 267, 270, 275, 278, 283, 284, 286, 287, 288, 292, 293, 295, 302, 305, 307, 312, 314, 318, 320, 325, 327, 329, 331, 333, 338, 340], "improv": [117, 123], "includ": [283, 284], "increas": [297, 298], "index": [283, 284], "industri": [8, 169, 170], "ineffici": [241, 242], "infer": [4, 108, 109, 111, 114, 117, 121, 122, 147, 150, 202, 215, 241, 242, 243, 244, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 264, 266, 289, 290, 318, 319, 324, 325, 329, 330, 331, 337, 338, 347, 348], "inferenceprint": [257, 258], "inform": [283, 284], "infrastructur": [17, 20, 24, 27, 66, 70, 108, 109, 114], "ingress": [43, 46, 53, 58, 66, 73], "init": [248, 249], "initi": [248, 249, 251, 252, 267, 270], "inlin": [289, 290], "input": [251, 252, 289, 290, 305, 306, 318, 319, 338, 339], "inspect": [12, 90, 107, 202, 214, 241, 242, 325, 327, 331, 333, 338, 341], "instal": [0, 1, 35, 38, 43, 46, 48, 49, 53, 58, 60, 61, 66, 69, 73, 76, 91, 133, 136, 146, 151, 152, 260, 268, 276, 300], "instanc": [17, 22, 28, 29, 35, 36, 241, 242, 289, 290, 292, 293, 295, 297, 298, 299, 304], "instead": [241, 242, 251, 252], "instruct": [85, 89, 97, 103], "integr": [16, 202, 216, 228, 241, 242, 283, 284, 305, 309], "interfac": [283, 284], "intro": [7, 10, 14, 15, 16, 184, 185, 229, 233], "introduct": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 80, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 103, 106, 108, 109, 110, 133, 134, 137, 138, 147, 148, 151, 152, 153, 154, 159, 160, 169, 170, 176, 177, 196, 197, 202, 203, 215, 228, 229, 230, 235, 236], "introductori": 13, "invert": [312, 313], "involv": [283, 284], "io": [3, 159, 165, 248, 249, 283, 284, 289, 290], "irvin": [325, 327], "item": [318, 319, 320, 324], "its": [241, 242, 283, 284], "job": [5, 13, 85, 89, 91, 97, 103, 104, 105, 257, 258, 259], "join": [267, 273, 318, 324], "json": [125, 129, 293, 295], "jupyt": [0, 1, 151, 152], "just": [254, 255, 283, 284, 285, 292, 301], "jvm": [8, 169, 173], "k8": [24, 26], "keep": [257, 258], "kei": [5, 6, 13, 16, 17, 19, 86, 89, 98, 103, 108, 109, 112, 116, 117, 120, 124, 125, 128, 129, 130, 132, 235, 239], "kubectl": [66, 72], "kubernet": [24, 25, 26, 27, 43, 46, 53, 58, 66, 67, 108, 109, 113], "kv": [108, 109, 112], "l6": [251, 252], "label": [293, 295, 338, 339], "lake": [8, 169, 170], "lakehous": [8, 169, 170], "landscap": [8, 169, 170], "languag": [108, 109, 110], "larg": [3, 108, 109, 110, 159, 165, 241, 242, 251, 252, 257, 258, 283, 284], "last": [91, 202, 226], "latenc": [108, 109, 113], "latest": [248, 249, 283, 284, 289, 290, 305, 310, 325, 330], "launch": [1, 5, 13, 80, 92, 117, 121, 122, 133, 136, 142, 145, 151, 152, 202, 213, 221, 225, 241, 242, 283, 284, 305, 309, 312, 316, 318, 322, 331, 335, 338, 344], "layer": [8, 24, 26, 169, 170, 171, 172], "lazi": [10, 184, 189, 254, 255, 257, 258], "learn": [8, 79, 91, 125, 127, 128, 129, 130, 169, 171, 202, 203, 216, 222, 241, 242, 251, 252, 257, 258, 283, 284, 305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339, 348], "leav": [90, 107], "legend": [1, 151, 152], "level": [3, 159, 161], "leverag": [241, 242, 257, 258], "lib": 348, "librari": [4, 12, 147, 148, 149, 241, 242, 244, 247, 262, 267, 270, 275, 278, 283, 284, 286, 287, 288, 292, 293, 295, 302], "librariesimport": [245, 246, 286, 287], "librariesin": [286, 287], "lifecycl": [5, 90, 106, 202, 212], "lightn": [6, 235, 236, 238, 239, 305, 309, 348], "lightningmodul": [305, 308, 312, 315], "lightweight": [251, 252], "like": [283, 284, 293, 295], "limit": [10, 184, 190], "list": [89, 105], "lite": [338, 339], "ll": [79, 125, 127, 202, 203, 216, 222], "llama": [117, 119], "llm": [108, 109, 110, 111, 113, 114, 115, 117, 118, 120, 123, 125, 126, 128, 131, 348], "load": [5, 6, 7, 9, 10, 13, 14, 15, 43, 46, 53, 58, 176, 179, 184, 188, 202, 215, 219, 223, 229, 231, 235, 239, 241, 242, 244, 248, 249, 250, 251, 252, 257, 258, 263, 267, 270, 283, 284, 289, 290, 305, 307, 318, 320, 325, 327, 331, 333, 338, 340], "load_dataset": [245, 246, 248, 249], "loader": 13, "local": [0, 1, 13, 81, 83, 93, 95, 117, 121, 133, 136, 146, 151, 152, 241, 242, 248, 249, 254, 255, 256, 283, 284], "localhost": [293, 295], "log": [84, 89, 96, 104, 142, 144, 145], "logic": [289, 290], "loop": [2, 5, 6, 13, 153, 158, 202, 206, 217, 223, 235, 238, 312, 316, 318, 322, 325, 328, 331, 335], "lora": [125, 128], "loss": [305, 309, 312, 316, 318, 322, 331, 335, 338, 345], "love": [293, 295], "mac": [1, 151, 152, 254, 255, 256], "machin": [8, 24, 25, 26, 169, 171, 241, 242, 251, 252, 254, 255, 257, 258, 259, 283, 284], "machinerai": [257, 258], "mai": [283, 284], "main": [89, 90, 104, 107, 275, 281], "make": [241, 242, 283, 284], "manag": [1, 3, 24, 26, 90, 106, 108, 109, 113, 151, 152, 159, 165, 283, 284, 318, 322], "mani": [3, 159, 167, 251, 252, 254, 255, 297, 298], "manual": [202, 226, 254, 255, 256], "map_batch": [241, 242, 251, 252, 254, 255, 257, 258], "materi": [10, 15, 184, 192, 241, 242, 257, 258, 259], "materialized_d": [257, 258], "matrix": [318, 319, 321, 325, 329], "matter": [125, 127, 129, 130], "max_model_len": [117, 123], "maxim": [241, 242], "medium": [117, 118, 119, 120], "memori": [8, 108, 109, 113, 169, 170, 241, 242, 244, 254, 255, 257, 258, 259, 266, 325, 328], "memorydb": [17, 22], "metadata": [257, 258, 259], "method": [241, 242, 251, 252], "metric": [5, 13, 84, 96, 142, 144, 145, 202, 211, 212, 214, 275, 279, 318, 322], "migrat": [5, 6, 13, 235, 239, 305, 306, 318, 319, 325, 326, 331, 332, 338, 339], "min": [331, 333], "mini": [325, 327], "miniforg": [1, 151, 152], "minilm": [251, 252], "ml": [241, 242, 257, 258, 260, 268, 276, 283, 284, 286, 287, 289, 290, 300], "mnist": [13, 202, 204, 205], "mode": [9, 10, 15, 176, 180, 184, 189], "model": [4, 5, 6, 7, 13, 14, 108, 109, 110, 112, 117, 119, 120, 123, 125, 131, 147, 148, 150, 202, 205, 209, 229, 233, 235, 238, 239, 241, 242, 244, 251, 252, 254, 255, 256, 257, 258, 265, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 295, 296, 297, 298, 301, 303, 318, 319, 321, 325, 328, 331, 334, 338, 339], "modeland": [283, 284], "modeldefin": [254, 255], "modelfastapi": [289, 290], "modelrespons": [293, 295], "modelserv": [293, 294], "modelsfrom": [286, 287], "modern": [241, 242, 257, 258], "modifi": [202, 223], "modul": [202, 215, 228], "monitor": [84, 96, 117, 123], "more": [5, 6, 15, 24, 27, 117, 123, 125, 128, 129, 130, 132, 235, 239, 283, 284, 289, 290, 297, 298], "most": [293, 295], "move": [241, 242], "movi": [318, 324], "movielen": [318, 320], "mp": [254, 255, 256, 257, 258], "mpsworker_devic": [254, 255], "multi": [146, 331, 332], "multipl": [13, 241, 242, 251, 252, 283, 284], "mutat": [257, 258, 259], "mysentimentmodel": [289, 290, 293, 294], "n": [257, 258, 318, 324], "name": [89, 90, 104, 107], "navig": [0, 89, 104], "necessari": [248, 249], "need": [24, 26, 90, 107, 254, 255, 283, 284], "nest": [3, 159, 166], "new": [0, 1, 12, 17, 22, 43, 44, 66, 67, 89, 104, 146, 151, 152, 305, 306], "next": [89, 104, 108, 109, 116, 117, 124, 125, 132, 133, 136, 202, 228, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "nginx": [43, 46, 53, 58, 66, 73], "node": [53, 57, 82, 90, 94, 107, 133, 136, 254, 255, 331, 332], "nois": [305, 306], "normal": [312, 314, 331, 333], "note": [2, 3, 5, 6, 10, 153, 158, 159, 164, 165, 167, 184, 188, 191, 202, 212, 235, 239, 257, 258, 259, 293, 295], "notebook": [0, 1, 12, 81, 85, 86, 89, 93, 97, 98, 105, 151, 152, 241, 242, 257, 258, 259, 267, 269, 283, 284, 293, 295, 297, 298], "now": [89, 104, 133, 136], "npfrom": [245, 246], "num_gpu": [254, 255], "num_replica": [297, 298], "number": [241, 242, 251, 252, 289, 290, 297, 298], "numpi": [245, 246], "nvidia": [43, 46, 53, 58, 254, 255], "nyc": [12, 331, 332, 333], "o": [1, 151, 152], "object": [3, 24, 26, 83, 91, 95, 159, 160, 161, 167, 241, 242, 257, 258, 305, 306, 312, 313, 318, 319], "observ": [133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 348], "onc": [3, 89, 104, 159, 167, 241, 242, 251, 252], "one": [241, 242, 331, 333], "ones": [283, 284], "onli": [142, 145, 202, 212], "onlin": [283, 284, 285, 289, 290, 292, 301, 348], "open": [89, 104], "oper": [9, 10, 15, 24, 25, 43, 48, 53, 60, 66, 76, 176, 182, 184, 193, 257, 258], "optim": [108, 109, 112, 113, 117, 123, 241, 242, 251, 252], "option": [1, 17, 22, 24, 25, 27, 43, 46, 53, 58, 66, 74, 83, 91, 95, 133, 136, 151, 152, 241, 242, 254, 255, 289, 290, 297, 298, 325, 327], "orchestr": [8, 108, 109, 114, 169, 172], "order": [9, 10, 15, 176, 182, 184, 193], "organ": [88, 100, 101], "origin": [257, 258, 259], "other": [117, 124, 257, 258, 259, 283, 284], "our": [13, 89, 104, 117, 119, 296], "out": [7, 89, 104, 105, 229, 232, 241, 242, 244, 257, 258, 259, 266], "outlin": [241, 242, 243, 244, 261, 267, 269, 275, 277, 283, 284, 285, 292, 301], "outlook": [15, 17, 18, 24, 27, 117, 124], "output": [0, 125, 129, 241, 242], "over": [8, 89, 90, 104, 107, 169, 174, 331, 333], "overview": [1, 2, 4, 5, 6, 11, 12, 13, 16, 17, 18, 88, 91, 101, 117, 119, 125, 127, 133, 135, 146, 147, 149, 151, 152, 153, 155, 196, 199, 235, 238, 247, 288], "own": [283, 284], "packag": [133, 136], "panda": [267, 274], "parallel": [2, 5, 6, 13, 108, 109, 112, 117, 123, 153, 158, 202, 203, 212, 235, 239, 241, 242, 248, 249, 254, 255, 257, 258, 283, 284], "param": [293, 295], "paramet": [254, 255, 289, 290], "parquet": [305, 307, 318, 320, 325, 327, 331, 333, 338, 340, 341], "part": [85, 86, 89, 90, 97, 98, 104, 105, 107], "partit": [241, 242, 248, 249], "pass": [3, 90, 107, 159, 161, 162], "passeng": [331, 333], "past": [89, 104], "path": 91, "pattern": [2, 3, 153, 158, 159, 161, 167, 305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339, 348], "pendulum": [312, 313, 314], "per": [202, 206, 275, 280, 318, 319], "perform": [251, 252, 254, 255, 257, 258], "persist": [10, 13, 15, 184, 194, 202, 212, 305, 307, 338, 340], "phase": [108, 109, 111], "pip": [3, 159, 164], "pipelin": [3, 91, 117, 123, 142, 144, 159, 167, 286, 287], "pixel": [305, 308], "plane": [17, 22], "platform": [83, 95, 248, 249], "plot": [305, 309, 312, 316, 318, 322, 331, 335, 338, 345], "plugin": [43, 46, 53, 58], "point": [13, 241, 242, 318, 320], "polici": [53, 57, 305, 306, 312, 313, 317, 348], "posit": [293, 295], "positionalencod": [331, 334], "post": [289, 290, 325, 330], "power": [241, 242], "practic": [125, 131, 257, 258], "pre": [286, 287, 288], "predict": [5, 6, 12, 202, 215, 235, 239, 241, 242, 283, 284, 289, 290, 293, 295], "predictor": [331, 337], "prefil": [108, 109, 111], "prepar": [202, 219, 331, 333, 338, 342], "prepare_data_load": [202, 210], "prepare_model": [202, 209], "preprocess": [267, 274], "prerequisit": [1, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 79, 117, 121, 133, 136, 142, 143, 146, 151, 152], "preview": [125, 127], "previou": [293, 295], "print": [257, 258, 293, 295], "problem": [305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339], "process": [3, 8, 108, 109, 111, 125, 131, 159, 167, 169, 173, 174, 241, 242, 248, 249, 251, 252, 254, 255, 257, 258, 260, 267, 268, 269, 276, 283, 284, 300, 305, 306, 348], "product": [5, 6, 7, 9, 10, 13, 15, 16, 176, 183, 184, 195, 229, 234, 235, 240, 241, 242, 283, 284], "profil": 146, "project": [87, 88, 99, 101], "prometheu": [133, 136], "properli": [293, 295], "provid": [17, 23, 43, 44, 283, 284], "public": [241, 242, 257, 258], "publish": [0, 90, 107], "purpos": [8, 17, 19, 169, 173], "put": [2, 153, 158], "py": [89, 90, 104, 107], "python": [89, 104, 105, 283, 284], "pytorch": [5, 6, 7, 11, 13, 14, 196, 197, 229, 232, 233, 235, 236, 238, 239, 260, 268, 275, 276, 277, 283, 284, 300, 331, 333, 348], "qualiti": [125, 131], "quantiz": [117, 123], "queri": [108, 109, 115, 293, 295, 296], "quick": [4, 147, 150, 331, 333], "rai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 24, 26, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 84, 89, 91, 96, 104, 108, 109, 110, 114, 115, 117, 118, 120, 121, 125, 126, 128, 133, 136, 137, 138, 139, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 158, 159, 160, 167, 168, 169, 170, 174, 175, 176, 177, 178, 183, 184, 185, 186, 187, 191, 195, 196, 197, 198, 199, 202, 203, 206, 212, 215, 216, 217, 218, 219, 220, 221, 222, 228, 229, 230, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 248, 249, 250, 254, 255, 256, 257, 258, 259, 260, 261, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 282, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 297, 298, 299, 300, 301, 304, 305, 306, 307, 309, 312, 313, 316, 318, 319, 320, 322, 325, 326, 327, 328, 329, 331, 332, 333, 335, 337, 338, 339, 342, 347, 348], "random": [331, 333], "rang": [283, 284], "rank": [202, 212, 318, 319], "rate": [318, 319, 320], "rayfrom": [286, 287], "rayimport": [245, 246], "rayserv": [289, 290], "read": [9, 10, 15, 176, 182, 184, 193], "real": [312, 314], "rec": 348, "recap": 12, "receiv": [283, 284], "recommend": [1, 91, 125, 131, 151, 152, 318, 319, 324], "reduc": [117, 123, 257, 258], "regist": [17, 23, 28, 31, 35, 40, 43, 47, 53, 59, 66, 75, 146], "registr": 146, "regress": [4, 147, 148], "relat": [117, 119, 124], "remot": [2, 5, 153, 156, 157], "remov": [331, 337], "repartit": [241, 242, 248, 249], "repeatedli": [251, 252], "replica": [11, 117, 123, 196, 199, 297, 298], "report": [5, 13, 202, 211, 212], "repositori": [90, 91, 107], "request": [3, 117, 121, 146, 159, 165, 283, 284, 289, 290, 292, 293, 295, 296, 304], "requestsw": [293, 295], "requir": [1, 35, 38, 43, 44, 53, 57, 66, 69, 108, 109, 113, 125, 131, 133, 136, 151, 152, 254, 255], "resampl": [331, 333], "research": [241, 242], "reserv": [241, 242, 283, 284], "resiz": [305, 307, 338, 340], "resnet": [202, 205], "resourc": [3, 10, 17, 20, 21, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 87, 99, 108, 109, 116, 117, 124, 125, 132, 159, 165, 184, 190, 191, 241, 242], "respons": [283, 284, 293, 295], "rest": [283, 284], "restart": [293, 295], "restor": [202, 226], "result": [2, 3, 5, 6, 13, 153, 157, 159, 167, 202, 214, 235, 239, 241, 242, 293, 295, 331, 337], "resum": [202, 226, 305, 310, 318, 323, 331, 336], "resumpt": [338, 346], "retri": [3, 159, 163, 202, 224], "retriev": 146, "return": [254, 255], "reus": [251, 252], "revers": [305, 306, 311, 312, 313, 317], "right": [241, 242, 283, 284], "roc": 13, "role": [17, 22, 53, 57, 88, 101], "rout": [289, 290], "row": [9, 10, 15, 176, 182, 184, 193, 241, 242, 254, 255, 256, 257, 258, 259], "run": [5, 6, 12, 24, 26, 35, 39, 85, 89, 90, 97, 103, 104, 107, 117, 122, 142, 144, 202, 215, 235, 239, 241, 242, 244, 251, 252, 254, 255, 256, 257, 258, 259, 266, 283, 284, 293, 294, 331, 337, 338, 347], "runconfig": [202, 212], "runningrai": [297, 298], "runtim": [3, 159, 164], "runtimedeploymenthandl": [293, 294], "s3": [17, 22], "same": [89, 105, 257, 258, 259], "sampl": [146, 202, 204, 289, 290, 305, 306, 311, 312, 313, 317], "sampler": [305, 311], "saniti": [305, 307, 331, 333, 338, 340], "save": [13, 202, 212, 224], "scalabl": [108, 109, 113, 241, 242, 257, 258, 283, 284], "scale": [5, 6, 13, 117, 123, 202, 208, 235, 239, 241, 242, 244, 254, 255, 256, 257, 258, 265, 283, 284, 289, 290, 291, 292, 297, 298, 303, 312, 313], "scalingconfig": [202, 208], "schedul": [13, 85, 89, 97, 105], "scikit": [283, 284], "score": [293, 295], "script": [89, 104], "sdk": [89, 105], "seamlessli": [241, 242], "second": [254, 255, 267, 271], "secur": [17, 22], "see": [289, 290, 293, 295], "select": [90, 107, 125, 131], "send": [117, 121, 283, 284, 289, 290, 292, 293, 295, 296, 304], "sentenc": [241, 242, 245, 246, 251, 252, 254, 255], "sentence_transform": [245, 246], "sentencetransform": [241, 242, 245, 246, 254, 255, 257, 258], "sentiment": [248, 249, 283, 284, 293, 295, 297, 298], "sequenc": [331, 332], "seri": [331, 332, 348], "serv": [0, 4, 8, 11, 16, 108, 109, 110, 111, 113, 114, 115, 117, 118, 120, 121, 125, 126, 128, 142, 145, 146, 147, 148, 150, 169, 175, 196, 197, 198, 199, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 297, 298, 299, 301, 304, 348], "servefrom": [286, 287], "server": [1, 151, 152], "serveserv": [297, 298], "servic": [11, 16, 86, 89, 90, 91, 98, 103, 106, 107, 117, 122, 196, 200, 283, 284, 291], "set": [1, 14, 117, 120, 122, 125, 130, 133, 136, 151, 152, 254, 255, 256, 289, 290], "setup": [88, 100, 133, 136, 146, 275, 279, 305, 306, 307, 312, 314, 318, 319, 325, 326, 331, 332, 338, 339], "sever": [283, 284], "share": [83, 95, 305, 311, 318, 324], "should": [254, 255, 256, 293, 295], "show": [241, 242, 254, 255, 257, 258, 259], "showcas": [254, 255, 283, 284], "shuffl": [9, 10, 15, 176, 182, 184, 193, 305, 307], "shut": [1, 117, 121, 122, 151, 152, 241, 242], "shutdown": [109, 115, 244, 257, 258, 259, 266, 267, 274, 275, 282, 283, 284, 292, 293, 295, 297, 298, 299, 304], "sign": 91, "signific": [241, 242], "silicon": [241, 242, 254, 255], "similarli": [289, 290], "simpl": [1, 142, 144, 151, 152, 283, 284], "simpli": [283, 284], "simul": [283, 284, 292, 293, 295, 296, 304], "singl": [5, 6, 13, 235, 238], "size": [117, 118, 119, 120, 124, 248, 249, 254, 255, 325, 327], "slide": [331, 333], "slow": [241, 242], "so": [254, 255, 256, 283, 284], "solv": [305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339], "sourc": [90, 106], "spark": [8, 169, 174], "specif": [2, 10, 89, 104, 153, 158, 184, 190, 191], "speedup": [241, 242], "spin": [90, 107, 241, 242, 251, 252], "split": [0, 248, 249, 305, 307, 312, 314, 318, 320, 325, 327, 338, 341], "stabl": [6, 235, 238], "start": [2, 7, 14, 86, 90, 91, 98, 107, 108, 109, 115, 133, 136, 153, 154, 229, 232, 233, 241, 242, 275, 282, 283, 284, 293, 295, 325, 328, 348], "starter": [90, 107], "state": [10, 15, 184, 191, 202, 224, 251, 252, 312, 313], "statu": [89, 104], "step": [12, 108, 109, 115, 116, 117, 124, 125, 132, 133, 136, 202, 228, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "still": [297, 298], "stop": [297, 298, 299], "storag": [5, 13, 24, 26, 83, 95, 202, 212, 227, 305, 311, 318, 324], "store": [3, 24, 26, 83, 95, 159, 160, 161, 241, 242, 257, 258], "strategi": [117, 123], "stream": [8, 169, 173], "structur": [8, 9, 79, 88, 101, 125, 129, 146, 169, 170, 176, 177], "style": [312, 313], "submit": [85, 89, 97, 104, 105], "subnet": [17, 22], "successfulli": [89, 104], "summar": [257, 258], "summari": [17, 22, 88, 102, 117, 124, 241, 242, 244, 259, 266, 267, 274, 275, 282, 292, 299, 304], "summaryin": [297, 298], "summarythi": [257, 258], "support": [17, 20, 283, 284], "sy": 348, "system": [318, 319], "tab": [84, 89, 90, 96, 104, 107], "tabl": 14, "tabular": [325, 326, 348], "take": [202, 203, 216, 222, 254, 255, 305, 306, 311, 312, 313, 317, 318, 319, 324, 325, 326, 330, 331, 332, 337, 338, 339, 347], "take_batch": [254, 255], "takeawai": [108, 109, 116, 117, 124, 125, 132], "task": [3, 12, 125, 131, 159, 160, 162, 163, 164, 165, 166], "taxi": [12, 331, 332, 333], "teacher": [331, 335], "team": 91, "templat": [90, 107, 117, 124], "tensor": [305, 306], "tensorflow": [283, 284], "termin": [89, 90, 104, 107, 133, 136], "terraform": [28, 30, 35, 39, 43, 45, 53, 56, 66, 70], "test": [0, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 94, 254, 255, 256, 283, 284, 292, 293, 295, 296, 304], "text": [108, 109, 111, 241, 242, 251, 252, 254, 255, 257, 258, 293, 295], "textembedd": [254, 255, 257, 258], "tfvar": [35, 39, 66, 70], "thatcan": [283, 284], "thei": [283, 284], "them": [254, 255], "thi": [1, 12, 85, 86, 90, 97, 98, 106, 107, 151, 152, 241, 242, 251, 252, 254, 255, 257, 258, 259, 283, 284, 289, 290, 297, 298, 305, 306, 311, 312, 313, 317, 318, 319, 324, 325, 326, 330, 331, 332, 337, 338, 339, 347], "think": [241, 242, 283, 284], "through": [85, 89, 90, 97, 103, 106, 241, 242, 257, 258], "throughput": [241, 242, 257, 258], "throw": [257, 258], "time": [241, 242, 331, 332, 348], "tip": 12, "titl": [318, 324], "togeth": [2, 153, 158], "token": [267, 274, 275, 280], "toler": [202, 222, 225, 228, 338, 346], "too": [3, 159, 167, 257, 258], "tool": [125, 130, 241, 242], "top": [3, 159, 161, 283, 284, 318, 324], "topic": [117, 123, 125, 132], "torch": [6, 235, 238], "torchfrom": [245, 246], "torchtrain": [6, 13, 202, 213, 221, 235, 239, 305, 309, 312, 316, 338, 344], "toward": [293, 295], "trace": [142, 145, 146], "track": [89, 104], "traffic": [283, 284, 297, 298], "train": [4, 5, 6, 13, 147, 148, 150, 202, 203, 206, 211, 212, 213, 214, 215, 216, 217, 221, 222, 223, 225, 226, 228, 235, 236, 237, 238, 239, 240, 248, 249, 260, 268, 275, 276, 277, 280, 281, 282, 286, 287, 288, 300, 305, 306, 307, 309, 312, 313, 316, 317, 318, 319, 320, 322, 323, 325, 326, 327, 328, 330, 331, 332, 335, 336, 337, 338, 339, 341, 344, 345, 348], "train_loop": [305, 309], "train_loop_config": [202, 207], "train_loop_per_work": [338, 343], "trainer": [202, 213, 325, 328], "transform": [9, 10, 15, 176, 180, 184, 190, 191, 202, 220, 251, 252, 257, 258, 286, 287, 289, 290, 331, 332, 334, 338, 341], "transformersfrom": [245, 246], "trigger": [257, 258], "tripl": [318, 319], "troubleshoot": [53, 63, 66, 71], "tune": [4, 7, 14, 147, 148, 150, 229, 230, 233, 234, 348], "tupl": [312, 313], "tutori": [202, 228], "tweet_ev": [248, 249], "two": [108, 109, 111, 254, 255, 267, 273], "type": [17, 20, 125, 129, 245, 246, 325, 327], "typic": [241, 242], "uci": [325, 327], "ul": [241, 242, 283, 284], "under": 13, "univers": [325, 327], "unstructur": [10, 184, 185], "until": [254, 255], "up": [1, 4, 14, 16, 43, 51, 53, 64, 84, 85, 86, 90, 91, 96, 97, 98, 107, 117, 120, 122, 125, 130, 133, 136, 147, 150, 151, 152, 202, 215, 227, 228, 241, 242, 251, 252, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "upcom": [9, 176, 183], "updat": [13, 90, 91, 106, 283, 284], "upgrad": [66, 74, 117, 123], "uri": [318, 320], "url": [89, 104], "us": [1, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 22, 24, 26, 89, 90, 103, 105, 106, 107, 117, 123, 125, 128, 129, 130, 131, 142, 145, 147, 148, 151, 152, 169, 174, 176, 178, 182, 183, 184, 193, 202, 203, 229, 233, 235, 237, 241, 242, 248, 249, 251, 252, 254, 255, 257, 258, 283, 284, 285, 289, 290, 292, 293, 295, 297, 298, 301, 305, 306, 312, 313, 318, 319, 320, 325, 326, 331, 332, 338, 339], "usag": 0, "user": [88, 101, 146, 283, 284, 318, 319, 320, 324], "uv": [1, 151, 152], "v": [8, 24, 25, 26, 89, 104, 169, 174, 175], "v2": [251, 252], "val": [305, 307, 312, 316], "valid": [318, 320, 322, 325, 327, 331, 335, 338, 341, 345], "valu": [108, 109, 112], "vanilla": [4, 7, 147, 150, 229, 232], "vector": [241, 242], "verifi": [1, 43, 49, 53, 61, 89, 104, 151, 152, 325, 330], "version": [283, 284], "view": [202, 214], "viewer": [84, 96], "virtual": [24, 25, 26], "vision": [305, 306, 338, 339, 348], "visual": [13, 14, 202, 204, 215, 305, 307, 318, 320, 325, 327, 329, 331, 333, 337, 338, 340, 347], "vllm": [108, 109, 114], "vm": [24, 26], "vpc": [17, 22], "vram": [254, 255, 256], "vscode": [81, 90, 93, 107], "wa": [89, 104], "wai": [283, 284], "wait": [3, 159, 167], "walk": [85, 89, 90, 97, 103, 106, 257, 258], "want": [251, 252], "warehous": [8, 169, 170], "we": [1, 12, 89, 90, 104, 107, 117, 124, 125, 127, 132, 151, 152, 241, 242, 283, 284, 286, 287, 297, 298], "weather": [125, 130], "web": [142, 145, 283, 284, 291], "webservic": [286, 287, 289, 290, 291, 292, 303], "welcom": [1, 79, 151, 152], "well": [283, 284], "what": [8, 9, 17, 19, 24, 26, 79, 108, 109, 111, 117, 122, 124, 125, 127, 132, 133, 136, 169, 174, 175, 176, 178, 202, 203, 216, 222, 283, 284, 285, 292, 301, 305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339], "when": [5, 6, 8, 9, 10, 11, 15, 16, 24, 26, 169, 174, 176, 183, 184, 186, 196, 198, 202, 203, 235, 237, 241, 242, 251, 252, 297, 298], "where": [202, 228, 283, 284, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "which": [24, 26, 241, 242, 283, 284], "why": [1, 8, 91, 108, 109, 113, 117, 119, 125, 127, 128, 129, 130, 151, 152, 169, 174, 175, 283, 284, 285, 292, 301, 305, 306], "wide": [283, 284], "window": [108, 109, 112, 125, 131, 331, 333], "work": [0, 10, 184, 187, 202, 203, 241, 242, 251, 252, 254, 255, 256, 305, 306], "worker": [5, 6, 82, 90, 94, 107, 133, 136, 202, 206, 235, 239, 254, 255, 256, 275, 280, 325, 328], "worker_devic": [254, 255], "workflow": [0, 11, 82, 89, 94, 104, 196, 201, 241, 242, 257, 258, 283, 284], "workload": [24, 26, 88, 101, 142, 144, 283, 284, 305, 306, 312, 313, 318, 319, 325, 326, 331, 332, 338, 339, 348], "workspac": [80, 89, 90, 91, 92, 104, 105, 107], "wrap": [202, 209, 228, 305, 311, 312, 317, 318, 324, 325, 330, 331, 337, 338, 347], "write": [9, 176, 181, 325, 327], "xgboost": [4, 147, 148, 150, 325, 326, 328], "yaml": [90, 107], "york": 12, "you": [79, 85, 89, 90, 97, 103, 104, 106, 202, 203, 216, 222, 241, 242, 248, 249, 251, 252, 254, 255, 283, 284, 293, 295, 305, 306, 311, 312, 313, 317, 318, 319, 324, 325, 326, 330, 331, 332, 337, 338, 339, 347], "your": [1, 17, 23, 24, 27, 53, 57, 84, 85, 86, 89, 90, 91, 96, 97, 98, 103, 104, 107, 151, 152, 241, 242, 251, 252], "zero": [297, 298]}})