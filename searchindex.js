Search.setIndex({"alltitles": {"0. Overview": [[2, "overview"], [17, "overview"], [18, "overview"], [175, "overview"], [177, null]], "0. What is Ray Data?": [[9, "what-is-ray-data"], [198, "what-is-ray-data"], [200, null]], "01 \u00b7 Define Training Loop with Ray Data": [[238, "define-training-loop-with-ray-data"], [240, null]], "01 \u00b7 Imports": [[224, "imports"], [226, null]], "01 \u00b7 Modify Training Loop to Enable Checkpoint Loading": [[245, "modify-training-loop-to-enable-checkpoint-loading"], [247, null]], "02 \u00b7 Build DataLoader from Ray Data": [[238, "build-dataloader-from-ray-data"], [241, null]], "02 \u00b7 Download MNIST Dataset": [[224, "download-mnist-dataset"], [226, "download-mnist-dataset"]], "02 \u00b7 Save Full Checkpoint with Extra State": [[245, "save-full-checkpoint-with-extra-state"], [248, null]], "03 \u00b7 Configure Automatic Retries with FailureConfig": [[245, "configure-automatic-retries-with-failureconfig"], [248, "configure-automatic-retries-with-failureconfig"]], "03 \u00b7 Prepare Dataset for Ray Data": [[238, "prepare-dataset-for-ray-data"], [242, null]], "03 \u00b7 Visualize Sample Digits": [[224, "visualize-sample-digits"], [226, "visualize-sample-digits"]], "04 \u00b7 Define ResNet-18 Model for MNIST": [[224, "define-resnet-18-model-for-mnist"], [227, null]], "04 \u00b7 Launch Fault-Tolerant Training": [[245, "launch-fault-tolerant-training"], [249, null]], "04 \u00b7 Load Dataset into Ray Data": [[238, "load-dataset-into-ray-data"], [242, "load-dataset-into-ray-data"]], "04-d1 Generative computer-vision pattern with Ray Train": [[329, null], [330, null]], "04-d2 Diffusion-Policy Pattern with Ray Train": [[336, null], [337, null]], "04a Computer-vision pattern with Ray Train": [[362, null], [363, null]], "04b Tabular workload pattern with Ray Train": [[349, null], [350, null]], "04c Time-Series workload pattern with Ray Train": [[355, null], [356, null]], "04e Recommendation system pattern with Ray Train": [[342, null], [343, null]], "05 \u00b7 Define Image Transformation": [[238, "define-image-transformation"], [243, null]], "05 \u00b7 Define the Ray Train Loop (DDP per-worker)": [[224, "define-the-ray-train-loop-ddp-per-worker"], [228, null]], "05 \u00b7 Manual Restoration from Checkpoints": [[245, "manual-restoration-from-checkpoints"], [250, null]], "06 \u00b7 Apply Transformations with Ray Data": [[238, "apply-transformations-with-ray-data"], [243, "apply-transformations-with-ray-data"]], "06 \u00b7 Define train_loop_config": [[224, "define-train-loop-config"], [229, null]], "06 \u00b7 Resume Training from the Last Checkpoint": [[245, "resume-training-from-the-last-checkpoint"], [250, "resume-training-from-the-last-checkpoint"]], "07 \u00b7 Clean Up Cluster Storage": [[245, "clean-up-cluster-storage"], [251, null]], "07 \u00b7 Configure Scaling with ScalingConfig": [[224, "configure-scaling-with-scalingconfig"], [230, null]], "07 \u00b7 Configure TorchTrainer with Ray Data": [[238, "configure-torchtrainer-with-ray-data"], [244, null]], "08 \u00b7 Launch Training with Ray Data": [[238, "launch-training-with-ray-data"], [244, "launch-training-with-ray-data"]], "08 \u00b7 Wrap the Model with prepare_model()": [[224, "wrap-the-model-with-prepare-model"], [231, null]], "09 \u00b7 Build the DataLoader with prepare_data_loader()": [[224, "build-the-dataloader-with-prepare-data-loader"], [232, null]], "1. Architecture": [[299, "architecture"], [302, null]], "1. Cloud Object Store": [[86, "cloud-object-store"], [87, "cloud-object-store"]], "1. Create Anyscale Resources with Terraform": [[28, "create-anyscale-resources-with-terraform"], [30, null], [43, "create-anyscale-resources-with-terraform"], [45, null], [53, "create-anyscale-resources-with-terraform"], [56, null]], "1. Creating Remote Functions": [[2, "creating-remote-functions"], [175, "creating-remote-functions"], [178, null]], "1. Dataset tuples": [[336, "dataset-tuples"], [337, "dataset-tuples"]], "1. Deploy to Kubernetes with Anyscale Operator": [[24, "deploy-to-kubernetes-with-anyscale-operator"], [25, "deploy-to-kubernetes-with-anyscale-operator"]], "1. How to Use Ray Data?": [[9, "how-to-use-ray-data"], [198, "how-to-use-ray-data"], [200, "how-to-use-ray-data"]], "1. Imports": [[342, "imports"], [344, null], [349, "imports"], [351, null], [355, "imports"], [357, null], [362, "imports"], [364, null]], "1. Imports and setup": [[329, "imports-and-setup"], [331, null], [336, "imports-and-setup"], [338, null]], "1. Installation": [[35, "installation"], [38, null], [66, "installation"], [69, null]], "1. Key-Value (KV) Caching": [[101, "key-value-kv-caching"], [102, "key-value-kv-caching"], [105, "key-value-kv-caching"]], "1. Loading and visualizing data": [[14, "loading-and-visualizing-data"]], "1. Loading the data": [[7, "loading-the-data"], [253, "loading-the-data"], [255, null]], "1. Memory Management": [[101, "memory-management"], [102, "memory-management"], [106, "memory-management"]], "1. Model Quality Benchmarks": [[118, "model-quality-benchmarks"], [124, "model-quality-benchmarks"]], "1. Object store": [[3, "object-store"], [181, "object-store"], [183, null]], "1. Overview of Ray Serve": [[16, "overview-of-ray-serve"]], "1. Overview of the Ray AI Libraries": [[4, "overview-of-the-ray-ai-libraries"], [12, "overview-of-the-ray-ai-libraries"], [171, "overview-of-the-ray-ai-libraries"], [173, null]], "1. PyTorch introductory example (single GPU)": [[13, "pytorch-introductory-example-single-gpu"]], "1. Ray Serve for Orchestration": [[101, "ray-serve-for-orchestration"], [102, "ray-serve-for-orchestration"], [107, "ray-serve-for-orchestration"]], "1. Reduce max_model_len": [[110, "reduce-max-model-len"], [116, "reduce-max-model-len"]], "1. Sign Up for Anyscale": [[100, "sign-up-for-anyscale"]], "1. Split Notebooks and Generate Navigation": [[0, "split-notebooks-and-generate-navigation"]], "1. User Profile Retrieval": [[168, "user-profile-retrieval"]], "1. What is an Anyscale Cloud?": [[17, "what-is-an-anyscale-cloud"], [19, null]], "1. When to Consider Ray Data": [[10, "when-to-consider-ray-data"], [206, "when-to-consider-ray-data"], [208, null]], "1. When to Consider Ray Serve": [[11, "when-to-consider-ray-serve"], [218, "when-to-consider-ray-serve"], [220, null]], "1. When to use Ray Data": [[15, "when-to-use-ray-data"]], "1. When to use Ray Train": [[5, "when-to-use-ray-train"], [6, "when-to-use-ray-train"], [259, "when-to-use-ray-train"], [261, null]], "1.1 Configure Google Cloud Authentication": [[35, "configure-google-cloud-authentication"], [38, "configure-google-cloud-authentication"]], "1.1. Configure Google Cloud Authentication": [[66, "configure-google-cloud-authentication"], [69, "configure-google-cloud-authentication"]], "1.1. Pattern: pass an object as a top-level argument": [[3, "pattern-pass-an-object-as-a-top-level-argument"], [181, "pattern-pass-an-object-as-a-top-level-argument"], [183, "pattern-pass-an-object-as-a-top-level-argument"]], "1.2 Enable Required APIs": [[35, "enable-required-apis"], [38, "enable-required-apis"]], "1.2: Enable Required APIs": [[66, "enable-required-apis"], [69, "enable-required-apis"]], "10 \u00b7 Report Training Metrics": [[224, "report-training-metrics"], [233, null]], "10. Clean up": [[336, "clean-up"], [341, "clean-up"]], "10. Conclusion": [[53, "conclusion"], [65, null]], "10. Helper: Ray-prepared DataLoaders": [[362, "helper-ray-prepared-dataloaders"], [366, null]], "10. Launch distributed Training with TorchTrainer": [[329, "launch-distributed-training-with-torchtrainer"], [333, "launch-distributed-training-with-torchtrainer"]], "10. Plot train and validation loss curves": [[342, "plot-train-and-validation-loss-curves"], [346, "plot-train-and-validation-loss-curves"]], "10. Ray Train training loop (with teacher forcing)": [[355, "ray-train-training-loop-with-teacher-forcing"], [359, null]], "10. Start distributed training": [[349, "start-distributed-training"], [352, "start-distributed-training"]], "101 - Anyscale Organization and Cloud Setup": [[96, null], [97, null]], "101 \u2013  Introduction to Anyscale Services": [[92, null], [93, null]], "101 \u2013 Collaboration on Anyscale": [[94, null], [95, null]], "101 \u2013 Compute Configs and Execution Environments in Anyscale": [[84, null], [85, null]], "101 \u2013 Debug and Monitor Your Anyscale Application": [[88, null], [89, null]], "101 \u2013 Developing Application with Anyscale": [[82, null], [83, null]], "101 \u2013 Introduction to Anyscale Jobs": [[90, null], [91, null]], "101 \u2013 Storage Options in the Anyscale Platform": [[86, null], [87, null]], "101 \u2014 Introduction to Anyscale Workspaces": [[80, null], [81, null]], "11 \u00b7 Save Checkpoints and Report Metrics": [[224, "save-checkpoints-and-report-metrics"], [234, null]], "11. Evaluate the trained model": [[349, "evaluate-the-trained-model"], [352, "evaluate-the-trained-model"]], "11. Launch training on 8 GPUs": [[355, "launch-training-on-8-gpus"], [359, "launch-training-on-8-gpus"]], "11. Plot loss curves": [[329, "plot-loss-curves"], [333, "plot-loss-curves"]], "11. Resume training from checkpoint": [[342, "resume-training-from-checkpoint"], [347, null]], "11. train_loop_per_worker": [[362, "train-loop-per-worker"], [367, null]], "12 \u00b7 Save Checkpoints on Rank-0 Only": [[224, "save-checkpoints-on-rank-0-only"], [234, "save-checkpoints-on-rank-0-only"]], "12. Confusion matrix visualization": [[349, "confusion-matrix-visualization"], [353, null]], "12. Inference: recommend top-N items for a user": [[342, "inference-recommend-top-n-items-for-a-user"], [348, null]], "12. Launch distributed training with TorchTrainer": [[362, "launch-distributed-training-with-torchtrainer"], [368, null]], "12. Plot training and validation loss": [[355, "plot-training-and-validation-loss"], [359, "plot-training-and-validation-loss"]], "12. Resume from latest checkpoint": [[329, "resume-from-latest-checkpoint"], [334, null]], "13 \u00b7 Configure Persistent Storage with RunConfig": [[224, "configure-persistent-storage-with-runconfig"], [234, "configure-persistent-storage-with-runconfig"]], "13. CPU batch inference with Ray Data": [[349, "cpu-batch-inference-with-ray-data"], [353, "cpu-batch-inference-with-ray-data"]], "13. Join top-N item IDs with movie titles": [[342, "join-top-n-item-ids-with-movie-titles"], [348, "join-top-n-item-ids-with-movie-titles"]], "13. Plot training and validation loss curves": [[362, "plot-training-and-validation-loss-curves"], [369, null]], "13. Resume training from checkpoint": [[355, "resume-training-from-checkpoint"], [360, null]], "13. Reverse diffusion sampler": [[329, "reverse-diffusion-sampler"], [335, null]], "14 \u00b7 Create the TorchTrainer": [[224, "create-the-torchtrainer"], [235, null]], "14. Clean up shared storage": [[342, "clean-up-shared-storage"], [348, "clean-up-shared-storage"]], "14. Demonstrate fault-tolerant resumption": [[362, "demonstrate-fault-tolerant-resumption"], [370, null]], "14. Feature-importance diagnostics": [[349, "feature-importance-diagnostics"], [353, "feature-importance-diagnostics"]], "14. Generate and display samples from the best checkpoint": [[329, "generate-and-display-samples-from-the-best-checkpoint"], [335, "generate-and-display-samples-from-the-best-checkpoint"]], "14. Inference helper \u2014 Ray Data batch predictor on GPU": [[355, "inference-helper-ray-data-batch-predictor-on-gpu"], [361, null]], "15 \u00b7 Launch Training with trainer.fit()": [[224, "launch-training-with-trainer-fit"], [235, "launch-training-with-trainer-fit"]], "15. Batch inference with Ray Data": [[362, "batch-inference-with-ray-data"], [371, null]], "15. Clean up shared storage": [[329, "clean-up-shared-storage"], [335, "clean-up-shared-storage"]], "15. Continue training from the latest checkpoint": [[349, "continue-training-from-the-latest-checkpoint"], [354, null]], "15. Run distributed inference and visualize results": [[355, "run-distributed-inference-and-visualize-results"], [361, "run-distributed-inference-and-visualize-results"]], "16 \u00b7 Inspect the Training Results": [[224, "inspect-the-training-results"], [236, null]], "16. Cleanup: remove all training artifacts": [[355, "cleanup-remove-all-training-artifacts"], [361, "cleanup-remove-all-training-artifacts"]], "16. Run and visualize Ray Data inference": [[362, "run-and-visualize-ray-data-inference"], [371, "run-and-visualize-ray-data-inference"]], "16. Verify post-training inference": [[349, "verify-post-training-inference"], [354, "verify-post-training-inference"]], "17 \u00b7 View Metrics as a DataFrame": [[224, "view-metrics-as-a-dataframe"], [236, "view-metrics-as-a-dataframe"]], "17. Clean up": [[349, "clean-up"], [354, "clean-up"], [362, "clean-up"], [371, "clean-up"]], "18 \u00b7 Load a Checkpoint for Inference": [[224, "load-a-checkpoint-for-inference"], [237, null]], "19 \u00b7 Run Inference and Visualize Predictions": [[224, "run-inference-and-visualize-predictions"], [237, "run-inference-and-visualize-predictions"]], "2. Attach Required IAM Policies to Your existing EKS\u2019s Node Role": [[53, "attach-required-iam-policies-to-your-existing-eks-s-node-role"], [57, null]], "2. Build the Book": [[0, "build-the-book"]], "2. Chaining Tasks and Passing Data": [[3, "chaining-tasks-and-passing-data"], [181, "chaining-tasks-and-passing-data"], [184, null]], "2. Clone the Repository (Optional)": [[100, "clone-the-repository-optional"]], "2. Cloud Deployment Types": [[17, "cloud-deployment-types"], [20, null]], "2. Continuous Batching": [[101, "continuous-batching"], [102, "continuous-batching"], [105, "continuous-batching"]], "2. Create Anyscale Resources with Terraform": [[35, "create-anyscale-resources-with-terraform"], [39, null], [66, "create-anyscale-resources-with-terraform"], [70, null]], "2. Distributed Data Parallel Training with Ray Train and PyTorch (multiple GPUs)": [[13, "distributed-data-parallel-training-with-ray-train-and-pytorch-multiple-gpus"]], "2. End-to-end example: predicting taxi tips in New York": [[12, "end-to-end-example-predicting-taxi-tips-in-new-york"]], "2. Executing Remote Functions": [[2, "executing-remote-functions"], [175, "executing-remote-functions"], [179, null]], "2. Generate a real pendulum dataset": [[336, "generate-a-real-pendulum-dataset"], [338, "generate-a-real-pendulum-dataset"]], "2. How to work with Ray Data": [[10, "how-to-work-with-ray-data"], [206, "how-to-work-with-ray-data"], [209, null]], "2. Implement an Classifier service": [[16, "implement-an-classifier-service"]], "2. Install Kubernetes Components": [[43, "install-kubernetes-components"], [46, null]], "2. Latency Requirements": [[101, "latency-requirements"], [102, "latency-requirements"], [106, "latency-requirements"]], "2. Library Imports": [[299, "library-imports"], [302, "library-imports"]], "2. Load 10 % of Food-101": [[329, "load-10-of-food-101"], [331, "load-10-of-food-101"], [362, "load-10-of-food-101"], [364, "load-10-of-food-101"]], "2. Load MovieLens 100K dataset": [[342, "load-movielens-100k-dataset"], [344, "load-movielens-100k-dataset"]], "2. Load NYC taxi passenger counts (30-min)": [[355, "load-nyc-taxi-passenger-counts-30-min"], [357, "load-nyc-taxi-passenger-counts-30-min"]], "2. Load the University of California, Irvine (UCI) Cover type dataset": [[349, "load-the-university-of-california-irvine-uci-cover-type-dataset"], [351, "load-the-university-of-california-irvine-uci-cover-type-dataset"]], "2. Loading Data": [[9, "loading-data"], [15, "loading-data"], [198, "loading-data"], [201, null]], "2. Overview of Ray Serve": [[11, "overview-of-ray-serve"], [218, "overview-of-ray-serve"], [221, null]], "2. Quick end-to-end example": [[4, "quick-end-to-end-example"], [171, "quick-end-to-end-example"], [174, null]], "2. Register the Anyscale Cloud": [[28, "register-the-anyscale-cloud"], [31, null]], "2. Setting up a PyTorch model": [[14, "setting-up-a-pytorch-model"]], "2. Shared File Storage": [[86, "shared-file-storage"], [87, "shared-file-storage"]], "2. Single GPU Training with PyTorch": [[5, "single-gpu-training-with-pytorch"]], "2. Single GPU Training with PyTorch Lightning": [[6, "single-gpu-training-with-pytorch-lightning"], [259, "single-gpu-training-with-pytorch-lightning"], [262, null]], "2. Starting out with vanilla PyTorch": [[7, "starting-out-with-vanilla-pytorch"], [253, "starting-out-with-vanilla-pytorch"], [256, null]], "2. Task and Domain Alignment": [[118, "task-and-domain-alignment"], [124, "task-and-domain-alignment"]], "2. Training objective": [[336, "training-objective"], [337, "training-objective"]], "2. Use Quantized Models": [[110, "use-quantized-models"], [116, "use-quantized-models"]], "2. User Registration": [[168, "user-registration"]], "2. Virtual Machines (VM) vs. Kubernetes (K8s)": [[24, "virtual-machines-vm-vs-kubernetes-k8s"], [26, null]], "2. vLLM as the inference engine": [[101, "vllm-as-the-inference-engine"], [102, "vllm-as-the-inference-engine"], [107, "vllm-as-the-inference-engine"]], "2.1 Control Layer: What Anyscale Manages or Needs Access To": [[24, "control-layer-what-anyscale-manages-or-needs-access-to"], [26, "control-layer-what-anyscale-manages-or-needs-access-to"]], "2.1 Create terraform.tfvars": [[35, "create-terraform-tfvars"], [39, "create-terraform-tfvars"]], "2.1 Install the Cluster Autoscaler": [[43, "install-the-cluster-autoscaler"], [46, "install-the-cluster-autoscaler"]], "2.1 Overview": [[6, "overview"], [259, "overview"], [262, "overview"]], "2.1 Vanilla XGboost code": [[4, "vanilla-xgboost-code"], [171, "vanilla-xgboost-code"], [174, "vanilla-xgboost-code"]], "2.1. Overview": [[5, "overview"]], "2.1: Create terraform.tfvars": [[66, "create-terraform-tfvars"], [70, "create-terraform-tfvars"]], "2.2 Data Layer: Storage, Object Stores, and External Dependencies": [[24, "data-layer-storage-object-stores-and-external-dependencies"], [26, "data-layer-storage-object-stores-and-external-dependencies"]], "2.2 Hyperparameter tuning with Ray Tune": [[4, "hyperparameter-tuning-with-ray-tune"], [171, "hyperparameter-tuning-with-ray-tune"], [174, "hyperparameter-tuning-with-ray-tune"]], "2.2 Install the AWS Load Balancer Controller": [[43, "install-the-aws-load-balancer-controller"], [46, "install-the-aws-load-balancer-controller"]], "2.2 Note on blocks": [[10, "note-on-blocks"], [206, "note-on-blocks"], [210, "note-on-blocks"]], "2.2 Run Terraform Commands": [[35, "run-terraform-commands"], [39, "run-terraform-commands"]], "2.2. Build model and load it on the GPU": [[5, "build-model-and-load-it-on-the-gpu"]], "2.2. Create a torch dataloader": [[6, "create-a-torch-dataloader"], [259, "create-a-torch-dataloader"], [262, "create-a-torch-dataloader"]], "2.2: Deploy Infrastructure": [[66, "deploy-infrastructure"], [70, "deploy-infrastructure"]], "2.3 Define a stable diffusion model": [[6, "define-a-stable-diffusion-model"], [259, "define-a-stable-diffusion-model"], [262, "define-a-stable-diffusion-model"]], "2.3 Install the Nginx Ingress Controller": [[43, "install-the-nginx-ingress-controller"], [46, "install-the-nginx-ingress-controller"]], "2.3 Workload Execution Layer: How Ray Runs on Each Backend": [[24, "workload-execution-layer-how-ray-runs-on-each-backend"], [26, "workload-execution-layer-how-ray-runs-on-each-backend"]], "2.3. Create Dataset and DataLoader": [[5, "create-dataset-and-dataloader"]], "2.3. Distributed training with Ray Train": [[4, "distributed-training-with-ray-train"], [171, "distributed-training-with-ray-train"], [174, "distributed-training-with-ray-train"]], "2.4 (Optional) Install the Nvidia Device Plugin": [[43, "optional-install-the-nvidia-device-plugin"], [46, "optional-install-the-nvidia-device-plugin"]], "2.4 Serving an ensemble model with Ray Serve": [[4, "serving-an-ensemble-model-with-ray-serve"], [171, "serving-an-ensemble-model-with-ray-serve"], [174, "serving-an-ensemble-model-with-ray-serve"]], "2.4 When to use which": [[24, "when-to-use-which"], [26, "when-to-use-which"]], "2.4. Create metrics and checkpointing": [[5, "create-metrics-and-checkpointing"]], "2.4. Define a PyTorch Lightning training loop": [[6, "define-a-pytorch-lightning-training-loop"], [259, "define-a-pytorch-lightning-training-loop"], [262, "define-a-pytorch-lightning-training-loop"]], "2.5 Batch inference with Ray Data": [[4, "batch-inference-with-ray-data"], [171, "batch-inference-with-ray-data"], [174, "batch-inference-with-ray-data"]], "2.5. Run the training loop": [[5, "run-the-training-loop"]], "2.6 Clean up": [[4, "clean-up"], [171, "clean-up"], [174, "clean-up"]], "2.6. Use checkpointed model to generate predictions": [[5, "use-checkpointed-model-to-generate-predictions"]], "20 \u00b7 Clean Up the Ray Actor": [[224, "clean-up-the-ray-actor"], [237, "clean-up-the-ray-actor"]], "3. (Optional) More Kubernetes Deployments Components": [[24, "optional-more-kubernetes-deployments-components"], [27, null]], "3. A Demonstrative Example of Resource Creation with AWS EC2": [[17, "a-demonstrative-example-of-resource-creation-with-aws-ec2"], [21, null]], "3. Advanced features of Ray Serve": [[16, "advanced-features-of-ray-serve"]], "3. Anyscale for Infrastructure": [[101, "anyscale-for-infrastructure"], [102, "anyscale-for-infrastructure"], [107, "anyscale-for-infrastructure"]], "3. Context Window Requirements": [[118, "context-window-requirements"], [124, "context-window-requirements"]], "3. Distributed Data Parallel Training with Ray Train and PyTorch": [[5, "distributed-data-parallel-training-with-ray-train-and-pytorch"]], "3. Distributed Training with Ray Train and PyTorch Lightning": [[6, "distributed-training-with-ray-train-and-pytorch-lightning"], [259, "distributed-training-with-ray-train-and-pytorch-lightning"], [263, null]], "3. Enable Pipeline Parallelism": [[110, "enable-pipeline-parallelism"], [116, "enable-pipeline-parallelism"]], "3. Getting Results": [[2, "getting-results"], [175, "getting-results"], [179, "getting-results"]], "3. Hyperparameter tuning with Ray Tune": [[7, "hyperparameter-tuning-with-ray-tune"], [253, "hyperparameter-tuning-with-ray-tune"], [257, null]], "3. Implement an image classification service": [[11, "implement-an-image-classification-service"], [218, "implement-an-image-classification-service"], [222, null]], "3. Install Kubernetes Components": [[53, "install-kubernetes-components"], [58, null]], "3. Install Ray and the Anyscale CLI (Recommended)": [[100, "install-ray-and-the-anyscale-cli-recommended"]], "3. Introduction to Ray Tune": [[14, "introduction-to-ray-tune"]], "3. Lazy execution mode": [[10, "lazy-execution-mode"], [206, "lazy-execution-mode"], [211, null]], "3. Loading data": [[10, "loading-data"], [206, "loading-data"], [210, null]], "3. Local Cluster Storage": [[86, "local-cluster-storage"], [87, "local-cluster-storage"]], "3. Metrics Setup": [[299, "metrics-setup"], [303, null]], "3. Model parallelization or alternatives": [[101, "model-parallelization-or-alternatives"], [102, "model-parallelization-or-alternatives"], [105, "model-parallelization-or-alternatives"]], "3. Normalize and split": [[336, "normalize-and-split"], [338, "normalize-and-split"]], "3. Overview of the training loop in Ray Train": [[13, "overview-of-the-training-loop-in-ray-train"]], "3. Point to Parquet dataset URI": [[342, "point-to-parquet-dataset-uri"], [344, "point-to-parquet-dataset-uri"]], "3. Register the Anyscale Cloud": [[35, "register-the-anyscale-cloud"], [40, null], [43, "register-the-anyscale-cloud"], [47, null]], "3. Resample to hourly, then normalize": [[355, "resample-to-hourly-then-normalize"], [357, "resample-to-hourly-then-normalize"]], "3. Resize and encode images": [[329, "resize-and-encode-images"], [331, "resize-and-encode-images"], [362, "resize-and-encode-images"], [364, "resize-and-encode-images"]], "3. Reverse diffusion (sampling)": [[336, "reverse-diffusion-sampling"], [337, "reverse-diffusion-sampling"]], "3. Running an experiment with Ray AI libraries": [[12, "running-an-experiment-with-ray-ai-libraries"]], "3. Scalability Demands": [[101, "scalability-demands"], [102, "scalability-demands"], [106, "scalability-demands"]], "3. Serve Locally for Testing": [[0, "serve-locally-for-testing"]], "3. Task retries": [[3, "task-retries"], [181, "task-retries"], [185, null]], "3. Test": [[28, "test"], [32, null]], "3. Transforming Data": [[9, "transforming-data"], [15, "transforming-data"], [198, "transforming-data"], [202, null]], "3. Troubleshooting GPU Availability": [[66, "troubleshooting-gpu-availability"], [71, null]], "3. Visualize class balance": [[349, "visualize-class-balance"], [351, "visualize-class-balance"]], "3.1 Distributed Data Parallel Training": [[6, "distributed-data-parallel-training"], [259, "distributed-data-parallel-training"], [263, "distributed-data-parallel-training"]], "3.1 IAM Role Definition": [[17, "iam-role-definition"], [22, null]], "3.1 Install the Cluster Autoscaler": [[53, "install-the-cluster-autoscaler"], [58, "install-the-cluster-autoscaler"]], "3.1. Overview of the training loop in Ray Train": [[5, "overview-of-the-training-loop-in-ray-train"]], "3.1.1\u202f\u202fAnyscale Control Plane Role (anyscale-iam-role-id)": [[17, "anyscale-control-plane-role-anyscale-iam-role-id"], [22, "anyscale-control-plane-role-anyscale-iam-role-id"]], "3.1.2\u202f\u202fInstance Role (instance-iam-role-id)": [[17, "instance-role-instance-iam-role-id"], [22, "instance-role-instance-iam-role-id"]], "3.10. Activity: Run the distributed training with more workers": [[5, "activity-run-the-distributed-training-with-more-workers"]], "3.2 Install the AWS Load Balancer Controller": [[53, "install-the-aws-load-balancer-controller"], [58, "install-the-aws-load-balancer-controller"]], "3.2 Ray Train Migration": [[6, "ray-train-migration"], [259, "ray-train-migration"], [263, "ray-train-migration"]], "3.2. Configure scale and GPUs": [[5, "configure-scale-and-gpus"]], "3.2.1. Note on Ray Train key concepts": [[5, "note-on-ray-train-key-concepts"]], "3.2\u202fVPC": [[17, "vpc"], [22, "vpc"]], "3.3 Install the Nginx Ingress Controller": [[53, "install-the-nginx-ingress-controller"], [58, "install-the-nginx-ingress-controller"]], "3.3 Subnets": [[17, "subnets"], [22, "subnets"]], "3.3. Configure scale and GPUs": [[6, "configure-scale-and-gpus"], [259, "configure-scale-and-gpus"], [263, "configure-scale-and-gpus"]], "3.3. Migrating the model to Ray Train": [[5, "migrating-the-model-to-ray-train"]], "3.3.1. Note on Ray Train key concepts": [[6, "note-on-ray-train-key-concepts"], [259, "note-on-ray-train-key-concepts"], [263, "note-on-ray-train-key-concepts"]], "3.4 (Optional) Install the Nvidia Device Plugin": [[53, "optional-install-the-nvidia-device-plugin"], [58, "optional-install-the-nvidia-device-plugin"]], "3.4 Create and fit a Ray Train TorchTrainer": [[6, "create-and-fit-a-ray-train-torchtrainer"], [259, "create-and-fit-a-ray-train-torchtrainer"], [263, "create-and-fit-a-ray-train-torchtrainer"]], "3.4. Migrating the dataset to Ray Train": [[5, "migrating-the-dataset-to-ray-train"]], "3.4\u202fSecurity Groups": [[17, "security-groups"], [22, "security-groups"]], "3.5. Access the training results": [[6, "access-the-training-results"], [259, "access-the-training-results"], [263, "access-the-training-results"]], "3.5. Reporting checkpoints and metrics": [[5, "reporting-checkpoints-and-metrics"]], "3.5.1. Note on the checkpoint lifecycle": [[5, "note-on-the-checkpoint-lifecycle"]], "3.5\u202fS3": [[17, "s3"], [22, "s3"]], "3.6. Configure remote storage": [[5, "configure-remote-storage"]], "3.6. Load the checkpointed model to generate predictions": [[6, "load-the-checkpointed-model-to-generate-predictions"], [259, "load-the-checkpointed-model-to-generate-predictions"], [263, "load-the-checkpointed-model-to-generate-predictions"]], "3.6\u202fEFS (Optional)": [[17, "efs-optional"], [22, "efs-optional"]], "3.7. Activity: Run the distributed training with more workers": [[6, "activity-run-the-distributed-training-with-more-workers"], [259, "activity-run-the-distributed-training-with-more-workers"], [263, "activity-run-the-distributed-training-with-more-workers"]], "3.7. Launching the distributed training job": [[5, "launching-the-distributed-training-job"]], "3.7\u202fMemoryDB (Optional)": [[17, "memorydb-optional"], [22, "memorydb-optional"]], "3.8 Summary": [[17, "summary"], [22, "summary"]], "3.8. Access the training results": [[5, "access-the-training-results"]], "3.9. Use checkpointed model to generate predictions": [[5, "id1"]], "4. Cleanup": [[28, "cleanup"], [33, null]], "4. Context Window Considerations": [[101, "context-window-considerations"], [102, "context-window-considerations"], [105, "context-window-considerations"]], "4. Cost Optimization": [[101, "cost-optimization"], [102, "cost-optimization"], [106, "cost-optimization"]], "4. Data Operations: Grouping, Aggregation, and Shuffling": [[15, "data-operations-grouping-aggregation-and-shuffling"]], "4. Development workflow": [[11, "development-workflow"], [218, "development-workflow"], [223, null]], "4. DiffusionPolicy LightningModule": [[336, "diffusionpolicy-lightningmodule"], [339, null]], "4. Diving deeper into Ray Tune concepts": [[14, "diving-deeper-into-ray-tune-concepts"]], "4. Examples Outlook: Deploying to Your Infrastructure": [[24, "examples-outlook-deploying-to-your-infrastructure"], [27, "examples-outlook-deploying-to-your-infrastructure"]], "4. Hardware and Cost Considerations": [[118, "hardware-and-cost-considerations"], [124, "hardware-and-cost-considerations"]], "4. Install the Anyscale Operator": [[43, "install-the-anyscale-operator"], [48, null]], "4. Local File Store": [[86, "local-file-store"], [87, "local-file-store"]], "4. Migrating the model and dataset to Ray Train": [[13, "migrating-the-model-and-dataset-to-ray-train"]], "4. Putting It All Together": [[2, "putting-it-all-together"], [175, "putting-it-all-together"], [180, null]], "4. Quick visual sanity-check": [[355, "quick-visual-sanity-check"], [357, "quick-visual-sanity-check"]], "4. Ray Serve in Production": [[16, "ray-serve-in-production"]], "4. Ray Train in Production": [[5, "ray-train-in-production"], [6, "ray-train-in-production"], [259, "ray-train-in-production"], [264, null]], "4. Ray Tune in Production": [[7, "ray-tune-in-production"], [253, "ray-tune-in-production"], [258, null]], "4. Register Anyscale Cloud to Your Cloud Provider": [[17, "register-anyscale-cloud-to-your-cloud-provider"], [23, null]], "4. Register the Anyscale Cloud": [[53, "register-the-anyscale-cloud"], [59, null]], "4. Scale with More Replicas": [[110, "scale-with-more-replicas"], [116, "scale-with-more-replicas"]], "4. Task Runtime Environments": [[3, "task-runtime-environments"], [181, "task-runtime-environments"], [186, null]], "4. Test": [[35, "test"], [41, null]], "4. Training function per worker": [[299, "training-function-per-worker"], [304, null]], "4. Transforming data": [[10, "transforming-data"], [206, "transforming-data"], [212, null]], "4. Visual sanity check": [[329, "visual-sanity-check"], [331, "visual-sanity-check"], [362, "visual-sanity-check"], [364, "visual-sanity-check"]], "4. Visualize dataset: ratings, users, and items": [[342, "visualize-dataset-ratings-users-and-items"], [344, "visualize-dataset-ratings-users-and-items"]], "4. Write train / validation Parquet files": [[349, "write-train-validation-parquet-files"], [351, "write-train-validation-parquet-files"]], "4. Writing Data": [[9, "writing-data"], [198, "writing-data"], [203, null]], "4. kubectl Configuration": [[66, "kubectl-configuration"], [72, null]], "4.1 On resource specification": [[10, "on-resource-specification"], [206, "on-resource-specification"], [212, "on-resource-specification"]], "4.1. Note about Ray ID Specification": [[2, "note-about-ray-id-specification"], [175, "note-about-ray-id-specification"], [180, "note-about-ray-id-specification"]], "4.1. Note about pip dependencies": [[3, "note-about-pip-dependencies"], [181, "note-about-pip-dependencies"], [186, "note-about-pip-dependencies"]], "4.2 On concurrency limiting": [[10, "on-concurrency-limiting"], [206, "on-concurrency-limiting"], [212, "on-concurrency-limiting"]], "4.2. Anti-pattern: Calling ray.get in a loop harms parallelism": [[2, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"], [175, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"], [180, "anti-pattern-calling-ray-get-in-a-loop-harms-parallelism"]], "5. Cleanup": [[35, "cleanup"], [42, null]], "5. Conclusion": [[28, "conclusion"], [34, null]], "5. Create Ray Dataset from Parquet and encode IDs": [[342, "create-ray-dataset-from-parquet-and-encode-ids"], [344, "create-ray-dataset-from-parquet-and-encode-ids"]], "5. Data Operations: Shuffling, Grouping and Aggregation": [[9, "data-operations-shuffling-grouping-and-aggregation"], [198, "data-operations-shuffling-grouping-and-aggregation"], [204, null]], "5. Distributed Train loop with checkpointing": [[336, "distributed-train-loop-with-checkpointing"], [340, null]], "5. Hyperparameter tuning the PyTorch model using Ray Tune": [[14, "hyperparameter-tuning-the-pytorch-model-using-ray-tune"]], "5. Install NGINX Ingress Controller": [[66, "install-nginx-ingress-controller"], [73, null]], "5. Install the Anyscale Operator": [[53, "install-the-anyscale-operator"], [60, null]], "5. Load the train and validation splits as Ray Datasets": [[349, "load-the-train-and-validation-splits-as-ray-datasets"], [351, "load-the-train-and-validation-splits-as-ray-datasets"]], "5. Main Training Function": [[299, "main-training-function"], [305, null]], "5. Persist to Parquet": [[329, "persist-to-parquet"], [331, "persist-to-parquet"], [362, "persist-to-parquet"], [364, "persist-to-parquet"]], "5. Persisting Data": [[15, "persisting-data"]], "5. Reporting checkpoints and metrics": [[13, "reporting-checkpoints-and-metrics"]], "5. Resource allocation and management": [[3, "resource-allocation-and-management"], [181, "resource-allocation-and-management"], [187, null]], "5. Sliding-window dataset to Parquet": [[355, "sliding-window-dataset-to-parquet"], [357, "sliding-window-dataset-to-parquet"]], "5. Stateful transformations with Ray Actors": [[10, "stateful-transformations-with-ray-actors"], [206, "stateful-transformations-with-ray-actors"], [213, null]], "5. Upgrade Hardware": [[110, "upgrade-hardware"], [116, "upgrade-hardware"]], "5. Verify the Installation": [[43, "verify-the-installation"], [49, null]], "5.1 Resource specification for stateful transformations": [[10, "resource-specification-for-stateful-transformations"], [206, "resource-specification-for-stateful-transformations"], [213, "resource-specification-for-stateful-transformations"]], "5.1. Note on resources requests, available resources, configuring large clusters": [[3, "note-on-resources-requests-available-resources-configuring-large-clusters"], [181, "note-on-resources-requests-available-resources-configuring-large-clusters"], [187, "note-on-resources-requests-available-resources-configuring-large-clusters"]], "5.2 Note on autoscaling for stateful transformations": [[10, "note-on-autoscaling-for-stateful-transformations"], [206, "note-on-autoscaling-for-stateful-transformations"], [213, "note-on-autoscaling-for-stateful-transformations"]], "5.2. Fractional resources": [[3, "fractional-resources"], [181, "fractional-resources"], [187, "fractional-resources"]], "5.3. IO bound tasks and fractional resources": [[3, "io-bound-tasks-and-fractional-resources"], [181, "io-bound-tasks-and-fractional-resources"], [187, "io-bound-tasks-and-fractional-resources"]], "6. (Optional) Upgrade Anyscale Dependencies": [[66, "optional-upgrade-anyscale-dependencies"], [74, null]], "6. Custom Food101Dataset for Parquet": [[362, "custom-food101dataset-for-parquet"], [365, null]], "6. Inspect dataset sizes (optional)": [[349, "inspect-dataset-sizes-optional"], [351, "inspect-dataset-sizes-optional"]], "6. Launch Ray TorchTrainer": [[336, "launch-ray-torchtrainer"], [340, "launch-ray-torchtrainer"]], "6. Launching the distributed training job": [[13, "launching-the-distributed-training-job"]], "6. Load and decode with Ray Data": [[329, "load-and-decode-with-ray-data"], [331, "load-and-decode-with-ray-data"]], "6. Materializing data": [[10, "materializing-data"], [206, "materializing-data"], [214, null]], "6. Nested Tasks": [[3, "nested-tasks"], [181, "nested-tasks"], [188, null]], "6. PyTorch Dataset over Parquet": [[355, "pytorch-dataset-over-parquet"], [357, "pytorch-dataset-over-parquet"]], "6. Start Training": [[299, "start-training"], [306, null]], "6. Test": [[43, "test"], [50, null]], "6. Train/validation split using Ray Data": [[342, "train-validation-split-using-ray-data"], [344, "train-validation-split-using-ray-data"]], "6. Verify the Installation": [[53, "verify-the-installation"], [61, null]], "6. When to use Ray Data": [[9, "when-to-use-ray-data"], [198, "when-to-use-ray-data"], [205, null]], "7. Accessing the training results": [[13, "accessing-the-training-results"]], "7. Clean up": [[43, "clean-up"], [51, null]], "7. Data Operations: grouping, aggregation, and shuffling": [[10, "data-operations-grouping-aggregation-and-shuffling"], [206, "data-operations-grouping-aggregation-and-shuffling"], [215, null]], "7. Define matrix factorization model": [[342, "define-matrix-factorization-model"], [345, null]], "7. Image transform": [[362, "image-transform"], [365, "image-transform"]], "7. Inspect a mini-batch": [[349, "inspect-a-mini-batch"], [351, "inspect-a-mini-batch"]], "7. Inspect one random batch": [[355, "inspect-one-random-batch"], [357, "inspect-one-random-batch"]], "7. Pattern: Pipeline data processing and waiting for results": [[3, "pattern-pipeline-data-processing-and-waiting-for-results"], [181, "pattern-pipeline-data-processing-and-waiting-for-results"], [189, null]], "7. Plot train / val loss": [[336, "plot-train-val-loss"], [340, "plot-train-val-loss"]], "7. Ray Data in Production": [[9, "ray-data-in-production"], [198, "ray-data-in-production"], [205, "ray-data-in-production"]], "7. Register the Anyscale Cloud": [[66, "register-the-anyscale-cloud"], [75, null]], "7. Shuffle and Train/Val split": [[329, "shuffle-and-train-val-split"], [331, "shuffle-and-train-val-split"]], "7. Shutdown Ray Cluster": [[299, "shutdown-ray-cluster"], [306, "shutdown-ray-cluster"]], "7. Test": [[53, "test"], [62, null]], "7.1 Batch Processing Pattern": [[3, "batch-processing-pattern"], [181, "batch-processing-pattern"], [189, "batch-processing-pattern"]], "7.1. Custom batching using groupby.": [[10, "custom-batching-using-groupby"], [206, "custom-batching-using-groupby"], [215, "custom-batching-using-groupby"]], "7.2 Note on fetching too many objects at once with ray.get causes failure": [[3, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"], [181, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"], [189, "note-on-fetching-too-many-objects-at-once-with-ray-get-causes-failure"]], "7.2. Aggregations": [[10, "aggregations"], [206, "aggregations"], [215, "aggregations"]], "7.3. Shuffling data": [[10, "shuffling-data"], [206, "shuffling-data"], [215, "shuffling-data"]], "7.3.1. File based shuffle on read": [[10, "file-based-shuffle-on-read"], [206, "file-based-shuffle-on-read"], [215, "file-based-shuffle-on-read"]], "7.3.2. Shuffling block order": [[10, "shuffling-block-order"], [206, "shuffling-block-order"], [215, "shuffling-block-order"]], "7.3.3. Shuffle all rows globally": [[10, "shuffle-all-rows-globally"], [206, "shuffle-all-rows-globally"], [215, "shuffle-all-rows-globally"]], "8. Conclusion": [[43, "conclusion"], [52, null]], "8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)": [[342, "define-ray-train-loop-with-validation-checkpointing-and-ray-managed-metrics"], [346, null]], "8. Define the Ray Train worker loop (Arrow-based, memory-efficient)": [[349, "define-the-ray-train-worker-loop-arrow-based-memory-efficient"], [352, null]], "8. Install the Anyscale Operator": [[66, "install-the-anyscale-operator"], [76, null]], "8. Persisting data": [[10, "persisting-data"], [206, "persisting-data"], [216, null]], "8. Pixel diffusion LightningModule": [[329, "pixel-diffusion-lightningmodule"], [332, null]], "8. Ray Actors": [[3, "ray-actors"], [181, "ray-actors"], [190, null]], "8. Ray Train in Production": [[13, "ray-train-in-production"]], "8. Ray-prepared DataLoader": [[355, "ray-prepared-dataloader"], [357, "ray-prepared-dataloader"]], "8. Reverse diffusion helper": [[336, "reverse-diffusion-helper"], [341, null]], "8. Summary": [[299, "summary"], [306, "summary"]], "8. Test": [[66, "test"], [77, null]], "8. Train/validation split": [[362, "train-validation-split"], [365, "train-validation-split"]], "8. Troubleshooting": [[53, "troubleshooting"], [63, null]], "8. Upcoming Features in Ray Data": [[9, "upcoming-features-in-ray-data"], [198, "upcoming-features-in-ray-data"], [205, "upcoming-features-in-ray-data"]], "9. Clean up": [[53, "clean-up"], [64, null]], "9. Cleanup": [[66, "cleanup"], [78, null]], "9. Configure XGBoost and build the Trainer": [[349, "configure-xgboost-and-build-the-trainer"], [352, "configure-xgboost-and-build-the-trainer"]], "9. Inspect a DataLoader batch": [[362, "inspect-a-dataloader-batch"], [365, "inspect-a-dataloader-batch"]], "9. Launch distributed training with Ray Train": [[342, "launch-distributed-training-with-ray-train"], [346, "launch-distributed-training-with-ray-train"]], "9. PositionalEncoding and Transformer model": [[355, "positionalencoding-and-transformer-model"], [358, null]], "9. Ray Data in production": [[10, "ray-data-in-production"], [206, "ray-data-in-production"], [217, null]], "9. Ray Train train_loop (Lightning + Ray integration)": [[329, "ray-train-train-loop-lightning-ray-integration"], [333, null]], "9. Sample an action from the trained policy": [[336, "sample-an-action-from-the-trained-policy"], [341, "sample-an-action-from-the-trained-policy"]], "API Endpoints": [[168, "api-endpoints"]], "Activity: Update the training loop to compute the area under the curve of ROC (AUROC)": [[13, "activity-update-the-training-loop-to-compute-the-area-under-the-curve-of-roc-auroc"]], "Adding New Notebooks or Courses": [[0, "adding-new-notebooks-or-courses"]], "Additional Setup (Optional)": [[155, "additional-setup-optional"], [158, "additional-setup-optional"]], "Additional dependencies": [[126, "additional-dependencies"], [127, "additional-dependencies"]], "Advanced LLM Features with Ray Serve LLM": [[118, null], [119, null]], "Advanced Topics: Monitoring & Optimization": [[110, "advanced-topics-monitoring-optimization"], [116, null]], "Aggregations": [[15, "aggregations"]], "Annotated experiment table": [[14, "annotated-experiment-table"]], "Anyscale 101 Learning Path": [[100, "anyscale-101-learning-path"]], "Anyscale Administrator Overview": [[17, null], [18, null]], "Anyscale For Admins": [[372, "anyscale-for-admins"]], "Anyscale Getting Started": [[372, "anyscale-getting-started"]], "Anyscale Observability": [[159, "anyscale-observability"], [162, null]], "Anyscale Projects": [[94, "anyscale-projects"], [95, "anyscale-projects"]], "Anyscale Ray Serve Observability": [[164, "anyscale-ray-serve-observability"], [167, "anyscale-ray-serve-observability"]], "Apache Arrow": [[8, "apache-arrow"], [191, "apache-arrow"], [192, "apache-arrow"]], "Application": [[147, "application"], [150, null]], "Applications": [[11, "applications"], [218, "applications"], [221, "applications"]], "Architecture": [[168, "architecture"], [268, "architecture"], [271, "architecture"], [286, null], [312, "architecture"], [316, "architecture"], [326, null]], "Architecture Overview": [[271, null], [312, null]], "Architecture### Import librariesIn addition to ray and serve, we also import FastAPI to create webservice and Hugging Face transformers to download ML models.# Import ray serve and FastAPI librariesimport rayfrom ray import servefrom fastapi import FastAPI# library for pre-trained modelsfrom transformers import pipeline": [[310, null], [311, null]], "ArchitectureArchitecture Diagram": [[269, null], [270, null]], "Available Endpoints": [[168, "available-endpoints"]], "Batch Inference Class": [[268, "batch-inference-class"], [277, "batch-inference-class"], [288, null]], "Batch Inference ClassMany machine learning models are optimized for processing a batch of inputs at once. When working with a large dataset, there could be many batches of data. Instead of loading machine learning models repeatedly to run each batch of data, you want to spin up a number of actor processes that are initialized once with your model and reused to process multiple batches. To implement this, you can use the map_batches API with a \u201cCallable\u201d class method that implements:- __init__: Initialize any expensive state.- __call__: Perform the stateful transformation.In this example, a lightweight sentence transformer model, all-MiniLM-L6-v2 is used to generate embeddings of text data.": [[275, null], [276, null]], "Batch Inference with Ray Data": [[267, null], [267, "id1"], [268, null], [285, null]], "Batch Inference with Ray Data\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb Launch Locally: You can run this notebook locally.\ud83d\ude80 Launch on Cloud: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)This example shows how to do batch inference with Ray Data.Batch inference with Ray Data enables you to efficiently generate predictions from machine learning models on large datasets by processing multiple data points at once. Instead of running inference on one row at a time, which can be slow and resource-inefficient, batch inference leverages vectorized computation and parallelism to maximize throughput. This is especially useful when working with modern deep learning models, which are optimized for batch processing on CPUs, GPUs, or Apple Silicon devices.The typical workflow begins by loading your dataset\u2014such as a public dataset from Hugging Face\u2014into a Ray Dataset. Ray Data can automatically partition the data for parallel processing, or you can repartition it explicitly to control the number of data blocks. Once the data is loaded, you define a callable class (such as a text embedding model) that loads the machine learning model in its constructor and implements a __call__ method to process each batch. Ray Data\u2019s map_batches API is then used to apply this callable to each batch of data, with options to control concurrency and resource allocation (e.g., number of GPUs).This approach allows you to spin up multiple concurrent model instances, each processing different batches of data in parallel. The result is a significant speedup in inference time, especially for large datasets. After inference, you can materialize the results, inspect the output, and shut down the Ray cluster to free up resources. Batch inference with Ray Data is scalable, flexible, and integrates seamlessly with modern ML workflows, making it a powerful tool for production and research environments alike.": [[265, null], [266, null]], "Batch Processing": [[8, "batch-processing"], [191, "batch-processing"], [195, "batch-processing"]], "Batch embeddings": [[128, "batch-embeddings"], [131, null]], "Batch inference": [[128, null], [129, null]], "Batching": [[137, "batching"], [141, null]], "Build and load our model on a single GPU": [[13, "build-and-load-our-model-on-a-single-gpu"]], "Building a FastAPI Web Service and Deploying a Model": [[315, null]], "CI/CD": [[147, "ci-cd"], [154, null]], "Challenges in LLM Serving": [[101, "challenges-in-llm-serving"], [102, "challenges-in-llm-serving"], [106, null]], "Challenges with JVM": [[8, "challenges-with-jvm"], [191, "challenges-with-jvm"], [195, "challenges-with-jvm"]], "Clean Up": [[88, "clean-up"], [89, "clean-up"], [90, "clean-up"], [91, "clean-up"], [92, "clean-up"], [93, "clean-up"]], "Clean up": [[16, "clean-up"]], "Cloning/Duplicating Resources": [[94, "cloning-duplicating-resources"], [95, "cloning-duplicating-resources"]], "Cloud": [[96, "cloud"], [98, "cloud"]], "Collaborating with Your Team": [[100, "collaborating-with-your-team"]], "Composing Deployments": [[16, "composing-deployments"]], "Compute by Function": [[8, "compute-by-function"], [191, "compute-by-function"], [193, "compute-by-function"]], "Conclusion: Next Steps": [[118, "conclusion-next-steps"], [125, null]], "Concurrency Optimization Strategies": [[110, "concurrency-optimization-strategies"], [116, "concurrency-optimization-strategies"]], "Configuration Breakdown": [[110, "configuration-breakdown"], [113, "configuration-breakdown"]], "Configuration for Medium-Sized Models": [[110, "configuration-for-medium-sized-models"], [113, "configuration-for-medium-sized-models"]], "Configure Ray Serve LLM with LoRA": [[118, "configure-ray-serve-llm-with-lora"], [121, "configure-ray-serve-llm-with-lora"]], "Configure persistent storage": [[13, "configure-persistent-storage"]], "Configure scale and GPUs": [[13, "configure-scale-and-gpus"]], "Configure the Worker Node(s)": [[84, "configure-the-worker-node-s"], [85, "configure-the-worker-node-s"]], "Convert the Hugging Face dataset to a Ray Dataset": [[274, "convert-the-hugging-face-dataset-to-a-ray-dataset"]], "Convert to Pandas DataFrame": [[291, "convert-to-pandas-dataframe"], [298, "convert-to-pandas-dataframe"]], "Convert to Ray Dataset": [[291, "convert-to-ray-dataset"], [295, null]], "Course Welcome and Overview": [[1, "course-welcome-and-overview"], [169, "course-welcome-and-overview"], [170, "course-welcome-and-overview"]], "Create a Second Dataset": [[291, "create-a-second-dataset"], [295, "create-a-second-dataset"]], "Create a batch data and call the model": [[268, "create-a-batch-data-and-call-the-model"], [280, "create-a-batch-data-and-call-the-model"], [289, null]], "Create a batch data and call the modelDefine a Ray Data map_batches function to embed text using the SentenceTransformer model. This function will be applied to each batch of data in the Ray Data dataset. It will take a batch of sentences, encode them into embeddings, and return the batch with the embeddings added.Showcasing two options of to do batch inference based on if the ray cluster has have GPU nodes or if it has just CPU nodes. The second option also works on a local ray cluster on an Apple Silicon Mac with MPS.# setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPSworker_device = \u201ccpu\u201d # or \u201ccuda\u201d if you have a nvidia gpu on worker nodes# batch_size should be set based on VRAM if worker_device == \u201ccuda\u201d: # if you have a nvidia gpu on worker nodes    # adjust batch_size based on the VRAM available on the GPU    ds = ds.map_batches(TextEmbedder, num_gpus=1, concurrency=2, batch_size=64) # 2 nodes with 1 GPU eachelse:    ds = ds.map_batches(TextEmbedder, concurrency=2, batch_size=64) # either cpu or mps (on a mac)": [[278, null], [279, null]], "Create custom security group": [[17, "create-custom-security-group"], [22, "create-custom-security-group"]], "Creating Anyscale Resources": [[28, "creating-anyscale-resources"], [30, "creating-anyscale-resources"], [35, "creating-anyscale-resources"], [39, "creating-anyscale-resources"], [43, "creating-anyscale-resources"], [45, "creating-anyscale-resources"]], "Creating a Compute Config": [[84, "creating-a-compute-config"], [85, "creating-a-compute-config"]], "Creating a Container Image": [[84, "creating-a-container-image"], [85, "creating-a-container-image"]], "Creating a Data Batch and Calling the Model": [[280, null]], "Custom batching using groupby": [[15, "custom-batching-using-groupby"]], "Custom batching using groupby and aggregations": [[9, "custom-batching-using-groupby-and-aggregations"], [198, "custom-batching-using-groupby-and-aggregations"], [204, "custom-batching-using-groupby-and-aggregations"]], "Customization": [[0, "customization"]], "Customizing autoscaling": [[16, "customizing-autoscaling"]], "Data Engineering Compute": [[8, "data-engineering-compute"], [191, "data-engineering-compute"], [193, "data-engineering-compute"]], "Data Pipeline Observability (Ray Data)": [[164, "data-pipeline-observability-ray-data"], [166, null]], "Data Processing and ML examples with Ray": [[284, null], [292, null], [300, null], [324, null]], "Data Processing with Ray Data": [[8, "data-processing-with-ray-data"], [191, "data-processing-with-ray-data"], [196, null], [291, null], [293, null]], "Data flow": [[8, "data-flow"], [191, "data-flow"], [197, "data-flow"]], "Data ingestion": [[128, "data-ingestion"], [130, null]], "Data lakes": [[8, "data-lakes"], [191, "data-lakes"], [192, "data-lakes"]], "Data storage": [[128, "data-storage"], [133, null]], "Data warehouses": [[8, "data-warehouses"], [191, "data-warehouses"], [192, "data-warehouses"]], "Databases": [[8, "databases"], [191, "databases"], [192, "databases"]], "Dataloaders": [[299, "dataloaders"], [304, "dataloaders"]], "Dataset": [[9, "dataset"], [198, "dataset"], [201, "dataset"]], "Decode Phase": [[101, "decode-phase"], [102, "decode-phase"], [104, "decode-phase"]], "Default settings for Ray Tune": [[14, "default-settings-for-ray-tune"]], "Defining a data loader": [[13, "defining-a-data-loader"]], "Defining the Batch Inference Class": [[277, null]], "Deploy a Medium-Sized LLM with Ray Serve LLM": [[110, null], [111, null]], "Deploy the model": [[316, "deploy-the-model"], [320, "deploy-the-model"], [327, "deploy-the-model"]], "Deploy the modelserve.run(MySentimentModel.bind()) # Bind the deployment to the Ray Serve runtimeDeploymentHandle(deployment=\u2019MySentimentModel\u2019)": [[317, null], [318, null]], "Deploying Applications with Services": [[100, "deploying-applications-with-services"]], "Deploying Our Model and Testing it": [[320, null]], "Deploying Pipelines with Jobs": [[100, "deploying-pipelines-with-jobs"]], "Deploying at scale": [[268, "deploying-at-scale"], [280, "deploying-at-scale"], [289, "deploying-at-scale"]], "Deploying at scale- The batch size for encoding can be adjusted based on the available memory and performance requirements.- The device parameter ensures that the model runs on the correct device (CPU, GPU, or MPS).- The concurrency parameter controls how many batches are processed in parallel. If there are 2 nodes with 1 GPU each or 1 node with 2 GPUs, then set concurrency = 2 and num_gpus=1.- map_batches() is a lazy function and not executed until needed (example, using take or show).Run inference on a batch of 128 rows. This will return a batch of 128 rows with the embeddings added to the caller\u2019s machine.# Run inference on a batch of 128 rows for testing.ds.take_batch(128)": [[278, "deploying-at-scale-the-batch-size-for-encoding-can-be-adjusted-based-on-the-available-memory-and-performance-requirements-the-device-parameter-ensures-that-the-model-runs-on-the-correct-device-cpu-gpu-or-mps-the-concurrency-parameter-controls-how-many-batches-are-processed-in-parallel-if-there-are-2-nodes-with-1-gpu-each-or-1-node-with-2-gpus-then-set-concurrency-2-and-num-gpus-1-map-batches-is-a-lazy-function-and-not-executed-until-needed-example-using-take-or-show-run-inference-on-a-batch-of-128-rows-this-will-return-a-batch-of-128-rows-with-the-embeddings-added-to-the-caller-s-machine-run-inference-on-a-batch-of-128-rows-for-testing-ds-take-batch-128"], [279, "deploying-at-scale-the-batch-size-for-encoding-can-be-adjusted-based-on-the-available-memory-and-performance-requirements-the-device-parameter-ensures-that-the-model-runs-on-the-correct-device-cpu-gpu-or-mps-the-concurrency-parameter-controls-how-many-batches-are-processed-in-parallel-if-there-are-2-nodes-with-1-gpu-each-or-1-node-with-2-gpus-then-set-concurrency-2-and-num-gpus-1-map-batches-is-a-lazy-function-and-not-executed-until-needed-example-using-take-or-show-run-inference-on-a-batch-of-128-rows-this-will-return-a-batch-of-128-rows-with-the-embeddings-added-to-the-caller-s-machine-run-inference-on-a-batch-of-128-rows-for-testing-ds-take-batch-128"]], "Deploying to Anyscale Services": [[110, "deploying-to-anyscale-services"], [115, null]], "Deployment": [[168, "deployment"]], "Deployment Example Structure": [[79, "deployment-example-structure"]], "Deployment Options: Virtual Machines vs. Kubernetes": [[24, null], [25, null]], "Deployments": [[11, "deployments"], [16, "deployments"], [147, "deployments"], [149, null], [218, "deployments"], [221, "deployments"]], "Developing in Anyscale Workspaces": [[100, "developing-in-anyscale-workspaces"]], "Development": [[126, "development"], [127, "development"]], "Disabling Notebook Execution and Outputs": [[0, "disabling-notebook-execution-and-outputs"]], "Distributed Computing Frameworks": [[8, "distributed-computing-frameworks"], [191, "distributed-computing-frameworks"], [195, null]], "Distributed Data-Parallel Training with Ray Train": [[224, "distributed-data-parallel-training-with-ray-train"], [234, "distributed-data-parallel-training-with-ray-train"]], "Distributed training": [[137, null], [138, null]], "Distributed training with Ray Train, PyTorch and Hugging Face": [[299, null], [301, null]], "Distributed training with Ray Train, PyTorch and HuggingFace": [[284, "distributed-training-with-ray-train-pytorch-and-huggingface"], [292, "distributed-training-with-ray-train-pytorch-and-huggingface"], [300, "distributed-training-with-ray-train-pytorch-and-huggingface"], [324, "distributed-training-with-ray-train-pytorch-and-huggingface"]], "Diving deeper into Ray Tune concepts": [[7, "diving-deeper-into-ray-tune-concepts"], [253, "diving-deeper-into-ray-tune-concepts"], [257, "diving-deeper-into-ray-tune-concepts"]], "Dual-Subnet Architecture": [[17, "dual-subnet-architecture"], [22, "dual-subnet-architecture"]], "Enabling LLM Monitoring": [[110, "enabling-llm-monitoring"], [116, "enabling-llm-monitoring"]], "Environment state and action": [[336, "environment-state-and-action"], [337, "environment-state-and-action"]], "Evaluation": [[137, "evaluation"], [146, null]], "Example": [[159, "example"], [163, null]], "Example Workflow": [[0, "example-workflow"]], "Example: Car type description": [[118, "example-car-type-description"], [122, "example-car-type-description"]], "Example: Code Assistant LoRA": [[118, "example-code-assistant-lora"], [121, "example-code-assistant-lora"]], "Example: Deploying LoRA Adapters": [[118, "example-deploying-lora-adapters"], [121, null]], "Example: Getting Structured JSON Output": [[118, "example-getting-structured-json-output"], [122, null]], "Example: Setting up Tool Calling": [[118, "example-setting-up-tool-calling"], [123, null]], "Example: Weather Assistant with Tool Calling": [[118, "example-weather-assistant-with-tool-calling"], [123, "example-weather-assistant-with-tool-calling"]], "Execution mode": [[9, "execution-mode"], [15, "execution-mode"], [198, "execution-mode"], [202, "execution-mode"]], "Exercise": [[7, "exercise"], [14, "exercise"], [253, "exercise"], [257, "exercise"]], "Expected Output": [[118, "expected-output"], [122, "expected-output"]], "Exploring the Anyscale Log Viewer": [[88, "exploring-the-anyscale-log-viewer"], [89, "exploring-the-anyscale-log-viewer"]], "Exploring the Anyscale Metrics Tab": [[88, "exploring-the-anyscale-metrics-tab"], [89, "exploring-the-anyscale-metrics-tab"]], "Exploring the Ray Dashboard": [[88, "exploring-the-ray-dashboard"], [89, "exploring-the-ray-dashboard"]], "FastAPI webservice and deploy a model": [[315, "fastapi-webservice-and-deploy-a-model"], [316, "fastapi-webservice-and-deploy-a-model"], [327, null]], "FastAPI webservice and deploy a modelFastAPI is used to create a webservice \u2018app\u2019 to accept HTTP requests.MySentimentModel class loads the ML model and defines predict function for online inference. @serve.deployment decorator defines the Ray Serve deployment.@app.get() is used to create a GET \u2018/predict\u2019 route. Similarly, @app.post() can be used POST requests. See https://docs.ray.io/en/latest/serve/http-guide.html for more details.In this example, application_logic() function is used to define a sample transformation or business logic that can be applied before sending the input to the ML model for inference. See inline comments for further explanation.### Scaling deploymentnum_replicas parameter sets the number of instances of the deployment. FastAPI and RayServe automatically load balances to send requests to each instance. There are more options to set the accelerator_type to GPU and even use fractional GPUs. See configuration options here: https://docs.ray.io/en/latest/serve/configure-serve-deployment.html .": [[313, null], [314, null]], "Features": [[0, "features"]], "File based shuffle on read": [[9, "file-based-shuffle-on-read"], [15, "file-based-shuffle-on-read"], [198, "file-based-shuffle-on-read"], [204, "file-based-shuffle-on-read"]], "Filter Ray Dataset": [[291, "filter-ray-dataset"], [296, null]], "Forward process: adding noise": [[329, "forward-process-adding-noise"], [330, "forward-process-adding-noise"]], "Foundations": [[372, "foundations"]], "General-Purpose Distributed Computing": [[8, "general-purpose-distributed-computing"], [191, "general-purpose-distributed-computing"], [195, "general-purpose-distributed-computing"]], "Get User Profile": [[168, "get-user-profile"]], "Getting Started": [[100, "getting-started"]], "Getting Started with Ray Serve LLM": [[101, "getting-started-with-ray-serve-llm"], [102, "getting-started-with-ray-serve-llm"], [108, null]], "Getting started": [[7, "getting-started"], [14, "getting-started"], [253, "getting-started"], [257, "getting-started"]], "How Navigation Works": [[0, "how-navigation-works"]], "How Other Sizes Differ": [[110, "how-other-sizes-differ"], [117, "how-other-sizes-differ"]], "How Resources are defined": [[17, "how-resources-are-defined"], [20, "how-resources-are-defined"]], "How to Choose an LLM?": [[118, "how-to-choose-an-llm"], [124, null]], "How to migrate this computer vision workload to a distributed setup using Ray on Anyscale": [[362, "how-to-migrate-this-computer-vision-workload-to-a-distributed-setup-using-ray-on-anyscale"], [363, "how-to-migrate-this-computer-vision-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this diffusion-policy workload to a distributed setup using Ray on Anyscale": [[329, "how-to-migrate-this-diffusion-policy-workload-to-a-distributed-setup-using-ray-on-anyscale"], [330, "how-to-migrate-this-diffusion-policy-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this recommendation system workload to a distributed setup using Ray on Anyscale": [[342, "how-to-migrate-this-recommendation-system-workload-to-a-distributed-setup-using-ray-on-anyscale"], [343, "how-to-migrate-this-recommendation-system-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this tabular workload to a distributed setup using Ray on Anyscale": [[349, "how-to-migrate-this-tabular-workload-to-a-distributed-setup-using-ray-on-anyscale"], [350, "how-to-migrate-this-tabular-workload-to-a-distributed-setup-using-ray-on-anyscale"]], "How to migrate this time-series workload to a distributed multi-node setup using Ray on Anyscale": [[355, "how-to-migrate-this-time-series-workload-to-a-distributed-multi-node-setup-using-ray-on-anyscale"], [356, "how-to-migrate-this-time-series-workload-to-a-distributed-multi-node-setup-using-ray-on-anyscale"]], "How to scale this policy learning workload using Ray on Anyscale": [[336, "how-to-scale-this-policy-learning-workload-using-ray-on-anyscale"], [337, "how-to-scale-this-policy-learning-workload-using-ray-on-anyscale"]], "How we use Ray AI Libraries for this task": [[12, "how-we-use-ray-ai-libraries-for-this-task"]], "Hyperparameter tune the PyTorch model using Ray Tune": [[7, "hyperparameter-tune-the-pytorch-model-using-ray-tune"], [253, "hyperparameter-tune-the-pytorch-model-using-ray-tune"], [257, "hyperparameter-tune-the-pytorch-model-using-ray-tune"]], "Import Libraries": [[268, "import-libraries"], [271, "import-libraries"], [286, "import-libraries"]], "Import Librariesimport rayimport torchfrom typing import Dictimport numpy as npfrom sentence_transformers import SentenceTransformer # huggingface sentence transformersfrom datasets import load_dataset # huggingface datasets": [[269, "import-librariesimport-rayimport-torchfrom-typing-import-dictimport-numpy-as-npfrom-sentence-transformers-import-sentencetransformer-huggingface-sentence-transformersfrom-datasets-import-load-dataset-huggingface-datasets"], [270, "import-librariesimport-rayimport-torchfrom-typing-import-dictimport-numpy-as-npfrom-sentence-transformers-import-sentencetransformer-huggingface-sentence-transformersfrom-datasets-import-load-dataset-huggingface-datasets"]], "Import libraries": [[312, "import-libraries"], [316, "import-libraries"], [326, "import-libraries"]], "Import ray serve and FastAPI libraries": [[312, "import-ray-serve-and-fastapi-libraries"]], "Improving Concurrency": [[110, "improving-concurrency"], [116, "improving-concurrency"]], "In-memory data formats": [[8, "in-memory-data-formats"], [191, "in-memory-data-formats"], [192, "in-memory-data-formats"]], "Inference: ranking items per user": [[342, "inference-ranking-items-per-user"], [343, "inference-ranking-items-per-user"]], "Initialize Ray and Load a Dataset": [[291, "initialize-ray-and-load-a-dataset"], [294, "initialize-ray-and-load-a-dataset"]], "Input: Images as tensors": [[329, "input-images-as-tensors"], [330, "input-images-as-tensors"]], "Input: user\u2013item\u2013rating triples": [[342, "input-useritemrating-triples"], [343, "input-useritemrating-triples"]], "Inputs": [[362, "inputs"], [363, "inputs"]], "Inspecting the features of the NYC taxi dataset": [[12, "inspecting-the-features-of-the-nyc-taxi-dataset"]], "Installation": [[0, "installation"], [284, "installation"], [292, "installation"], [300, "installation"], [324, "installation"]], "Integrating with FastAPI": [[16, "integrating-with-fastapi"]], "Intro to Ray Data": [[15, null]], "Intro to Ray Data:  Ray Data + Unstructured Data": [[10, null], [206, null], [207, null]], "Intro to Ray Serve": [[16, null]], "Intro to Ray Tune": [[7, "intro-to-ray-tune"], [14, null], [253, "intro-to-ray-tune"], [257, "intro-to-ray-tune"]], "Introduction": [[82, "introduction"], [83, "introduction"], [84, "introduction"], [85, "introduction"], [86, "introduction"], [87, "introduction"], [88, "introduction"], [89, "introduction"]], "Introduction to Ray Core (Advancement): Object store, Tasks, Actors": [[3, null], [181, null], [182, null]], "Introduction to Ray Core: Getting Started": [[2, null], [175, null], [176, null]], "Introduction to Ray Data: Industry Landscape": [[8, null], [191, null], [192, null]], "Introduction to Ray Data: Ray Data + Structured Data": [[9, null], [198, null], [199, null]], "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving": [[101, null], [102, null], [103, null]], "Introduction to Ray Serve with PyTorch": [[11, null], [218, null], [219, null]], "Introduction to Ray Train": [[13, null]], "Introduction to Ray Train + PyTorch": [[5, null]], "Introduction to Ray Train: Ray Train + PyTorch Lightning": [[6, null], [259, null], [260, null]], "Introduction to Ray Tune": [[7, null], [253, null], [254, null]], "Introduction to Ray: Developer": [[1, null], [169, null], [170, null]], "Introduction to the Ray AI Libraries": [[12, null]], "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model": [[4, null], [171, null], [172, null]], "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster": [[43, null], [44, null]], "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster": [[66, null], [67, null]], "Introduction: Deploy Anyscale Ray on AWS EC2 Instances": [[28, null], [29, null]], "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster": [[53, null], [54, null]], "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)": [[35, null], [36, null]], "Introduction: Why Anyscale?": [[100, "introduction-why-anyscale"]], "Join Two Ray Datasets": [[291, "join-two-ray-datasets"], [297, null]], "Key Benefits": [[118, "key-benefits"], [118, "id1"], [118, "id3"], [121, "key-benefits"], [122, "key-benefits"], [123, "key-benefits"]], "Key Components": [[110, "key-components"], [113, "key-components"]], "Key Concepts and Optimizations": [[101, "key-concepts-and-optimizations"], [102, "key-concepts-and-optimizations"], [105, null]], "Key Functions": [[17, "key-functions"], [19, "key-functions"]], "Key Ray Serve Features": [[16, "key-ray-serve-features"]], "Key Takeaways": [[101, "key-takeaways"], [102, "key-takeaways"], [109, null], [110, "key-takeaways"], [117, "key-takeaways"], [118, "key-takeaways"], [125, "key-takeaways"]], "Key characteristics of Anyscale Services": [[92, "key-characteristics-of-anyscale-services"], [93, "key-characteristics-of-anyscale-services"]], "Key points": [[13, "key-points"]], "Labels": [[362, "labels"], [363, "labels"]], "Lakehouses": [[8, "lakehouses"], [191, "lakehouses"], [192, "lakehouses"]], "Last Updated 6/19": [[100, null]], "Launch Grafana": [[155, "launch-grafana"], [158, "launch-grafana"]], "Launching Ray Serve": [[110, "launching-ray-serve"], [114, "launching-ray-serve"]], "Launching a Anyscale Workspace": [[80, "launching-a-anyscale-workspace"], [81, "launching-a-anyscale-workspace"]], "Launching a Web Application using Ray Serve": [[164, "launching-a-web-application-using-ray-serve"], [167, "launching-a-web-application-using-ray-serve"]], "Launching a distributed training job with a TorchTrainer.": [[13, "launching-a-distributed-training-job-with-a-torchtrainer"]], "Launching the Service": [[110, "launching-the-service"], [115, "launching-the-service"]], "Learn More": [[118, "learn-more"], [118, "id2"], [118, "id4"], [121, "learn-more"], [122, "learn-more"], [123, "learn-more"]], "Learning Approach": [[118, "learning-approach"], [120, "learning-approach"]], "Learning Path Overview and Objectives": [[100, "learning-path-overview-and-objectives"]], "Library Imports": [[291, "library-imports"], [294, null]], "Llm Serving": [[372, "llm-serving"]], "Load a dataset": [[268, "load-a-dataset"], [274, "load-a-dataset"], [287, null]], "Load a datasetLoad a dataset from hugging face or local and convert into Ray Dataset. A Ray cluster automatically initialized on local or on Anyscale platform. You can also use ray.init() To explicitly create or connect to an existing Ray cluster.https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html#ray.init# load a Hugging Face datasethf_dataset = load_dataset(\u201ccardiffnlp/tweet_eval\u201d, \u201csentiment\u201d, split=\u201dtrain\u201d)# Convert the Hugging Face dataset to a Ray Datasetds = ray.data.from_huggingface(hf_dataset).repartition(2) # repartition to 2 blocks for parallel processing. Not necessary if already partitioned due to the size of the dataset.": [[272, null], [273, null]], "Loading a Dataset": [[274, null]], "Loading and visualizing MNIST data": [[13, "loading-and-visualizing-mnist-data"]], "Local Deployment & Inference": [[110, "local-deployment-inference"], [114, null]], "Local Development": [[168, "local-development"]], "Local IDE (VSCode / Cursor)": [[82, "local-ide-vscode-cursor"], [83, "local-ide-vscode-cursor"]], "Logging Configuration": [[164, "logging-configuration"], [167, "logging-configuration"]], "Machine Learning and AI Compute": [[8, "machine-learning-and-ai-compute"], [191, "machine-learning-and-ai-compute"], [193, "machine-learning-and-ai-compute"]], "Managing Dependencies": [[1, "managing-dependencies"], [169, "managing-dependencies"], [170, "managing-dependencies"]], "Materializing Data": [[15, "materializing-data"]], "Model": [[137, "model"], [140, null]], "Model Recommendations by Use Case": [[118, "model-recommendations-by-use-case"], [124, "model-recommendations-by-use-case"]], "Model Selection Framework": [[118, "model-selection-framework"], [124, "model-selection-framework"]], "Model Size Comparison": [[110, "model-size-comparison"], [112, "model-size-comparison"]], "Model registry": [[137, "model-registry"], [142, null]], "Model: embedding-based matrix factorization": [[342, "model-embedding-based-matrix-factorization"], [343, "model-embedding-based-matrix-factorization"]], "Monitoring and Debugging": [[128, "monitoring-and-debugging"], [134, null]], "More Advanced Topics": [[118, "more-advanced-topics"], [125, "more-advanced-topics"]], "More about Datasets": [[15, "more-about-datasets"]], "Multi-Actor Ray Serve Tracing Example": [[168, null]], "Multi-modal AI pipeline": [[126, null], [127, null]], "Multimodal Ai Workloads": [[372, "multimodal-ai-workloads"]], "Next Steps": [[101, "next-steps"], [102, "next-steps"], [109, "next-steps"], [110, "next-steps"], [117, "next-steps"], [118, "next-steps"], [125, "next-steps"]], "No infrastructure headaches": [[126, "no-infrastructure-headaches"], [127, "no-infrastructure-headaches"]], "Note on the Checkpoint Lifecycle": [[224, "note-on-the-checkpoint-lifecycle"], [234, "note-on-the-checkpoint-lifecycle"]], "Note that this does not mutate the original Dataset.": [[283, "note-that-this-does-not-mutate-the-original-dataset"]], "Notebook": [[82, "notebook"], [83, "notebook"]], "Now launch a Ray worker node in the terminal:": [[155, "now-launch-a-ray-worker-node-in-the-terminal"], [158, "now-launch-a-ray-worker-node-in-the-terminal"]], "Observability": [[147, "observability"], [152, null], [372, "observability"]], "Observability Introduction": [[155, null], [156, null]], "Observability Overview": [[155, "observability-overview"], [157, null]], "On Ray Data vs Spark": [[8, "on-ray-data-vs-spark"], [191, "on-ray-data-vs-spark"], [196, "on-ray-data-vs-spark"]], "Online Model Serving with Ray Serve": [[309, null], [309, "id1"], [316, null], [325, null]], "Online Model Serving with Ray Serve\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb Launch Locally: You can run this notebook locally.\ud83d\ude80 Launch on Cloud: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)Model serving is the process of deploying machine learning models to production so that they can be accessed and used by applications or users. It involves creating an API or interface that allows users to send requests to the modeland receive predictions in response. There are several libraries and frameworks available for model serving, each with its own features and capabilities. In this notebook, we showcase Ray Serve and FastAPI to deploy a sentiment analysismachine learning (ML) model.": [[307, null], [308, null]], "Online serving": [[147, null], [148, null]], "Option 1: Create a New VPC": [[17, "option-1-create-a-new-vpc"], [22, "option-1-create-a-new-vpc"]], "Option 2: Use Existing VPC": [[17, "option-2-use-existing-vpc"], [22, "option-2-use-existing-vpc"]], "Organization": [[96, "organization"], [98, "organization"]], "Our Example: Llama-3.1-70B": [[110, "our-example-llama-3-1-70b"], [112, "our-example-llama-3-1-70b"]], "Out of memory errors": [[268, "out-of-memory-errors"], [283, "out-of-memory-errors"], [290, "out-of-memory-errors"]], "Out of memory errorsGPU (or MPS or CPU) memory has to keep the machine learning model and the batch of data in memory during the inference. If the batch_size is too large, it can run out of memory and throw out of memory errors. In that case, reduce the batch_size.### Shutdown Ray cluster# avoids collisons with other notebooks running ray jobs on the same machineray.shutdown()": [[281, "out-of-memory-errorsgpu-or-mps-or-cpu-memory-has-to-keep-the-machine-learning-model-and-the-batch-of-data-in-memory-during-the-inference-if-the-batch-size-is-too-large-it-can-run-out-of-memory-and-throw-out-of-memory-errors-in-that-case-reduce-the-batch-size-shutdown-ray-cluster-avoids-collisons-with-other-notebooks-running-ray-jobs-on-the-same-machineray-shutdown"], [282, "out-of-memory-errorsgpu-or-mps-or-cpu-memory-has-to-keep-the-machine-learning-model-and-the-batch-of-data-in-memory-during-the-inference-if-the-batch-size-is-too-large-it-can-run-out-of-memory-and-throw-out-of-memory-errors-in-that-case-reduce-the-batch-size-shutdown-ray-cluster-avoids-collisons-with-other-notebooks-running-ray-jobs-on-the-same-machineray-shutdown"]], "Outline": [[267, "outline"], [268, "outline"], [285, "outline"], [299, "outline"], [301, "outline"], [309, "outline"], [316, "outline"], [325, "outline"]], "Outline    Architecture    Import Libraries    FastAPI service to accept HTTP requests and scaling with Ray Serve    Simulate Client: Send test requests    Shutdown the Serve app and the ray cluster</ul></div>": [[307, "outline-architecture-import-libraries-fastapi-service-to-accept-http-requests-and-scaling-with-ray-serve-simulate-client-send-test-requests-shutdown-the-serve-app-and-the-ray-cluster"], [308, "outline-architecture-import-libraries-fastapi-service-to-accept-http-requests-and-scaling-with-ray-serve-simulate-client-send-test-requests-shutdown-the-serve-app-and-the-ray-cluster"]], "Outline of the notebook": [[291, "outline-of-the-notebook"], [293, "outline-of-the-notebook"]], "Outline<b>In this notebook, we go through a typical ML batch inference workflow:</b>    Architecture    Import Libraries    Load a public dataset from Hugging Face and move it into Ray Data object store.    Batch Inference Class        - Create a Ray actor class to load a ML model. In this example, we use SentenceTransformer library from Hugging Face to load a sentence embedding model.    Create batches of data to do inference.    Deploying at Scale    Inference on the entire dataset    Out of memory errors    Summary</ul></div>": [[265, "outlinein-this-notebook-we-go-through-a-typical-ml-batch-inference-workflow-architecture-import-libraries-load-a-public-dataset-from-hugging-face-and-move-it-into-ray-data-object-store-batch-inference-class-create-a-ray-actor-class-to-load-a-ml-model-in-this-example-we-use-sentencetransformer-library-from-hugging-face-to-load-a-sentence-embedding-model-create-batches-of-data-to-do-inference-deploying-at-scale-inference-on-the-entire-dataset-out-of-memory-errors-summary"], [266, "outlinein-this-notebook-we-go-through-a-typical-ml-batch-inference-workflow-architecture-import-libraries-load-a-public-dataset-from-hugging-face-and-move-it-into-ray-data-object-store-batch-inference-class-create-a-ray-actor-class-to-load-a-ml-model-in-this-example-we-use-sentencetransformer-library-from-hugging-face-to-load-a-sentence-embedding-model-create-batches-of-data-to-do-inference-deploying-at-scale-inference-on-the-entire-dataset-out-of-memory-errors-summary"]], "Outlook": [[17, "outlook"], [18, "outlook"]], "Outlook:  Ray Data in Production": [[15, "outlook-ray-data-in-production"]], "Overview": [[168, "overview"]], "Overview: Advanced Features Preview": [[118, "overview-advanced-features-preview"], [120, null]], "Overview: Why Medium-Sized Models?": [[110, "overview-why-medium-sized-models"], [112, null]], "Part 1. Creating and Submitting your first job": [[90, "part-1-creating-and-submitting-your-first-job"], [91, "part-1-creating-and-submitting-your-first-job"]], "Part 1: Starting your first Anyscale Service": [[92, "part-1-starting-your-first-anyscale-service"], [93, "part-1-starting-your-first-anyscale-service"]], "Part 2. Automation and Scheduling": [[90, "part-2-automation-and-scheduling"], [91, "part-2-automation-and-scheduling"]], "Practical Selection Process": [[118, "practical-selection-process"], [124, "practical-selection-process"]], "Prefill Phase": [[101, "prefill-phase"], [102, "prefill-phase"], [104, "prefill-phase"]], "Preprocess": [[137, "preprocess"], [139, null]], "Preprocessing with a Tokenizer": [[291, "preprocessing-with-a-tokenizer"], [298, null]], "Prerequisites": [[1, "prerequisites"], [28, "prerequisites"], [29, "prerequisites"], [35, "prerequisites"], [37, null], [43, "prerequisites"], [44, "prerequisites"], [53, "prerequisites"], [55, null], [66, "prerequisites"], [68, null], [79, "prerequisites"], [110, "prerequisites"], [114, "prerequisites"], [164, "prerequisites"], [165, "prerequisites"], [168, "prerequisites"], [169, "prerequisites"], [170, "prerequisites"]], "Prerequisites and Assumptions": [[155, "prerequisites-and-assumptions"], [158, "prerequisites-and-assumptions"]], "Production": [[126, "production"], [127, "production"]], "Production Job": [[137, "production-job"], [145, null]], "Production jobs": [[128, "production-jobs"], [135, null]], "Production services": [[147, "production-services"], [153, null]], "Projects": [[96, "projects"], [98, "projects"]], "Providers": [[43, "providers"], [44, "providers"]], "Publishing": [[0, "publishing"]], "Purpose": [[17, "purpose"], [19, "purpose"]], "Pytorch Lightning": [[372, "pytorch-lightning"]], "Query the deployed model": [[320, "query-the-deployed-model"]], "Ray Ai Libs": [[372, "ray-ai-libs"]], "Ray Core": [[372, "ray-core"]], "Ray Data": [[128, "ray-data"], [132, null], [372, "ray-data"]], "Ray Data Batch Inference": [[372, "ray-data-batch-inference"]], "Ray Data Logs": [[164, "ray-data-logs"], [166, "ray-data-logs"]], "Ray Data Metrics": [[164, "ray-data-metrics"], [166, "ray-data-metrics"]], "Ray Data Processing": [[372, "ray-data-processing"]], "Ray Distributed Training": [[372, "ray-distributed-training"]], "Ray Enablement Content": [[372, null]], "Ray Enablement Content: Jupyter Book Publishing": [[0, null]], "Ray Observability": [[159, "ray-observability"], [161, null]], "Ray Serve": [[8, "ray-serve"], [147, "ray-serve"], [151, null], [191, "ray-serve"], [197, null], [372, "ray-serve"]], "Ray Serve Alerts": [[164, "ray-serve-alerts"], [167, "ray-serve-alerts"]], "Ray Serve LLM + Anyscale Architecture": [[101, "ray-serve-llm-anyscale-architecture"], [102, "ray-serve-llm-anyscale-architecture"], [107, null]], "Ray Serve Logs": [[164, "ray-serve-logs"], [167, "ray-serve-logs"]], "Ray Serve Metrics": [[164, "ray-serve-metrics"], [167, "ray-serve-metrics"]], "Ray Serve Online Serving": [[372, "ray-serve-online-serving"]], "Ray Serve Tracing (Anyscale Only)": [[164, "ray-serve-tracing-anyscale-only"], [167, "ray-serve-tracing-anyscale-only"]], "Ray Serve vs Ray Data": [[8, "ray-serve-vs-ray-data"], [191, "ray-serve-vs-ray-data"], [197, "ray-serve-vs-ray-data"]], "Ray Train": [[137, "ray-train"], [144, null], [372, "ray-train"]], "Ray Tune": [[372, "ray-tune"]], "Ray Workloads Data Dashboard": [[164, "ray-workloads-data-dashboard"], [166, "ray-workloads-data-dashboard"]], "Ray and Anyscale Observability Introduction": [[159, null], [160, null]], "Ray and Anyscale Observability in Detail": [[164, null], [165, null]], "Recap": [[12, "recap"]], "Register New User": [[168, "register-new-user"]], "Related Examples": [[110, "related-examples"], [112, "related-examples"]], "Related Examples & Templates": [[110, "related-examples-templates"], [117, "related-examples-templates"]], "Replicas": [[11, "replicas"], [218, "replicas"], [221, "replicas"]], "Reporting metrics": [[13, "reporting-metrics"]], "Request Flow": [[168, "request-flow"]], "Requirements": [[43, "requirements"], [44, "requirements"]], "Resources": [[101, "resources"], [102, "resources"], [109, "resources"], [110, "resources"], [117, "resources"], [118, "resources"], [125, "resources"]], "Reverse diffusion: sampling new images": [[329, "reverse-diffusion-sampling-new-images"], [330, "reverse-diffusion-sampling-new-images"]], "Run a simple Data Pipeline": [[164, "run-a-simple-data-pipeline"], [166, "run-a-simple-data-pipeline"]], "Run inference on a batch of 128 rows for testing.": [[280, "run-inference-on-a-batch-of-128-rows-for-testing"]], "Run inference on the entire dataset": [[268, "run-inference-on-the-entire-dataset"], [283, "run-inference-on-the-entire-dataset"], [283, "id1"], [290, null]], "Run inference on the entire datasetExecute and materialize this dataset into object store memory. This operation will trigger execution of the lazy transformations performed on this dataset. The embedding model \u2018TextEmbedder\u2019 in map_batches() is called on the entire dataset.# Run inference on the entire dataset# Note that this does not mutate the original Dataset.materialized_ds = ds.materialize()# metadata after inferenceprint(\u2018** Original dataset:\u2019, ds)print(\u2018\\n** Materialized dataset:\u2019, materialized_ds)# Show a few rows of the materialized dataset with embeddingsmaterialized_ds.show(3)": [[281, null], [282, null]], "Running Inference on Anyscale": [[110, "running-inference-on-anyscale"], [115, "running-inference-on-anyscale"]], "Running inference on the entire dataset": [[283, null]], "Running this notebook": [[12, "running-this-notebook"]], "Sample Requests": [[168, "sample-requests"]], "Saving a checkpoint in a local directory": [[13, "saving-a-checkpoint-in-a-local-directory"]], "Scaling deployment": [[315, "scaling-deployment"], [316, "scaling-deployment"], [327, "scaling-deployment"]], "Scheduling the training loop on a single GPU": [[13, "scheduling-the-training-loop-on-a-single-gpu"]], "Sending Requests": [[110, "sending-requests"], [114, "sending-requests"]], "Setting Up Local Ray Observability": [[155, "setting-up-local-ray-observability"], [158, null]], "Setting Up a Local Ray server using Jupyter Notebook": [[1, "setting-up-a-local-ray-server-using-jupyter-notebook"], [169, "setting-up-a-local-ray-server-using-jupyter-notebook"], [170, "setting-up-a-local-ray-server-using-jupyter-notebook"]], "Setting up Ray Serve LLM": [[110, "setting-up-ray-serve-llm"], [113, null]], "Setting up the Configuration File": [[110, "setting-up-the-configuration-file"], [115, "setting-up-the-configuration-file"]], "Setup and Installation": [[168, "setup-and-installation"]], "Show a few rows of the materialized dataset with embeddings": [[283, "show-a-few-rows-of-the-materialized-dataset-with-embeddings"]], "Shuffle all rows globally": [[9, "shuffle-all-rows-globally"], [198, "shuffle-all-rows-globally"], [204, "shuffle-all-rows-globally"]], "Shuffle rows globally": [[15, "shuffle-rows-globally"]], "Shuffling block order": [[9, "shuffling-block-order"], [15, "shuffling-block-order"], [198, "shuffling-block-order"], [204, "shuffling-block-order"]], "Shuffling data": [[9, "shuffling-data"], [15, "shuffling-data"], [198, "shuffling-data"], [204, "shuffling-data"]], "Shutdown Ray": [[291, "shutdown-ray"], [298, "shutdown-ray"]], "Shutdown Ray cluster": [[268, "shutdown-ray-cluster"], [283, "shutdown-ray-cluster"], [290, "shutdown-ray-cluster"]], "Shutdown and Summary": [[323, null]], "Shutdown the Ray Serve instances and Ray Cluster": [[316, "shutdown-the-ray-serve-instances-and-ray-cluster"], [323, "shutdown-the-ray-serve-instances-and-ray-cluster"], [328, "shutdown-the-ray-serve-instances-and-ray-cluster"]], "Shutdown the Ray Serve instances and Ray Cluster# stop ray serveserve.shutdown()  # Shutdown Ray Serve when done, ray cluster will still be runningray.shutdown()  # Shutdown Ray cluster": [[321, null], [322, null]], "Shutting Down": [[110, "shutting-down"], [114, "shutting-down"]], "Shutting Down the Service": [[110, "shutting-down-the-service"], [115, "shutting-down-the-service"]], "Similar images": [[128, "similar-images"], [136, null]], "Simulate Client: Send test requests": [[316, "simulate-client-send-test-requests"], [320, "simulate-client-send-test-requests"], [328, null]], "Simulate Client: Send test requestsWe use requests library to send HTTP requests to the deployed model.Note: if you encounter any errors with serve not able to start, most likely it is due to previous instance of serve not being shutdown properly. Restart the notebook or see towards the end of notebook to see how to gracefully shutdown ray serve and the ray cluster.import requests # used to send HTTP requests to the deployed model# Query the deployed modelresponse = requests.get(\u201dhttp://localhost:8000/predict\u201d, params={\u201ctext\u201d: \u201cI love Ray Serve!\u201d})print(response.json())  # Should print the sentiment analysis result{\u2018text\u2019: \u2018i love ray serve!\u2019, \u2018sentiment\u2019: [{\u2018label\u2019: \u2018POSITIVE\u2019, \u2018score\u2019: 0.9998507499694824}]}": [[317, "simulate-client-send-test-requestswe-use-requests-library-to-send-http-requests-to-the-deployed-model-note-if-you-encounter-any-errors-with-serve-not-able-to-start-most-likely-it-is-due-to-previous-instance-of-serve-not-being-shutdown-properly-restart-the-notebook-or-see-towards-the-end-of-notebook-to-see-how-to-gracefully-shutdown-ray-serve-and-the-ray-cluster-import-requests-used-to-send-http-requests-to-the-deployed-model-query-the-deployed-modelresponse-requests-get-http-localhost-8000-predict-params-text-i-love-ray-serve-print-response-json-should-print-the-sentiment-analysis-result-text-i-love-ray-serve-sentiment-label-positive-score-0-9998507499694824"], [319, null]], "Start by launching the Ray head node in the terminal:": [[155, "start-by-launching-the-ray-head-node-in-the-terminal"], [158, "start-by-launching-the-ray-head-node-in-the-terminal"]], "Stateful transformations with actors": [[15, "stateful-transformations-with-actors"]], "Step 1: Configuration": [[101, "step-1-configuration"], [102, "step-1-configuration"], [108, "step-1-configuration"]], "Step 1: Install Required Packages": [[155, "step-1-install-required-packages"], [158, "step-1-install-required-packages"]], "Step 2: Deployment": [[101, "step-2-deployment"], [102, "step-2-deployment"], [108, "step-2-deployment"]], "Step 2: Launch Prometheus": [[155, "step-2-launch-prometheus"], [158, "step-2-launch-prometheus"]], "Step 3: Launch Ray Cluster": [[155, "step-3-launch-ray-cluster"], [158, "step-3-launch-ray-cluster"]], "Step 3: Querying": [[101, "step-3-querying"], [102, "step-3-querying"], [108, "step-3-querying"]], "Step 4: Install and Launch Grafana": [[155, "step-4-install-and-launch-grafana"], [158, "step-4-install-and-launch-grafana"]], "Step 4: Shutdown": [[102, "step-4-shutdown"], [108, "step-4-shutdown"]], "Steps to run:": [[12, "steps-to-run"]], "Streaming Applications": [[8, "streaming-applications"], [191, "streaming-applications"], [195, "streaming-applications"]], "Structure of a data lake": [[8, "structure-of-a-data-lake"], [191, "structure-of-a-data-lake"], [192, "structure-of-a-data-lake"]], "Summary": [[268, "summary"], [283, "summary"], [290, "summary"], [291, "summary"], [298, "summary"], [316, "summary"], [323, "summary"], [328, "summary"]], "Summary & Outlook": [[110, "summary-outlook"], [117, null]], "SummaryIn this notebook, we deployed a sentiment analysis model from Hugging Face using Ray Serve and FastAPI. Using num_replicas we scaled the number of instances of the model. There are many more options to autoscale to increase the replicas when the traffic is high and downscale to zero when there is no traffic.": [[321, "summaryin-this-notebook-we-deployed-a-sentiment-analysis-model-from-hugging-face-using-ray-serve-and-fastapi-using-num-replicas-we-scaled-the-number-of-instances-of-the-model-there-are-many-more-options-to-autoscale-to-increase-the-replicas-when-the-traffic-is-high-and-downscale-to-zero-when-there-is-no-traffic"], [322, "summaryin-this-notebook-we-deployed-a-sentiment-analysis-model-from-hugging-face-using-ray-serve-and-fastapi-using-num-replicas-we-scaled-the-number-of-instances-of-the-model-there-are-many-more-options-to-autoscale-to-increase-the-replicas-when-the-traffic-is-high-and-downscale-to-zero-when-there-is-no-traffic"]], "SummaryThis notebook demonstrates how to perform efficient batch inference on large datasets using Ray Data. It walks through loading a public dataset from Hugging Face, converting it into a Ray Dataset, and defining a callable class to load and apply a machine learning model (SentenceTransformer) for embedding text. The notebook shows how to use Ray Data\u2019s map_batches API to process data in parallel batches, leveraging available CPUs or GPUs for high-throughput inference. It also covers best practices for scaling, handling memory constraints, and summarizes how Ray Data enables scalable, distributed batch inference for modern ML workflows.": [[281, "summarythis-notebook-demonstrates-how-to-perform-efficient-batch-inference-on-large-datasets-using-ray-data-it-walks-through-loading-a-public-dataset-from-hugging-face-converting-it-into-a-ray-dataset-and-defining-a-callable-class-to-load-and-apply-a-machine-learning-model-sentencetransformer-for-embedding-text-the-notebook-shows-how-to-use-ray-datas-map-batches-api-to-process-data-in-parallel-batches-leveraging-available-cpus-or-gpus-for-high-throughput-inference-it-also-covers-best-practices-for-scaling-handling-memory-constraints-and-summarizes-how-ray-data-enables-scalable-distributed-batch-inference-for-modern-ml-workflows"], [282, "summarythis-notebook-demonstrates-how-to-perform-efficient-batch-inference-on-large-datasets-using-ray-data-it-walks-through-loading-a-public-dataset-from-hugging-face-converting-it-into-a-ray-dataset-and-defining-a-callable-class-to-load-and-apply-a-machine-learning-model-sentencetransformer-for-embedding-text-the-notebook-shows-how-to-use-ray-datas-map-batches-api-to-process-data-in-parallel-batches-leveraging-available-cpus-or-gpus-for-high-throughput-inference-it-also-covers-best-practices-for-scaling-handling-memory-constraints-and-summarizes-how-ray-data-enables-scalable-distributed-batch-inference-for-modern-ml-workflows"]], "Supported Infrastructure Types": [[17, "supported-infrastructure-types"], [20, "supported-infrastructure-types"]], "Testing the Container Image and Compute Config with an Anyscale Workflow": [[84, "testing-the-container-image-and-compute-config-with-an-anyscale-workflow"], [85, "testing-the-container-image-and-compute-config-with-an-anyscale-workflow"]], "The Compute Layer": [[8, "the-compute-layer"], [191, "the-compute-layer"], [193, null]], "The LLM Text Generation Process": [[101, "the-llm-text-generation-process"], [102, "the-llm-text-generation-process"], [104, "the-llm-text-generation-process"]], "The Orchestration Layer": [[8, "the-orchestration-layer"], [191, "the-orchestration-layer"], [194, null]], "The data layer": [[8, "the-data-layer"], [191, "the-data-layer"], [192, "the-data-layer"]], "The following instructions will walk you through running your first job. This notebook covers the following:": [[90, "the-following-instructions-will-walk-you-through-running-your-first-job-this-notebook-covers-the-following"], [91, "the-following-instructions-will-walk-you-through-running-your-first-job-this-notebook-covers-the-following"]], "This notebook covers the following:": [[92, "this-notebook-covers-the-following"], [93, "this-notebook-covers-the-following"]], "Tokenizer": [[299, "tokenizer"], [304, "tokenizer"]], "Trace Structure": [[168, "trace-structure"]], "Tracing Configuration": [[168, "tracing-configuration"]], "Train Generative Cv": [[372, "train-generative-cv"]], "Train Policy Learning": [[372, "train-policy-learning"]], "Train Rec Sys": [[372, "train-rec-sys"]], "Train Tabular": [[372, "train-tabular"]], "Train Time Series": [[372, "train-time-series"]], "Train Vision Pattern": [[372, "train-vision-pattern"]], "Training": [[137, "training"], [143, null]], "Training objective": [[329, "training-objective"], [330, "training-objective"], [342, "training-objective"], [343, "training-objective"]], "Two Phases of LLM Inference": [[101, "two-phases-of-llm-inference"], [102, "two-phases-of-llm-inference"], [104, "two-phases-of-llm-inference"]], "Usage": [[0, "usage"]], "Users and Roles": [[96, "users-and-roles"], [98, "users-and-roles"]], "Using LoRA Adapters": [[118, "using-lora-adapters"], [121, "using-lora-adapters"]], "Using Structured Output": [[118, "using-structured-output"], [122, "using-structured-output"]], "Using Tool Calling": [[118, "using-tool-calling"], [123, "using-tool-calling"]], "Using fractions of a GPU": [[16, "using-fractions-of-a-gpu"]], "VSCode": [[82, "vscode"], [83, "vscode"]], "Web Application Observability (Ray Serve)": [[164, "web-application-observability-ray-serve"], [167, null]], "Welcome to Anyscale Administration": [[79, null]], "What We Accomplished": [[110, "what-we-accomplished"], [117, "what-we-accomplished"], [118, "what-we-accomplished"], [125, "what-we-accomplished"]], "What We\u2019ll Cover": [[118, "what-we-ll-cover"], [120, "what-we-ll-cover"]], "What You\u2019ll Learn": [[79, "what-you-ll-learn"]], "What does the model learn?": [[362, "what-does-the-model-learn"], [363, "what-does-the-model-learn"]], "What is LLM Serving?": [[101, "what-is-llm-serving"], [102, "what-is-llm-serving"], [104, null]], "What is Ray Data ?": [[8, "what-is-ray-data"], [191, "what-is-ray-data"], [196, "what-is-ray-data"]], "What is Ray Serve ?": [[8, "what-is-ray-serve"], [191, "what-is-ray-serve"], [197, "what-is-ray-serve"]], "What is Ray Serve?": [[309, "what-is-ray-serve"], [316, "what-is-ray-serve"], [325, "what-is-ray-serve"]], "What is Ray Serve?Ray Serve is a scalable model serving library that allows you to deploy and manage machine learning models in production.With Ray Serve, you can easily create a scalable and distributed serving architecture thatcan handle high traffic and large workloads. It is built on top of Ray, a distributed computing frameworkthat allows you to run Python code in parallel across multiple machines. Ray Serve provides a simple API for deploying and managing models, as well as features like autoscaling,load balancing, and versioning.Ray Serve is designed to be easy to use and integrate with existing machine learning workflows.It supports a wide range of machine learning frameworks, including TensorFlow, PyTorch, and Scikit-learn.Ray Serve also provides a simple way to deploy models as REST APIs, using FastAPI,making it easy to integrate with web applications and other services.More information: https://docs.ray.io/en/latest/serve/index.html": [[307, "what-is-ray-serve-ray-serve-is-a-scalable-model-serving-library-that-allows-you-to-deploy-and-manage-machine-learning-models-in-production-with-ray-serve-you-can-easily-create-a-scalable-and-distributed-serving-architecture-thatcan-handle-high-traffic-and-large-workloads-it-is-built-on-top-of-ray-a-distributed-computing-frameworkthat-allows-you-to-run-python-code-in-parallel-across-multiple-machines-ray-serve-provides-a-simple-api-for-deploying-and-managing-models-as-well-as-features-like-autoscaling-load-balancing-and-versioning-ray-serve-is-designed-to-be-easy-to-use-and-integrate-with-existing-machine-learning-workflows-it-supports-a-wide-range-of-machine-learning-frameworks-including-tensorflow-pytorch-and-scikit-learn-ray-serve-also-provides-a-simple-way-to-deploy-models-as-rest-apis-using-fastapi-making-it-easy-to-integrate-with-web-applications-and-other-services-more-information-https-docs-ray-io-en-latest-serve-index-html"], [308, "what-is-ray-serve-ray-serve-is-a-scalable-model-serving-library-that-allows-you-to-deploy-and-manage-machine-learning-models-in-production-with-ray-serve-you-can-easily-create-a-scalable-and-distributed-serving-architecture-thatcan-handle-high-traffic-and-large-workloads-it-is-built-on-top-of-ray-a-distributed-computing-frameworkthat-allows-you-to-run-python-code-in-parallel-across-multiple-machines-ray-serve-provides-a-simple-api-for-deploying-and-managing-models-as-well-as-features-like-autoscaling-load-balancing-and-versioning-ray-serve-is-designed-to-be-easy-to-use-and-integrate-with-existing-machine-learning-workflows-it-supports-a-wide-range-of-machine-learning-frameworks-including-tensorflow-pytorch-and-scikit-learn-ray-serve-also-provides-a-simple-way-to-deploy-models-as-rest-apis-using-fastapi-making-it-easy-to-integrate-with-web-applications-and-other-services-more-information-https-docs-ray-io-en-latest-serve-index-html"]], "What is an Anyscale Service?": [[110, "what-is-an-anyscale-service"], [115, "what-is-an-anyscale-service"]], "What problem are you solving? (Diffusion as image de-noising)": [[329, "what-problem-are-you-solving-diffusion-as-image-de-noising"], [330, "what-problem-are-you-solving-diffusion-as-image-de-noising"]], "What problem are you solving? (Forest cover classification with XGBoost)": [[349, "what-problem-are-you-solving-forest-cover-classification-with-xgboost"], [350, "what-problem-are-you-solving-forest-cover-classification-with-xgboost"]], "What problem are you solving? (Inverted Pendulum, Diffusion-Style)": [[336, "what-problem-are-you-solving-inverted-pendulum-diffusion-style"], [337, "what-problem-are-you-solving-inverted-pendulum-diffusion-style"]], "What problem are you solving? (NYC taxi demand forecasting with a Transformer)": [[355, "what-problem-are-you-solving-nyc-taxi-demand-forecasting-with-a-transformer"], [356, "what-problem-are-you-solving-nyc-taxi-demand-forecasting-with-a-transformer"]], "What problem are you solving? (image classification with Food-101-Lite)": [[362, "what-problem-are-you-solving-image-classification-with-food-101-lite"], [363, "what-problem-are-you-solving-image-classification-with-food-101-lite"]], "What problem are you solving? (matrix factorization for recommendations)": [[342, "what-problem-are-you-solving-matrix-factorization-for-recommendations"], [343, "what-problem-are-you-solving-matrix-factorization-for-recommendations"]], "What you learn and take away": [[329, "what-you-learn-and-take-away"], [330, "what-you-learn-and-take-away"], [336, "what-you-learn-and-take-away"], [337, "what-you-learn-and-take-away"], [342, "what-you-learn-and-take-away"], [343, "what-you-learn-and-take-away"], [349, "what-you-learn-and-take-away"], [350, "what-you-learn-and-take-away"], [355, "what-you-learn-and-take-away"], [356, "what-you-learn-and-take-away"], [362, "what-you-learn-and-take-away"], [363, "what-you-learn-and-take-away"]], "What you\u2019ll learn & take away": [[224, "what-youll-learn-take-away"], [225, "what-youll-learn-take-away"], [238, "what-youll-learn-take-away"], [239, "what-youll-learn-take-away"], [245, "what-youll-learn-take-away"], [246, "what-youll-learn-take-away"]], "What\u2019s Next": [[155, "what-s-next"], [158, "what-s-next"]], "What\u2019s XGBoost?": [[349, "what-s-xgboost"], [350, "what-s-xgboost"]], "What\u2019s a policy?": [[336, "what-s-a-policy"], [337, "what-s-a-policy"]], "What\u2019s a sequence-to-sequence Transformer?": [[355, "what-s-a-sequence-to-sequence-transformer"], [356, "what-s-a-sequence-to-sequence-transformer"]], "When to use Ray Core over Ray Data ?": [[8, "when-to-use-ray-core-over-ray-data"], [191, "when-to-use-ray-core-over-ray-data"], [196, "when-to-use-ray-core-over-ray-data"]], "When to use Ray Serve?": [[16, "when-to-use-ray-serve"]], "Where can you take this next?": [[329, "where-can-you-take-this-next"], [335, "where-can-you-take-this-next"], [336, "where-can-you-take-this-next"], [341, "where-can-you-take-this-next"], [342, "where-can-you-take-this-next"], [348, "where-can-you-take-this-next"], [349, "where-can-you-take-this-next"], [354, "where-can-you-take-this-next"], [355, "where-can-you-take-this-next"], [361, "where-can-you-take-this-next"], [362, "where-can-you-take-this-next"], [371, "where-can-you-take-this-next"]], "Why Choose Medium-Sized Models?": [[110, "why-choose-medium-sized-models"], [112, "why-choose-medium-sized-models"]], "Why Ray Data ?": [[8, "why-ray-data"], [191, "why-ray-data"], [196, "why-ray-data"]], "Why Ray Serve ?": [[8, "why-ray-serve"], [191, "why-ray-serve"], [197, "why-ray-serve"]], "Why Ray?": [[1, "why-ray"], [169, "why-ray"], [170, "why-ray"]], "Why Structured Output Matters": [[118, "why-structured-output-matters"], [122, "why-structured-output-matters"]], "Why These Features Matter": [[118, "why-these-features-matter"], [120, "why-these-features-matter"]], "Why Tool Calling Matters": [[118, "why-tool-calling-matters"], [123, "why-tool-calling-matters"]], "Why Use LoRA Adapters?": [[118, "why-use-lora-adapters"], [121, "why-use-lora-adapters"]], "Why not Kubernetes ?": [[101, "why-not-kubernetes"], [102, "why-not-kubernetes"], [106, "why-not-kubernetes"]], "Why not use just FastAPI or Flask?": [[309, "why-not-use-just-fastapi-or-flask"], [316, "why-not-use-just-fastapi-or-flask"], [325, "why-not-use-just-fastapi-or-flask"]], "Why not use just FastAPI or Flask?We could have simply used FastAPI or Flask to create a REST API for the model,but Ray Serve provides additional features like autoscaling and load balancing that make it a better choice for production deployments. Ray Serve also allows you to easilydeploy multiple models and manage their versions, which can be useful in a production environment where you may need to deploy multiple models or update existing ones.\u201d\u201d\u201d": [[307, "why-not-use-just-fastapi-or-flask-we-could-have-simply-used-fastapi-or-flask-to-create-a-rest-api-for-the-model-but-ray-serve-provides-additional-features-like-autoscaling-and-load-balancing-that-make-it-a-better-choice-for-production-deployments-ray-serve-also-allows-you-to-easilydeploy-multiple-models-and-manage-their-versions-which-can-be-useful-in-a-production-environment-where-you-may-need-to-deploy-multiple-models-or-update-existing-ones"], [308, "why-not-use-just-fastapi-or-flask-we-could-have-simply-used-fastapi-or-flask-to-create-a-rest-api-for-the-model-but-ray-serve-provides-additional-features-like-autoscaling-and-load-balancing-that-make-it-a-better-choice-for-production-deployments-ray-serve-also-allows-you-to-easilydeploy-multiple-models-and-manage-their-versions-which-can-be-useful-in-a-production-environment-where-you-may-need-to-deploy-multiple-models-or-update-existing-ones"]], "Why this works": [[329, "why-this-works"], [330, "why-this-works"]], "Workloads": [[96, "workloads"], [98, "workloads"], [372, "workloads"]], "Wrap up and next steps": [[329, "wrap-up-and-next-steps"], [335, "wrap-up-and-next-steps"], [336, "wrap-up-and-next-steps"], [341, "wrap-up-and-next-steps"], [342, "wrap-up-and-next-steps"], [348, "wrap-up-and-next-steps"], [349, "wrap-up-and-next-steps"], [354, "wrap-up-and-next-steps"], [355, "wrap-up-and-next-steps"], [361, "wrap-up-and-next-steps"], [362, "wrap-up-and-next-steps"], [371, "wrap-up-and-next-steps"]], "avoids collisons with other notebooks running ray jobs on the same machine": [[283, "avoids-collisons-with-other-notebooks-running-ray-jobs-on-the-same-machine"]], "batch_size should be set based on VRAM": [[280, "batch-size-should-be-set-based-on-vram"]], "library for pre-trained models": [[312, "library-for-pre-trained-models"]], "load a Hugging Face dataset": [[274, "load-a-hugging-face-dataset"]], "metadata after inference": [[283, "metadata-after-inference"]], "setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPS": [[280, "setting-manually-so-that-code-works-on-ray-clusters-with-both-cpu-or-gpu-workers-or-on-a-local-mac-with-mps"]], "stop ray serve": [[323, "stop-ray-serve"]], "\u25b6\ufe0f 3. Activate the Environment": [[1, "activate-the-environment"], [169, "activate-the-environment"], [170, "activate-the-environment"]], "\u2705 1. Install Conda": [[1, "install-conda"], [169, "install-conda"], [170, "install-conda"]], "\u2705 7. Verify Ray Installation with a Simple Example": [[1, "verify-ray-installation-with-a-simple-example"], [169, "verify-ray-installation-with-a-simple-example"], [170, "verify-ray-installation-with-a-simple-example"]], "\u2705 Module 01 \u00b7 Introduction to Ray Train": [[245, "module-01-introduction-to-ray-train"], [252, "module-01-introduction-to-ray-train"]], "\u2705 Module 02 \u00b7 Integrating Ray Train with Ray Data": [[245, "module-02-integrating-ray-train-with-ray-data"], [252, "module-02-integrating-ray-train-with-ray-data"]], "\u2705 Module 03 \u00b7 Fault Tolerance in Ray Train": [[245, "module-03-fault-tolerance-in-ray-train"], [252, "module-03-fault-tolerance-in-ray-train"]], "\ud83c\udf89 Wrapping Up & Next Steps": [[245, "wrapping-up-next-steps"], [252, null]], "\ud83d\udccb Notebook Compute Requirements Legend": [[1, "notebook-compute-requirements-legend"], [169, "notebook-compute-requirements-legend"], [170, "notebook-compute-requirements-legend"]], "\ud83d\udccc Overview of Structure": [[96, "overview-of-structure"], [98, null]], "\ud83d\udcda 01 \u00b7 Introduction to Ray Train": [[224, null], [225, null]], "\ud83d\udcda Next Tutorials in the Course": [[245, "next-tutorials-in-the-course"], [252, "next-tutorials-in-the-course"]], "\ud83d\udce6 4. Install UV and Dependencies": [[1, "install-uv-and-dependencies"], [169, "install-uv-and-dependencies"], [170, "install-uv-and-dependencies"]], "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data": [[238, null], [239, null]], "\ud83d\udd0e Integrating Ray Train with Ray Data": [[238, "id1"], [239, "id1"]], "\ud83d\udd0e When to use Ray Train": [[224, "when-to-use-ray-train"], [225, "when-to-use-ray-train"]], "\ud83d\udda5\ufe0f 5. (Optional but Recommended) Add Your Conda Environment to Jupyter": [[1, "optional-but-recommended-add-your-conda-environment-to-jupyter"], [169, "optional-but-recommended-add-your-conda-environment-to-jupyter"], [170, "optional-but-recommended-add-your-conda-environment-to-jupyter"]], "\ud83d\udda5\ufe0f How Distributed Data Parallel (DDP) Works": [[224, "how-distributed-data-parallel-ddp-works"], [225, "how-distributed-data-parallel-ddp-works"]], "\ud83d\ude80 6. Launch Jupyter Notebook": [[1, "launch-jupyter-notebook"], [169, "launch-jupyter-notebook"], [170, "launch-jupyter-notebook"]], "\ud83d\ude80 Where to go next": [[245, "where-to-go-next"], [252, "where-to-go-next"]], "\ud83d\udee0\ufe0f 2. Create a New Conda Environment": [[1, "create-a-new-conda-environment"], [169, "create-a-new-conda-environment"], [170, "create-a-new-conda-environment"]], "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train": [[245, null], [246, null]], "\ud83e\udde0 Summary": [[96, "summary"], [99, null]], "\ud83e\udde9 Miniforge Installation (It depends on your OS. In this case, we use ARM Macs)": [[1, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"], [169, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"], [170, "miniforge-installation-it-depends-on-your-os-in-this-case-we-use-arm-macs"]], "\ud83e\uddf9 8. Shut Down and Clean Up": [[1, "shut-down-and-clean-up"], [169, "shut-down-and-clean-up"], [170, "shut-down-and-clean-up"]]}, "docnames": ["README", "courses/deprecated/Developer_Intro_to_Ray/00_Introduction", "courses/deprecated/Developer_Intro_to_Ray/00a_Intro_Ray_Core_Basics", "courses/deprecated/Developer_Intro_to_Ray/00b_Intro_Ray_Core_Advancement", "courses/deprecated/Developer_Intro_to_Ray/01_Intro_Ray_AI_Libs_Overview", "courses/deprecated/Developer_Intro_to_Ray/02a_Intro_Ray_Train_with_PyTorch", "courses/deprecated/Developer_Intro_to_Ray/02b_Intro_Ray_Train_with_PyTorch_Lightning", "courses/deprecated/Developer_Intro_to_Ray/03_Intro_Ray_Tune", "courses/deprecated/Developer_Intro_to_Ray/04a_Intro_Ray_Data_Industry_Landscape", "courses/deprecated/Developer_Intro_to_Ray/04b_Intro_Ray_Data_Structured", "courses/deprecated/Developer_Intro_to_Ray/04c_Intro_Ray_Data_Unstructured", "courses/deprecated/Developer_Intro_to_Ray/05_Intro_Ray_Serve_PyTorch", "courses/deprecated/ray-101/1_AI_Libs_Intro", "courses/deprecated/ray-101/2_Intro_Train", "courses/deprecated/ray-101/3_Intro_Tune", "courses/deprecated/ray-101/4_Intro_Data", "courses/deprecated/ray-101/5_Intro_Serve", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/anyscale_administrator_overview", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/anyscale_vm_vs_k8s", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/deploy_to_ec2", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/deploy_to_GCE", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/deploy_to_a_new_EKS", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/deploy_to_an_existing_EKS", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/deploy_to_new_GKE", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12", "courses/foundations/Anyscale_For_Admins/README", "courses/foundations/Anyscale_Getting_Started/00_Intro_to_Workspaces/101_01_anyscale_intro_workspace", "courses/foundations/Anyscale_Getting_Started/00_Intro_to_Workspaces/output/101_01_anyscale_intro_workspace_01", "courses/foundations/Anyscale_Getting_Started/01_dev_intro/101_02_anyscale_development_intro", "courses/foundations/Anyscale_Getting_Started/01_dev_intro/output/101_02_anyscale_development_intro_01", "courses/foundations/Anyscale_Getting_Started/02_compute_runtime/101_03_anyscale_compute_runtime_intro", "courses/foundations/Anyscale_Getting_Started/02_compute_runtime/output/101_03_anyscale_compute_runtime_intro_01", "courses/foundations/Anyscale_Getting_Started/03_storage_options/101_04_anyscale_storage_options", "courses/foundations/Anyscale_Getting_Started/03_storage_options/output/101_04_anyscale_storage_options_01", "courses/foundations/Anyscale_Getting_Started/04_logging_metrics/101_05_anyscale_logging_metrics", "courses/foundations/Anyscale_Getting_Started/04_logging_metrics/output/101_05_anyscale_logging_metrics_01", "courses/foundations/Anyscale_Getting_Started/05_intro_jobs/101_06_anyscale_intro_jobs", "courses/foundations/Anyscale_Getting_Started/05_intro_jobs/output/101_06_anyscale_intro_jobs_01", "courses/foundations/Anyscale_Getting_Started/06_intro_services/101_07_anyscale_intro_services", "courses/foundations/Anyscale_Getting_Started/06_intro_services/output/101_07_anyscale_intro_services_01", "courses/foundations/Anyscale_Getting_Started/07_collaboration/101_08_anyscale_collaboration", "courses/foundations/Anyscale_Getting_Started/07_collaboration/output/101_08_anyscale_collaboration_01", "courses/foundations/Anyscale_Getting_Started/08_org_setup/101_09_anyscale_org_setup", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_01", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_02", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_03", "courses/foundations/Anyscale_Getting_Started/README", "courses/foundations/LLM_Serving/00_intro_serve_llm/README", "courses/foundations/LLM_Serving/00_intro_serve_llm/notebook", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_01", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_02", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_03", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_04", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_05", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_06", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_07", "courses/foundations/LLM_Serving/01_deploy_medium_llm/notebook", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_01", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_02", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_03", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_04", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_05", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_06", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_07", "courses/foundations/LLM_Serving/02_advanced_llm_features/notebook", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_01", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_02", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_03", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_04", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_05", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_06", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_07", "courses/foundations/Multimodal AI Workloads/00_overview/README", "courses/foundations/Multimodal AI Workloads/00_overview/output/README_01", "courses/foundations/Multimodal AI Workloads/01_batch_inference/01-Batch-Inference", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_01", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_02", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_03", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_04", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_05", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_06", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_07", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_08", "courses/foundations/Multimodal AI Workloads/02_distributed_training/02-Distributed-Training", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_01", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_02", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_03", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_04", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_05", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_06", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_07", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_08", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_09", "courses/foundations/Multimodal AI Workloads/03_online_serving/03-Online-Serving", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_01", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_02", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_03", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_04", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_05", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_06", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_07", "courses/foundations/Observability/01_Intro_and_setup/01_general_intro_and_setup", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_01", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_02", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_03", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/2_Ray_Anyscale_Observability_Overview", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/03_Ray_Anyscale_Observability_in_Details", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/tracing_example/README", "courses/foundations/Ray_AI_Libs/00_intro/README", "courses/foundations/Ray_AI_Libs/00_intro/output/README_01", "courses/foundations/Ray_AI_Libs/01_Overview/01_Intro_Ray_AI_Libs_Overview", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_01", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_02", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_03", "courses/foundations/Ray_Core/00_Basics/00_Intro_Ray_Core_Basics", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_01", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_02", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_03", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_04", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_05", "courses/foundations/Ray_Core/01_Advanced/00a_Intro_Ray_Core_Advancement", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_01", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_02", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_03", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_04", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_05", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_06", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_07", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_08", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_09", "courses/foundations/Ray_Data/00_Landscape/04a_Intro_Ray_Data_Industry_Landscape", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_01", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_02", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_03", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_04", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_05", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_06", "courses/foundations/Ray_Data/01_Structured/04b_Intro_Ray_Data_Structured", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_01", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_02", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_03", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_04", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_05", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_06", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_07", "courses/foundations/Ray_Data/02_Unstructured/04c_Intro_Ray_Data_Unstructured", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_01", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_02", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_03", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_04", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_05", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_06", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_07", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_08", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_09", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_10", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_11", "courses/foundations/Ray_Serve/00_Serve/05_Intro_Ray_Serve_PyTorch", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_01", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_02", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_03", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_04", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_05", "courses/foundations/Ray_Train/01_intro/01_intro_to_ray_train", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_01", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_02", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_03", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_04", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_05", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_06", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_07", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_08", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_09", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_10", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_11", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_12", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_13", "courses/foundations/Ray_Train/02_train_and_data/02_integrating_ray_train_with_ray_data", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_01", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_02", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_03", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_04", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_05", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_06", "courses/foundations/Ray_Train/03_fault_tolerance/03_fault_tolerance_in_ray_train", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_01", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_02", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_03", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_04", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_05", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_06", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_07", "courses/foundations/Ray_Tune/00_Tune/03_Intro_Ray_Tune", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_01", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_02", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_03", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_04", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_05", "courses/workloads/PyTorch_Lightning/00_workload/02b_Intro_Ray_Train_with_PyTorch_Lightning", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_Ray_Data_batch_inference", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/lesson", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/output/lesson_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/slides", "courses/workloads/Ray_Data_Batch_Inference/00_workload/README", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_01", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_02", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_03", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_04", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_05", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_06", "courses/workloads/Ray_Data_Processing/00_workload/02_Ray_Data_data_processing", "courses/workloads/Ray_Data_Processing/00_workload/README", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_01", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_02", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_03", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_04", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_05", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_06", "courses/workloads/Ray_Distributed_Training/00_workload/04_Ray_Train_distributed_training", "courses/workloads/Ray_Distributed_Training/00_workload/README", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_01", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_02", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_03", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_04", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_05", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_06", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/output/lesson_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/output/lesson_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/output/lesson_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_Ray_Serve_online_serving", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/output/lesson_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/output/lesson_02", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/lesson", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/output/lesson_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/slides", "courses/workloads/Ray_Serve_Online_Serving/00_workload/README", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_01", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_02", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_03", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_04", "courses/workloads/Train_Generative_CV/00_workload/04d1_generative_cv_pattern", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_01", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_02", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_03", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_04", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_05", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_06", "courses/workloads/Train_Policy_Learning/00_workload/04d2_policy_learning_pattern", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_01", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_02", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_03", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_04", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_05", "courses/workloads/Train_Rec_sys/00_workload/04e_rec_sys_workload_pattern", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_01", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_02", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_03", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_04", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_05", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_06", "courses/workloads/Train_Tabular/00_workload/04b_tabular_workload_pattern", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_01", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_02", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_03", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_04", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_05", "courses/workloads/Train_Time_Series/00_workload/04c_time_series_workload_pattern", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_01", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_02", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_03", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_04", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_05", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_06", "courses/workloads/Train_Vision_Pattern/00_workload/04a_vision_pattern", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_01", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_02", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_03", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_04", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_05", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_06", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_07", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_08", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_09", "index"], "envversion": {"sphinx": 62, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["README.md", "courses/deprecated/Developer_Intro_to_Ray/00_Introduction.ipynb", "courses/deprecated/Developer_Intro_to_Ray/00a_Intro_Ray_Core_Basics.ipynb", "courses/deprecated/Developer_Intro_to_Ray/00b_Intro_Ray_Core_Advancement.ipynb", "courses/deprecated/Developer_Intro_to_Ray/01_Intro_Ray_AI_Libs_Overview.ipynb", "courses/deprecated/Developer_Intro_to_Ray/02a_Intro_Ray_Train_with_PyTorch.ipynb", "courses/deprecated/Developer_Intro_to_Ray/02b_Intro_Ray_Train_with_PyTorch_Lightning.ipynb", "courses/deprecated/Developer_Intro_to_Ray/03_Intro_Ray_Tune.ipynb", "courses/deprecated/Developer_Intro_to_Ray/04a_Intro_Ray_Data_Industry_Landscape.ipynb", "courses/deprecated/Developer_Intro_to_Ray/04b_Intro_Ray_Data_Structured.ipynb", "courses/deprecated/Developer_Intro_to_Ray/04c_Intro_Ray_Data_Unstructured.ipynb", "courses/deprecated/Developer_Intro_to_Ray/05_Intro_Ray_Serve_PyTorch.ipynb", "courses/deprecated/ray-101/1_AI_Libs_Intro.ipynb", "courses/deprecated/ray-101/2_Intro_Train.ipynb", "courses/deprecated/ray-101/3_Intro_Tune.ipynb", "courses/deprecated/ray-101/4_Intro_Data.ipynb", "courses/deprecated/ray-101/5_Intro_Serve.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/anyscale_administrator_overview.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_00_Anyscale_Admin_Overview/output/anyscale_administrator_overview_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/anyscale_vm_vs_k8s.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_01_VM_vs_K8s/output/anyscale_vm_vs_k8s_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/deploy_to_ec2.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_02a_Deploy_to_VM_AWS_EC2/output/deploy_to_ec2_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/deploy_to_GCE.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_02b_Deploy_to_VM_GCP_GCE/output/deploy_to_GCE_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/deploy_to_a_new_EKS.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_08.ipynb", "courses/foundations/Anyscale_For_Admins/02_03a_Deploy_to_K8s_New_AWS_EKS/output/deploy_to_a_new_EKS_09.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/deploy_to_an_existing_EKS.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_08.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_09.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_10.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_11.ipynb", "courses/foundations/Anyscale_For_Admins/02_03b_Deploy_to_K8s_Existing_AWS_EKS/output/deploy_to_an_existing_EKS_12.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/deploy_to_new_GKE.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_01.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_02.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_03.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_04.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_05.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_06.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_07.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_08.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_09.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_10.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_11.ipynb", "courses/foundations/Anyscale_For_Admins/02_03c_Deploy_to_K8s_New_GCP_GKE/output/deploy_to_new_GKE_12.ipynb", "courses/foundations/Anyscale_For_Admins/README.md", "courses/foundations/Anyscale_Getting_Started/00_Intro_to_Workspaces/101_01_anyscale_intro_workspace.ipynb", "courses/foundations/Anyscale_Getting_Started/00_Intro_to_Workspaces/output/101_01_anyscale_intro_workspace_01.ipynb", "courses/foundations/Anyscale_Getting_Started/01_dev_intro/101_02_anyscale_development_intro.ipynb", "courses/foundations/Anyscale_Getting_Started/01_dev_intro/output/101_02_anyscale_development_intro_01.ipynb", "courses/foundations/Anyscale_Getting_Started/02_compute_runtime/101_03_anyscale_compute_runtime_intro.ipynb", "courses/foundations/Anyscale_Getting_Started/02_compute_runtime/output/101_03_anyscale_compute_runtime_intro_01.ipynb", "courses/foundations/Anyscale_Getting_Started/03_storage_options/101_04_anyscale_storage_options.ipynb", "courses/foundations/Anyscale_Getting_Started/03_storage_options/output/101_04_anyscale_storage_options_01.ipynb", "courses/foundations/Anyscale_Getting_Started/04_logging_metrics/101_05_anyscale_logging_metrics.ipynb", "courses/foundations/Anyscale_Getting_Started/04_logging_metrics/output/101_05_anyscale_logging_metrics_01.ipynb", "courses/foundations/Anyscale_Getting_Started/05_intro_jobs/101_06_anyscale_intro_jobs.ipynb", "courses/foundations/Anyscale_Getting_Started/05_intro_jobs/output/101_06_anyscale_intro_jobs_01.ipynb", "courses/foundations/Anyscale_Getting_Started/06_intro_services/101_07_anyscale_intro_services.ipynb", "courses/foundations/Anyscale_Getting_Started/06_intro_services/output/101_07_anyscale_intro_services_01.ipynb", "courses/foundations/Anyscale_Getting_Started/07_collaboration/101_08_anyscale_collaboration.ipynb", "courses/foundations/Anyscale_Getting_Started/07_collaboration/output/101_08_anyscale_collaboration_01.ipynb", "courses/foundations/Anyscale_Getting_Started/08_org_setup/101_09_anyscale_org_setup.ipynb", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_01.ipynb", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_02.ipynb", "courses/foundations/Anyscale_Getting_Started/08_org_setup/output/101_09_anyscale_org_setup_03.ipynb", "courses/foundations/Anyscale_Getting_Started/README.md", "courses/foundations/LLM_Serving/00_intro_serve_llm/README.md", "courses/foundations/LLM_Serving/00_intro_serve_llm/notebook.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_01.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_02.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_03.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_04.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_05.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_06.ipynb", "courses/foundations/LLM_Serving/00_intro_serve_llm/output/notebook_07.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/notebook.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_01.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_02.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_03.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_04.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_05.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_06.ipynb", "courses/foundations/LLM_Serving/01_deploy_medium_llm/output/notebook_07.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/notebook.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_01.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_02.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_03.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_04.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_05.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_06.ipynb", "courses/foundations/LLM_Serving/02_advanced_llm_features/output/notebook_07.ipynb", "courses/foundations/Multimodal AI Workloads/00_overview/README.ipynb", "courses/foundations/Multimodal AI Workloads/00_overview/output/README_01.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/01-Batch-Inference.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_01.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_02.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_03.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_04.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_05.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_06.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_07.ipynb", "courses/foundations/Multimodal AI Workloads/01_batch_inference/output/01-Batch-Inference_08.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/02-Distributed-Training.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_01.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_02.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_03.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_04.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_05.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_06.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_07.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_08.ipynb", "courses/foundations/Multimodal AI Workloads/02_distributed_training/output/02-Distributed-Training_09.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/03-Online-Serving.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_01.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_02.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_03.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_04.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_05.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_06.ipynb", "courses/foundations/Multimodal AI Workloads/03_online_serving/output/03-Online-Serving_07.ipynb", "courses/foundations/Observability/01_Intro_and_setup/01_general_intro_and_setup.ipynb", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_01.ipynb", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_02.ipynb", "courses/foundations/Observability/01_Intro_and_setup/output/01_general_intro_and_setup_03.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/2_Ray_Anyscale_Observability_Overview.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_01.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_02.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_03.ipynb", "courses/foundations/Observability/02_Ray_Anyscale_Introduction/output/2_Ray_Anyscale_Observability_Overview_04.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/03_Ray_Anyscale_Observability_in_Details.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_01.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_02.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/output/03_Ray_Anyscale_Observability_in_Details_03.ipynb", "courses/foundations/Observability/03_Ray_Anyscale_Observability_in_Detail/tracing_example/README.md", "courses/foundations/Ray_AI_Libs/00_intro/README.ipynb", "courses/foundations/Ray_AI_Libs/00_intro/output/README_01.ipynb", "courses/foundations/Ray_AI_Libs/01_Overview/01_Intro_Ray_AI_Libs_Overview.ipynb", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_01.ipynb", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_02.ipynb", "courses/foundations/Ray_AI_Libs/01_Overview/output/01_Intro_Ray_AI_Libs_Overview_03.ipynb", "courses/foundations/Ray_Core/00_Basics/00_Intro_Ray_Core_Basics.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_01.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_02.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_03.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_04.ipynb", "courses/foundations/Ray_Core/00_Basics/output/00_Intro_Ray_Core_Basics_05.ipynb", "courses/foundations/Ray_Core/01_Advanced/00a_Intro_Ray_Core_Advancement.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_01.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_02.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_03.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_04.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_05.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_06.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_07.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_08.ipynb", "courses/foundations/Ray_Core/01_Advanced/output/00a_Intro_Ray_Core_Advancement_09.ipynb", "courses/foundations/Ray_Data/00_Landscape/04a_Intro_Ray_Data_Industry_Landscape.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_01.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_02.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_03.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_04.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_05.ipynb", "courses/foundations/Ray_Data/00_Landscape/output/04a_Intro_Ray_Data_Industry_Landscape_06.ipynb", "courses/foundations/Ray_Data/01_Structured/04b_Intro_Ray_Data_Structured.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_01.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_02.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_03.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_04.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_05.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_06.ipynb", "courses/foundations/Ray_Data/01_Structured/output/04b_Intro_Ray_Data_Structured_07.ipynb", "courses/foundations/Ray_Data/02_Unstructured/04c_Intro_Ray_Data_Unstructured.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_01.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_02.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_03.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_04.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_05.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_06.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_07.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_08.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_09.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_10.ipynb", "courses/foundations/Ray_Data/02_Unstructured/output/04c_Intro_Ray_Data_Unstructured_11.ipynb", "courses/foundations/Ray_Serve/00_Serve/05_Intro_Ray_Serve_PyTorch.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_01.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_02.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_03.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_04.ipynb", "courses/foundations/Ray_Serve/00_Serve/output/05_Intro_Ray_Serve_PyTorch_05.ipynb", "courses/foundations/Ray_Train/01_intro/01_intro_to_ray_train.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_01.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_02.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_03.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_04.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_05.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_06.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_07.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_08.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_09.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_10.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_11.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_12.ipynb", "courses/foundations/Ray_Train/01_intro/output/01_intro_to_ray_train_13.ipynb", "courses/foundations/Ray_Train/02_train_and_data/02_integrating_ray_train_with_ray_data.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_01.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_02.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_03.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_04.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_05.ipynb", "courses/foundations/Ray_Train/02_train_and_data/output/02_integrating_ray_train_with_ray_data_06.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/03_fault_tolerance_in_ray_train.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_01.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_02.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_03.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_04.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_05.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_06.ipynb", "courses/foundations/Ray_Train/03_fault_tolerance/output/03_fault_tolerance_in_ray_train_07.ipynb", "courses/foundations/Ray_Tune/00_Tune/03_Intro_Ray_Tune.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_01.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_02.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_03.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_04.ipynb", "courses/foundations/Ray_Tune/00_Tune/output/03_Intro_Ray_Tune_05.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/02b_Intro_Ray_Train_with_PyTorch_Lightning.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_01.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_02.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_03.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_04.ipynb", "courses/workloads/PyTorch_Lightning/00_workload/output/02b_Intro_Ray_Train_with_PyTorch_Lightning_05.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/00_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_Ray_Data_batch_inference.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/01_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/02_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/03_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/04_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/lesson.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/05_lesson/slides.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/README.md", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_01.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_02.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_03.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_04.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_05.ipynb", "courses/workloads/Ray_Data_Batch_Inference/00_workload/output/01_Ray_Data_batch_inference_06.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/02_Ray_Data_data_processing.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/README.md", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_01.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_02.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_03.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_04.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_05.ipynb", "courses/workloads/Ray_Data_Processing/00_workload/output/02_Ray_Data_data_processing_06.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/04_Ray_Train_distributed_training.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/README.md", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_01.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_02.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_03.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_04.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_05.ipynb", "courses/workloads/Ray_Distributed_Training/00_workload/output/04_Ray_Train_distributed_training_06.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/00_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/01_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/02_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_Ray_Serve_online_serving.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/output/lesson_02.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/03_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/lesson.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/output/lesson_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/04_lesson/slides.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/README.md", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_01.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_02.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_03.ipynb", "courses/workloads/Ray_Serve_Online_Serving/00_workload/output/03_Ray_Serve_online_serving_04.ipynb", "courses/workloads/Train_Generative_CV/00_workload/04d1_generative_cv_pattern.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_01.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_02.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_03.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_04.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_05.ipynb", "courses/workloads/Train_Generative_CV/00_workload/output/04d1_generative_cv_pattern_06.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/04d2_policy_learning_pattern.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_01.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_02.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_03.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_04.ipynb", "courses/workloads/Train_Policy_Learning/00_workload/output/04d2_policy_learning_pattern_05.ipynb", "courses/workloads/Train_Rec_sys/00_workload/04e_rec_sys_workload_pattern.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_01.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_02.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_03.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_04.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_05.ipynb", "courses/workloads/Train_Rec_sys/00_workload/output/04e_rec_sys_workload_pattern_06.ipynb", "courses/workloads/Train_Tabular/00_workload/04b_tabular_workload_pattern.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_01.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_02.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_03.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_04.ipynb", "courses/workloads/Train_Tabular/00_workload/output/04b_tabular_workload_pattern_05.ipynb", "courses/workloads/Train_Time_Series/00_workload/04c_time_series_workload_pattern.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_01.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_02.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_03.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_04.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_05.ipynb", "courses/workloads/Train_Time_Series/00_workload/output/04c_time_series_workload_pattern_06.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/04a_vision_pattern.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_01.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_02.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_03.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_04.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_05.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_06.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_07.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_08.ipynb", "courses/workloads/Train_Vision_Pattern/00_workload/output/04a_vision_pattern_09.ipynb", "index.md"], "indexentries": {}, "objects": {}, "objnames": {}, "objtypes": {}, "terms": {"": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 28, 30, 31, 34, 35, 39, 40, 43, 45, 46, 47, 56, 58, 59, 66, 75, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 100, 101, 102, 104, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 136, 137, 138, 139, 140, 143, 146, 147, 148, 150, 151, 152, 153, 154, 159, 162, 163, 164, 166, 167, 169, 170, 171, 172, 174, 175, 180, 181, 183, 184, 185, 187, 189, 190, 191, 192, 196, 197, 198, 201, 202, 203, 204, 206, 208, 210, 211, 213, 215, 218, 220, 221, 222, 224, 225, 226, 227, 228, 232, 233, 234, 237, 238, 239, 241, 245, 252, 253, 255, 256, 257, 259, 261, 262, 263, 267, 268, 280, 283, 285, 287, 289, 290, 291, 294, 297, 298, 299, 301, 303, 304, 305, 329, 330, 331, 333, 335, 340, 341, 342, 343, 344, 345, 346, 347, 348, 351, 352, 353, 354, 357, 359, 361, 362, 363, 364, 365, 366, 367, 369, 371], "0": [1, 3, 4, 5, 6, 7, 10, 12, 13, 14, 15, 16, 22, 28, 29, 30, 35, 37, 40, 43, 44, 45, 46, 47, 53, 55, 56, 58, 59, 66, 68, 75, 80, 81, 84, 85, 88, 89, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 128, 130, 133, 136, 137, 139, 140, 141, 143, 144, 146, 147, 149, 150, 155, 158, 159, 163, 168, 169, 170, 171, 174, 181, 185, 187, 189, 190, 199, 203, 206, 211, 212, 213, 225, 226, 227, 228, 232, 233, 235, 237, 238, 242, 243, 245, 247, 248, 252, 253, 255, 256, 257, 259, 262, 263, 268, 287, 289, 291, 294, 295, 298, 299, 304, 306, 316, 320, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 340, 341, 342, 344, 346, 348, 349, 350, 351, 352, 355, 357, 358, 359, 361, 362, 363, 364, 365, 367, 371], "00": [12, 13, 14, 15, 86, 87, 137, 144, 146, 168, 268, 289, 299, 306, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364, 372], "000": [7, 14, 224, 226, 253, 255, 336, 338, 342, 344, 349, 351], "0000000000000": [28, 30], "00000000000000000": [28, 30], "0001": [6, 10, 206, 213, 259, 263], "00026041e": [137, 146], "0003573892": 14, "0003590581": 14, "0003788471": 14, "0003824231": 14, "0004189012": 14, "00043082e": [137, 146], "00046369e": [268, 290], "00054121e": [137, 146], "00087994e": [268, 290], "001": [137, 144], "00129196e": [268, 290], "00172612e": [137, 146], "00217544e": [268, 290], "00348413e": [137, 146], "00403815e": [268, 290], "00439209e": [137, 146], "0059": 12, "00592375e": [137, 146], "00596860e": [268, 290], "00612268e": [137, 146], "00641076e": [268, 290], "006742": 13, "00719017e": [268, 290], "00724374e": [268, 290], "00728178e": [268, 290], "00749106": [268, 289], "00753223": [268, 289], "00785351e": [137, 146], "007877049646500664": 14, "00787705": 14, "00813907e": [137, 146], "00816345e": [137, 146], "00844238": [268, 289], "00926834e": [268, 290], "0092816": [268, 289], "00958297e": [268, 290], "00974117e": [268, 290], "00982723e": [268, 290], "00994138e": [268, 290], "00_developer_intro_to_rai": [284, 292, 300, 324], "00_overview": 372, "00a": 372, "00z": 168, "01": [4, 6, 13, 126, 127, 137, 146, 171, 174, 259, 263, 268, 290, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364, 372], "01000227e": [268, 290], "01085338e": [137, 146], "01103884e": [268, 290], "01104600e": [268, 290], "01141734e": [268, 290], "01148352e": [268, 290], "01190887": [268, 289], "01222771": [268, 289], "01231135": [268, 289], "01273207e": [268, 290], "01295078e": [137, 146], "01347007e": [268, 290], "01351717e": [268, 290], "01387227e": [268, 290], "01402104e": [268, 290], "01455652": [268, 289], "01504247": [268, 289], "01505721e": [268, 290], "01544438e": [268, 290], "01616676e": [268, 290], "01646197e": [268, 290], "01848297e": [268, 290], "01875377e": [268, 290], "01878762e": [137, 146], "01880312e": [137, 146], "01893711e": [137, 146], "01946776e": [268, 290], "01948935e": [137, 146], "01951000e": [268, 290], "01958193e": [268, 290], "01_batch_infer": 372, "02": [4, 126, 127, 137, 146, 171, 174, 268, 290, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364, 372], "02095584": [268, 289], "02193573e": [137, 146], "02202111": [268, 289], "02216589e": [137, 146], "02220649e": [137, 146], "02316421e": [268, 290], "02334268e": [137, 146], "02338964e": [268, 290], "02352677e": [268, 290], "02359867e": [137, 146], "02369718e": [137, 146], "0242": 12, "02428794e": [137, 146], "02448604e": [268, 290], "02480531e": [268, 290], "02481507e": [268, 290], "02496293": [268, 289], "02508835e": [268, 290], "02556132": [268, 289], "02750473e": [268, 290], "02791084": [268, 289], "02834240e": [137, 146], "02842702e": [268, 290], "0299": [137, 141], "02993000e": [137, 146], "02_distributed_train": 372, "02_service_hello_world": [92, 93], "02b": 372, "02d": [342, 344], "03": [4, 12, 14, 126, 127, 137, 146, 171, 174, 268, 290, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364, 372], "03000000000000000": [28, 30], "03162946e": [268, 290], "03174406e": [137, 146], "03241184e": [268, 290], "03302041": [268, 289], "03319024e": [268, 290], "03347346e": [268, 290], "03351809e": [268, 290], "03357503": [268, 289], "03508779e": [137, 146], "03519934e": [137, 146], "0362": [137, 141], "03620186e": [268, 290], "03722222e": [268, 290], "0385": [137, 141], "03865302e": [137, 146], "03901269e": [268, 290], "03915609e": [268, 290], "03924675": [268, 289], "03927016e": [137, 146], "03974594e": [268, 290], "03_online_serv": 372, "03d": [329, 333, 336, 340], "04": [137, 146, 268, 290, 331, 339, 342, 344, 349, 351, 355, 357, 362, 364, 372], "04023737e": [268, 290], "04127836e": [268, 290], "04148921": [268, 289], "04188204e": [268, 290], "04267104e": [268, 290], "04279362e": [268, 290], "04313433e": [268, 290], "04401015e": [268, 290], "04419766e": [268, 290], "04514116e": [268, 290], "04530790e": [137, 146], "04760937e": [268, 290], "04781413e": [268, 290], "04831458": [268, 289], "04853529e": [137, 146], "04871886e": [268, 290], "04a": 372, "04b": 372, "04c": 372, "04d1": 372, "04d2": 372, "04e": 372, "05": [6, 9, 12, 13, 137, 140, 164, 166, 198, 201, 204, 259, 262, 299, 306, 329, 331, 336, 340, 342, 344, 349, 351, 355, 357, 362, 364, 372], "050227": [268, 289], "05029851e": [268, 290], "05031021e": [268, 290], "05048873e": [268, 290], "05110919e": [268, 290], "05117615e": [268, 290], "05286286": [268, 289], "05403318e": [268, 290], "05412337e": [268, 290], "0564279": 14, "05699580e": [268, 290], "0582891": 14, "05838008e": [268, 290], "05897461e": [268, 290], "05951829e": [268, 290], "05955295e": [268, 290], "05959303e": [137, 146], "05962829": [268, 289], "05964907e": [268, 290], "05_069833_2422": 13, "06": [1, 13, 86, 87, 169, 170, 329, 331, 336, 340, 342, 344, 355, 357, 362, 365, 372], "06012297e": [268, 290], "06039613e": [268, 290], "06096685": [268, 289], "06099542": [268, 289], "06128380e": [268, 290], "06155156e": [268, 290], "06164196e": [268, 290], "06194988": [268, 289], "06202352e": [268, 290], "06234232e": [268, 290], "06241195e": [268, 290], "06251295": [268, 289], "06282867e": [268, 290], "06317782e": [268, 290], "06359579e": [268, 290], "06367093324661255": 13, "063671": 13, "06383444e": [268, 290], "06388348e": [137, 146], "06465332e": [268, 290], "06585157e": [137, 146], "06659506e": [268, 290], "06678507e": [268, 290], "06857569e": [268, 290], "06872221": [268, 289], "06887527e": [268, 290], "07": [118, 123, 299, 306, 329, 331, 336, 340, 342, 345, 349, 351, 355, 357, 362, 365, 372], "07005756e": [268, 290], "07007243e": [137, 146], "07039157e": [268, 290], "07176238": [268, 289], "07179873e": [137, 146], "07271361e": [137, 146], "07316985e": [268, 290], "07334603e": [268, 290], "07348490e": [268, 290], "07383470982313156": [147, 150], "07393348e": [137, 146], "07420641e": [268, 290], "07442781e": [137, 146], "07510021": [268, 289], "07512747e": [268, 290], "07565679e": [268, 290], "07582638e": [268, 290], "07590961e": [268, 290], "07614997e": [268, 290], "07706316e": [137, 146], "07735191e": [268, 290], "07769895e": [268, 290], "07796153e": [268, 290], "07813133e": [268, 290], "07829855": [268, 289], "07924390e": [137, 146], "07964912e": [137, 146], "08": [110, 116, 137, 144, 168, 268, 290, 329, 332, 336, 341, 342, 346, 355, 357, 362, 365, 372], "08080895": [268, 289], "08113792": [268, 289], "08117312e": [268, 290], "08142687": [268, 289], "08161136e": [268, 290], "08262652e": [137, 146], "08306534e": [268, 290], "08318681e": [268, 290], "08351701e": [137, 146], "08386130e": [268, 290], "08393911e": [268, 290], "08408488e": [137, 146], "08411038e": [137, 146], "08423311e": [268, 290], "08562492e": [268, 290], "08593434e": [268, 290], "08646134e": [137, 146], "08834168e": [268, 290], "08835796e": [137, 146], "08847059e": [268, 290], "08849846e": [268, 290], "08855490e": [268, 290], "08866049e": [268, 290], "08888834e": [268, 290], "08900222e": [268, 290], "08926150e": [268, 290], "08956832e": [137, 146], "08967713e": [268, 290], "09": [13, 14, 329, 333, 336, 341, 342, 346, 349, 352, 355, 358, 362, 365, 372], "09058516e": [268, 290], "09124994e": [137, 146], "09158831e": [268, 290], "09305708e": [268, 290], "09318195e": [268, 290], "09376505e": [268, 290], "09415853e": [137, 146], "09640113e": [268, 290], "09668531e": [268, 290], "09694359e": [268, 290], "09729558e": [268, 290], "09752440e": [137, 146], "09788750e": [268, 290], "09830695e": [137, 146], "09841380e": [268, 290], "09894407e": [137, 146], "09954223e": [268, 290], "09_200164_18044": [299, 306], "0a000000000000000": [28, 30], "0aa72cef9b8921af5": [128, 136, 137, 146], "0b5c2c9a5a27cfba2": [128, 133], "0bd7bde3f2c914b3": [28, 30], "0e941ed71ef3480e": [128, 136], "0f8bb12ddf9a451e9": [28, 30, 43, 45, 53, 56], "0ffe5abae6e899f5a": [128, 136, 137, 146], "0m": [13, 14, 16, 128, 129, 133, 136, 137, 138, 139, 143, 146, 147, 148, 150, 299, 306], "0x72c6d85fc9d0": 16, "0x72c6d85fcf90": 16, "0x72c6d85fd3d0": 16, "0x72c6d85fd550": 16, "0x72c6d85fe590": 16, "0x72c6d85ff250": 16, "0x72c6d85ff3d0": 16, "0x72c6d8608750": 16, "0x72c6d8609050": 16, "0x72c6d860b6d0": 16, "0x72c6d860b7d0": 16, "0x72c6d8610490": 16, "0x72c6d8610a50": 16, "0x72c6d8611310": 16, "0x72c6d8611ad0": 16, "0x72c6d8611b90": 16, "0x72c6d8612050": 16, "0x72c6d8613690": 16, "0x72c6d8620a90": 16, "0x72c6d8620e10": 16, "0x72c6d86218d0": 16, "0x72c6d8621b90": 16, "0x72c6d8622ad0": 16, "0x72c6d8623590": 16, "0x72c6d86281d0": 16, "0x72c6d8628710": 16, "0x72c6d862a1d0": 16, "0x72c6d862ac50": 16, "0x72c6d862b790": 16, "0x72c6d862b7d0": 16, "0x72c6d862c690": 16, "0x72c6d872db50": 16, "0x72c6d8747390": 16, "0x72c6d87500d0": 16, "0x72c6d8752290": 16, "0x72c6d8752e50": 16, "0x72c6d8757dd0": 16, "0x72c6d87793d0": 16, "0x72c6d8779b90": 16, "0x72c6d877a010": 16, "0x72c6d877a6d0": 16, "0x72c6d877b010": 16, "0x72c6d877bc10": 16, "0x72c6d8785e50": 16, "0x72c6d8785fd0": 16, "0x72c6d8786a50": 16, "0x72c6d8787c90": 16, "0x72c6d8794350": 16, "0x72c6d8795110": 16, "0x72c6d8796b50": 16, "0x72c6d8797150": 16, "0x72c6d8797ed0": 16, "0x72c6d87a0690": 16, "0x72c6d87a14d0": 16, "0x72c6d87a1b50": 16, "0x72c6d87a1f50": 16, "0x72c6d87a2c10": 16, "0x72c6d87a3d50": 16, "0x72c6d87a3ed0": 16, "0x72c6d87a8690": 16, "0x72c6d87a96d0": 16, "0x72c6d87a9cd0": 16, "0x72c6d87aa0d0": 16, "0x72c6d87aa4d0": 16, "0x72c6d87aad50": 16, "0x72c6d87ab690": 16, "0x72c6d87ac4d0": 16, "0x72c6d87adb90": 16, "0x72c6d87ae4d0": 16, "0x72c6d87ae710": 16, "0x72c6d87c0110": 16, "0x72c6d87c1110": 16, "0x72c6d87c1250": 16, "0x72c6d87c18d0": 16, "0x72c6d87c2350": 16, "0x72c6d87c3a50": 16, "0x72c6d87d0690": 16, "0x72c6d87d0d90": 16, "0x72c6d87d1c50": 16, "0x72c6d87d1cd0": 16, "0x72c6d87d3190": 16, "0x72c6d87d3fd0": 16, "0x72c6d87d8250": 16, "0x72c6d87d8e90": 16, "0x72c6d87d96d0": 16, "0x72c6d87da1d0": 16, "0x72c6d87e4810": 16, "0x72c6d87e4f90": 16, "0x72c6d87e62d0": 16, "0x72c6d87e64d0": 16, "0x72c6e01cbd50": 16, "0x72c6e034a510": 16, "0x72c6e034b950": 16, "0x72c6e0351e10": 16, "0x72c6e0353410": 16, "0x72c6e035ca50": 16, "0x72c6e035d5d0": 16, "0x72c6e03660d0": 16, "0x72c72000ddd0": 16, "0x72c73032a850": 16, "0xxxxxxxx": [28, 30], "0xxxxxxxxx": [28, 30], "0xxxxxxxxxx": [28, 30], "1": [29, 37, 42, 44, 50, 55, 62, 68, 73, 78, 88, 89, 113, 114, 115, 117, 121, 122, 123, 128, 130, 131, 133, 136, 137, 138, 139, 140, 143, 144, 146, 147, 149, 150, 153, 159, 163, 164, 165, 167, 179, 184, 188, 199, 219, 222, 224, 225, 226, 227, 230, 231, 235, 236, 237, 238, 243, 244, 245, 247, 248, 250, 254, 256, 257, 268, 280, 287, 289, 290, 291, 294, 296, 297, 298, 303, 305, 306, 330, 332, 333, 335, 339, 340, 341, 343, 345, 346, 348, 350, 352, 353, 354, 356, 358, 359, 361, 363, 365, 367, 368, 371], "10": [1, 3, 4, 7, 10, 11, 12, 13, 14, 15, 16, 17, 22, 35, 39, 43, 44, 45, 55, 56, 66, 68, 70, 86, 87, 88, 89, 128, 133, 136, 137, 146, 159, 163, 164, 166, 168, 169, 170, 171, 174, 181, 183, 185, 189, 190, 206, 208, 212, 218, 222, 227, 253, 255, 257, 268, 272, 273, 274, 287, 289, 291, 297, 298, 299, 306, 330, 334, 335, 338, 340, 344, 348, 350, 351, 354, 357, 361, 363, 372], "100": [3, 7, 10, 14, 15, 16, 88, 89, 118, 121, 168, 181, 190, 206, 212, 213, 224, 231, 253, 257, 291, 295, 299, 304, 336, 341, 342, 344], "1000": [9, 10, 15, 88, 89, 159, 163, 198, 201, 206, 210, 329, 332, 336, 338, 339], "10000": [355, 358], "100000000000": [6, 259, 262], "100k": [343, 348], "100th": [291, 295], "101": [17, 22, 330, 335, 367, 371, 372], "1010": [291, 298], "10127169e": [137, 146], "10129036e": [268, 290], "10140711e": [137, 146], "10182568e": [137, 146], "101_01_anyscale_intro_workspac": 100, "101_02_anyscale_development_intro": 100, "101_03_anycale_compute_runtime_intro": 100, "101_04_anyscale_storage_opt": 100, "101_05_anyscale_logging_and_metr": 100, "101_06_anyscale_intro_job": 100, "101_07_anyscale_intro_servic": 100, "101_08_anyscale_collaboration_intro": 100, "101_09_anyscale_org_setup": 100, "102": [17, 22], "1024": [3, 6, 9, 10, 35, 39, 181, 183, 198, 201, 202, 206, 212, 259, 262, 355, 358], "10279503e": [268, 290], "10307": 14, "104": [128, 133, 136, 137, 139], "10526211e": [268, 290], "10536157": [268, 289], "10537948e": [268, 290], "105m": [164, 167, 168], "10628068e": [137, 146], "10776436e": [268, 290], "10807291e": [268, 290], "10863163e": [268, 290], "10879738e": [268, 290], "108934": 13, "10893423855304718": 13, "10937572e": [137, 146], "10954670e": [268, 290], "10956261e": [268, 290], "10_000": [6, 259, 262, 263, 336, 338], "10am": [268, 289], "10m": [342, 348], "11": [1, 12, 13, 14, 137, 141, 144, 169, 170, 268, 289, 299, 306, 372], "11016287e": [268, 290], "11058047e": [268, 290], "11085677e": [268, 290], "110m": [164, 167, 168], "112": [128, 136, 137, 143, 146], "11493243e": [268, 290], "11518174e": [137, 146], "11712754e": [268, 290], "11721872": [268, 289], "11745796e": [268, 290], "11773282e": [137, 146], "11788076e": [268, 290], "118": [128, 130], "11807573e": [137, 146], "11824808e": [137, 146], "11897744e": [268, 290], "119": [128, 130], "11912012e": [137, 146], "11_10": [299, 306], "11th": [268, 289], "12": [1, 4, 12, 13, 43, 46, 53, 58, 66, 73, 86, 87, 128, 136, 137, 143, 144, 169, 170, 171, 174, 284, 292, 300, 324, 344, 357, 372], "120": [128, 130, 136, 137, 146], "12014441e": [268, 290], "12090182e": [137, 146], "12174596e": [268, 290], "12183236e": [268, 290], "12196040e": [137, 146], "12234001e": [268, 290], "1228194534778595": [147, 150], "123": [128, 130, 168], "123456": [35, 38], "12422097e": [137, 146], "12468980e": [268, 290], "12480514e": [268, 290], "125": [128, 130], "12500": [291, 294, 297, 298], "12501": [291, 298], "12502": [291, 298], "12503": [291, 298], "12504": [291, 298], "12587933e": [268, 290], "12685782e": [268, 290], "127": [1, 128, 130, 147, 150, 155, 158, 169, 170], "128": [5, 7, 13, 14, 128, 130, 224, 229, 253, 256, 257, 268, 289, 336, 339, 355, 359, 361], "12821269e": [268, 290], "12832280e": [268, 290], "12841654e": [268, 290], "128k": [101, 102, 105, 118, 124], "12907687e": [268, 290], "12912727e": [268, 290], "12939501e": [268, 290], "12959743e": [137, 146], "129887": 13, "12m47": [128, 136], "12m52": [128, 136], "12th": [268, 289], "12x": 14, "12xlarg": [128, 133, 136, 137, 139, 143, 146, 159, 163], "13": [12, 14, 43, 46, 53, 58, 128, 136, 137, 146, 351, 372], "13000": [291, 297, 298], "13086134e": [268, 290], "13093004e": [137, 146], "13095595e": [268, 290], "13100": [291, 297, 298], "13238472e": [268, 290], "13421358e": [137, 146], "13470632e": [137, 146], "13547181e": [268, 290], "13586960e": [268, 290], "13600": [291, 297, 298], "13700": [291, 297, 298], "13730657e": [137, 146], "138": [13, 128, 136, 137, 146], "13803817e": [268, 290], "13828215e": [268, 290], "13841531e": [268, 290], "13845997e": [137, 146], "13898420e": [137, 146], "13922286e": [137, 146], "13934547e": [137, 146], "13b": [110, 112, 117, 118, 124], "13m0": [137, 146], "13m35": [147, 150], "13m37": [128, 136], "13m5": [137, 146], "14": [110, 112, 268, 289, 299, 306, 331, 336, 338, 344, 351, 357, 364], "140": [110, 112], "14019522e": [268, 290], "14075851e": [137, 146], "140gb": [110, 112, 114], "14105418e": [137, 146], "14159100e": [268, 290], "14268738e": [268, 290], "144": [128, 130], "14403330e": [137, 146], "14443852e": [268, 290], "14489758e": [137, 146], "14533243e": [268, 290], "14597031e": [137, 146], "146": [128, 130], "14656349e": [268, 290], "14693624e": [137, 146], "14703774e": [268, 290], "14777484e": [268, 290], "14787792e": [268, 290], "14892137e": [268, 290], "149": [128, 130], "14971709e": [268, 290], "14gb": [101, 102, 106], "14th": [268, 289], "15": [3, 5, 12, 13, 28, 29, 35, 39, 43, 44, 45, 53, 55, 56, 66, 70, 118, 123, 155, 158, 181, 189, 268, 287, 289, 299, 306, 353], "150": [128, 130, 268, 287, 289], "15048778e": [137, 146], "15072963e": [268, 290], "150m": 168, "151": [128, 130], "15157820e": [268, 290], "15213782e": [137, 146], "15247765e": [268, 290], "153": [128, 130], "15391724e": [268, 290], "15394783": [268, 289], "15416703e": [137, 146], "15428728e": [268, 290], "15451038e": [268, 290], "15464866e": [137, 146], "155": [9, 198, 201], "15531293e": [268, 290], "15545475e": [137, 146], "15553670e": [268, 290], "15556864e": [268, 290], "15585802e": [268, 290], "155m": 168, "156": 12, "15658525e": [268, 290], "15747452e": [268, 290], "15786707e": [268, 290], "158": [128, 130], "15815112e": [137, 146], "15844142e": [268, 290], "15874708e": [268, 290], "15949178e": [137, 146], "15_08": 12, "15_15": 12, "15m32": [128, 136], "15x": 13, "16": [12, 13, 14, 110, 116, 229, 268, 289, 299, 306, 329, 331, 350, 352, 364, 365], "160": [6, 110, 112, 128, 130, 136, 137, 143, 259, 262], "161": [128, 130], "16129310e": [268, 290], "16136428e": [137, 146], "16142738e": [268, 290], "162": [128, 130], "16249922e": [268, 290], "16273381e": [268, 290], "16315296e": [268, 290], "163491": 12, "163492": 12, "16403189e": [137, 146], "16499318e": [137, 146], "16707636e": [268, 290], "168": [128, 136, 137, 146, 355, 356, 357], "16848189e": [268, 290], "1693310400": 168, "16946062e": [268, 290], "16959971e": [137, 146], "16970104e": [268, 290], "16m0": [137, 146], "16m2": [128, 136], "16m5": [137, 146], "16m52": [128, 136], "16m7": [128, 136], "16th": [268, 287, 289], "16xlarg": [159, 163], "17": [12, 17, 22, 43, 46, 47, 53, 58, 59, 66, 75, 86, 87, 110, 116, 268, 289, 291, 294], "17139973e": [137, 146], "17145060e": [268, 290], "172": [17, 22], "17217854e": [137, 146], "17218718e": [268, 290], "17278847e": [268, 290], "17298350e": [137, 146], "17306670e": [268, 290], "1732276209": 13, "1732276227": 13, "17342269e": [268, 290], "1737": [137, 141], "17392734e": [137, 146], "17458829e": [268, 290], "17499933e": [137, 146], "17503238e": [268, 290], "17549804e": [268, 290], "175m": [164, 167, 168], "17605399e": [268, 290], "17623755e": [268, 290], "17677706e": [268, 290], "17716263e": [268, 290], "17771959e": [137, 146], "1783": [137, 141], "17952654e": [268, 290], "17967087e": [137, 146], "17982033e": [268, 290], "17th": [268, 289], "17x": [128, 130], "18": [13, 35, 40, 43, 47, 53, 59, 66, 75, 86, 87, 110, 116, 225, 226, 231, 268, 289, 362, 363], "18025970e": [268, 290], "180m": [164, 167, 168], "18165373e": [268, 290], "18200418e": [268, 290], "18251920e": [137, 146], "18252768e": [268, 290], "18264270e": [268, 290], "1833": [137, 141], "18500029e": [268, 290], "18641007e": [137, 146], "18707759e": [268, 290], "1879": [1, 169, 170], "1881": [137, 141], "18899436e": [268, 290], "18926680e": [268, 290], "18932852e": [268, 290], "18th": [268, 289], "19": [12, 14, 86, 87, 110, 116, 268, 289, 329, 331, 355, 357, 362, 364], "19056532e": [137, 146], "19172417e": [268, 290], "19192507e": [268, 290], "19229007e": [137, 146], "19254841e": [268, 290], "19263542e": [137, 146], "19275388e": [268, 290], "192gb": [128, 133, 136, 137, 139, 143, 146], "19408388e": [268, 290], "19601890e": [268, 290], "19630387e": [268, 290], "19634366e": [137, 146], "1967": [291, 294], "19670653e": [268, 290], "19684739e": [268, 290], "19716156e": [268, 290], "19767813e": [268, 290], "19797611e": [268, 290], "19835320e": [268, 290], "19861914e": [268, 290], "19884178e": [268, 290], "199": [128, 136, 137, 146], "19939610e": [137, 146], "1995": [268, 289], "19986786e": [268, 290], "19m52": [128, 136], "1_000_000": [3, 181, 189], "1d": [336, 339], "1e": [5, 6, 7, 13, 14, 137, 140, 143, 224, 228, 238, 240, 245, 247, 253, 256, 257, 259, 262, 299, 305, 336, 339, 342, 346, 355, 359, 362, 368], "1f": [355, 357], "1gb": [9, 198, 201], "1h34m20": 13, "1m": [342, 348], "1m10": 16, "1m15": [137, 139], "1mb": [159, 163], "1pb": [159, 163], "1st": [3, 181, 184, 268, 287, 289], "1xt4": [12, 13, 14, 128, 136, 137, 146], "2": [29, 30, 42, 44, 45, 55, 56, 78, 80, 81, 92, 93, 112, 115, 117, 121, 122, 123, 128, 130, 136, 137, 139, 143, 144, 146, 147, 150, 153, 159, 163, 164, 165, 167, 185, 188, 199, 203, 219, 222, 224, 225, 227, 229, 237, 238, 241, 254, 255, 257, 268, 274, 280, 287, 289, 290, 291, 296, 297, 298, 305, 306, 313, 314, 315, 316, 327, 330, 335, 343, 346, 348, 352, 358, 359, 361, 363, 366, 367, 371, 372], "20": [1, 7, 12, 13, 14, 17, 22, 86, 87, 110, 116, 137, 143, 144, 169, 170, 253, 255, 257, 299, 306, 329, 331, 336, 338, 342, 344, 346, 349, 351, 355, 359], "200": [268, 287, 289, 291, 295], "20022464e": [137, 146], "20059741e": [137, 146], "20093006e": [268, 290], "200m": 168, "20103974e": [268, 290], "20113872e": [268, 290], "2012": [17, 22, 268, 289], "20136715e": [137, 146], "2014": [355, 356, 357], "2015": [268, 289], "20152864e": [268, 290], "2017": [268, 287, 289], "20175812e": [268, 290], "2021": [4, 12, 171, 174], "2023": [9, 198, 205], "2024": [9, 10, 12, 13, 14, 15, 168, 198, 205, 206, 217], "20241398e": [137, 146], "2025": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 69, 86, 87, 100, 101, 102, 103, 110, 111, 118, 119, 123, 137, 144, 155, 156, 159, 160, 164, 165, 169, 170, 171, 172, 175, 176, 181, 182, 191, 192, 198, 199, 206, 207, 253, 254, 259, 260, 267, 268, 285, 291, 293, 299, 301, 306, 309, 316, 325], "20302368700504303": 14, "20312032e": [268, 290], "20320046e": [137, 146], "20370263e": [268, 290], "20407227e": [268, 290], "20495850e": [137, 146], "20567256e": [268, 290], "20587808e": [268, 290], "205m": 168, "20716389e": [268, 290], "20745975e": [137, 146], "2076": [137, 141], "20779848e": [137, 146], "20950916e": [137, 146], "20m42": [128, 136], "20m47": [128, 136], "20m52": [128, 136], "20yr": [268, 289], "21": [13, 17, 22, 299, 306], "210m": 168, "21142501e": [137, 146], "21206143e": [268, 290], "213": [128, 130], "21334969e": [268, 290], "21351314e": [137, 146], "21377383e": [268, 290], "21380231e": [268, 290], "214": [128, 130], "21405767e": [268, 290], "2147483648": [3, 181, 187], "21503563e": [268, 290], "21508265e": [268, 290], "21584712e": [268, 290], "216": [128, 130], "21600205e": [268, 290], "21627375e": [137, 146], "21637220e": [268, 290], "217": [128, 130], "21715315e": [137, 146], "21745682e": [268, 290], "21776196e": [268, 290], "21796946e": [268, 290], "219": [128, 130], "21971425e": [268, 290], "21988237e": [137, 146], "21m32": [128, 136], "21m37": [128, 136], "21st": [268, 289], "22": [13, 137, 144, 268, 289, 299, 306], "220": [128, 130], "22013104e": [137, 146], "22061956e": [137, 146], "221": [128, 130], "22192677e": [268, 290], "222": [14, 128, 130], "22204229e": [137, 146], "22234142e": [268, 290], "22277103e": [268, 290], "223": [128, 130], "22332841e": [268, 290], "224": [329, 331, 335, 362, 363, 364, 365], "22424790e": [137, 146], "22458421e": [268, 290], "225": [128, 130, 362, 365], "22552105e": [268, 290], "22592783e": [137, 146], "227": [128, 130], "228": [128, 130], "22804798e": [268, 290], "22867932e": [268, 290], "22891478e": [268, 290], "229": [362, 365], "22918031e": [268, 290], "2292557954788208": [147, 150], "22967193e": [268, 290], "2298": [137, 141], "22_06": 13, "22_11": 13, "22nd": [268, 289], "23": [12, 268, 289, 329, 331, 362, 364], "230": [128, 130], "23042464e": [137, 146], "23053056e": [268, 290], "23069037e": [268, 290], "23083012e": [268, 290], "23107583e": [268, 290], "23113478e": [268, 290], "23149785e": [137, 146], "232": [128, 130], "23204021e": [268, 290], "233": [128, 130], "23302741e": [268, 290], "23314434e": [268, 290], "234": [128, 130], "23478852e": [268, 290], "235": [128, 130], "235m": [164, 167, 168], "23639209e": [268, 290], "23694149e": [268, 290], "23694418e": [268, 290], "237": [128, 130], "23702966e": [268, 290], "23758684e": [137, 146], "23793101e": [268, 290], "23815991e": [268, 290], "239": [128, 130], "23926058e": [137, 146], "23960292e": [268, 290], "23972368e": [137, 146], "23982316e": [268, 290], "23x": [299, 306], "24": [3, 12, 17, 22, 181, 187, 355, 356, 357], "24085984e": [268, 290], "240m": [164, 167, 168], "24192730e": [268, 290], "24219281e": [268, 290], "24348718e": [137, 146], "24473451e": [268, 290], "245m": [164, 167, 168], "24615501e": [268, 290], "24660867e": [137, 146], "24661736e": [137, 146], "24854468e": [268, 290], "24885803e": [268, 290], "24984046e": [268, 290], "24xlarg": [159, 163], "25": [12, 13, 14, 43, 44, 53, 55, 66, 68, 137, 139, 299, 306], "250": [291, 295], "25000": [291, 294, 295, 296], "25093237e": [137, 146], "25101686e": [137, 146], "25175510e": [268, 290], "25248020e": [268, 290], "25258800e": [268, 290], "25268223e": [268, 290], "25294994e": [268, 290], "25365251e": [137, 146], "25387060e": [268, 290], "25401214e": [268, 290], "255": [7, 14, 15, 253, 255, 329, 331], "25536823e": [137, 146], "25569642e": [137, 146], "256": [6, 13, 137, 140, 143, 144, 159, 163, 259, 262, 263, 329, 331, 362, 364], "25669474e": [268, 290], "25707304e": [268, 290], "25723777e": [268, 290], "25795130e": [268, 290], "25806283e": [268, 290], "25825119e": [137, 146], "25883007e": [137, 146], "25912409e": [268, 290], "25934454e": [268, 290], "25_924022_2383": 12, "25m22": [128, 136], "25m27": [128, 136], "25th": [268, 289], "26": [13, 14, 110, 112, 299, 306], "26000811e": [268, 290], "26009388e": [268, 290], "26060665e": [137, 146], "26151049e": [137, 146], "26156314e": [268, 290], "26208720e": [268, 290], "26232028e": [137, 146], "26244992e": [268, 290], "26258478e": [268, 290], "26309279e": [268, 290], "26314560e": [268, 290], "26320729e": [268, 290], "26351237e": [268, 290], "26371822e": [268, 290], "264131": 14, "26429361e": [268, 290], "26502474e": [268, 290], "26629007e": [137, 146], "26750994e": [137, 146], "26772380e": [137, 146], "26816351e": [268, 290], "26885992e": [137, 146], "26948217e": [268, 290], "26990714e": [268, 290], "27": [12, 13, 128, 133, 136, 268, 287, 289], "27106623e": [268, 290], "27159020e": [268, 290], "27211": 14, "27212": 14, "27213278e": [268, 290], "27219909e": [268, 290], "27320850e": [137, 146], "27394149e": [268, 290], "27448589e": [137, 146], "27486494e": [268, 290], "27514724e": [268, 290], "27598614e": [268, 290], "27724269e": [137, 146], "27818017e": [268, 290], "2784426808357239": 13, "278443": 13, "27902825e": [268, 290], "27904241e": [268, 290], "27935463e": [268, 290], "27x": [299, 306], "28": [10, 11, 13, 14, 15, 16, 164, 167, 206, 212, 218, 222, 224, 226, 299, 306], "28050": 14, "28076579e": [268, 290], "28086493e": [268, 290], "28125295e": [268, 290], "28131025e": [268, 290], "28160176e": [268, 290], "28278339e": [137, 146], "28330866e": [268, 290], "28378880e": [137, 146], "28415197e": [268, 290], "28529142e": [268, 290], "28545947e": [268, 290], "28572544e": [268, 290], "28628640e": [268, 290], "28712449e": [268, 290], "28749549e": [268, 290], "28796948e": [268, 290], "28858958e": [268, 290], "28869668e": [268, 290], "28947487e": [268, 290], "28947702e": [268, 290], "28m22": [128, 136], "28m27": [128, 136], "28th": [268, 289], "28x28": [7, 14, 253, 255], "29": [12, 13, 14, 118, 123], "29297644e": [268, 290], "29473785e": [268, 290], "29535252e": [268, 290], "29538319e": [137, 146], "29549801e": [137, 146], "29641497e": [137, 146], "29715446e": [268, 290], "29792884e": [268, 290], "29807210e": [268, 290], "29825398e": [268, 290], "29964035e": [268, 290], "29_09": 14, "29t10": 168, "2a": [43, 45, 53, 56, 128, 133, 136, 137, 139, 143, 146], "2b": [43, 45, 53, 56], "2cpu": [43, 50, 53, 62], "2d": [7, 14, 253, 255, 342, 348], "2e": [329, 332], "2f": [3, 88, 89, 137, 146, 181, 183, 342, 348, 349, 353], "2m12": [128, 133], "2m17": [128, 133], "2m57": [128, 133], "2nd": [3, 181, 184, 268, 289], "2xlarg": [128, 136, 137, 139, 146], "2ykut_ijz8q8gwt5vphvitzshksddol6msszjxzwe5a": [110, 115], "3": [31, 42, 44, 51, 55, 64, 68, 78, 84, 85, 113, 114, 115, 120, 121, 122, 125, 128, 130, 137, 139, 140, 141, 143, 144, 146, 147, 150, 153, 159, 163, 164, 167, 168, 180, 189, 199, 212, 213, 219, 224, 225, 226, 227, 237, 238, 240, 245, 247, 248, 254, 256, 264, 268, 283, 284, 289, 290, 291, 292, 294, 298, 300, 305, 306, 324, 330, 332, 335, 339, 340, 341, 343, 346, 348, 352, 359, 361, 363, 368], "30": [13, 118, 123, 137, 139, 268, 289, 299, 306, 342, 344, 356, 361], "30062854e": [137, 146], "30101690e": [137, 146], "30219358e": [137, 146], "30242959e": [137, 146], "30407149e": [137, 146], "30422497e": [137, 146], "30472469e": [137, 146], "30478994e": [268, 290], "30517557e": [268, 290], "30540405e": [268, 290], "30551": 13, "30557770e": [268, 290], "30565623e": [268, 290], "30582720e": [268, 290], "30603": 13, "30618355e": [268, 290], "30638674e": [268, 290], "30642965e": [137, 146], "3069": [137, 141], "30711952e": [137, 146], "30715715e": [268, 290], "30746688e": [268, 290], "30834863e": [268, 290], "308870": 13, "30887049436569214": 13, "30980236e": [268, 290], "30981060e": [268, 290], "30999158e": [268, 290], "30min": [355, 357], "30th": [268, 289], "31": [1, 12, 13, 14, 128, 136, 137, 146, 169, 170, 299, 306], "31001920e": [137, 146], "31019164e": [268, 290], "31141504e": [268, 290], "31161788e": [137, 146], "31172134e": [268, 290], "31209707e": [268, 290], "31232068e": [137, 146], "31235719e": [137, 146], "31236637e": [137, 146], "31332219e": [137, 146], "31449399e": [268, 290], "31453001e": [137, 146], "31499174e": [268, 290], "31506741e": [137, 146], "31562141e": [268, 290], "31659403e": [268, 290], "31720269e": [137, 146], "31913936e": [268, 290], "31973000e": [268, 290], "31st": [268, 289], "32": [3, 6, 10, 110, 116, 118, 121, 128, 133, 137, 144, 181, 183, 190, 206, 212, 259, 262, 329, 332, 333, 336, 340, 349, 352], "320": [6, 259, 262], "32145682e": [268, 290], "32192443e": [268, 290], "32214013e": [137, 146], "32244647e": [268, 290], "32266051e": [268, 290], "32296453e": [268, 290], "32373542e": [268, 290], "32400897e": [137, 146], "32401919e": [268, 290], "32454751e": [268, 290], "32564947e": [268, 290], "326001912355423": 14, "32635012e": [268, 290], "32654747e": [268, 290], "3266499161421599": 14, "32665": 14, "32722983e": [268, 290], "32756231e": [268, 290], "32768": [110, 113, 116, 118, 123], "32768166e": [268, 290], "32868871e": [137, 146], "32875013e": [137, 146], "32888782e": [137, 146], "32890965e": [268, 290], "32902160e": [268, 290], "32918817e": [268, 290], "3297": [137, 141], "32b": [118, 123, 124], "32gb": [14, 128, 136, 137, 139, 146], "32k": [101, 102, 105, 118, 124], "32m": [13, 14, 299, 306], "33": [268, 287, 289, 290], "33019698e": [137, 146], "33023707e": [268, 290], "33163792e": [268, 290], "33167297e": [268, 290], "33217961e": [137, 146], "33281359e": [268, 290], "33300245e": [268, 290], "33315668e": [268, 290], "3336": [137, 141], "33374238e": [137, 146], "33572224e": [268, 290], "33688403e": [268, 290], "33728483e": [268, 290], "33760041e": [268, 290], "33768886e": [268, 290], "33827804e": [268, 290], "33832851e": [268, 290], "33951919e": [268, 290], "34": [13, 14, 15, 128, 136, 137, 141, 268, 290], "34206163e": [268, 290], "34323069e": [268, 290], "3434": [137, 141], "34348310e": [268, 290], "34402555e": [137, 146], "34404474e": [137, 146], "34449054e": [268, 290], "34450802e": [137, 146], "34561165e": [268, 290], "34613437e": [268, 290], "34668782e": [268, 290], "34740751e": [268, 290], "34842591e": [268, 290], "34856862e": [137, 146], "34950879e": [137, 146], "34999743e": [268, 290], "35": [13, 14, 268, 290, 299, 306], "35016027e": [268, 290], "35024523e": [268, 290], "35026570e": [268, 290], "35150540e": [137, 146], "35173336e": [137, 146], "35185423e": [268, 290], "35189386e": [268, 290], "35268092e": [268, 290], "35481167e": [268, 290], "35504": [137, 144], "35658276e": [137, 146], "35665376e": [268, 290], "35665385e": [268, 290], "35833579e": [268, 290], "35872006e": [137, 146], "35873899e": [268, 290], "35882646e": [137, 146], "35890651e": [268, 290], "36": [14, 110, 116, 137, 140, 144, 299, 306], "36022663e": [137, 146], "36150215e": [268, 290], "36222595e": [137, 146], "36240765e": [268, 290], "365191": 13, "36595646e": [268, 290], "36629641e": [137, 146], "36664629e": [137, 146], "36710434e": [268, 290], "36786141e": [268, 290], "36829392e": [268, 290], "36855166e": [268, 290], "36857450e": [268, 290], "36868265e": [268, 290], "36873224e": [268, 290], "3688": [137, 141], "36969042e": [137, 146], "36m": [13, 14, 16, 128, 133, 136, 137, 139, 143, 146, 147, 150, 299, 306], "37": [13, 14, 110, 116], "37044120e": [137, 146], "37080820e": [268, 290], "37142141e": [268, 290], "37153175e": [268, 290], "37196398e": [268, 290], "37221038e": [137, 146], "37391533e": [268, 290], "37459409e": [137, 146], "37605290e": [268, 290], "37751494e": [268, 290], "37764162e": [268, 290], "37784477e": [268, 290], "37850042e": [268, 290], "37893021e": [137, 146], "37964366e": [268, 290], "37986809e": [137, 146], "38": [299, 306], "38109175e": [268, 290], "38115362e": [268, 290], "38248950e": [137, 146], "38281021e": [268, 290], "384": [110, 116, 268, 289, 290], "38448": 14, "384480": 14, "38451725e": [268, 290], "38496622e": [268, 290], "38554320e": [268, 290], "38563488e": [268, 290], "38644290e": [137, 146], "38687068e": [268, 290], "38798335e": [268, 290], "38858718e": [268, 290], "38910706e": [268, 290], "38927615e": [137, 146], "38930988e": [137, 146], "39053045e": [268, 290], "39066431e": [268, 290], "39178723e": [137, 146], "39178753e": [137, 146], "39268537e": [268, 290], "39287962e": [268, 290], "39421923e": [268, 290], "39532143e": [268, 290], "39561000e": [268, 290], "39590132e": [268, 290], "39660065e": [268, 290], "39745891e": [268, 290], "39747020e": [268, 290], "39796034e": [268, 290], "39855982e": [268, 290], "3989": [137, 141], "39904258e": [137, 146], "39905545e": [268, 290], "39945886e": [137, 146], "3b": [118, 122], "3d": [336, 339], "3f": [349, 352, 353, 354], "3m10": [137, 139], "3m40": [137, 143], "3m45": [137, 143], "3rd": [268, 289], "3x": [147, 151], "3x3": [224, 226, 237], "4": [12, 42, 73, 78, 88, 89, 111, 112, 113, 114, 121, 122, 123, 128, 129, 130, 131, 133, 137, 138, 139, 140, 143, 146, 147, 148, 153, 164, 167, 168, 172, 185, 199, 213, 219, 224, 225, 254, 257, 260, 268, 289, 290, 291, 298, 332, 333, 340, 343, 346, 348, 352, 359, 361, 365], "40": [13, 168, 268, 289, 291, 294], "4000": [3, 181, 187], "40002957e": [137, 146], "40064341e": [268, 290], "40084913e": [268, 290], "40099749e": [137, 146], "400b": [110, 112, 117, 118, 124], "40222309e": [268, 290], "40240113e": [268, 290], "40254933e": [268, 290], "403": [13, 14], "40336857e": [268, 290], "40409318e": [268, 290], "40434751e": [137, 146], "40456108e": [268, 290], "40510444e": [268, 290], "40537590e": [268, 290], "40543866e": [137, 146], "406": [362, 365], "40600796e": [268, 290], "40880044e": [268, 290], "40937243e": [268, 290], "40g": [110, 113, 114], "41": [1, 169, 170, 299, 306], "41017914e": [137, 146], "41169238e": [268, 290], "41174936e": [137, 146], "41280317e": [268, 290], "41516277e": [268, 290], "41520910e": [268, 290], "41526775e": [268, 290], "41544282e": [137, 146], "41575071e": [268, 290], "41593361e": [137, 146], "41598": [299, 306], "41599": [299, 306], "415m": 168, "41709536e": [268, 290], "41786465e": [268, 290], "41920993e": [268, 290], "41922843e": [268, 290], "41933542e": [268, 290], "41968962e": [268, 290], "41985670e": [268, 290], "42": [4, 171, 174, 342, 344, 349, 351, 362, 365], "42026821e": [137, 146], "42042309e": [137, 146], "42074719e": [137, 146], "420m": 168, "42177847e": [268, 290], "4219": [137, 141], "422321": [299, 306], "42241838e": [268, 290], "42471355e": [268, 290], "42482564e": [268, 290], "42522454e": [137, 146], "42548003e": [268, 290], "42604055e": [268, 290], "42662169e": [268, 290], "42837034e": [268, 290], "42856956e": [268, 290], "42857780e": [268, 290], "42904809e": [268, 290], "42934144e": [137, 146], "42943636e": [268, 290], "43057": 12, "43168001e": [268, 290], "43248991e": [268, 290], "43266308e": [137, 146], "43267226e": [268, 290], "43328887e": [268, 290], "43380117e": [137, 146], "43496174e": [137, 146], "43732": 13, "43747482e": [268, 290], "43779554e": [268, 290], "43806068e": [268, 290], "43821533e": [268, 290], "43902507e": [268, 290], "43916206e": [268, 290], "43940079e": [268, 290], "43996522e": [268, 290], "44": [12, 14], "44072895e": [268, 290], "44087312e": [268, 290], "44139796e": [137, 146], "44193405e": [137, 146], "44237953e": [268, 290], "44269560e": [268, 290], "44299744e": [268, 290], "443": [17, 22], "44326049e": [268, 290], "44500265e": [268, 290], "44628939e": [268, 290], "44769201e": [268, 290], "44773570": 13, "44875658e": [268, 290], "44877301e": [268, 290], "44888324e": [268, 290], "44945610e": [268, 290], "45": [13, 28, 31, 268, 289], "45010501e": [137, 146], "45237213e": [268, 290], "45387161e": [268, 290], "45463008e": [268, 290], "45558545e": [268, 290], "45565206e": [268, 290], "45596695e": [268, 290], "456": [168, 362, 365], "45615": [268, 287, 290], "45630976e": [268, 290], "45667797e": [268, 290], "45728597e": [137, 146], "45732313e": [137, 146], "45758486e": [137, 146], "45803327e": [268, 290], "45804777e": [268, 290], "45845979e": [268, 290], "45928478e": [268, 290], "45977615e": [268, 290], "45981687e": [268, 290], "46": [43, 46, 53, 58, 299, 306], "46038486e": [268, 290], "46128924e": [268, 290], "46165411e": [268, 290], "46190545e": [268, 290], "46210968e": [268, 290], "46212946e": [268, 290], "46281177e": [268, 290], "46350823e": [268, 290], "46354072e": [268, 290], "46371266e": [268, 290], "46490431e": [268, 290], "46582437e": [137, 146], "46654081e": [268, 290], "46689427e": [137, 146], "46736982e": [268, 290], "46787928e": [268, 290], "46938870e": [268, 290], "46950224e": [137, 146], "46954274e": [268, 290], "46988708e": [137, 146], "47110615e": [268, 290], "47215438e": [137, 146], "47293536e": [268, 290], "47399181e": [268, 290], "47439016e": [268, 290], "47477984e": [268, 290], "47517592e": [137, 146], "475m": 168, "47602344e": [268, 290], "47635157e": [268, 290], "47678024e": [268, 290], "47783903e": [268, 290], "47834991e": [268, 290], "47863048e": [137, 146], "47896763e": [268, 290], "47925606e": [268, 290], "47997355e": [268, 290], "48": [13, 110, 116, 159, 163, 355, 356, 357], "48037392e": [137, 146], "48048115e": [268, 290], "48062438e": [137, 146], "480m": 168, "48107335e": [137, 146], "48195577e": [137, 146], "48344308e": [137, 146], "48345065e": [137, 146], "485": [362, 365], "485m": 168, "48663167e": [268, 290], "48783788e": [137, 146], "48813944e": [268, 290], "48855758e": [268, 290], "48856053e": [268, 290], "48863769e": [137, 146], "48883224e": [137, 146], "48921371e": [137, 146], "48cpu": [128, 133, 136, 137, 139, 143, 146], "49": [13, 101, 102, 108, 110, 115, 299, 306], "49026504e": [268, 290], "49106100e": [268, 290], "49187856e": [268, 290], "49242997e": [137, 146], "49249397e": [268, 290], "49257514e": [268, 290], "49290550e": [137, 146], "49307863e": [268, 290], "49331436e": [268, 290], "49361154e": [268, 290], "49398082e": [137, 146], "49425897e": [268, 290], "49499828e": [137, 146], "49631714e": [268, 290], "49642932e": [268, 290], "49676067e": [137, 146], "49779003e": [268, 290], "49808554e": [268, 290], "49876371e": [268, 290], "49959813e": [268, 290], "4f": [342, 346], "4k": [101, 102, 105, 118, 124], "4m30": [137, 143], "4m37": 14, "4th": [268, 289], "4xt4": [128, 133, 136, 137, 139, 143, 146], "5": [2, 7, 12, 16, 29, 37, 44, 55, 68, 118, 121, 122, 124, 128, 136, 137, 144, 146, 147, 153, 164, 167, 168, 175, 180, 184, 185, 190, 199, 212, 224, 228, 232, 237, 238, 243, 253, 255, 256, 257, 268, 289, 290, 291, 297, 298, 304, 333, 335, 338, 343, 346, 359, 361, 368, 369, 371], "50": [10, 13, 15, 28, 30, 110, 116, 206, 210, 291, 297, 313, 314, 315, 316, 317, 319, 320, 327, 328, 329, 335, 336, 341, 349, 352, 354], "500": [7, 159, 163, 253, 257, 329, 331, 362, 364, 365], "50048220e": [137, 146], "50099332e": [268, 290], "50164117e": [268, 290], "50424745e": [268, 290], "50576949e": [268, 290], "50653918e": [268, 290], "50681949e": [137, 146], "50758056e": [137, 146], "50785267e": [268, 290], "50843680e": [137, 146], "50856599e": [137, 146], "50892793e": [268, 290], "50946071e": [268, 290], "50_per_index": [10, 15, 16, 206, 210, 212, 215], "51002133e": [268, 290], "51042950e": [268, 290], "51119480e": [268, 290], "51127565e": [137, 146], "512": [13, 128, 136, 137, 140, 143, 144, 238, 244, 245, 248, 250, 342, 346], "51281480e": [268, 290], "51315774e": [268, 290], "51320172e": [268, 290], "51369202e": [137, 146], "51379186e": [137, 146], "51383309e": [268, 290], "51503164e": [268, 290], "51518283e": [268, 290], "51617597e": [268, 290], "51706925e": [137, 146], "51739728e": [268, 290], "51786283e": [268, 290], "51865722e": [268, 290], "52": [128, 130], "52094781e": [137, 146], "52096841e": [268, 290], "52135583e": [268, 290], "52154651e": [268, 290], "522000": [137, 144], "52212310e": [137, 146], "52294970e": [137, 146], "52319247e": [137, 146], "52324782e": [268, 290], "525325868955": [17, 22], "52539432e": [268, 290], "52647242e": [268, 290], "52814901e": [137, 146], "53": [13, 128, 130], "53022516e": [137, 146], "53134540e": [137, 146], "53193595e": [268, 290], "53348368e": [137, 146], "53377999e": [268, 290], "53381238e": [268, 290], "53421593e": [137, 146], "53534813e": [268, 290], "53647423e": [268, 290], "53650725e": [137, 146], "53716086e": [268, 290], "53754605e": [268, 290], "53796408e": [137, 146], "53836450e": [137, 146], "53959617e": [268, 290], "53998228e": [268, 290], "54": [12, 128, 130, 147, 151, 299, 306, 349, 350, 351], "5404948": 12, "54087022e": [137, 146], "54091814e": [137, 146], "54113623e": [268, 290], "54230055e": [268, 290], "54291149e": [268, 290], "54304842e": [268, 290], "54419112e": [137, 146], "54450669e": [268, 290], "54470068e": [137, 146], "54546804e": [268, 290], "54573391e": [268, 290], "54621774e": [268, 290], "54645248e": [268, 290], "54685497e": [268, 290], "5492": [137, 141], "55": 13, "55025972e": [268, 290], "55034113e": [137, 146], "55071461e": [137, 146], "55099240e": [268, 290], "550_000": [6, 259, 263], "5529555": 12, "5552995": 12, "55531311e": [137, 146], "55601753e": [268, 290], "55613178e": [268, 290], "55635225e": [268, 290], "55699139e": [268, 290], "5588189": 12, "55886114e": [137, 146], "55900936e": [268, 290], "55923614e": [268, 290], "55929470e": [137, 146], "56": [128, 133], "56011016e": [268, 290], "56031644e": [137, 146], "56041365e": [268, 290], "56115811e": [268, 290], "56188577e": [268, 290], "56204057e": [137, 146], "56274483e": [268, 290], "56360346e": [137, 146], "56389399e": [268, 290], "56662523e": [268, 290], "56688479e": [268, 290], "56718080e": [268, 290], "56892097e": [137, 146], "56896329e": [268, 290], "56896693e": [268, 290], "56920916e": [268, 290], "57": [110, 116, 128, 130], "57008413e": [268, 290], "57041806e": [137, 146], "57156193e": [137, 146], "57156566e": [268, 290], "57160601e": [268, 290], "57175567e": [268, 290], "57203451e": [137, 146], "57226282e": [268, 290], "57393692e": [268, 290], "57440994e": [268, 290], "57462114e": [137, 146], "57502690e": [268, 290], "57504702e": [137, 146], "57536250e": [268, 290], "57737350e": [268, 290], "57877821e": [268, 290], "57970206e": [268, 290], "58": 12, "580": [159, 163, 349, 350, 351], "58003160e": [268, 290], "58086956e": [268, 290], "580k": [349, 351], "580x580x3": [159, 163], "58155808e": [268, 290], "58189368e": [268, 290], "58204031e": [137, 146], "58261052e": [268, 290], "58301930e": [268, 290], "58354291e": [137, 146], "58395988e": [137, 146], "58452298e": [268, 290], "58531007e": [268, 290], "58614498e": [268, 290], "58722073e": [137, 146], "58806413e": [268, 290], "58818898e": [268, 290], "58824509e": [268, 290], "58904049e": [268, 290], "58911937e": [137, 146], "59": [12, 118, 123, 128, 130], "59060508e": [268, 290], "59105931e": [268, 290], "59115836e": [137, 146], "59260547e": [268, 290], "59284808e": [137, 146], "59323025e": [137, 146], "593301": [137, 144], "59349391e": [268, 290], "59405363e": [137, 146], "59438765e": [268, 290], "59552705e": [268, 290], "59555081e": [268, 290], "59640062e": [137, 146], "59683321e": [137, 146], "59728657e": [268, 290], "59733031e": [268, 290], "59756446e": [137, 146], "59785998e": [137, 146], "59831755e": [268, 290], "59940967e": [137, 146], "59980466e": [268, 290], "5x": [147, 151], "6": [14, 15, 16, 137, 146, 155, 158, 164, 167, 168, 199, 268, 289, 290, 338, 350, 359, 364], "60": [7, 14, 128, 130, 136, 137, 146, 164, 167, 224, 226, 253, 255], "60005671e": [137, 146], "60166726e": [137, 146], "60254669e": [268, 290], "60417205": 12, "60533416e": [137, 146], "60535234e": [137, 146], "60535276e": [137, 146], "60559933e": [268, 290], "60586494e": [137, 146], "60700288e": [268, 290], "60769555e": [268, 290], "60900429e": [268, 290], "60926636e": [268, 290], "60929009e": [268, 290], "61124960e": [268, 290], "61173157e": [268, 290], "61207989e": [137, 146], "61210120e": [268, 290], "61377022e": [268, 290], "61495838e": [268, 290], "61530429e": [137, 146], "61626707e": [268, 290], "61644816e": [268, 290], "61783993e": [268, 290], "61808310e": [268, 290], "62014505e": [268, 290], "62131321e": [268, 290], "62141061e": [137, 146], "62198240e": [137, 146], "62209135e": [268, 290], "62317707e": [268, 290], "62408248e": [268, 290], "62470581e": [268, 290], "62640566e": [137, 146], "62676229e": [268, 290], "62701386e": [137, 146], "62736428e": [137, 146], "62778306e": [137, 146], "62782574e": [268, 290], "6288": [137, 141], "6290559": 12, "629055917263031": 12, "62961054e": [137, 146], "63": [299, 306], "63008086e": [268, 290], "63077107e": [268, 290], "63363028e": [268, 290], "63391770e": [268, 290], "63410094e": [268, 290], "63487554e": [137, 146], "63496330e": [268, 290], "63529071e": [268, 290], "63559180e": [268, 290], "63560931e": [137, 146], "63716504e": [268, 290], "63769671e": [268, 290], "6379": [155, 158], "63797104e": [137, 146], "63952804e": [137, 146], "64": [5, 6, 7, 13, 14, 128, 131, 137, 139, 146, 224, 227, 253, 256, 257, 259, 262, 268, 280, 289, 342, 345, 346, 355, 358, 362, 368, 371], "640": [6, 259, 262], "64019221e": [137, 146], "64042781e": [268, 290], "641551": 13, "64302550e": [137, 146], "64337176e": [137, 146], "64353992e": [268, 290], "64574068e": [268, 290], "64641732e": [137, 146], "64772916e": [137, 146], "64824829e": [268, 290], "64865339e": [137, 146], "64927173e": [268, 290], "65101083e": [268, 290], "65246567e": [137, 146], "65288996e": [268, 290], "65297012e": [268, 290], "65307403e": [137, 146], "65330970e": [137, 146], "65584633e": [137, 146], "65683129e": [268, 290], "65831": 14, "65908886e": [268, 290], "66008": 12, "66033891e": [268, 290], "66034375e": [268, 290], "66039094e": [137, 146], "660569": 12, "662196": 12, "66246349e": [268, 290], "662499": 12, "66315883e": [137, 146], "66424632e": [137, 146], "66462481e": [137, 146], "66510823e": [268, 290], "66672035e": [268, 290], "66712171e": [268, 290], "66808441e": [268, 290], "66871315e": [268, 290], "66888866e": [268, 290], "66935217e": [137, 146], "67037442e": [137, 146], "67044029e": [137, 146], "67074795e": [268, 290], "67096058e": [268, 290], "67168414e": [268, 290], "67199332e": [137, 146], "67200039e": [268, 290], "67249476e": [268, 290], "67394597e": [268, 290], "67415068e": [137, 146], "67530221e": [268, 290], "67625724e": [268, 290], "67702921e": [268, 290], "67820978e": [137, 146], "67874378e": [137, 146], "68044616e": [268, 290], "682": [342, 344], "68261507e": [268, 290], "68279577e": [268, 290], "68410710e": [268, 290], "68456510e": [137, 146], "68468618e": [137, 146], "68614852e": [137, 146], "68623075e": [268, 290], "68863142e": [268, 290], "68928802e": [268, 290], "68990344e": [268, 290], "69025257e": [137, 146], "69066399e": [137, 146], "69076526e": [137, 146], "69132429e": [137, 146], "69230062e": [268, 290], "69240460e": [137, 146], "69288528e": [137, 146], "69341841e": [137, 146], "69485952e": [268, 290], "69494259e": [137, 146], "69655108e": [268, 290], "69689246e": [268, 290], "69690510e": [268, 290], "69710606e": [137, 146], "69849765e": [268, 290], "69882979e": [268, 290], "69915810e": [137, 146], "69974936e": [268, 290], "6_000108_000000": [6, 259, 262], "6_2024": 14, "6f": [88, 89, 299, 304], "6m52": [128, 133], "6th": [268, 289], "6x": 14, "7": [7, 12, 14, 15, 137, 141, 146, 164, 167, 168, 199, 224, 227, 253, 256, 257, 268, 289, 290, 333, 346, 352, 359, 364], "70": [110, 112], "70009223e": [137, 146], "70078446e": [268, 290], "70130879e": [268, 290], "70160577e": [268, 290], "70176884e": [268, 290], "70194232e": [137, 146], "70207208e": [268, 290], "70480572e": [268, 290], "70483862e": [268, 290], "70502144e": [268, 290], "70606668e": [268, 290], "70623609e": [268, 290], "70640138e": [268, 290], "70672083e": [137, 146], "70758316e": [268, 290], "70819569e": [137, 146], "70849383e": [268, 290], "70991046e": [268, 290], "70b": [101, 102, 105, 113, 114, 115, 116, 117, 118, 124], "71": 12, "71107422e": [268, 290], "71122622e": [137, 146], "71239424e": [137, 146], "71276686e": [268, 290], "71327189e": [268, 290], "71361454e": [268, 290], "71367359e": [268, 290], "71599907e": [268, 290], "71614686e": [268, 290], "71635705e": [137, 146], "71677093e": [268, 290], "71802861e": [268, 290], "71831726e": [268, 290], "72043703e": [268, 290], "72060728e": [268, 290], "72087287e": [268, 290], "72130489e": [137, 146], "72232352e": [268, 290], "72453296e": [137, 146], "72505680e": [268, 290], "72557199e": [137, 146], "72602186e": [268, 290], "72664893e": [268, 290], "72761095e": [137, 146], "72793153e": [268, 290], "72884393e": [137, 146], "72918749e": [268, 290], "72939146e": [137, 146], "72964428e": [268, 290], "73005784e": [268, 290], "73056117e": [268, 290], "73160497e": [268, 290], "73187131e": [137, 146], "73239648e": [137, 146], "73338448e": [268, 290], "73339425e": [268, 290], "73386657e": [268, 290], "73405361e": [137, 146], "73471044e": [268, 290], "73475703e": [268, 290], "73528048e": [268, 290], "73541475e": [268, 290], "73574477e": [137, 146], "73729736e": [137, 146], "73842654e": [268, 290], "73909385e": [268, 290], "73954112e": [268, 290], "73962507e": [268, 290], "74006203e": [268, 290], "74086124e": [137, 146], "74092592e": [268, 290], "74106744e": [268, 290], "74198601e": [268, 290], "74435990e": [268, 290], "74475431e": [137, 146], "74639919e": [268, 290], "74725649e": [268, 290], "74946982e": [137, 146], "75": [110, 116, 362, 371], "75087003e": [268, 290], "75092961e": [268, 290], "75342406e": [268, 290], "75359203e": [268, 290], "75361482e": [268, 290], "75378935e": [268, 290], "75653571e": [268, 290], "75755769e": [268, 290], "75817132e": [137, 146], "75831634e": [137, 146], "75859511e": [137, 146], "75899668e": [268, 290], "76054996e": [137, 146], "76055871e": [268, 290], "76067518e": [268, 290], "76138058e": [268, 290], "76263633e": [268, 290], "76323075e": [268, 290], "76346910e": [268, 290], "76389585e": [268, 290], "76404205e": [268, 290], "76421027e": [137, 146], "76428038e": [137, 146], "76448804e": [268, 290], "76463524e": [268, 290], "764641": [299, 306], "76478487e": [137, 146], "76568236e": [268, 290], "76666151e": [137, 146], "76702512e": [268, 290], "76708573e": [137, 146], "76715726e": [268, 290], "76780378e": [268, 290], "76795331e": [268, 290], "768": [110, 116], "76873404e": [137, 146], "76875392e": [268, 290], "76900255e": [268, 290], "76936167e": [268, 290], "76972622e": [268, 290], "77": [6, 259, 262], "77095975e": [268, 290], "7721": 14, "7722": 14, "7725": 14, "77374096e": [268, 290], "774": [1, 169, 170], "77459675e": [268, 290], "77555919e": [268, 290], "77573276e": [137, 146], "77590072e": [137, 146], "77606642e": [268, 290], "77690476e": [137, 146], "77768275e": [137, 146], "77784879e": [268, 290], "77830246e": [268, 290], "77848917e": [137, 146], "77938917e": [137, 146], "77998394e": [137, 146], "78": [128, 130], "78041089e": [268, 290], "78072055e": [268, 290], "78197911e": [268, 290], "78284839e": [268, 290], "78317158e": [137, 146], "7834368348121643": 13, "783437": 13, "78439620e": [268, 290], "78455579e": [137, 146], "78474542e": [268, 290], "78646958e": [137, 146], "78715137e": [268, 290], "78778227e": [268, 290], "78793629e": [268, 290], "78836194e": [268, 290], "78844824e": [137, 146], "78927866e": [268, 290], "79095644e": [137, 146], "79144512e": [268, 290], "79200619e": [137, 146], "79223316e": [268, 290], "79288665e": [268, 290], "79409343e": [268, 290], "79455946e": [268, 290], "79587227e": [137, 146], "79640555e": [137, 146], "79642552e": [137, 146], "79656491e": [268, 290], "79822421e": [137, 146], "79875319e": [268, 290], "79880509e": [268, 290], "79887915e": [137, 146], "79888725e": [268, 290], "79926023e": [137, 146], "799808": [299, 306], "79x": [110, 116], "7am": [268, 289], "7b": [101, 102, 106, 110, 112, 117, 118, 124], "7th": [268, 287, 289, 290], "7x": [5, 6, 13, 259, 264], "8": [6, 12, 15, 110, 111, 112, 113, 114, 116, 117, 118, 124, 128, 133, 136, 137, 139, 143, 144, 146, 147, 153, 168, 183, 187, 199, 204, 224, 226, 229, 230, 237, 259, 262, 263, 268, 289, 290, 291, 297, 305, 330, 331, 333, 337, 338, 340, 343, 344, 356, 358, 363, 364, 368, 369], "80": [110, 116, 128, 130, 329, 331, 336, 338, 342, 344, 349, 351], "800": [110, 112], "8000": [0, 4, 11, 12, 16, 101, 102, 108, 110, 114, 118, 121, 122, 123, 147, 150, 164, 167, 171, 174, 218, 222, 316, 320, 328], "80058852e": [268, 290], "80134752e": [268, 290], "80346647e": [268, 290], "80356482e": [268, 290], "80584863e": [268, 290], "80600139e": [268, 290], "80705532e": [137, 146], "80770779e": [268, 290], "80772168e": [268, 290], "80778313e": [268, 290], "80779386e": [268, 290], "8080": [137, 144, 155, 158], "80813038e": [137, 146], "80910456e": [137, 146], "80974126e": [137, 146], "80b": [110, 112, 118, 124], "81101489e": [268, 290], "81141561e": [137, 146], "81150190e": [268, 290], "81153491e": [268, 290], "81209888e": [268, 290], "81231391e": [137, 146], "81385726e": [268, 290], "81385851e": [137, 146], "81416702e": [137, 146], "8147535920143127": 13, "814754": 13, "81557357e": [137, 146], "81605020e": [268, 290], "81611199e": [268, 290], "81677291e": [268, 290], "81750199e": [268, 290], "81875241e": [137, 146], "8192": [101, 102, 108, 118, 121, 122], "81969379e": [268, 290], "81974494e": [268, 290], "81992236e": [268, 290], "82": [128, 130], "82090014e": [137, 146], "82237032e": [268, 290], "82248098e": [268, 290], "82329589e": [137, 146], "82377563e": [268, 290], "82394725e": [137, 146], "82457250e": [137, 146], "82634446e": [268, 290], "8265": [1, 155, 158, 169, 170], "82662451e": [137, 146], "82670507e": [268, 290], "82704625e": [268, 290], "82709087e": [268, 290], "82714777e": [268, 290], "82736081e": [268, 290], "82819171e": [268, 290], "82860744e": [137, 146], "82876396e": [268, 290], "82900697e": [268, 290], "82929088e": [268, 290], "82934299e": [268, 290], "82964134e": [137, 146], "82985464e": [268, 290], "83": [13, 128, 130], "83045536e": [268, 290], "83105981e": [137, 146], "83186028e": [268, 290], "83414386e": [268, 290], "83462293e": [268, 290], "83483610e": [268, 290], "83486040e": [268, 290], "83672294e": [137, 146], "837": [110, 116], "83878148e": [137, 146], "83987024e": [268, 290], "84": [137, 146], "84016372e": [268, 290], "84025085e": [268, 290], "84072098e": [268, 290], "84077434e": [268, 290], "84100178e": [268, 290], "84181255e": [137, 146], "84200376e": [268, 290], "84204574e": [268, 290], "84257627e": [268, 290], "84342591e": [268, 290], "84438897e": [268, 290], "84440348e": [137, 146], "84560393e": [268, 290], "84609653e": [268, 290], "84649059e": [268, 290], "84714490e": [137, 146], "84724203e": [268, 290], "84787306e": [268, 290], "84826868e": [268, 290], "84889567e": [137, 146], "85": [118, 121, 128, 130], "85011083e": [268, 290], "85030222e": [268, 290], "85076341e": [137, 146], "85247302e": [137, 146], "85358205e": [137, 146], "85489166e": [137, 146], "85491417e": [268, 290], "85511327e": [137, 146], "85630648e": [268, 290], "85643661e": [137, 146], "85674404e": [268, 290], "85712914e": [268, 290], "85737485e": [268, 290], "8588": [137, 144], "858816514880031760": [137, 144], "86": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 118, 123], "86025219e": [268, 290], "86067504e": [268, 290], "86104554e": [137, 146], "86110076e": [268, 290], "86166140e": [268, 290], "86302094e": [268, 290], "86359452e": [268, 290], "86366066e": [137, 146], "86417520e": [137, 146], "86425143e": [137, 146], "86543170e": [268, 290], "86662281e": [137, 146], "86763499e": [268, 290], "86765429e": [137, 146], "87054926e": [137, 146], "87068461e": [268, 290], "87136489e": [268, 290], "87145798e": [268, 290], "87155861e": [268, 290], "87222108e": [268, 290], "87272584e": [268, 290], "87305135e": [268, 290], "87395632e": [268, 290], "87472761e": [137, 146], "87498319e": [137, 146], "87503409e": [268, 290], "87604594e": [137, 146], "87622940e": [268, 290], "87672335e": [137, 146], "87687856e": [268, 290], "87722724e": [268, 290], "87785244e": [137, 146], "87823978e": [268, 290], "87826851e": [268, 290], "87948608e": [137, 146], "88": [128, 130], "88060308e": [268, 290], "88125470e": [268, 290], "88136353e": [268, 290], "88193573e": [268, 290], "88231084e": [268, 290], "88282757e": [268, 290], "88321698e": [137, 146], "88339034e": [268, 290], "88483773e": [268, 290], "88547347e": [268, 290], "88624009e": [268, 290], "88648832e": [137, 146], "88702995e": [137, 146], "88834351e": [268, 290], "88834363e": [137, 146], "88852262e": [268, 290], "88920692e": [137, 146], "89050034e": [268, 290], "89084566e": [137, 146], "89146360e": [268, 290], "89203757e": [137, 146], "89227986e": [268, 290], "89326443e": [268, 290], "89382686e": [137, 146], "89384127e": [268, 290], "89393270e": [268, 290], "89414667e": [268, 290], "89481646e": [137, 146], "89499091e": [137, 146], "895000": [137, 144], "89564747e": [268, 290], "89572328e": [137, 146], "89613281e": [268, 290], "89739510e": [268, 290], "89740060e": [268, 290], "89891556e": [268, 290], "89910729e": [268, 290], "89954895e": [137, 146], "8b": [101, 102, 108, 118, 121, 124], "8cpu": [14, 128, 136, 137, 139, 146], "8gb": [43, 50, 53, 62], "8k": [101, 102, 105, 118, 124], "8m20": [137, 146], "8m25": [137, 146], "8m30": [137, 146], "8th": [268, 287, 289], "8xl40": [110, 116], "9": [3, 7, 12, 14, 15, 28, 29, 35, 37, 43, 44, 46, 47, 55, 58, 59, 68, 75, 128, 136, 137, 146, 147, 153, 159, 163, 168, 181, 190, 224, 226, 227, 238, 242, 253, 255, 256, 268, 290, 299, 304, 331, 335, 357, 363, 364], "90": [43, 44, 118, 122, 159, 163, 268, 289], "90151963e": [268, 290], "90165711e": [268, 290], "90290013e": [268, 290], "90306157e": [137, 146], "90392175e": [268, 290], "90476887e": [268, 290], "90511677e": [268, 290], "90528610e": [268, 290], "90639007e": [137, 146], "90640": 14, "90656148e": [268, 290], "90659517e": [268, 290], "90668219e": [268, 290], "90754700e": [137, 146], "90833239e": [268, 290], "91": [13, 128, 130], "91010717e": [268, 290], "91011913e": [268, 290], "91103810e": [137, 146], "91166097e": [137, 146], "91205712e": [268, 290], "91232206e": [137, 146], "91410846e": [137, 146], "91437262e": [268, 290], "91462375e": [268, 290], "91497517e": [137, 146], "91614306e": [137, 146], "91614705e": [268, 290], "91802080e": [268, 290], "91829026e": [137, 146], "91938744e": [268, 290], "92105675e": [137, 146], "92106171e": [268, 290], "92230538e": [268, 290], "92250460e": [268, 290], "92267558e": [268, 290], "92389667e": [137, 146], "92452052e": [268, 290], "92482489e": [137, 146], "92608377e": [268, 290], "92633970e": [268, 290], "92644155e": [137, 146], "92704949e": [268, 290], "92810488e": [137, 146], "92848936e": [268, 290], "92968845e": [268, 290], "92msuccessfulli": [128, 129, 137, 138, 147, 148], "92mview": [128, 129, 137, 138, 147, 148], "931": [137, 144], "93108886e": [268, 290], "93218452e": [268, 290], "93256009e": [268, 290], "93267390e": [268, 290], "93274853e": [268, 290], "93358018e": [268, 290], "93396618e": [268, 290], "93402106e": [137, 146], "93505753e": [268, 290], "93599756e": [268, 290], "93615": 12, "93653901e": [268, 290], "93655512e": [268, 290], "93691164e": [137, 146], "93784940e": [137, 146], "93872128e": [268, 290], "93877090e": [268, 290], "93991698e": [268, 290], "94008095e": [268, 290], "94048835e": [268, 290], "94161111e": [137, 146], "94202918e": [268, 290], "942508": 13, "9425080418586731": 13, "943": [342, 344], "94304550e": [137, 146], "94449548e": [268, 290], "94474315e": [268, 290], "94507216e": [268, 290], "94511395e": [268, 290], "94528699e": [137, 146], "94559568e": [268, 290], "9478": [126, 127], "94806529e": [268, 290], "94919703e": [268, 290], "94921815e": [268, 290], "94926137e": [268, 290], "949393": [299, 306], "94980036e": [268, 290], "94988877e": [137, 146], "94998607e": [137, 146], "94999364e": [137, 146], "95": [128, 130], "950654": 13, "9506543874740601": 13, "95128240e": [268, 290], "95235109e": [137, 146], "95309842e": [268, 290], "95409912e": [268, 290], "95493992e": [268, 290], "95530374e": [268, 290], "95612733e": [268, 290], "95642184e": [268, 290], "95678936e": [268, 290], "95717700e": [268, 290], "95755831e": [268, 290], "95777296e": [268, 290], "95807119e": [268, 290], "9590334296226501": [316, 317, 319, 320, 328], "959243851260": [28, 30], "96": 15, "96016713e": [268, 290], "96020412e": [137, 146], "96078059e": [268, 290], "96112816e": [268, 290], "96223371e": [268, 290], "96230914e": [268, 290], "96423475e": [268, 290], "96519734e": [268, 290], "96568063e": [268, 290], "96603352e": [137, 146], "96672040e": [268, 290], "96707787e": [268, 290], "96853566e": [137, 146], "96858853e": [137, 146], "96917129e": [268, 290], "96927994e": [137, 146], "96963590e": [137, 146], "97122377e": [268, 290], "97128675e": [268, 290], "97161290e": [268, 290], "97195402e": [137, 146], "97206768e": [137, 146], "97234564e": [268, 290], "97312343e": [268, 290], "97332066e": [137, 146], "97360516e": [137, 146], "97468323e": [268, 290], "97478577e": [268, 290], "97603959e": [268, 290], "97606084e": [268, 290], "97651729e": [268, 290], "97699012e": [268, 290], "97720259e": [137, 146], "97741865e": [268, 290], "97746691e": [137, 146], "97773625e": [268, 290], "97781667e": [137, 146], "97899076e": [268, 290], "97899440e": [268, 290], "97910169e": [268, 290], "97926176e": [268, 290], "97952076e": [268, 290], "97973699e": [268, 290], "98": [15, 137, 146], "98070179e": [268, 290], "98125350e": [137, 146], "98221380e": [268, 290], "98229402e": [137, 146], "98234466e": [268, 290], "98250581e": [268, 290], "98300147e": [137, 146], "98440845e": [268, 290], "98496": 14, "98530062e": [268, 290], "98561847e": [137, 146], "98645657e": [268, 290], "98712543e": [268, 290], "988": 15, "98850322e": [137, 146], "99006638e": [268, 290], "99074054e": [268, 290], "99094741e": [268, 290], "99195677e": [137, 146], "99340957e": [268, 290], "99445761e": [268, 290], "99487356e": [268, 290], "99530393e": [268, 290], "99537045e": [268, 290], "99769610e": [137, 146], "999": [336, 338], "99955577e": [268, 290], "9996353387832642": [316, 317, 319, 320, 328], "9998507499694824": [316, 320, 328], "9m10": [137, 146], "9m15": [137, 146], "A": [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 24, 26, 35, 37, 41, 53, 62, 82, 83, 84, 85, 96, 98, 101, 102, 103, 105, 106, 110, 111, 112, 118, 119, 121, 124, 128, 129, 137, 138, 139, 147, 148, 168, 171, 172, 174, 175, 178, 181, 183, 187, 190, 198, 199, 201, 206, 207, 210, 218, 219, 221, 224, 227, 234, 236, 253, 254, 257, 259, 260, 263, 268, 274, 275, 276, 277, 287, 288, 289, 291, 297, 298, 299, 301, 329, 332, 335, 336, 337, 339, 341, 342, 344, 345, 346, 348, 349, 351, 355, 356, 359], "AND": [92, 93], "And": [128, 129, 137, 138, 147, 148, 268, 289, 291, 294], "As": [1, 15, 82, 83, 90, 91, 118, 121, 128, 130, 159, 163, 169, 170, 291, 294, 298, 355, 361], "At": [2, 3, 5, 13, 17, 22, 164, 167, 175, 177, 181, 185, 268, 289, 291, 298, 329, 330, 336, 337, 362, 363], "Be": [268, 289], "But": [7, 14, 128, 130, 137, 139, 146, 253, 256, 268, 289, 291, 294, 297, 298], "By": [3, 8, 10, 90, 91, 100, 164, 167, 181, 187, 191, 195, 206, 210, 212, 224, 225, 226, 234, 349, 352, 355, 356, 362, 363, 364, 365, 369, 371], "For": [1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 15, 17, 18, 20, 24, 25, 26, 27, 43, 47, 53, 59, 66, 69, 75, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 106, 110, 113, 114, 115, 118, 121, 122, 123, 124, 155, 158, 164, 166, 167, 169, 170, 171, 174, 175, 177, 181, 186, 188, 191, 197, 198, 202, 203, 204, 205, 206, 213, 215, 218, 223, 224, 234, 238, 239, 268, 284, 289, 291, 292, 293, 298, 299, 300, 302, 304, 324, 336, 337, 342, 343, 349, 353, 362, 363], "IN": [164, 167], "IT": [268, 289], "If": [1, 2, 3, 4, 5, 7, 8, 9, 14, 28, 29, 35, 42, 43, 44, 50, 53, 55, 62, 63, 66, 68, 71, 78, 82, 83, 84, 85, 86, 87, 88, 89, 100, 118, 121, 128, 129, 137, 138, 147, 148, 169, 170, 171, 172, 175, 179, 181, 186, 187, 191, 193, 196, 197, 198, 203, 224, 230, 233, 245, 247, 249, 250, 253, 257, 268, 280, 283, 289, 290, 291, 294, 296, 297, 298, 299, 305, 329, 335, 342, 346, 348, 355, 359, 362, 363, 369, 370], "In": [3, 4, 5, 9, 10, 11, 15, 24, 27, 28, 30, 43, 45, 53, 56, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 96, 98, 100, 101, 102, 109, 110, 112, 118, 120, 121, 159, 161, 171, 174, 181, 184, 187, 188, 196, 197, 198, 204, 206, 211, 212, 213, 215, 218, 223, 224, 225, 226, 229, 234, 236, 238, 239, 242, 245, 246, 252, 267, 268, 277, 283, 285, 287, 288, 289, 290, 291, 294, 295, 296, 297, 298, 299, 304, 309, 312, 315, 316, 323, 325, 326, 327, 328, 329, 335, 336, 337, 341, 342, 348, 349, 350, 352, 355, 356, 359, 362, 363], "It": [0, 4, 5, 6, 7, 8, 9, 10, 15, 16, 17, 19, 22, 28, 29, 43, 44, 47, 53, 54, 59, 66, 67, 75, 82, 83, 84, 85, 96, 97, 110, 112, 118, 122, 137, 138, 164, 167, 171, 174, 176, 191, 196, 198, 200, 201, 206, 209, 212, 213, 224, 228, 229, 232, 253, 257, 259, 262, 268, 280, 283, 289, 290, 291, 294, 298, 299, 301, 304, 305, 306, 309, 316, 325, 336, 339, 342, 345, 349, 350, 355, 358], "Its": [8, 191, 196, 291, 298], "NOT": [268, 289], "No": [0, 9, 24, 26, 110, 117, 128, 130, 155, 158, 198, 203, 238, 240, 268, 289, 329, 335, 336, 337, 341, 342, 343, 362, 369, 371], "Not": [3, 5, 6, 17, 18, 181, 190, 245, 251, 259, 260, 262, 268, 274, 287, 289, 362, 371], "OR": [35, 38, 66, 69], "Of": [291, 297], "On": [13, 17, 20, 24, 27, 66, 69, 84, 85, 94, 95, 101, 102, 105, 110, 116, 342, 346, 362, 363], "One": [82, 83, 96, 98, 118, 121, 291, 294, 298, 362, 368], "Or": [0, 86, 87, 101, 110, 113, 118, 121], "Such": [291, 298], "THE": [268, 289, 291, 297], "TO": [268, 289], "That": [3, 100, 181, 183, 268, 289, 291, 297, 298], "The": [0, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 17, 20, 22, 23, 24, 25, 26, 27, 28, 31, 35, 39, 40, 43, 46, 47, 53, 58, 59, 63, 66, 69, 75, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 99, 100, 105, 106, 107, 108, 110, 113, 115, 116, 118, 121, 122, 123, 126, 127, 128, 130, 131, 134, 147, 152, 155, 157, 158, 159, 163, 164, 166, 167, 168, 171, 173, 174, 175, 178, 179, 181, 184, 190, 195, 197, 198, 201, 202, 204, 206, 211, 212, 214, 224, 225, 227, 229, 230, 234, 235, 236, 237, 238, 239, 240, 242, 243, 245, 247, 250, 253, 255, 257, 258, 259, 263, 267, 268, 280, 283, 285, 289, 290, 291, 294, 297, 298, 299, 301, 303, 304, 305, 306, 329, 330, 336, 337, 341, 342, 343, 344, 345, 346, 348, 349, 350, 351, 352, 355, 358, 361, 362, 363, 371], "Their": [268, 289], "Then": [0, 1, 8, 11, 28, 31, 35, 40, 43, 47, 53, 57, 59, 66, 75, 169, 170, 191, 196, 218, 222, 268, 287, 289, 290, 291, 297, 342, 348], "There": [9, 10, 15, 90, 91, 198, 201, 203, 204, 206, 215, 268, 289, 291, 297, 298, 309, 315, 316, 323, 325, 327, 328], "These": [0, 3, 4, 8, 10, 15, 17, 22, 35, 39, 80, 81, 84, 85, 88, 89, 101, 102, 105, 110, 112, 155, 157, 171, 173, 181, 183, 187, 191, 195, 206, 211, 224, 226, 235, 284, 291, 292, 298, 300, 324, 342, 344], "To": [1, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 22, 25, 28, 30, 43, 45, 53, 56, 82, 83, 86, 87, 88, 89, 100, 101, 102, 108, 126, 127, 128, 133, 137, 139, 147, 150, 155, 157, 158, 164, 166, 167, 169, 170, 171, 174, 181, 184, 188, 191, 194, 198, 202, 204, 206, 211, 212, 213, 214, 215, 218, 223, 224, 234, 236, 238, 243, 245, 247, 248, 253, 257, 268, 274, 277, 287, 288, 289, 299, 302, 305, 342, 348, 362, 365, 370], "With": [1, 5, 16, 96, 99, 101, 102, 105, 118, 124, 159, 163, 169, 170, 224, 225, 237, 238, 239, 244, 245, 246, 249, 252, 291, 293, 298, 309, 316, 325, 342, 343, 349, 350, 362, 363], "_": [3, 7, 14, 16, 88, 89, 118, 121, 137, 143, 181, 185, 253, 257, 268, 289, 329, 330, 332, 335, 336, 337, 338, 342, 343, 355, 356, 362, 363, 371], "_2": [329, 330, 336, 337], "__call__": [4, 10, 11, 12, 15, 16, 128, 131, 137, 146, 164, 167, 171, 174, 206, 213, 218, 222, 267, 268, 277, 285, 288, 349, 353, 355, 361, 362, 371], "__file__": [86, 87], "__getitem__": [6, 259, 262, 355, 357, 362, 365], "__index_level_0__": [349, 352], "__init__": [3, 4, 6, 10, 11, 12, 15, 16, 86, 87, 128, 131, 137, 139, 140, 146, 147, 149, 150, 171, 174, 181, 190, 206, 213, 218, 222, 224, 237, 259, 262, 268, 277, 288, 313, 314, 315, 316, 327, 329, 332, 336, 339, 342, 345, 349, 353, 355, 357, 358, 361, 362, 365, 371], "__len__": [6, 259, 262, 355, 357, 362, 365], "__main__": [88, 89, 159, 163], "__name__": [88, 89, 159, 163], "_arrow_table_from_shard": [349, 352], "_build": 0, "_class_nam": [6, 259, 262], "_config": 0, "_diffusers_vers": [6, 259, 262], "_dmat_from_arrow": [349, 352], "_k": [336, 337], "_model": [4, 12, 171, 174], "_sample_timestep": [6, 259, 262], "_shared_step": [329, 332, 336, 339], "_static": 0, "_toc": 0, "a10": [362, 363], "a100": [110, 113, 114, 116, 362, 363], "a10g": [336, 337, 340, 341], "a_random_job_nam": [35, 41, 53, 62], "abandon": [291, 298], "abil": [3, 5, 6, 80, 81, 84, 85, 159, 161, 181, 188, 224, 225, 259, 261, 291, 294], "abl": [5, 8, 9, 28, 29, 43, 44, 53, 55, 63, 66, 68, 155, 158, 191, 196, 198, 204, 316, 320, 328], "abortmultipartupload": [17, 22], "about": [5, 7, 12, 13, 14, 16, 24, 26, 28, 34, 84, 85, 86, 87, 88, 89, 101, 102, 106, 110, 115, 118, 121, 123, 126, 127, 159, 161, 163, 176, 177, 182, 184, 187, 188, 189, 224, 226, 228, 236, 253, 257, 267, 268, 285, 289, 291, 293, 294, 297, 298, 299, 301, 309, 316, 325, 362, 363], "abov": [3, 5, 6, 101, 102, 106, 107, 118, 121, 128, 129, 131, 137, 138, 147, 148, 155, 158, 159, 163, 164, 167, 181, 187, 224, 225, 226, 227, 234, 236, 259, 263, 268, 289, 291, 298, 355, 359], "absenc": [164, 167, 268, 289, 349, 354], "absent": [291, 298], "absolut": [5, 355, 357], "abspath": [128, 129, 137, 138, 147, 148], "abstract": [4, 8, 15, 17, 19, 100, 101, 102, 106, 107, 110, 113, 126, 127, 171, 173, 191, 192, 336, 337, 349, 350, 355, 361], "absurd": [291, 298], "academ": [118, 124, 355, 356], "academi": [291, 298], "acc": [3, 13, 181, 190, 349, 352], "acc_metr": [362, 367], "acceler": [1, 2, 3, 6, 8, 10, 11, 28, 34, 66, 71, 84, 85, 100, 126, 127, 128, 132, 169, 170, 175, 177, 181, 187, 191, 192, 206, 213, 218, 220, 259, 262, 263, 329, 333, 336, 340], "accelerator_shap": [12, 13, 14], "accelerator_typ": [10, 12, 13, 14, 101, 102, 108, 110, 113, 116, 118, 121, 122, 123, 128, 131, 137, 139, 143, 146, 147, 149, 206, 213, 315, 316, 327], "accept": [1, 2, 7, 10, 11, 14, 15, 169, 170, 175, 180, 206, 212, 213, 218, 220, 224, 227, 237, 253, 257, 309, 315, 316, 325, 327, 355, 358], "access": [3, 8, 17, 20, 21, 22, 27, 53, 63, 82, 83, 86, 87, 88, 89, 94, 95, 96, 97, 98, 100, 110, 111, 113, 114, 118, 119, 121, 122, 123, 126, 127, 128, 135, 147, 151, 155, 158, 159, 163, 164, 167, 168, 181, 183, 190, 191, 192, 224, 229, 234, 236, 238, 244, 299, 304, 306, 309, 316, 325, 349, 351, 362, 363, 364], "accident": [349, 352], "acclaim": [291, 298], "accomplish": [245, 252], "accord": [2, 9, 10, 15, 84, 85, 175, 180, 198, 204, 206, 215, 224, 235, 238, 244], "accordingli": [118, 124, 299, 304, 336, 341], "account": [3, 8, 17, 21, 22, 24, 26, 28, 29, 30, 35, 36, 39, 43, 44, 45, 53, 55, 56, 66, 68, 70, 78, 100, 126, 127, 181, 190, 191, 196, 291, 294], "account_id": [17, 22], "accross": [101, 102, 105, 106], "accumul": [5, 13, 224, 228], "accur": [101, 102, 105, 355, 356], "accuraci": [7, 10, 13, 14, 15, 110, 112, 118, 124, 137, 146, 206, 215, 253, 256, 299, 301, 303, 305, 349, 350, 352, 353, 354, 362, 367, 371], "accuracy_scor": [349, 351, 352], "achiev": [3, 5, 6, 9, 10, 11, 15, 101, 102, 105, 137, 146, 181, 184, 189, 198, 204, 206, 213, 215, 218, 220, 224, 225, 259, 261], "acid": [8, 191, 192], "acl": [96, 98], "across": [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 17, 19, 22, 24, 26, 79, 84, 85, 86, 87, 88, 89, 96, 99, 101, 102, 106, 107, 110, 113, 116, 117, 118, 121, 124, 126, 127, 128, 130, 131, 134, 137, 146, 159, 161, 163, 168, 169, 170, 171, 174, 175, 177, 181, 183, 186, 188, 191, 192, 195, 198, 204, 205, 206, 208, 210, 214, 215, 217, 224, 225, 226, 227, 228, 229, 231, 234, 235, 236, 238, 239, 242, 243, 244, 245, 252, 259, 263, 291, 295, 298, 299, 301, 305, 306, 309, 316, 325, 329, 330, 331, 335, 336, 337, 338, 341, 342, 343, 344, 346, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 359, 361, 362, 363, 367, 369, 371], "act": [17, 22, 224, 229, 291, 298], "act_dim": [336, 339, 341], "act_fn": [6, 259, 262], "action": [16, 17, 22, 118, 121, 123, 159, 163, 291, 297, 298, 338, 339], "action_spac": [336, 338], "activ": [0, 2, 3, 7, 9, 10, 14, 80, 81, 90, 91, 101, 102, 106, 168, 175, 180, 181, 190, 198, 204, 206, 212, 253, 257, 342, 344], "actor": [5, 6, 8, 9, 11, 13, 16, 101, 102, 106, 128, 132, 159, 161, 183, 187, 191, 196, 198, 201, 207, 218, 220, 221, 227, 245, 252, 259, 263, 267, 268, 277, 285, 288, 349, 353, 354, 355, 356, 361, 362, 371], "actorpoolmapoper": [10, 206, 214], "actorpoolstrategi": [349, 351, 353, 354], "actress": [291, 294], "actual": [2, 7, 10, 14, 15, 28, 30, 35, 38, 43, 45, 48, 53, 56, 60, 66, 69, 72, 76, 118, 123, 128, 130, 175, 179, 206, 213, 224, 234, 253, 257, 268, 289, 291, 298, 336, 338, 342, 343], "ad": [6, 16, 17, 22, 28, 30, 34, 84, 85, 94, 95, 100, 128, 129, 137, 138, 146, 147, 148, 238, 239, 245, 252, 259, 263, 268, 280, 289, 336, 337, 349, 352, 353], "adam": [5, 7, 13, 14, 137, 143, 224, 226, 228, 238, 240, 245, 247, 253, 254, 256, 257, 329, 332, 336, 339, 342, 346, 355, 359, 362, 367], "adamw": [6, 259, 262], "adapt": [101, 102, 105, 119, 120, 125, 224, 226, 231, 299, 305, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "adapter_config": [118, 121], "adapter_model": [118, 121], "adapter_nam": [118, 121], "adaptiveavgpool2d": 13, "add": [2, 3, 5, 6, 8, 9, 10, 11, 13, 16, 17, 22, 35, 39, 43, 46, 48, 51, 53, 58, 60, 63, 64, 66, 69, 73, 76, 80, 81, 82, 83, 84, 85, 92, 93, 101, 102, 107, 118, 121, 123, 126, 127, 128, 130, 133, 136, 137, 139, 143, 146, 147, 153, 159, 163, 164, 166, 175, 178, 179, 180, 181, 183, 184, 186, 190, 191, 192, 198, 202, 203, 206, 212, 218, 222, 224, 226, 227, 229, 232, 237, 245, 252, 259, 262, 263, 284, 291, 292, 295, 300, 324, 329, 330, 335, 336, 338, 341, 342, 348, 349, 354, 355, 359, 361, 362, 371], "add_class": [128, 130, 137, 139, 146], "add_label": [10, 15, 206, 212, 215], "add_nois": [6, 259, 262], "add_ref": [3, 181, 188], "add_subplot": [5, 224, 226, 237], "addit": [5, 9, 17, 20, 80, 81, 82, 83, 84, 85, 88, 89, 100, 118, 121, 125, 159, 162, 198, 202, 224, 231, 245, 246, 247, 309, 312, 316, 325, 326, 342, 343, 348, 349, 354, 362, 371], "addition": [9, 10, 16, 17, 22, 28, 34, 126, 127, 159, 163, 198, 204, 206, 212, 299, 305, 342, 343], "addr": [329, 333], "address": [2, 8, 17, 22, 28, 30, 43, 45, 53, 56, 126, 127, 137, 138, 155, 158, 175, 177, 191, 192, 224, 235, 268, 289], "adebayor": [268, 289], "adher": [291, 298], "adjust": [6, 9, 118, 124, 198, 203, 224, 227, 259, 262, 268, 280, 289, 291, 297, 299, 306, 349, 352, 362, 371], "adjust_total_amount": [9, 198, 202], "adjusted_data": [9, 198, 203, 205], "adjusted_data_rai": [9, 198, 203, 205], "adjusted_total_amount": [9, 164, 166, 198, 202], "admin": [17, 18, 22, 96, 98, 126, 127], "administr": [291, 298, 372], "admittedli": [291, 294], "adopt": [9, 198, 201], "adv": [268, 287, 289], "advanc": [6, 7, 8, 14, 24, 26, 27, 28, 34, 84, 85, 101, 102, 109, 111, 117, 121, 164, 167, 191, 192, 196, 253, 256, 259, 262, 362, 371, 372], "advantag": [8, 110, 112, 191, 196], "adventureland": [268, 289], "adversari": [329, 330], "affect": [11, 218, 221], "affin": [13, 137, 140], "afford": [110, 112, 268, 289], "after": [1, 2, 3, 4, 5, 6, 11, 13, 16, 43, 47, 50, 53, 57, 59, 62, 66, 75, 82, 83, 84, 85, 88, 89, 101, 102, 105, 128, 129, 137, 138, 146, 147, 148, 159, 163, 164, 167, 168, 169, 170, 171, 174, 175, 180, 181, 183, 189, 218, 223, 224, 226, 228, 234, 237, 259, 262, 267, 268, 285, 289, 290, 291, 297, 299, 301, 306, 329, 330, 335, 336, 337, 342, 343, 344, 346, 348, 349, 351, 354, 355, 357, 359, 360, 362, 363, 367, 369], "afterward": [362, 363], "again": [3, 4, 9, 28, 33, 84, 85, 90, 91, 128, 130, 171, 172, 181, 187, 198, 202, 291, 298, 329, 334, 342, 347, 349, 350, 362, 370], "against": [110, 115, 118, 121, 245, 249, 268, 287, 289, 291, 297, 298, 336, 341, 342, 343, 346, 348, 355, 361, 362, 369], "agent": [101, 102, 105, 118, 124, 336, 337, 341], "aggreg": [8, 13, 137, 139, 146, 191, 195, 199, 207, 211, 224, 225, 329, 332, 333, 349, 353, 354, 362, 367], "aggregate_metr": [137, 146], "aggress": [10, 164, 167, 206, 212], "agil": [336, 337], "agnost": [147, 151, 355, 357], "ago": [291, 294], "agre": [291, 297], "aguero": [268, 289], "ahead": [110, 117, 118, 125, 291, 297], "ai": [1, 3, 9, 10, 11, 15, 16, 86, 87, 90, 91, 94, 95, 100, 101, 102, 108, 110, 117, 118, 120, 121, 123, 169, 170, 181, 190, 198, 205, 206, 210, 212, 213, 215, 218, 222], "air": 12, "airflow": [147, 154], "aj": [268, 287, 289], "ak": [17, 20], "ako": [268, 289], "akz9ul28": [147, 153], "alaska": [291, 297], "alb": [24, 27], "alberta": [291, 297], "album": [268, 289], "alciato": [268, 287, 289], "ald": [268, 289], "alert": [92, 93, 128, 135, 147, 153, 165, 245, 252], "algorithm": [7, 12, 13, 14, 24, 27, 100, 101, 102, 107, 253, 257, 299, 306, 349, 350], "alic": 168, "align": [8, 191, 195, 224, 226, 329, 333, 362, 364, 367], "alik": [267, 268, 285], "all": [0, 1, 3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 17, 20, 22, 24, 26, 28, 29, 35, 36, 38, 43, 44, 49, 50, 51, 53, 54, 61, 62, 64, 66, 67, 69, 71, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 96, 98, 100, 101, 102, 103, 104, 105, 110, 111, 118, 119, 121, 122, 125, 126, 127, 128, 129, 130, 134, 135, 136, 137, 138, 140, 146, 147, 148, 154, 155, 156, 158, 159, 160, 163, 164, 165, 168, 169, 170, 171, 172, 173, 176, 181, 182, 183, 188, 189, 191, 192, 199, 201, 205, 207, 212, 224, 225, 226, 228, 229, 233, 234, 236, 238, 244, 245, 248, 249, 250, 253, 254, 257, 259, 260, 263, 267, 268, 277, 284, 285, 288, 289, 291, 292, 293, 294, 297, 298, 299, 300, 301, 305, 309, 316, 324, 325, 329, 333, 336, 338, 341, 342, 343, 344, 346, 348, 349, 351, 352, 353, 354, 356, 357, 362, 363, 366, 367, 369, 371], "all_fil": [86, 87], "all_results_at_onc": [3, 181, 189], "alleg": [291, 294], "allegori": [291, 298], "alli": [291, 297], "allobjectact": [17, 22], "alloc": [7, 8, 10, 14, 17, 22, 24, 25, 28, 30, 43, 45, 53, 56, 182, 191, 197, 206, 208, 212, 224, 230, 237, 253, 257, 267, 268, 285, 349, 352, 355, 356], "allow": [3, 4, 5, 6, 8, 15, 16, 17, 22, 53, 63, 80, 81, 84, 85, 88, 89, 94, 95, 96, 98, 110, 116, 118, 120, 121, 128, 135, 137, 144, 147, 151, 164, 167, 171, 173, 174, 181, 183, 187, 191, 197, 224, 225, 230, 231, 234, 238, 244, 245, 248, 250, 259, 261, 267, 268, 285, 289, 291, 298, 299, 301, 304, 305, 306, 309, 313, 314, 315, 316, 325, 327, 329, 333, 342, 347, 349, 352, 355, 359, 361, 362, 363], "allowedhead": [17, 22, 53, 63], "allowedmethod": [17, 22, 53, 63], "allowedorigin": [17, 22, 53, 63], "allreduc": [224, 225], "alltoallapi": [10, 206, 215], "allus": [291, 298], "almost": [291, 298, 349, 351], "alon": [9, 10, 15, 198, 204, 206, 215], "along": [84, 85, 224, 233, 268, 289, 362, 363], "alongsid": [245, 246, 252, 349, 351, 362, 371], "alonso": [268, 289], "alphas_cumprod": [6, 259, 262], "alreadi": [1, 9, 10, 11, 15, 43, 46, 53, 58, 82, 83, 86, 87, 100, 159, 163, 169, 170, 198, 201, 206, 215, 218, 220, 224, 226, 228, 237, 245, 250, 268, 274, 287, 289, 342, 344, 348, 349, 353, 355, 357, 362, 370], "also": [3, 5, 9, 10, 11, 13, 15, 16, 28, 29, 35, 37, 43, 44, 50, 53, 55, 62, 66, 68, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 96, 97, 101, 102, 105, 126, 127, 128, 130, 132, 133, 135, 137, 144, 147, 154, 159, 163, 164, 165, 167, 181, 184, 198, 205, 206, 215, 218, 222, 224, 226, 236, 245, 248, 268, 274, 280, 283, 287, 289, 290, 291, 293, 294, 297, 298, 309, 312, 316, 325, 326, 329, 333, 342, 344, 345, 346, 349, 351, 355, 359], "altern": [1, 28, 29, 43, 44, 53, 55, 66, 68, 126, 127, 169, 170, 299, 304, 349, 354], "alwai": [0, 3, 118, 121, 122, 128, 133, 159, 162, 181, 187, 349, 354, 355, 357], "am": [291, 294], "amaz": [118, 125, 268, 289, 291, 297], "amazon": [8, 17, 20, 28, 29, 43, 44, 53, 55, 86, 87, 88, 89, 191, 192, 268, 289], "amazonaw": [13, 14, 17, 22, 128, 136, 147, 150, 153], "amazonelasticfilesystemclientreadwriteaccess": [53, 57, 63], "ambush": [291, 297], "america": [291, 294], "american": [291, 294, 297, 298], "ami": [24, 26, 268, 289], "amnt": [268, 289], "among": [9, 10, 15, 110, 113, 198, 204, 206, 215, 291, 297, 298], "amount": [3, 4, 5, 6, 8, 9, 10, 12, 86, 87, 171, 174, 181, 190, 191, 192, 198, 201, 206, 212, 224, 225, 259, 261, 355, 359, 361, 362, 363], "amp": [268, 289, 329, 335, 362, 371], "amus": [291, 298], "an": [0, 2, 5, 8, 9, 10, 13, 15, 18, 21, 22, 23, 24, 27, 28, 29, 35, 41, 43, 44, 52, 55, 65, 82, 83, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 105, 107, 108, 113, 114, 116, 119, 121, 126, 127, 128, 130, 131, 134, 135, 137, 139, 147, 148, 151, 155, 158, 159, 160, 163, 164, 166, 167, 173, 175, 177, 179, 182, 184, 185, 186, 190, 191, 192, 196, 197, 198, 199, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 219, 221, 223, 224, 225, 226, 268, 274, 275, 276, 277, 280, 284, 287, 288, 289, 291, 292, 294, 296, 297, 298, 299, 300, 305, 309, 313, 314, 315, 316, 324, 325, 327, 329, 330, 335, 337, 342, 343, 349, 350, 352, 353, 354, 355, 356, 362, 363, 365, 372], "anaconda3": [137, 144], "anal": [291, 298], "analys": [291, 298], "analysi": [8, 79, 88, 89, 101, 102, 105, 118, 121, 124, 191, 195, 291, 298, 309, 313, 314, 315, 316, 320, 323, 325, 327, 328], "analyt": [8, 191, 192, 193, 195], "analyz": [8, 118, 121, 155, 157, 191, 192, 224, 236], "anatom": [291, 294], "anatomi": [268, 289], "angel": [291, 297, 298], "angelbr": [291, 298], "anger": [268, 289], "anggrek": [268, 289], "angl": [336, 337], "angular": [336, 337], "ani": [1, 2, 3, 4, 7, 9, 10, 11, 14, 15, 16, 28, 30, 35, 39, 42, 43, 45, 53, 56, 66, 78, 80, 81, 82, 83, 94, 95, 100, 118, 121, 126, 127, 128, 132, 133, 137, 138, 147, 154, 155, 158, 159, 163, 164, 166, 169, 170, 171, 174, 175, 177, 178, 181, 187, 198, 201, 206, 213, 216, 218, 219, 222, 224, 229, 238, 243, 245, 251, 253, 254, 256, 257, 268, 277, 288, 291, 293, 294, 297, 298, 313, 314, 315, 316, 320, 327, 328, 329, 330, 333, 336, 341, 342, 343, 346, 348, 349, 350, 351, 352, 354, 355, 361, 362, 363, 371], "anniversari": [268, 289], "annot": [7, 24, 27, 253, 257, 349, 353], "anon": [6, 259, 262], "anonym": [12, 164, 166], "anoth": [9, 10, 24, 27, 84, 85, 90, 91, 128, 129, 137, 138, 146, 147, 148, 198, 202, 206, 211, 268, 287, 289, 291, 295, 297, 298, 329, 335], "answer": [101, 102, 105, 268, 289, 291, 294], "ant": [268, 289], "anthoni": [291, 297, 298], "anti": [3, 176, 181, 184, 189, 268, 289, 291, 298], "antiwoman": [291, 298], "anymor": [329, 335, 342, 348], "anyon": [268, 289, 291, 294], "anyscal": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 20, 21, 27, 32, 33, 34, 37, 38, 41, 42, 46, 49, 50, 51, 52, 55, 57, 58, 61, 62, 63, 64, 65, 68, 69, 72, 77, 78, 98, 99, 103, 108, 109, 111, 112, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 133, 134, 135, 137, 138, 142, 145, 147, 148, 151, 153, 154, 155, 156, 157, 158, 163, 166, 168, 169, 170, 171, 172, 174, 175, 176, 181, 182, 191, 192, 198, 199, 201, 204, 206, 207, 210, 212, 213, 215, 218, 219, 222, 224, 225, 226, 245, 252, 253, 254, 259, 260, 264, 267, 268, 274, 285, 287, 291, 293, 299, 301, 309, 316, 325, 335, 341, 348, 354, 361, 368], "anyscale_artifact_storag": [6, 86, 87, 159, 163, 259, 262], "anyscale_cloud_id": [35, 39], "anyscale_cloud_nam": [17, 23, 28, 29, 30, 31, 32, 33, 35, 36, 39, 40, 41, 42, 43, 44, 45, 47, 50, 51, 53, 54, 56, 59, 62, 64, 66, 67, 69, 70, 75, 77, 78], "anyscale_cloud_storage_bucket": [86, 87], "anyscale_cloud_storage_bucket_region": [86, 87], "anyscale_iam_rol": [53, 57], "anyscale_iam_role_arn": [17, 23], "anyscale_iam_s3_policy_arn": [53, 57], "anyscale_registration_command": [28, 30, 35, 39, 43, 45, 53, 56, 66, 70], "anyscale_s3_bucket_nam": [28, 30, 33, 43, 45, 51, 53, 56, 64], "anyscale_security_group": [17, 22], "anyscale_vpc": [17, 22], "anyscale_vpc_nam": [17, 22], "anyscalerai": [43, 51, 53, 64], "anyscaleuserdata": [110, 115, 147, 153], "anyth": [24, 27, 86, 87, 224, 229, 291, 294, 298, 362, 370], "anywher": [3, 90, 91, 128, 135, 181, 183], "apach": 195, "aperitif": [268, 289], "api": [3, 8, 9, 10, 11, 13, 15, 16, 24, 25, 26, 27, 37, 68, 78, 90, 91, 92, 93, 96, 98, 100, 101, 102, 107, 108, 110, 113, 118, 120, 122, 123, 125, 128, 135, 137, 145, 147, 150, 151, 153, 181, 190, 191, 194, 195, 196, 197, 198, 200, 205, 206, 212, 213, 218, 220, 267, 268, 274, 277, 283, 285, 287, 288, 290, 291, 296, 309, 316, 325, 329, 333, 342, 344, 349, 354, 355, 361, 362, 363, 371], "api_kei": [101, 102, 108, 110, 114, 115, 118, 121, 122, 123], "apigatewai": [164, 167, 168], "app": [2, 4, 11, 16, 53, 63, 88, 89, 92, 93, 101, 102, 106, 108, 110, 113, 114, 115, 116, 118, 121, 122, 123, 147, 150, 153, 164, 167, 168, 171, 174, 175, 177, 218, 223, 309, 315, 316, 325, 327, 329, 335], "app1": [11, 164, 167, 218, 223], "app_build": [11, 218, 223], "appapp": [313, 314], "apparatu": [336, 341], "appear": [101, 102, 106, 268, 289, 291, 294, 298], "append": [2, 3, 10, 86, 87, 118, 123, 128, 129, 137, 138, 146, 147, 148, 175, 180, 181, 183, 189, 206, 212, 329, 331, 336, 338, 342, 346, 355, 357, 362, 364, 371], "appl": [1, 169, 170, 267, 268, 280, 285, 289, 299, 301, 304], "appli": [0, 3, 4, 5, 6, 9, 10, 11, 12, 15, 16, 28, 30, 31, 35, 39, 40, 42, 43, 45, 47, 53, 56, 59, 66, 70, 75, 78, 128, 129, 130, 131, 132, 137, 139, 146, 171, 174, 181, 190, 198, 200, 201, 202, 204, 206, 209, 213, 215, 218, 222, 224, 225, 226, 228, 232, 235, 237, 259, 262, 267, 268, 280, 283, 285, 289, 290, 291, 293, 296, 298, 299, 304, 315, 316, 327, 329, 331, 336, 337, 338, 342, 343, 344, 355, 359, 362, 363, 365], "applic": [1, 3, 4, 9, 12, 15, 16, 24, 27, 28, 34, 35, 38, 66, 69, 80, 81, 84, 85, 90, 91, 92, 93, 94, 95, 101, 102, 106, 108, 110, 112, 113, 115, 117, 118, 120, 122, 123, 125, 126, 127, 149, 151, 152, 153, 154, 155, 157, 159, 161, 162, 165, 168, 169, 170, 171, 173, 181, 185, 192, 196, 197, 198, 200, 219, 222, 223, 309, 313, 314, 315, 316, 317, 319, 320, 325, 327, 328, 342, 348], "application_log": [315, 316, 327], "approach": [3, 8, 79, 128, 130, 137, 146, 181, 184, 191, 197, 267, 268, 285, 291, 293, 298, 299, 301, 305, 329, 330, 342, 343, 344, 349, 351, 362, 363], "appropri": [43, 50, 53, 62, 101, 102, 106, 110, 113, 126, 127, 128, 135], "approv": [28, 30, 33, 35, 39, 42, 43, 45, 51, 53, 64, 66, 70, 78], "approx": [3, 181, 183, 336, 337], "approxim": [101, 102, 106, 342, 343, 344], "april": [268, 289], "ar": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 22, 24, 26, 27, 28, 29, 30, 33, 35, 38, 39, 43, 44, 45, 49, 50, 51, 53, 55, 56, 57, 61, 62, 63, 64, 66, 68, 69, 71, 78, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 96, 98, 101, 102, 106, 110, 112, 113, 115, 116, 117, 118, 121, 122, 123, 125, 128, 132, 147, 151, 153, 154, 155, 158, 159, 163, 164, 165, 166, 167, 168, 169, 170, 175, 177, 179, 180, 181, 182, 183, 187, 189, 190, 191, 192, 194, 195, 196, 197, 198, 201, 202, 203, 204, 205, 206, 210, 211, 212, 213, 215, 218, 220, 221, 224, 225, 226, 227, 228, 229, 231, 232, 234, 235, 236, 237, 238, 240, 243, 245, 246, 250, 253, 257, 258, 259, 263, 264, 267, 268, 277, 280, 284, 285, 288, 289, 291, 292, 294, 297, 298, 299, 300, 304, 309, 315, 316, 317, 319, 320, 323, 324, 325, 327, 328, 331, 335, 338, 341, 346, 348, 354, 361, 364, 367, 371], "arang": [355, 358, 361], "arbitrari": [2, 118, 122, 175, 177], "architectur": [5, 6, 11, 13, 16, 21, 24, 26, 79, 103, 109, 218, 221, 224, 227, 259, 262, 263, 267, 269, 270, 285, 301, 309, 325, 329, 335, 342, 345, 355, 356, 358, 362, 371], "archuleta": [268, 289], "area": [9, 118, 125, 198, 204], "aree": 101, "aren": [53, 63, 355, 359], "arena": [118, 124], "arg": [118, 122, 137, 140, 146, 355, 358, 362, 371], "argmax": [5, 10, 11, 13, 15, 16, 137, 140, 143, 206, 213, 218, 222, 224, 237, 299, 303, 349, 352, 353, 362, 367, 371], "args_fp": [137, 140, 146], "argu": [291, 298], "arguabl": [291, 294], "argument": [2, 5, 6, 7, 10, 11, 14, 15, 16, 90, 91, 118, 121, 123, 175, 180, 182, 184, 187, 206, 213, 218, 222, 224, 229, 231, 253, 257, 259, 263, 268, 289, 291, 298, 299, 304], "arm": [291, 297], "arm64": [1, 169, 170], "arn": [17, 22, 28, 30, 43, 45, 53, 56, 57], "around": [5, 6, 92, 93, 118, 121, 128, 135, 224, 227, 259, 263, 291, 294, 297, 298, 362, 371], "arr": [224, 237, 329, 331], "arrai": [7, 9, 10, 11, 12, 14, 16, 118, 122, 128, 130, 136, 137, 146, 198, 202, 206, 212, 218, 222, 224, 226, 237, 238, 242, 243, 253, 255, 257, 268, 289, 290], "arrang": [3, 181, 188], "array_equ": [3, 181, 183], "array_split": [342, 344], "arriv": [8, 191, 195, 336, 341], "arrow_ref": [349, 352], "arsen": [268, 289], "art": [101, 102, 107, 291, 297], "articl": 16, "artifact": [5, 12, 13, 86, 87, 126, 127, 128, 133, 137, 138, 143, 144, 147, 154, 224, 226, 234, 235, 245, 251, 299, 306, 349, 354, 362, 371], "artifact_dir": [147, 150, 349, 354], "artifact_storag": [86, 87], "artifact_storage_path": [6, 259, 262], "artifact_uri": [137, 144, 146, 147, 150], "artifacts_dir": [137, 146, 147, 149, 150], "artifici": [336, 337], "artist": [291, 294], "as_directori": [5, 6, 13, 224, 237, 245, 247, 259, 263, 329, 333, 335, 336, 340, 341, 342, 346, 348, 349, 352, 355, 359, 361, 362, 367, 371], "as_fram": [349, 351], "as_index": [342, 346, 355, 359, 362, 369], "as_tensor": [6, 137, 141, 224, 237, 259, 262], "asarrai": [329, 331], "asc": [137, 144, 147, 150], "asgi": 168, "ask": [3, 181, 190, 291, 294, 329, 333, 362, 368], "aspen": [349, 350], "assert": [3, 10, 15, 16, 181, 189, 206, 212, 329, 335, 336, 341, 349, 351], "assess": [355, 359, 361, 362, 369], "assign": [9, 10, 15, 66, 73, 94, 95, 198, 203, 206, 210, 224, 230, 235, 238, 241, 342, 343, 349, 352, 362, 363], "assist": [124, 268, 289], "associ": [8, 82, 83, 159, 161, 191, 197, 224, 236, 342, 348], "assum": [3, 17, 22, 24, 26, 155, 158, 181, 183, 224, 226, 349, 353], "assumerol": [17, 22], "astral": [1, 169, 170], "astyp": [6, 11, 218, 222, 259, 262, 336, 338, 342, 344, 349, 353, 355, 361, 362, 371], "async": [4, 11, 12, 16, 147, 150, 164, 167, 171, 174, 218, 222], "asynchron": [7, 253, 256], "asyncio": [4, 8, 171, 172, 174, 191, 197], "athen": [268, 289], "atom": [8, 147, 154, 191, 192, 268, 289], "attach": [24, 26, 27, 63, 224, 234, 237, 245, 248, 342, 346, 355, 359, 362, 367], "attempt": [10, 128, 133, 136, 137, 139, 143, 146, 206, 211, 212, 245, 247, 355, 359], "attend": [291, 298], "attent": [101, 102, 109, 291, 294, 299, 304, 355, 356, 361], "attention_head_dim": [6, 259, 262], "attribut": [329, 333, 349, 354], "audienc": [291, 297, 298, 342, 348], "audio": [268, 289], "audit": [101, 102, 107], "augment": [238, 239, 245, 252, 362, 371], "august": [268, 289], "auschwitz": [291, 298], "auth": [35, 38, 66, 69, 72, 355, 357], "authent": [17, 22, 24, 27, 36, 37, 68, 82, 83, 86, 87, 110, 114, 115], "author": [110, 114, 118, 121, 147, 153, 168], "auto": [6, 16, 24, 26, 28, 30, 33, 35, 39, 42, 43, 45, 51, 53, 64, 66, 70, 78, 92, 93, 118, 123, 137, 138, 147, 153, 164, 166, 259, 262, 263, 329, 333, 336, 337, 340, 342, 346, 362, 363, 371], "auto_select_worker_config": [101, 102, 108, 110, 115], "autocal": [24, 26], "autocomplet": [118, 124], "autodiscoveri": [43, 46, 53, 58], "autograd": [224, 237], "autom": [24, 25, 26, 82, 83, 118, 123, 125, 349, 350, 355, 361], "automat": [0, 3, 5, 6, 9, 10, 13, 16, 17, 22, 24, 26, 27, 35, 39, 82, 83, 90, 91, 92, 93, 101, 102, 107, 110, 115, 126, 127, 137, 144, 147, 152, 181, 185, 198, 201, 206, 208, 213, 224, 225, 226, 227, 228, 229, 231, 232, 234, 236, 238, 240, 241, 242, 244, 246, 249, 252, 259, 261, 262, 267, 268, 274, 275, 276, 277, 285, 287, 288, 291, 294, 299, 301, 304, 315, 316, 327, 329, 330, 333, 335, 336, 337, 338, 340, 341, 342, 343, 346, 347, 348, 349, 350, 352, 354, 355, 356, 359, 360, 361, 362, 363, 366, 367, 368, 369, 370, 371], "automodelforsequenceclassif": [299, 302, 304], "autoreload": [128, 129, 137, 138, 147, 148], "autosc": [8, 11, 24, 26, 28, 34, 35, 42, 84, 85, 88, 89, 92, 93, 101, 102, 106, 108, 128, 132, 134, 147, 151, 191, 197, 208, 218, 221, 224, 225, 309, 316, 325], "autoscal": [3, 5, 6, 10, 13, 14, 16, 24, 26, 27, 28, 34, 49, 50, 51, 52, 61, 62, 63, 64, 65, 80, 81, 84, 85, 126, 127, 128, 133, 136, 137, 139, 143, 146, 147, 148, 150, 151, 159, 161, 181, 187, 206, 213, 259, 261, 316, 323, 328], "autoscaling_config": [16, 101, 102, 108, 110, 113, 116, 118, 123], "autotoken": [299, 302, 304], "autotun": [7, 253, 258], "auxiliari": [336, 341], "avail": [1, 5, 6, 8, 9, 10, 17, 22, 28, 30, 43, 45, 53, 56, 82, 83, 90, 91, 92, 93, 101, 102, 104, 107, 110, 114, 118, 121, 124, 126, 127, 128, 129, 137, 138, 147, 148, 151, 155, 158, 159, 162, 163, 164, 167, 169, 170, 182, 183, 184, 188, 189, 191, 197, 198, 204, 206, 212, 213, 224, 226, 227, 229, 234, 238, 244, 245, 247, 259, 263, 268, 280, 283, 289, 290, 291, 296, 299, 301, 304, 309, 316, 325, 329, 335, 336, 340, 342, 344, 346, 355, 360, 361, 362, 367, 371], "available_resourc": [3, 181, 187], "availi": [80, 81, 92, 93], "avalanch": [291, 297], "averag": [5, 6, 9, 198, 204, 224, 225, 228, 259, 263, 291, 294, 342, 346, 349, 353, 362, 367], "avg_loss": 5, "avg_train_loss": [342, 346, 355, 359], "avg_val_loss": [342, 346, 355, 359], "avgpool": 13, "avoid": [2, 3, 5, 6, 8, 9, 10, 11, 88, 89, 90, 91, 92, 93, 101, 102, 108, 137, 139, 147, 151, 164, 167, 175, 180, 181, 188, 191, 195, 198, 202, 203, 206, 208, 212, 218, 220, 224, 225, 234, 237, 245, 248, 251, 259, 261, 262, 268, 290, 291, 294, 297, 336, 337, 341, 342, 344, 349, 351, 352, 353, 355, 357, 362, 371], "aw": [8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 30, 33, 34, 45, 48, 50, 51, 52, 55, 56, 57, 60, 63, 64, 65, 66, 76, 79, 86, 87, 94, 95, 96, 98, 118, 121, 191, 192, 198, 201, 206, 210, 213, 218, 222, 291, 298], "awai": [10, 82, 83, 100, 126, 127, 206, 211, 268, 289, 291, 294, 298, 361], "await": [4, 11, 12, 16, 147, 150, 171, 174, 218, 222], "awak": [291, 298], "awar": [147, 151, 336, 337, 342, 348, 355, 357], "award": [268, 289, 291, 298], "aws_region": [17, 23, 28, 30, 43, 45, 46, 48, 53, 56, 58, 60, 118, 121], "aws_role_nam": [35, 39], "awsregion": [43, 46, 53, 58], "ax": [7, 13, 14, 224, 226, 253, 255, 329, 331, 335, 362, 364], "axi": [5, 7, 10, 11, 13, 14, 15, 16, 206, 211, 213, 218, 222, 224, 226, 237, 253, 255, 299, 303, 329, 331, 335, 349, 352, 353, 355, 361, 362, 364, 371], "axvlin": [355, 361], "aydin": [118, 121], "azur": [17, 20, 224, 234, 245, 252, 362, 371], "b": [2, 3, 11, 175, 178, 180, 181, 184, 187, 218, 222, 224, 237, 268, 289, 329, 331, 332, 342, 348, 355, 358, 359, 361, 362, 371], "babi": [268, 287, 289], "back": [3, 4, 12, 16, 92, 93, 164, 167, 171, 174, 181, 190, 238, 241, 242, 268, 287, 289, 290, 291, 297, 298, 329, 335, 336, 337, 341, 342, 344, 349, 352, 353, 362, 371], "backbon": [224, 227, 329, 335], "backdrop": [291, 298], "backend": [110, 116, 118, 121, 137, 144, 159, 162, 163, 164, 165, 299, 304, 305, 362, 371], "background": [2, 175, 179, 291, 298], "backpressur": [8, 101, 102, 106, 164, 166, 191, 196, 197], "backpropag": [299, 304], "backward": [5, 6, 7, 13, 14, 137, 143, 224, 225, 228, 238, 240, 245, 247, 253, 256, 257, 259, 263, 291, 298, 299, 304, 329, 330, 342, 346, 355, 359, 362, 367], "bad": [268, 289, 291, 298], "bai": [291, 298], "bake": [3, 181, 186], "balanc": [3, 11, 17, 22, 24, 27, 28, 34, 51, 52, 64, 65, 92, 93, 101, 102, 107, 110, 112, 113, 115, 118, 124, 181, 190, 218, 220, 309, 315, 316, 325, 327, 336, 337], "bale": [268, 289], "ball": [268, 289], "band": [268, 289], "bandwidth": [101, 102, 104], "bank": [291, 298], "bar": [128, 136, 137, 146, 147, 154, 342, 344, 349, 351, 362, 364], "barca": [268, 289], "barcelona": [118, 121, 268, 289], "barh": [349, 353], "barr": [291, 298], "barrier": [329, 333, 336, 340], "base": [1, 4, 5, 6, 7, 8, 14, 16, 24, 25, 26, 27, 43, 44, 53, 54, 66, 67, 79, 80, 81, 84, 85, 88, 89, 92, 93, 96, 98, 101, 102, 106, 107, 118, 120, 121, 124, 125, 128, 129, 131, 136, 137, 138, 139, 140, 143, 147, 148, 150, 151, 154, 159, 163, 164, 167, 169, 170, 171, 174, 191, 192, 194, 195, 197, 203, 209, 210, 213, 224, 225, 234, 237, 253, 257, 259, 261, 262, 263, 268, 289, 291, 293, 297, 299, 304, 306, 313, 314, 315, 316, 327, 329, 330, 331, 336, 337, 338, 348, 350, 351, 353, 355, 361, 362, 363, 369, 371], "base_dir": [362, 371], "base_model_id": [118, 121], "base_s3_path": [118, 121], "base_url": [92, 93, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123], "baselin": [7, 14, 224, 226, 253, 256, 336, 341, 342, 345], "basemodel": [4, 118, 122, 171, 172, 174], "bash": [1, 128, 129, 135, 137, 138, 145, 147, 148, 155, 158, 169, 170], "bash_profil": [1, 169, 170], "basic": [2, 5, 7, 10, 11, 13, 14, 15, 17, 21, 24, 27, 79, 90, 91, 100, 118, 119, 120, 125, 155, 158, 164, 165, 175, 176, 206, 215, 218, 223, 253, 254, 256, 257, 355, 357, 372], "basicblock": 13, "basicvariantgener": [7, 14, 253, 257], "basset": [137, 144, 146], "basset_10005": [137, 146], "bastion": [17, 22], "batch": [5, 6, 7, 11, 12, 13, 14, 16, 88, 89, 90, 91, 96, 98, 106, 107, 109, 126, 127, 130, 135, 136, 138, 139, 140, 143, 146, 147, 151, 164, 166, 172, 182, 187, 196, 197, 202, 205, 208, 211, 212, 213, 218, 220, 222, 224, 225, 228, 229, 232, 234, 237, 238, 239, 240, 241, 244, 245, 247, 252, 253, 255, 259, 262, 263, 283, 284, 290, 292, 299, 300, 301, 302, 304, 305, 324, 329, 331, 332, 336, 338, 339, 340, 341, 342, 343, 344, 346, 350, 352, 354, 356, 363], "batch_df": [329, 331], "batch_first": [355, 358], "batch_format": [9, 198, 202, 329, 331, 336, 338, 342, 344, 349, 351, 353, 354, 355, 361, 362, 371], "batch_idx": [6, 259, 262, 329, 332, 336, 339], "batch_metr": [137, 146], "batch_norm": [137, 140], "batch_pr": [10, 15, 206, 213], "batch_siz": [3, 5, 6, 7, 9, 10, 13, 14, 15, 16, 128, 131, 137, 139, 141, 143, 144, 146, 181, 189, 198, 202, 206, 211, 212, 213, 224, 228, 232, 238, 240, 241, 245, 247, 253, 255, 256, 257, 259, 262, 263, 268, 283, 289, 290, 299, 304, 329, 333, 336, 340, 342, 346, 349, 351, 355, 357, 359, 361, 362, 365, 366, 367, 368, 371], "batch_size_per_work": [6, 259, 263, 299, 304, 305], "batchnorm1d": [137, 140], "batchnorm2d": 13, "bathtub": [291, 298], "bathtuby": [291, 298], "batman": [268, 289], "batteri": [12, 147, 151], "battl": [268, 287, 289, 290, 291, 297, 298], "battleship": [291, 298], "bayesian": [7, 14, 253, 257], "bbc": [268, 289], "bc": [336, 341], "bd1": [268, 289], "beach": [118, 121], "bearer": [147, 153, 168], "bearer_token": [147, 153], "beast": [268, 289], "beauti": [268, 289, 291, 297, 298, 316, 317, 319, 320, 328], "bebr": [291, 298], "becaus": [3, 7, 9, 14, 24, 26, 28, 33, 43, 51, 53, 63, 64, 82, 83, 101, 102, 105, 108, 128, 130, 137, 139, 159, 163, 181, 185, 198, 203, 253, 257, 291, 294, 297, 298, 329, 331, 333, 334, 349, 354, 362, 364, 369], "becom": [3, 8, 9, 128, 129, 137, 138, 147, 148, 159, 163, 181, 183, 189, 191, 197, 198, 201, 224, 229, 237, 268, 289, 329, 330], "bee": [268, 287, 289], "been": [8, 82, 83, 84, 85, 94, 95, 191, 196, 291, 297, 298, 349, 352], "befor": [3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 69, 78, 80, 81, 100, 101, 102, 105, 128, 130, 132, 137, 139, 164, 165, 171, 172, 174, 181, 189, 198, 204, 206, 215, 224, 226, 234, 238, 239, 240, 243, 244, 245, 247, 248, 250, 253, 256, 257, 259, 263, 291, 294, 315, 316, 327, 329, 331, 342, 344, 349, 351, 355, 357, 362, 363, 364, 365], "beforehand": [82, 83, 118, 121], "begin": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 82, 83, 100, 118, 124, 164, 165, 267, 268, 285, 291, 298, 342, 346], "beginn": [284, 292, 300, 324], "behalf": [155, 157], "behavior": [3, 84, 85, 118, 120, 121, 155, 157, 159, 161, 164, 167, 181, 184, 245, 246, 336, 341, 342, 344, 362, 364, 367, 369], "behaviour": [329, 331, 349, 353], "behind": [24, 27, 101, 102, 106, 291, 297, 298, 342, 343], "being": [5, 8, 13, 16, 84, 85, 88, 89, 191, 196, 291, 294, 297, 298, 316, 320, 328, 329, 331, 362, 364], "believ": [291, 297], "belong": [362, 363], "below": [2, 3, 5, 7, 8, 14, 24, 26, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 100, 101, 102, 106, 110, 116, 164, 166, 175, 180, 181, 185, 190, 191, 192, 224, 228, 236, 253, 257, 329, 335, 349, 354, 362, 371], "ben": [268, 287, 289, 290, 291, 297], "benchmark": [122, 349, 354, 362, 363], "benefit": [1, 3, 4, 7, 10, 12, 14, 17, 22, 125, 169, 170, 171, 173, 181, 184, 206, 212, 253, 256], "bergman": [291, 294], "berni": [268, 289], "bert": [284, 292, 299, 300, 301, 304, 305, 306, 324], "besok": [268, 289], "best": [4, 5, 7, 9, 10, 12, 14, 15, 17, 20, 35, 39, 84, 85, 101, 118, 121, 124, 128, 130, 159, 162, 171, 174, 198, 204, 206, 215, 224, 225, 234, 236, 238, 242, 245, 252, 253, 257, 268, 283, 289, 290, 291, 297, 298, 299, 301, 336, 341, 342, 348, 349, 352, 354, 355, 359, 361, 362, 363, 369, 371], "best_ckpt": [329, 333, 335, 336, 340, 341, 349, 352, 353, 354, 355, 359, 362, 368], "best_ckpt_path": [355, 361, 362, 371], "best_result": [7, 14, 253, 257], "best_run": [137, 144, 146, 147, 150], "best_val_loss": [137, 143], "better": [3, 7, 8, 10, 14, 88, 89, 92, 93, 118, 120, 126, 127, 181, 187, 191, 195, 206, 212, 245, 252, 253, 256, 268, 289, 291, 294, 296, 297, 309, 316, 325, 329, 335, 336, 341, 342, 348, 349, 353], "between": [3, 7, 8, 9, 10, 11, 14, 15, 17, 18, 19, 22, 24, 26, 53, 63, 79, 86, 87, 101, 102, 106, 110, 112, 118, 120, 121, 159, 162, 163, 181, 184, 191, 192, 194, 195, 198, 201, 204, 206, 208, 212, 215, 218, 220, 224, 229, 253, 255, 257, 291, 294, 295, 342, 343, 349, 351, 355, 357], "beyonc": [268, 289], "beyond": [94, 95, 118, 119, 123, 268, 289, 291, 297], "bf16": [6, 259, 262, 263], "bfloat16": [362, 371], "bia": [5, 7, 13, 14, 137, 140, 224, 227, 253, 256, 257, 349, 351], "bias": [245, 252, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "bidder": [291, 297], "bieber": [268, 289], "big": [8, 191, 192, 196, 291, 297, 299, 301, 329, 330, 336, 337, 342, 343], "bigger": [268, 289], "bigl": [362, 363], "bigqueri": [8, 191, 192], "bigr": [362, 363], "bill": [35, 37, 66, 68, 96, 98], "billion": [110, 112, 159, 163], "bin": [0, 66, 69, 342, 344], "binari": [12, 128, 130, 349, 350, 354], "bind": [4, 8, 11, 12, 16, 92, 93, 147, 150, 164, 167, 171, 174, 191, 192, 218, 222, 316, 320, 327], "birthdai": [268, 289], "bit": [3, 181, 189, 291, 297, 298], "bitten": [291, 297], "bjork": [291, 298], "bjp": [268, 289], "black": [7, 14, 253, 255, 268, 289, 291, 298, 342, 344], "blair": [268, 289], "blank": [80, 81], "bless": [268, 289], "blind": [291, 298], "blob": [224, 234, 342, 344, 362, 371], "block": [2, 3, 5, 7, 11, 13, 16, 43, 51, 53, 64, 101, 102, 108, 110, 114, 116, 118, 121, 122, 123, 128, 130, 164, 166, 167, 175, 179, 180, 181, 182, 188, 189, 201, 202, 203, 208, 211, 212, 213, 218, 221, 222, 223, 224, 235, 238, 239, 240, 253, 256, 267, 268, 274, 285, 287, 291, 296, 329, 331, 342, 343, 344, 349, 352], "block_out_channel": [6, 259, 262], "blockbust": [291, 298], "blog": [5, 6, 7, 13, 253, 258, 259, 264], "blow": [268, 289], "blue": [101, 102, 105, 268, 287, 289, 329, 330, 362, 363], "bn1": 13, "bn2": 13, "board": [291, 298], "boat": [291, 297], "bob": [268, 289], "bodi": [11, 16, 218, 222, 291, 294, 298], "boi": [291, 294, 298], "boilerpl": [224, 231, 234, 329, 330, 336, 337, 349, 350, 362, 371], "bomb": [291, 298], "bon": [268, 289], "book": [268, 287, 289, 290, 291, 297], "bookkeep": [362, 366], "bool": [362, 371], "boolean": 12, "boost": [4, 12, 171, 174, 349, 350, 351, 352, 354], "booster": [4, 12, 171, 174, 349, 352, 353, 354], "boot": [268, 289], "booth": [268, 289], "bootstrap": [24, 26], "border": [291, 297], "border_colli": [128, 130, 137, 144, 147, 150], "border_collie_1055": [128, 130], "bore": [291, 297], "both": [0, 8, 10, 17, 22, 24, 26, 53, 57, 96, 98, 100, 110, 111, 118, 122, 128, 130, 155, 157, 159, 161, 164, 167, 191, 192, 197, 206, 208, 224, 234, 236, 238, 239, 242, 245, 246, 250, 268, 289, 299, 301, 305, 329, 331, 349, 351, 352], "boto3": [86, 87, 118, 121], "bottleneck": [137, 144, 147, 151, 155, 157, 159, 163, 349, 351], "bottom": [3, 94, 95, 181, 189], "bound": [17, 22, 147, 151, 182], "boundari": [8, 191, 195], "bouquet": [268, 289], "bout": [268, 287, 289, 290], "box": [5, 88, 89, 94, 95, 224, 225], "br": [291, 294, 297, 298], "brain": [291, 297], "branch": 0, "brand": [118, 122], "braun": [268, 289], "break": [6, 7, 14, 88, 89, 253, 255, 259, 262, 291, 298, 362, 365], "breakdown": [8, 128, 134, 191, 192], "breakneck": [291, 298], "breakpoint": [299, 304], "breed": [126, 127], "breez": [291, 294], "brennan": [291, 297], "brew": [4, 28, 29, 43, 44, 53, 55, 66, 68, 155, 158, 171, 172], "bridg": [8, 15, 16, 126, 127, 191, 192, 224, 229, 291, 298], "brief": [118, 121], "bring": [4, 171, 173, 224, 235, 268, 287, 289, 291, 297, 298, 349, 351], "brit": [268, 289], "british": [268, 289, 291, 298], "broadcast": [224, 225], "broader": [8, 191, 196], "brock": [268, 289], "brought": [291, 297], "brown": [268, 289, 291, 294], "brows": 168, "browser": [0, 1, 35, 38, 82, 83, 169, 170], "bryant": [268, 289], "bst": [4, 171, 174], "bubbl": [268, 289], "bucket": [17, 21, 22, 23, 28, 30, 33, 34, 35, 36, 39, 42, 43, 45, 51, 53, 56, 63, 64, 66, 70, 78, 86, 87, 118, 121, 128, 130, 133], "bucket_nam": [35, 39, 118, 121], "budget": [118, 124], "buf": [329, 331, 362, 364], "buffalo": [268, 289], "buffer": [8, 101, 102, 106, 191, 192, 238, 241], "bug": [155, 157], "bui": [268, 289, 291, 297], "build": [1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 16, 17, 21, 80, 81, 84, 85, 100, 110, 117, 118, 120, 123, 125, 147, 151, 169, 170, 171, 173, 174, 175, 180, 181, 182, 191, 192, 195, 197, 198, 200, 206, 208, 211, 218, 219, 221, 222, 223, 226, 228, 231, 240, 242, 259, 262, 291, 298, 329, 330, 331, 336, 337, 342, 343, 344, 348, 350, 351, 355, 357, 361, 362, 365, 367, 371], "build_app": [11, 218, 223], "build_data_load": [7, 14, 253, 255, 256], "build_data_loader_ray_train": [5, 13, 224, 228, 232], "build_data_loader_ray_train_ray_data": [238, 240, 241, 245, 247], "build_data_loader_torch": [5, 13], "build_dataload": [355, 357, 359, 362, 366, 367], "build_inference_dataset": [362, 371], "build_openai_app": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123], "build_resnet18": [5, 13, 224, 227, 231, 237], "builder": [11, 218, 223], "built": [0, 2, 4, 5, 6, 8, 9, 12, 16, 24, 27, 28, 34, 35, 39, 82, 83, 84, 85, 88, 89, 101, 102, 106, 128, 135, 137, 144, 145, 147, 153, 154, 155, 158, 159, 161, 168, 171, 173, 175, 177, 191, 192, 198, 200, 224, 225, 227, 238, 240, 259, 261, 263, 309, 316, 325, 336, 337, 340, 342, 347, 349, 351, 352, 353, 354, 355, 361, 362, 363, 367, 371], "bulk": [90, 91], "bundl": [84, 85], "bunni": [291, 294], "burden": 100, "burrito": [362, 363], "bursti": [80, 81, 84, 85, 101, 102, 106], "busi": [8, 11, 118, 123, 147, 149, 151, 191, 192, 218, 221, 315, 316, 327], "button": [82, 83, 84, 85, 92, 93, 94, 95, 128, 136, 137, 146, 147, 154], "bx1": [329, 332], "bx3xhxw": [329, 332], "bxauk": [147, 153], "bypass": [3, 181, 184], "bystand": [291, 298], "byte": [10, 164, 166, 206, 212, 329, 331, 335, 362, 364], "bytesio": [128, 136, 329, 331, 362, 364, 365, 371], "byth": [86, 87], "c": [1, 3, 11, 84, 85, 86, 87, 169, 170, 181, 187, 218, 222, 224, 237, 268, 289, 291, 297, 342, 346, 349, 351, 352, 355, 359, 362, 369, 371], "cab": [4, 9, 12, 164, 166, 171, 174, 198, 201, 204], "cabin": [291, 298], "cabl": [291, 294], "cach": [3, 15, 17, 21, 84, 85, 104, 106, 109, 110, 116, 181, 183, 245, 252, 329, 331, 336, 338, 342, 344, 348, 349, 351, 355, 357, 362, 364, 365, 371], "cactu": [268, 287, 289], "caesar": [362, 363], "cahse": [268, 289], "calcul": [5, 9, 13, 88, 89, 137, 146, 198, 202, 224, 228, 299, 303, 304], "call": [3, 4, 5, 6, 7, 9, 10, 13, 14, 15, 92, 93, 102, 109, 119, 120, 125, 164, 167, 168, 171, 174, 176, 179, 181, 184, 189, 190, 198, 204, 205, 206, 211, 212, 215, 224, 226, 228, 231, 234, 235, 238, 240, 244, 245, 247, 248, 249, 250, 253, 257, 259, 263, 283, 290, 291, 298, 299, 306, 329, 331, 333, 334, 342, 346, 349, 351, 354, 362, 364, 368], "call_id": [118, 123], "callabl": [10, 11, 15, 118, 123, 128, 131, 206, 213, 218, 223, 267, 268, 277, 283, 285, 288, 290], "callback": [6, 259, 263, 329, 332, 333, 336, 340, 341, 349, 352, 354], "caller": [268, 280, 289], "came": [268, 289], "camera": [291, 297, 298], "campu": [268, 289], "can": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 22, 24, 26, 27, 28, 29, 30, 32, 33, 34, 35, 39, 41, 42, 43, 44, 45, 50, 51, 52, 53, 55, 56, 57, 62, 63, 64, 65, 66, 68, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 114, 115, 116, 118, 119, 120, 121, 123, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 139, 142, 144, 145, 146, 147, 149, 152, 153, 154, 155, 157, 158, 159, 161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 201, 202, 203, 204, 206, 207, 211, 212, 213, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 229, 230, 233, 235, 236, 237, 238, 239, 242, 243, 245, 246, 247, 248, 250, 252, 253, 254, 256, 257, 259, 261, 262, 263, 267, 268, 274, 277, 280, 283, 285, 287, 288, 289, 290, 291, 293, 294, 296, 297, 298, 299, 301, 304, 305, 306, 309, 315, 316, 325, 327, 330, 331, 332, 333, 337, 339, 343, 344, 350, 351, 356, 357, 360, 363, 364, 365], "canadian": [291, 297], "canari": [147, 153], "cancel": [291, 297], "candid": [118, 121, 329, 333, 336, 340, 341], "cannon": [291, 298], "cannot": [1, 3, 28, 31, 33, 35, 40, 43, 47, 53, 59, 66, 75, 84, 85, 169, 170, 181, 183, 291, 294], "canopi": [349, 354], "cant": [291, 298], "canva": [5, 6, 13, 259, 264], "capabl": [8, 11, 24, 27, 82, 83, 86, 87, 110, 112, 117, 118, 119, 120, 123, 124, 125, 126, 127, 159, 160, 162, 168, 191, 192, 196, 218, 221, 299, 305, 309, 316, 325], "capac": [28, 30, 43, 45, 53, 56, 84, 85, 101, 102, 104, 105, 126, 127, 342, 343], "capit": [101, 102, 108, 118, 121], "captain": [291, 297], "caption_lat": [6, 259, 262], "captur": [94, 95, 147, 152, 164, 167, 329, 333, 342, 346, 355, 356, 361], "car_typ": [118, 122], "card": [9, 110, 113, 198, 201], "cardescript": [118, 122], "cardiffnlp": [268, 274, 287], "care": [224, 234, 291, 297, 298, 355, 357], "carli": [268, 289], "carriag": [291, 298], "carrow": [268, 289], "cartograph": [349, 350], "cartpol": [336, 341], "cartyp": [118, 122], "case": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 24, 26, 28, 30, 43, 45, 53, 56, 84, 85, 88, 89, 101, 102, 105, 110, 112, 117, 120, 121, 125, 147, 151, 159, 162, 171, 173, 174, 175, 177, 181, 184, 190, 191, 195, 198, 204, 205, 206, 213, 215, 218, 223, 245, 248, 253, 257, 258, 259, 263, 264, 268, 283, 289, 290, 291, 298, 299, 304, 342, 343, 349, 350, 355, 359], "cash": [9, 198, 201], "castl": [291, 297], "casual": [268, 289], "cat": [329, 332, 336, 339, 355, 359], "catalog": [342, 343], "catch": [268, 289], "categor": [8, 88, 89, 191, 193, 195], "categori": [10, 24, 26, 206, 211, 299, 301, 362, 363], "cattl": [291, 297], "cattleman": [291, 297], "caus": [5, 6, 28, 30, 43, 45, 53, 56, 159, 163, 182, 224, 225, 259, 261], "cd": [0, 11, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 90, 91, 92, 93, 100, 118, 125, 126, 127, 159, 163, 164, 167, 218, 223, 362, 371], "cdot": [329, 330, 336, 337], "ceil_mod": 13, "cell": [4, 5, 6, 7, 9, 10, 11, 13, 82, 83, 84, 85, 86, 87, 90, 91, 128, 129, 137, 138, 147, 148, 171, 174, 198, 205, 206, 217, 218, 223, 224, 226, 237, 253, 258, 259, 260, 264, 342, 344, 355, 357], "celsiu": [3, 118, 123, 181, 190], "cena": [268, 287, 289], "center": [224, 232, 268, 289, 291, 294, 329, 331, 362, 363, 364], "center_input_sampl": [6, 259, 262], "centercrop": [329, 331, 362, 364], "central": [118, 121, 159, 162, 224, 234], "centric": [8, 191, 195], "ceph": [8, 191, 192], "cerebr": [291, 297], "cert": [24, 27], "certain": [5, 6, 7, 9, 14, 198, 202, 253, 257, 259, 260, 291, 294, 313, 314, 315, 316, 327], "certif": [24, 27], "chain": [15, 147, 154, 182], "chair": [268, 289], "chalk": [291, 298], "challeng": [5, 6, 10, 11, 100, 103, 109, 126, 127, 206, 208, 218, 220, 224, 225, 238, 239, 259, 261], "chanc": [3, 181, 185, 268, 289], "chang": [3, 5, 6, 7, 8, 10, 11, 14, 28, 30, 35, 39, 43, 45, 53, 56, 63, 86, 87, 88, 89, 92, 93, 110, 115, 117, 118, 121, 128, 129, 137, 138, 143, 147, 148, 159, 161, 164, 167, 181, 183, 191, 192, 206, 216, 218, 220, 223, 224, 225, 253, 257, 259, 261, 263, 268, 289, 291, 297, 299, 305, 329, 330, 333, 335, 336, 337, 342, 343, 349, 350, 355, 361, 362, 363, 370, 371], "channel": [10, 13, 15, 16, 159, 163, 164, 167, 206, 212, 224, 226, 227, 237, 291, 298, 329, 330, 331, 332, 355, 361, 362, 363], "chao": [291, 298], "chap": [28, 29, 43, 44, 53, 55], "charact": [101, 102, 104, 291, 298, 316, 317, 319, 320, 328], "characterist": [8, 9, 11, 15, 101, 102, 104, 191, 192, 198, 201, 218, 221], "charg": [9, 198, 201], "charli": [268, 289], "charm": [268, 289, 291, 297], "chart": [43, 46, 48, 53, 58, 60, 66, 76, 349, 351], "chase": [268, 287, 289], "chat": [101, 102, 105, 108, 110, 114, 115, 118, 121, 122, 123, 124], "chatbot": [118, 123, 124], "cheap": [268, 289], "cheaper": [8, 191, 194], "cheapli": [291, 294], "cheat": [291, 297], "check": [3, 5, 6, 9, 10, 11, 13, 28, 32, 35, 41, 43, 49, 50, 53, 61, 62, 63, 66, 71, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 101, 102, 106, 118, 123, 164, 166, 168, 181, 187, 198, 201, 203, 206, 212, 213, 218, 222, 224, 226, 234, 235, 245, 247, 259, 263, 268, 289, 299, 304, 333, 336, 340, 342, 344, 349, 351, 354], "check_cal": [329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "check_val_every_n_epoch": [329, 333, 336, 340], "checkout": [88, 89, 92, 93], "checkpoint": [9, 10, 12, 14, 86, 87, 126, 127, 128, 130, 132, 137, 142, 143, 144, 198, 205, 206, 208, 225, 226, 227, 228, 235, 236, 238, 240, 244, 246, 249, 251, 252, 261, 262, 299, 306, 330, 331, 333, 337, 338, 341, 343, 344, 348, 350, 352, 356, 357, 359, 361, 362, 363, 364, 367, 368, 369, 370, 371], "checkpoint_": [362, 371], "checkpoint_000000": 13, "checkpoint_000001": 13, "checkpoint_at_end": [355, 359], "checkpoint_config": [329, 333, 336, 340, 342, 346, 349, 352, 355, 359, 362, 368], "checkpoint_dir": [362, 371], "checkpoint_dir_nam": 13, "checkpoint_frequ": [329, 333, 336, 340, 349, 352, 362, 368], "checkpoint_nam": [349, 352], "checkpoint_path": [5, 6, 13, 259, 263, 355, 361, 362, 371], "checkpoint_root": [362, 371], "checkpoint_score_": [355, 359], "checkpoint_score_attribut": [329, 333, 336, 340, 349, 352, 355, 359, 362, 368], "checkpoint_score_ord": [329, 333, 336, 340, 349, 352, 355, 359, 362, 368], "checkpointconfig": [329, 330, 331, 333, 336, 337, 338, 340, 342, 343, 344, 346, 349, 350, 351, 352, 355, 357, 359, 362, 364, 368], "cheekbon": [268, 289], "chelsea": [268, 289], "cheri": [268, 289], "chill": [268, 289], "chip": [1, 169, 170], "chloe": [291, 294], "chmod": [155, 158], "choic": [8, 9, 24, 26, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 126, 127, 191, 197, 198, 205, 309, 316, 325, 355, 356], "choos": [1, 10, 15, 17, 18, 20, 53, 63, 66, 73, 80, 81, 82, 83, 84, 85, 101, 102, 106, 119, 125, 126, 127, 169, 170, 206, 214, 291, 298, 342, 348], "choreo": [268, 289], "chose": [5, 13], "chown": [155, 158], "chri": [268, 289], "chrisbrown": [268, 289], "christian": [268, 289], "chromadb": [8, 191, 192], "chronolog": [159, 161], "chuck": [268, 289], "chunk": [8, 10, 15, 88, 89, 101, 102, 108, 110, 114, 115, 118, 121, 128, 130, 191, 195, 206, 215], "church": [268, 289], "churn": [355, 361], "chw": [329, 331], "ci": [90, 91, 118, 125, 126, 127, 245, 252, 362, 371], "ciara": [268, 289], "cidr": [17, 22], "cidr_block": [17, 22], "cif": [268, 289], "cifar": [238, 244, 245, 248, 250], "cifar10": [238, 242, 245, 251], "cinema": [291, 294, 297, 298], "cinemat": [291, 298], "cinematograph": [291, 298], "cinematographi": [291, 297, 298], "cineworld": [268, 289], "citi": [4, 9, 12, 118, 123, 171, 174, 198, 201, 268, 289, 291, 298, 355, 356], "citizenship": [268, 289], "ckpt": [5, 6, 13, 259, 263, 329, 330, 333, 335, 336, 337, 340, 341, 342, 346, 349, 352, 353, 355, 359, 361, 362, 367], "ckpt_dir": [5, 6, 13, 224, 237, 245, 247, 259, 263, 329, 335, 336, 341, 342, 346, 348, 355, 359, 361, 362, 367, 371], "ckpt_file": [329, 335, 336, 341], "ckpt_out": [342, 346, 355, 359, 362, 367], "ckpt_path": [6, 259, 263, 329, 333, 336, 340], "ckpt_root": [329, 333, 336, 340], "cl": [137, 140, 146], "claim": [8, 191, 196, 291, 294, 297], "clamp": [329, 335], "clarifi": [96, 97], "class": [3, 4, 5, 6, 7, 10, 11, 12, 14, 15, 16, 118, 122, 128, 130, 131, 137, 139, 140, 146, 147, 149, 150, 164, 167, 171, 174, 181, 190, 206, 210, 213, 218, 222, 224, 227, 235, 237, 238, 242, 253, 255, 257, 259, 262, 263, 267, 283, 285, 289, 290, 315, 316, 327, 329, 330, 331, 332, 335, 336, 339, 342, 345, 350, 353, 355, 357, 358, 361, 362, 363, 364, 365, 371], "class_nam": [3, 181, 190], "class_to_label": [137, 139, 143, 144, 146], "classic": [336, 337, 341, 342, 343, 349, 350], "classif": [4, 12, 15, 126, 127, 171, 174, 219, 224, 226, 227, 228, 299, 301, 302, 304, 306, 367], "classifi": [7, 11, 14, 126, 127, 137, 139, 147, 149, 150, 218, 222, 253, 256, 299, 301, 362, 363], "classificationmodel": [137, 140, 143, 146], "classmat": [291, 294], "classmethod": [137, 140, 146], "classpredictor": [147, 149, 150], "claud": [118, 124], "cld": [86, 87, 110, 115, 147, 153], "cld_g54aiirwj1s8t9ktgzikqur41k": [86, 87], "cld_kvedzwag2qa8i5bjxuevf5i7": [128, 129, 137, 138, 147, 148], "cldrsrc_12345abcdefgh67890ijklmnop": [43, 47, 48, 53, 59, 60, 66, 75, 76], "clean": [0, 11, 28, 33, 35, 42, 66, 78, 128, 133, 137, 139, 142, 218, 221, 234, 291, 298, 330, 332, 333, 344, 355, 359, 361, 363, 364, 367, 370], "cleaner": [224, 226], "cleanli": [224, 234, 342, 344], "cleanup": [4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 24, 26, 79, 90, 91, 171, 174, 198, 205, 206, 217, 218, 223, 224, 226, 245, 251, 253, 258, 259, 264, 329, 335, 336, 341, 342, 348, 349, 354, 362, 371], "clear": [0, 96, 99, 291, 297, 336, 341, 349, 354, 355, 361, 362, 363, 371], "clearli": [164, 165, 291, 298], "cli": [17, 18, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 74, 82, 83, 86, 87, 88, 89, 90, 91, 126, 127, 128, 135, 147, 154, 164, 167, 168], "click": [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 17, 22, 28, 29, 43, 44, 53, 55, 63, 66, 68, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 101, 102, 103, 110, 111, 116, 118, 119, 128, 129, 137, 138, 147, 148, 155, 158, 164, 166, 167, 171, 172, 175, 176, 180, 181, 182, 190, 198, 199, 206, 207, 212, 218, 219, 253, 254, 257, 259, 260, 263, 267, 268, 285, 291, 293, 299, 301, 309, 316, 325], "client": [86, 87, 101, 102, 107, 108, 110, 114, 115, 118, 121, 122, 123, 309, 325], "cliff": [268, 289], "clip": [128, 131, 136, 137, 139, 147, 150], "clipboard": [92, 93], "clipmodel": [128, 131, 137, 139, 147, 148, 149], "clipprocessor": [128, 131, 137, 139, 147, 148, 149], "clitori": [291, 294], "clock": [291, 298], "clog": [291, 298], "clone": [0, 92, 93, 336, 341], "close": [224, 234, 268, 289, 291, 298], "cloud": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 18, 21, 22, 24, 26, 27, 29, 30, 32, 33, 34, 36, 37, 39, 41, 42, 44, 45, 48, 49, 50, 51, 52, 54, 56, 60, 61, 62, 64, 65, 67, 68, 70, 76, 77, 78, 79, 80, 81, 84, 85, 94, 95, 99, 100, 101, 102, 103, 107, 108, 110, 111, 115, 118, 119, 121, 126, 127, 128, 129, 130, 133, 137, 139, 147, 153, 159, 163, 169, 170, 171, 172, 175, 176, 181, 182, 198, 199, 206, 207, 218, 219, 224, 225, 234, 245, 252, 253, 254, 259, 260, 267, 268, 285, 291, 293, 299, 301, 305, 309, 316, 325, 329, 335, 342, 348, 349, 350, 362, 363], "cloud_deployment_id": [43, 48, 53, 60, 66, 76], "cloud_nam": [43, 50, 53, 62], "clouddeploymentid": [43, 45, 48, 53, 56, 60, 66, 76], "cloudflar": [8, 191, 192], "cloudform": [17, 22], "cloudfound": [17, 22, 35, 39], "cloudprovid": [43, 45, 48, 53, 56, 60, 66, 76], "cloudresourcemanag": [35, 38, 66, 69], "cloudwatch": [17, 22], "club": [268, 287, 289, 291, 297], "cluster": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 35, 41, 42, 45, 49, 50, 51, 52, 55, 56, 57, 61, 62, 63, 64, 65, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 94, 95, 101, 102, 103, 107, 108, 110, 111, 115, 118, 119, 126, 127, 128, 129, 130, 132, 133, 134, 136, 137, 138, 139, 143, 146, 147, 148, 150, 157, 159, 161, 162, 164, 165, 169, 170, 171, 172, 174, 175, 176, 178, 179, 182, 183, 184, 186, 188, 190, 191, 195, 198, 199, 203, 205, 206, 207, 208, 210, 211, 212, 213, 214, 217, 218, 219, 221, 224, 225, 226, 227, 230, 232, 234, 235, 236, 237, 238, 239, 242, 243, 244, 248, 252, 253, 254, 259, 260, 261, 267, 274, 285, 287, 289, 291, 293, 301, 305, 309, 320, 325, 329, 330, 331, 335, 336, 337, 338, 341, 342, 343, 344, 346, 348, 349, 350, 351, 354, 355, 356, 362, 363, 368, 371], "cluster_id": [88, 89], "cluster_storag": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 86, 87, 128, 133, 137, 139, 142, 144, 147, 150, 164, 166, 171, 174, 198, 203, 206, 213, 218, 222, 224, 226, 227, 232, 234, 236, 238, 242, 245, 251, 259, 263, 264, 329, 331, 333, 335, 336, 340, 341, 342, 344, 346, 348, 349, 351, 352, 354, 355, 356, 357, 362, 363, 364, 365, 367, 368, 371], "clusternam": [43, 46, 53, 58], "clusteronc": [28, 32], "cm": [349, 353], "cm_norm": [349, 353], "cmap": [5, 7, 10, 13, 14, 16, 206, 211, 224, 226, 237, 253, 255, 349, 353], "cnn": [224, 226, 329, 332, 335, 362, 363], "co": [118, 121, 336, 337, 338, 341, 355, 358], "coach": [268, 289], "coars": [11, 218, 221], "cocki": [291, 297], "code": [0, 1, 2, 3, 5, 6, 7, 9, 11, 13, 14, 16, 17, 18, 79, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 110, 115, 117, 120, 124, 126, 127, 137, 143, 159, 163, 164, 167, 169, 170, 172, 175, 177, 181, 187, 190, 198, 201, 218, 222, 223, 224, 225, 228, 229, 230, 231, 232, 233, 238, 239, 245, 252, 253, 257, 259, 261, 263, 268, 289, 309, 316, 325, 329, 330, 336, 337, 342, 343, 348, 349, 350, 351, 355, 356, 357, 361, 362, 363, 371], "coder": [118, 124], "cogent": [291, 294], "coher": [291, 297, 298], "coi": [291, 297], "col": [5, 6, 224, 226, 237, 259, 262, 342, 346, 355, 359, 362, 369], "colbert": [268, 289], "cold": [101, 102, 108], "collabor": [96, 98, 99, 342, 343, 345, 348, 372], "collat": [6, 259, 262, 299, 304], "collate_fn": [137, 141, 143, 146, 147, 148, 149, 299, 304], "colleagu": [268, 289], "collect": [2, 3, 9, 10, 16, 17, 19, 155, 157, 158, 175, 177, 180, 181, 183, 198, 201, 206, 210, 215, 224, 226, 233, 235, 237, 268, 289, 336, 340, 341, 349, 352, 353, 362, 367, 372], "collector": [224, 237], "colli": [147, 150], "collison": [268, 290], "colour": [291, 298], "column": [4, 6, 9, 10, 82, 83, 128, 131, 137, 139, 146, 171, 174, 198, 201, 204, 206, 211, 224, 236, 238, 242, 259, 262, 268, 275, 276, 277, 288, 291, 295, 298, 329, 331, 333, 335, 336, 340, 342, 344, 346, 348, 349, 351, 352, 355, 357, 359, 361, 362, 364, 365, 369, 371], "column_nam": [349, 352], "column_stack": [349, 352], "columnar": [8, 191, 192, 238, 242, 329, 331, 349, 351, 355, 357, 362, 364], "com": [0, 1, 13, 14, 17, 22, 24, 27, 28, 29, 35, 37, 38, 39, 43, 44, 53, 55, 63, 66, 68, 69, 70, 88, 89, 92, 93, 100, 110, 115, 126, 127, 128, 129, 136, 137, 138, 147, 148, 150, 153, 168, 169, 170, 355, 357], "combin": [3, 8, 181, 187, 191, 192, 224, 234, 291, 297, 298, 329, 330], "combur": [268, 289], "comcast": [268, 289], "come": [3, 24, 26, 84, 85, 88, 89, 100, 126, 127, 128, 134, 181, 186, 268, 289, 291, 294, 297, 298, 349, 354, 355, 361, 362, 371], "comedi": [291, 298], "comfort": [362, 371], "command": [17, 20, 23, 28, 30, 31, 40, 42, 43, 45, 46, 47, 51, 53, 56, 58, 59, 64, 66, 69, 75, 78, 82, 83, 88, 89, 90, 91, 92, 93, 128, 129, 135, 137, 138, 147, 148, 153, 155, 158, 159, 163, 164, 167, 336, 341], "commend": [291, 294], "comment": [92, 93, 315, 316, 327], "commerci": [8, 191, 192, 194], "commiss": [4, 9, 12, 171, 174, 198, 201], "commit": [329, 331, 362, 364], "common": [2, 3, 4, 9, 10, 53, 63, 66, 71, 118, 125, 155, 157, 164, 167, 171, 173, 175, 177, 181, 190, 198, 201, 203, 206, 210, 213, 268, 289, 291, 295, 297, 298], "common_prefix": [35, 39], "commonli": [8, 10, 191, 192, 195, 206, 209, 342, 345], "commun": [3, 8, 9, 10, 15, 17, 22, 101, 102, 105, 110, 116, 117, 118, 125, 181, 190, 191, 196, 198, 204, 206, 215, 355, 357], "compact": [11, 147, 151, 153, 218, 220, 329, 331, 335, 362, 363, 364], "compani": [24, 27, 96, 97], "compar": [7, 8, 14, 15, 84, 85, 101, 102, 104, 110, 112, 159, 160, 191, 195, 253, 257, 329, 335, 336, 341, 355, 359, 362, 369, 371], "comparison": [291, 298], "compat": [101, 102, 107, 110, 113, 118, 121, 164, 165, 238, 241, 362, 363], "compet": [8, 191, 196], "competit": [268, 289], "compil": [1, 137, 144, 169, 170, 284, 292, 300, 324], "complet": [1, 3, 4, 5, 6, 13, 24, 26, 28, 30, 43, 50, 53, 62, 82, 83, 88, 89, 90, 91, 100, 101, 102, 104, 105, 108, 110, 111, 114, 115, 118, 121, 122, 123, 124, 125, 159, 163, 168, 169, 170, 171, 174, 181, 188, 224, 225, 235, 245, 247, 248, 250, 252, 259, 261, 291, 298, 299, 306, 329, 333, 334, 335, 336, 340, 341, 362, 369], "complex": [4, 8, 11, 16, 24, 26, 100, 101, 102, 105, 107, 110, 112, 117, 118, 120, 124, 126, 127, 171, 174, 191, 192, 195, 196, 197, 218, 220, 329, 330], "compli": [6, 259, 263], "complianc": [17, 20, 110, 115, 118, 125], "compliant": [17, 22], "compon": [2, 6, 7, 8, 11, 14, 17, 20, 22, 25, 52, 65, 66, 72, 84, 85, 88, 89, 101, 102, 106, 107, 109, 128, 134, 137, 143, 146, 159, 161, 162, 168, 175, 177, 191, 192, 218, 220, 224, 226, 253, 257, 259, 263, 355, 357, 362, 364], "compos": [5, 6, 7, 10, 11, 13, 14, 15, 147, 149, 206, 207, 212, 218, 220, 221, 224, 226, 232, 238, 243, 253, 254, 255, 257, 259, 262, 329, 331, 362, 364, 365, 371], "composit": [8, 147, 149, 191, 197, 291, 298], "comprehens": [1, 9, 10, 15, 17, 21, 66, 67, 79, 101, 102, 103, 110, 116, 117, 118, 120, 121, 122, 123, 125, 168, 169, 170, 198, 201, 203, 206, 208, 216], "compress": [349, 351, 355, 361], "comput": [2, 3, 4, 5, 6, 9, 10, 11, 12, 15, 16, 17, 20, 22, 24, 26, 27, 28, 34, 38, 39, 42, 43, 45, 50, 53, 56, 62, 66, 69, 70, 71, 78, 79, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 96, 98, 99, 100, 101, 102, 104, 105, 107, 108, 126, 127, 128, 135, 136, 137, 139, 143, 145, 147, 149, 151, 153, 171, 174, 175, 177, 179, 180, 181, 182, 183, 187, 190, 192, 194, 196, 198, 204, 205, 206, 208, 212, 214, 215, 217, 218, 220, 224, 225, 227, 228, 238, 240, 259, 261, 263, 267, 268, 285, 291, 295, 299, 301, 303, 304, 305, 309, 316, 325, 331, 342, 343, 346, 348, 349, 352, 353, 354, 367, 371, 372], "computation": [7, 14, 253, 256], "compute_accuraci": [10, 15, 206, 215], "compute_config": [43, 50, 53, 62, 101, 102, 108, 110, 115, 159, 163], "compute_metr": [299, 303], "compute_nodes_service_account_email": [35, 39, 66, 70], "compute_tip_percentag": [9, 198, 202], "computeconfig": [43, 50, 53, 62], "con": [268, 289], "conc": [35, 41], "concat_t": [349, 352], "concept": [8, 10, 24, 26, 79, 103, 147, 154, 164, 165, 191, 194, 206, 207, 224, 227], "conceptu": 5, "concern": [2, 11, 175, 177, 218, 221, 291, 297], "concert": [268, 289], "concis": [118, 121], "conclud": [137, 146], "conclus": [119, 291, 293], "concomit": [268, 289], "concret": [17, 18], "concurr": [3, 4, 8, 9, 11, 12, 15, 16, 101, 102, 105, 117, 128, 130, 131, 137, 139, 146, 171, 174, 181, 187, 191, 197, 198, 205, 213, 218, 221, 224, 228, 267, 268, 280, 285, 289, 291, 296, 355, 361, 362, 371], "concurrency_limit": [10, 206, 212], "concuss": [268, 287, 289, 290], "conda": [155, 158], "condit": [3, 6, 17, 22, 118, 121, 181, 183, 259, 262, 291, 293, 329, 335], "conductor": [291, 298], "confid": [245, 246, 329, 335, 336, 341, 342, 348, 349, 354], "config": [4, 5, 6, 7, 11, 12, 13, 14, 16, 35, 38, 66, 69, 126, 127, 128, 135, 137, 143, 144, 145, 147, 153, 155, 158, 164, 167, 171, 174, 218, 223, 224, 227, 228, 229, 230, 235, 238, 240, 245, 247, 250, 253, 257, 259, 263, 299, 304, 329, 333, 336, 337, 340, 342, 346, 349, 352, 355, 359, 362, 364, 367], "configur": [7, 10, 11, 14, 16, 17, 19, 20, 21, 22, 24, 25, 27, 28, 29, 30, 34, 37, 39, 43, 44, 45, 50, 53, 55, 56, 62, 68, 70, 71, 80, 81, 88, 89, 90, 91, 92, 93, 107, 109, 111, 117, 120, 125, 128, 135, 137, 143, 145, 147, 149, 153, 155, 158, 159, 163, 182, 206, 208, 212, 218, 221, 223, 225, 226, 227, 229, 246, 250, 252, 253, 257, 262, 299, 301, 302, 304, 305, 306, 315, 316, 327, 329, 333, 336, 340, 342, 343, 346, 347, 350, 355, 356, 359, 362, 363, 367, 371], "configure_optim": [6, 259, 262, 329, 332, 336, 339], "confirm": [92, 93, 118, 121, 224, 226, 235, 245, 250, 329, 331, 333, 342, 344, 346, 349, 351, 354, 355, 357, 362, 364], "conflict": [224, 237], "confluent": [8, 191, 195], "confus": [10, 137, 146, 206, 212, 362, 371], "confusion_matrix": [349, 351, 353], "congratul": [110, 117, 118, 125], "congress": [268, 289], "conjur": [291, 298], "connect": [3, 5, 6, 10, 17, 22, 24, 27, 28, 34, 43, 46, 53, 58, 66, 72, 80, 81, 82, 83, 86, 87, 96, 98, 128, 129, 132, 137, 138, 147, 151, 181, 184, 206, 209, 224, 225, 238, 244, 259, 261, 268, 274, 287, 291, 298, 349, 353], "connector": [9, 10, 198, 205, 206, 209], "consecut": [268, 287, 289], "consid": [2, 3, 5, 6, 13, 110, 117, 118, 124, 164, 165, 175, 180, 181, 183, 185, 186, 187, 189, 219, 259, 263, 291, 294, 296, 297, 329, 331], "consider": [24, 26, 100], "consist": [7, 8, 14, 15, 79, 80, 81, 84, 85, 118, 120, 122, 125, 191, 192, 224, 225, 226, 227, 238, 239, 253, 255, 284, 292, 300, 324, 336, 341, 349, 352, 354, 355, 357, 358], "consol": [28, 31, 32, 35, 40, 41, 43, 47, 50, 53, 59, 62, 63, 66, 75, 77, 82, 83, 90, 91, 92, 93, 118, 125, 126, 127, 128, 129, 137, 138, 147, 148, 153, 155, 157, 164, 165, 168, 342, 346], "consolid": [147, 153], "conspicu": [268, 289], "constant": [6, 101, 102, 105, 259, 262, 291, 298, 355, 361], "constraint": [6, 101, 102, 106, 118, 124, 259, 262, 268, 283, 290, 291, 298], "construct": [3, 6, 181, 190, 224, 231, 232, 259, 262, 336, 337, 355, 359, 362, 365, 366], "constructor": [3, 10, 11, 15, 16, 181, 190, 206, 213, 218, 222, 267, 268, 285], "consum": [7, 9, 10, 14, 15, 28, 30, 43, 45, 53, 56, 198, 201, 203, 206, 209, 211, 238, 239, 244, 245, 252, 253, 257, 355, 357], "consumptionapi": [10, 206, 211, 215], "contain": [0, 3, 4, 5, 6, 10, 12, 13, 15, 24, 26, 27, 43, 51, 53, 64, 66, 69, 72, 80, 81, 82, 83, 86, 87, 92, 93, 118, 123, 147, 149, 171, 174, 181, 184, 206, 210, 224, 227, 236, 245, 250, 259, 263, 291, 298, 342, 344, 349, 351, 352, 355, 357, 361, 362, 363, 369, 372], "container": [11, 79, 218, 220], "containerfil": [101, 102, 108, 110, 115, 126, 127, 128, 135, 137, 145, 147, 153], "content": [10, 86, 87, 100, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 128, 136, 147, 153, 168, 206, 212, 342, 348, 362, 371], "context": [104, 106, 110, 113, 116, 159, 161, 163, 329, 333, 336, 340, 342, 348, 355, 356, 362, 363], "contextu": [155, 157, 159, 162, 163], "contigu": [10, 206, 210, 342, 343, 344], "continu": [5, 8, 13, 80, 81, 86, 87, 107, 109, 118, 124, 137, 144, 159, 163, 191, 197, 245, 247, 248, 249, 250, 268, 289, 291, 298, 329, 331, 336, 341, 342, 347, 362, 364, 367, 369, 370], "contrast": [8, 191, 195, 196, 291, 297, 298], "control": [4, 8, 9, 16, 19, 20, 21, 25, 27, 51, 52, 64, 65, 84, 85, 94, 95, 96, 98, 100, 101, 102, 106, 108, 118, 121, 159, 163, 164, 167, 171, 174, 191, 192, 198, 202, 224, 228, 238, 241, 267, 268, 280, 285, 289, 336, 337, 341, 342, 343, 349, 352, 362, 363], "controversi": [291, 294], "conv1": [5, 7, 13, 14, 224, 227, 253, 256, 257], "conv2": 13, "conv2d": [5, 7, 13, 14, 224, 227, 253, 256, 257, 329, 332], "convei": [291, 298], "conveni": [1, 4, 5, 128, 130, 169, 170, 171, 173, 224, 226], "convent": [224, 225, 291, 298], "converg": [329, 333, 335, 336, 340, 342, 346, 355, 359, 362, 369], "convers": [118, 121, 123, 124, 238, 243], "convert": [3, 6, 9, 13, 101, 102, 104, 128, 131, 136, 137, 139, 147, 149, 181, 190, 198, 202, 224, 232, 237, 238, 241, 242, 243, 259, 262, 268, 283, 287, 290, 293, 299, 304, 313, 314, 315, 316, 327, 329, 331, 333, 342, 343, 344, 349, 350, 352, 355, 357, 362, 364, 365, 371], "convert_to_label": [137, 139], "convnext": [362, 371], "convolut": [224, 227, 329, 330], "cool": [291, 298], "coordin": [17, 22, 329, 330, 336, 337, 349, 350, 352, 362, 363, 371], "copi": [3, 5, 8, 82, 83, 84, 85, 90, 91, 92, 93, 164, 166, 181, 183, 191, 192, 224, 225, 232, 268, 289, 342, 344, 346, 349, 352, 355, 359, 361, 362, 369, 371], "cor": [17, 22, 53, 63], "cordern": [128, 129, 137, 138, 147, 148], "core": [4, 6, 7, 10, 11, 12, 14, 88, 89, 126, 127, 171, 173, 177, 187, 206, 208, 218, 220, 224, 226, 253, 256, 257, 259, 262, 268, 274, 287, 329, 330, 331, 333, 335, 336, 341, 342, 343, 346, 349, 352, 362, 363, 364], "corner": [82, 83, 268, 289], "correct": [5, 8, 13, 159, 163, 191, 197, 224, 227, 228, 231, 232, 234, 238, 240, 245, 247, 248, 268, 280, 289, 342, 347, 349, 350, 353, 354, 355, 357, 362, 363, 366, 367], "correct_squar": [3, 181, 185], "correct_square_mod": [3, 181, 185], "correctli": [1, 53, 63, 96, 97, 137, 139, 169, 170, 224, 226, 235, 237, 245, 250, 329, 331, 342, 344, 346, 349, 351, 355, 357, 362, 364, 365], "correl": [7, 14, 253, 257], "correspond": [2, 8, 13, 175, 179, 191, 197, 224, 236, 342, 345], "corrupt": [329, 330, 331], "cosin": [128, 136], "cost": [2, 5, 6, 8, 13, 84, 85, 88, 89, 90, 91, 92, 93, 105, 107, 108, 109, 110, 112, 117, 121, 126, 127, 159, 163, 175, 177, 191, 192, 194, 196, 259, 264, 336, 341], "costum": [268, 287, 289], "could": [3, 9, 101, 102, 106, 128, 135, 137, 139, 145, 147, 153, 181, 185, 189, 198, 204, 268, 277, 288, 289, 291, 297, 298, 309, 316, 325], "couldn": [268, 289], "count": [2, 9, 10, 15, 128, 130, 155, 157, 175, 177, 198, 204, 205, 206, 215, 291, 298, 329, 331, 336, 338, 342, 344, 346, 349, 351, 353, 356], "countri": [118, 123, 268, 289, 291, 294, 342, 348], "countrymen": [291, 294], "coup": [118, 122], "coupl": [291, 298], "cours": [17, 20, 79, 118, 125, 128, 135, 155, 157, 158, 164, 165, 291, 297, 298, 372], "cover": [17, 20, 80, 81, 82, 83, 84, 85, 88, 89, 100, 101, 102, 109, 110, 111, 117, 125, 126, 127, 155, 156, 157, 268, 283, 284, 290, 291, 292, 298, 299, 300, 306, 324, 353, 354], "cover_typ": [349, 351], "covtyp": [349, 351, 352, 354], "covtype_xgb_cpu": [349, 352], "coward": [291, 298], "cp": [10, 11, 15, 16, 206, 213, 218, 222], "cpu": [3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 80, 81, 84, 85, 88, 89, 128, 129, 131, 132, 133, 136, 137, 139, 140, 143, 144, 146, 147, 149, 155, 157, 158, 159, 161, 171, 174, 181, 187, 188, 190, 198, 203, 205, 206, 208, 210, 212, 213, 218, 222, 223, 224, 227, 230, 231, 232, 234, 235, 237, 238, 239, 244, 245, 252, 253, 256, 257, 259, 262, 263, 267, 268, 275, 276, 277, 283, 285, 288, 289, 290, 291, 293, 299, 301, 304, 305, 306, 329, 335, 336, 337, 341, 342, 346, 348, 350, 351, 352, 354, 355, 359, 361, 362, 367, 371], "cpus_per_work": [349, 352], "craft": [291, 297, 298], "crappi": [291, 294], "crash": [8, 147, 153, 191, 195, 245, 249, 268, 287, 289, 290, 349, 350, 362, 370], "crazi": [268, 287, 289, 291, 298], "cream": [268, 289], "creat": [3, 4, 7, 8, 9, 10, 12, 13, 14, 15, 16, 19, 20, 21, 23, 24, 26, 27, 31, 32, 34, 36, 37, 40, 41, 42, 46, 47, 48, 50, 52, 57, 58, 59, 60, 62, 65, 68, 72, 73, 75, 76, 77, 78, 80, 81, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 96, 98, 101, 102, 105, 108, 110, 113, 114, 115, 117, 118, 121, 122, 123, 125, 126, 127, 128, 129, 135, 137, 138, 139, 142, 147, 148, 149, 155, 158, 159, 163, 171, 173, 174, 176, 180, 181, 183, 190, 191, 193, 194, 198, 200, 202, 206, 209, 226, 234, 237, 238, 241, 242, 245, 250, 252, 253, 257, 267, 274, 275, 276, 277, 284, 285, 287, 288, 292, 293, 300, 309, 312, 315, 316, 324, 325, 326, 327, 329, 331, 343, 349, 354, 355, 356, 362, 363, 365, 366, 371], "create_us": 168, "creation": [3, 79, 181, 183], "cred": [268, 287, 289], "credenti": [28, 29, 35, 38, 43, 44, 53, 55, 66, 69, 72], "credit": [9, 198, 201, 291, 298], "creepi": [291, 298], "creepybr": [291, 298], "creighton": [268, 289], "crime": [291, 298], "criteria": [10, 11, 101, 102, 104, 206, 208, 218, 220], "criterion": [5, 7, 14, 224, 228, 238, 240, 245, 247, 253, 256, 257, 362, 367], "critic": [8, 86, 87, 191, 197, 268, 289], "crop": [329, 331, 362, 363, 364], "cross": [17, 21, 22, 24, 26, 53, 63, 224, 226, 355, 357, 362, 363], "cross_attention_dim": [6, 259, 262], "crossattndownblock2d": [6, 259, 262], "crossattnupblock2d": [6, 259, 262], "crossentropyloss": [5, 7, 13, 14, 137, 143, 224, 226, 228, 238, 240, 245, 247, 253, 254, 256, 257, 362, 367], "crouch": [291, 298], "crow": [291, 297], "crucial": [118, 124, 137, 146, 291, 293, 299, 304], "crush": [268, 289], "cry": [291, 294], "css": 0, "csv": [5, 8, 10, 13, 86, 87, 191, 192, 206, 210, 224, 226, 238, 239, 342, 344, 348, 349, 354, 355, 356, 357, 362, 369, 371], "csv_path": [355, 357], "ctor": [362, 371], "ctrl": [1, 169, 170], "cu128": [101, 102, 108, 110, 115, 159, 163], "cub": [268, 289], "cuda": [5, 7, 10, 13, 14, 15, 16, 101, 102, 107, 128, 131, 137, 139, 146, 147, 149, 150, 206, 213, 224, 228, 230, 231, 234, 235, 237, 238, 240, 253, 256, 268, 275, 276, 277, 280, 288, 289, 299, 304, 329, 335, 336, 341, 355, 361, 362, 371], "cultur": [291, 294], "cumprod": [6, 259, 262], "cumul": [137, 143, 342, 348], "cup": [291, 298], "cure": [291, 298], "curiou": [245, 252, 291, 294], "curl": [1, 147, 153, 168, 169, 170], "current": [3, 5, 8, 9, 12, 13, 14, 16, 43, 46, 53, 58, 66, 69, 92, 93, 118, 123, 164, 166, 181, 187, 191, 196, 198, 204, 224, 233, 234, 238, 241, 245, 248, 299, 306, 329, 333, 336, 337, 340, 342, 344], "current_training_step": [6, 259, 262], "cursor": [126, 127], "curti": [268, 287, 289, 290], "curv": [224, 226, 236, 332, 355, 359, 363, 364], "custom": [3, 4, 6, 11, 12, 20, 21, 24, 26, 27, 28, 29, 35, 36, 39, 43, 44, 53, 54, 66, 67, 80, 81, 84, 85, 88, 89, 92, 93, 96, 98, 101, 102, 108, 110, 115, 117, 118, 121, 125, 137, 144, 159, 161, 164, 167, 171, 173, 181, 186, 187, 212, 218, 220, 223, 224, 227, 259, 262, 291, 293, 294, 298, 329, 330, 342, 343, 346, 349, 350, 354, 363, 371], "custom_hid": 0, "custom_light": 0, "custom_nam": [118, 121], "customer_ingress_cidr_rang": [17, 22], "cut": [5, 6, 13, 259, 264, 355, 357], "cv": [118, 121], "cv_job_match": [118, 121], "cybersecur": [118, 121], "d": [9, 10, 15, 16, 128, 130, 131, 137, 139, 143, 146, 147, 153, 159, 163, 164, 166, 168, 198, 201, 202, 204, 206, 210, 211, 212, 268, 272, 273, 274, 280, 283, 287, 289, 290, 291, 297, 298, 329, 331, 333, 336, 338, 340, 341, 342, 343, 348, 349, 352, 353, 355, 357, 361, 362, 364, 371], "d3a9a7d0": [35, 39, 66, 70], "d89d0_00000": 13, "d_": [5, 13], "d_model": [355, 358, 359, 361], "da": [268, 289], "dag": [8, 128, 130, 147, 154, 191, 194, 197], "dai": [118, 123, 268, 289, 291, 298], "daili": [349, 354], "dalla": [268, 289], "damn": [268, 289], "dancer": [291, 298], "daniel": [268, 289, 291, 297], "dark": [168, 291, 297, 298], "darwin": [268, 289], "dash": [291, 298], "dashboard": [1, 5, 6, 8, 16, 24, 27, 82, 83, 92, 93, 110, 116, 117, 128, 134, 137, 144, 147, 152, 155, 157, 158, 159, 161, 162, 163, 165, 167, 169, 170, 191, 193, 195, 224, 225, 245, 252, 259, 261, 336, 341], "dask": [8, 191, 195, 196], "data": [2, 11, 12, 16, 17, 21, 22, 43, 51, 53, 64, 82, 83, 84, 85, 86, 87, 100, 102, 108, 118, 122, 123, 124, 125, 126, 127, 131, 134, 136, 137, 139, 141, 143, 144, 146, 147, 150, 155, 157, 159, 163, 165, 168, 175, 177, 182, 183, 194, 195, 211, 213, 218, 220, 221, 226, 228, 230, 232, 247, 251, 254, 257, 258, 260, 262, 264, 274, 277, 283, 287, 288, 290, 294, 295, 298, 299, 301, 302, 304, 306, 330, 333, 335, 336, 337, 338, 340, 341, 343, 346, 348, 350, 351, 352, 354, 356, 357, 359, 363, 364, 365], "data_dir": [355, 357, 359, 361], "data_load": [5, 6, 7, 13, 14, 224, 228, 238, 240, 241, 245, 247, 253, 255, 256, 257, 259, 262], "data_path": [9, 198, 201, 204], "data_url": [342, 344], "databas": [10, 118, 120, 123, 128, 133, 168, 206, 209], "databaseservic": [164, 167, 168], "databrick": [8, 10, 191, 192, 206, 210], "datadog": [155, 157], "datafram": [4, 8, 9, 12, 171, 174, 191, 195, 198, 202, 225, 226, 238, 242, 293, 329, 331, 333, 342, 344, 346, 348, 349, 353, 354, 355, 357, 362, 371], "dataload": [7, 13, 14, 226, 228, 234, 239, 240, 243, 253, 254, 255, 257, 260, 263, 302, 329, 331, 336, 338, 359, 363, 364, 367], "dataset": [4, 6, 7, 8, 10, 14, 16, 86, 87, 126, 127, 128, 130, 131, 132, 136, 137, 138, 139, 143, 146, 147, 150, 153, 164, 166, 171, 174, 191, 192, 193, 194, 200, 202, 204, 205, 206, 208, 209, 210, 211, 212, 214, 215, 216, 225, 227, 229, 232, 234, 237, 239, 240, 241, 243, 244, 245, 248, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 262, 263, 267, 271, 277, 280, 285, 286, 288, 289, 293, 298, 299, 301, 302, 304, 305, 306, 329, 330, 331, 333, 335, 340, 341, 343, 346, 348, 350, 352, 354, 356, 359, 361, 362, 363, 364, 365, 366, 371], "dataset_": [164, 166], "dataset_iter": [238, 241], "dataset_uri": [342, 344], "datasourc": [9, 10, 15, 198, 203, 206, 210], "date": [13, 118, 123, 268, 289], "datetim": [5, 13, 224, 226, 355, 357], "david": [168, 268, 287, 289], "dawson": [291, 297], "day_of_week": 12, "db": 168, "ddim": [329, 335], "ddp": [5, 6, 137, 144, 226, 227, 230, 231, 234, 235, 245, 248, 252, 259, 263, 329, 333, 342, 344, 345, 348, 355, 356, 359, 361, 362, 367, 371], "ddpmschedul": [6, 259, 260, 262], "ddpstrategi": [6, 259, 262], "de": [3, 181, 183, 268, 289, 332, 335, 336, 337, 341, 355, 361], "deactiv": [1, 169, 170], "dead": [291, 298], "deadlock": [3, 181, 188], "deal": [8, 191, 197], "dear": [268, 289], "debat": [268, 289], "debug": [8, 11, 82, 83, 94, 95, 100, 118, 125, 126, 127, 137, 144, 155, 157, 158, 159, 161, 163, 164, 165, 167, 191, 195, 218, 223, 224, 226, 233, 237, 329, 330], "debugg": [126, 127], "debut": [268, 289], "decemb": [268, 289], "decid": [2, 7, 14, 15, 118, 123, 175, 177, 224, 227, 235, 253, 257, 268, 289, 291, 298, 336, 337, 355, 359], "decis": [24, 26, 349, 350, 351], "declar": [224, 230, 329, 330, 336, 337, 342, 343, 349, 350, 362, 363], "decod": [109, 330, 335, 355, 356, 357, 358, 359, 361], "decode_and_norm": [329, 331], "decoder_input": [355, 358, 359], "decor": [2, 3, 10, 11, 16, 175, 178, 181, 187, 190, 206, 211, 215, 218, 222, 315, 316, 327], "decoupl": [11, 218, 223, 245, 252], "decreas": [128, 130, 224, 236, 355, 359, 362, 369], "dedic": [7, 14, 90, 91, 92, 93, 110, 115, 118, 124, 147, 153, 155, 158, 224, 237, 253, 257, 291, 298], "dedupl": [14, 299, 306], "deep": [9, 110, 116, 198, 201, 224, 226, 267, 268, 285, 291, 298, 299, 301, 302, 305, 306, 342, 344, 348, 362, 364], "deeper": [8, 101, 102, 109, 118, 120, 125, 191, 193], "deepli": [159, 163], "deepseek": [118, 124], "deepspe": [5, 6, 13, 137, 144, 245, 252, 259, 264], "def": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 82, 83, 88, 89, 90, 91, 118, 123, 128, 130, 131, 136, 137, 139, 140, 141, 143, 146, 147, 149, 150, 159, 163, 164, 166, 167, 169, 170, 171, 174, 175, 178, 180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 198, 202, 206, 212, 213, 215, 218, 222, 224, 227, 228, 231, 232, 233, 234, 237, 238, 240, 241, 243, 245, 247, 248, 253, 255, 256, 257, 259, 262, 263, 268, 275, 276, 277, 288, 291, 298, 299, 303, 304, 305, 313, 314, 315, 316, 327, 329, 331, 332, 333, 335, 336, 338, 339, 340, 341, 342, 344, 345, 346, 349, 352, 353, 355, 357, 358, 359, 361, 362, 365, 366, 367, 371], "default": [0, 3, 5, 6, 7, 9, 10, 11, 12, 28, 30, 35, 38, 39, 43, 45, 53, 56, 66, 69, 80, 81, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 118, 123, 126, 127, 128, 129, 131, 133, 135, 137, 138, 139, 145, 147, 148, 153, 159, 163, 164, 167, 181, 185, 187, 198, 201, 202, 206, 210, 212, 218, 223, 224, 227, 245, 247, 253, 257, 259, 262, 291, 296, 299, 306, 329, 331, 333, 349, 351, 355, 356, 362, 371], "default_cluster_storag": [164, 166], "default_data_col": [299, 304], "default_root_dir": [6, 259, 262, 329, 333, 336, 340], "default_tracing_servic": 168, "defens": [268, 289, 355, 359], "defin": [2, 3, 4, 5, 7, 10, 11, 12, 14, 19, 43, 50, 53, 62, 66, 69, 80, 81, 84, 85, 90, 91, 92, 93, 101, 102, 105, 110, 113, 118, 122, 124, 128, 135, 137, 139, 140, 143, 144, 145, 147, 150, 153, 159, 161, 171, 174, 175, 180, 181, 185, 187, 190, 206, 212, 218, 222, 223, 225, 230, 231, 232, 234, 237, 239, 244, 253, 257, 267, 268, 280, 283, 285, 289, 290, 291, 298, 299, 301, 304, 305, 315, 316, 327, 329, 331, 333, 344, 350, 353, 354, 355, 356, 357, 361, 362, 365, 366, 367, 371], "definit": [84, 85, 118, 122, 123, 336, 338], "degre": [9, 10, 15, 198, 204, 206, 215], "del": [224, 237], "delai": [16, 291, 297], "deleg": [4, 171, 174], "delet": [5, 17, 22, 28, 30, 33, 35, 42, 43, 45, 51, 53, 56, 64, 66, 78, 86, 87, 224, 226, 245, 251, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "delete_object": [86, 87], "deleteobject": [17, 22], "delhi": [268, 289], "deliv": [159, 163, 291, 297], "deloy": [92, 93], "delta": [8, 101, 102, 108, 110, 114, 115, 118, 121, 191, 192, 349, 354], "demand": [1, 16, 24, 25, 27, 80, 81, 84, 85, 92, 93, 128, 132, 133, 136, 137, 139, 143, 146, 169, 170, 329, 335, 357], "demo": [17, 20, 224, 228, 329, 335, 336, 341, 342, 348], "demonstr": [6, 20, 86, 87, 110, 111, 118, 120, 126, 127, 159, 163, 164, 165, 166, 168, 224, 237, 259, 260, 268, 283, 290, 291, 295, 297, 298, 299, 301, 306, 329, 331, 334, 336, 340, 342, 344, 347, 348, 355, 360, 363, 364], "denizen": [291, 294], "denni": [268, 289], "deped": [84, 85], "depend": [0, 8, 10, 17, 22, 28, 30, 43, 45, 53, 56, 73, 80, 81, 82, 83, 84, 85, 100, 101, 102, 104, 106, 128, 129, 135, 137, 138, 145, 147, 148, 153, 168, 182, 188, 191, 194, 195, 196, 206, 212, 238, 239, 284, 292, 299, 300, 301, 324, 329, 331, 336, 338, 342, 344, 349, 351, 355, 356, 357, 362, 364], "depict": [291, 298], "deploi": [4, 11, 12, 16, 17, 19, 20, 21, 22, 30, 39, 45, 56, 79, 84, 85, 92, 93, 101, 102, 103, 104, 106, 107, 108, 109, 112, 114, 117, 119, 120, 122, 123, 125, 126, 127, 147, 148, 153, 164, 167, 168, 171, 174, 218, 219, 223, 267, 285, 309, 323, 325, 328, 336, 341, 349, 354, 355, 361, 372], "deploy": [0, 4, 8, 12, 18, 19, 22, 23, 26, 28, 30, 35, 36, 38, 39, 43, 45, 47, 48, 53, 56, 59, 60, 63, 66, 69, 70, 75, 76, 92, 93, 94, 95, 106, 107, 109, 111, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 150, 151, 152, 153, 159, 162, 164, 167, 171, 174, 191, 197, 222, 223, 309, 320, 325, 342, 348, 355, 361, 362, 371], "deployment_config": [101, 102, 108, 110, 113, 116, 118, 123], "deploymenthandl": [147, 150, 316, 320, 327], "deploymentrespons": 16, "depress": [291, 294], "depth": 13, "deriv": [342, 346], "derp": [28, 30, 43, 45, 53, 56], "desc": [5, 362, 364], "descent": [291, 298], "describ": [28, 30, 43, 45, 53, 56, 57, 291, 298, 336, 337], "descript": [1, 9, 17, 21, 22, 101, 102, 106, 121, 123, 147, 150, 169, 170, 198, 201, 336, 338], "deseri": [2, 8, 175, 177, 191, 195], "design": [1, 2, 8, 9, 10, 16, 79, 101, 102, 105, 118, 121, 125, 137, 144, 169, 170, 175, 177, 191, 192, 195, 196, 198, 200, 203, 206, 211, 291, 298, 299, 301, 305, 306, 309, 316, 325, 349, 352, 355, 356, 362, 367], "desir": [13, 362, 371], "desktop": [82, 83], "despit": [291, 298], "destin": [118, 121], "destroi": [28, 30, 33, 35, 42, 43, 51, 53, 64, 66, 78, 268, 289, 291, 298], "destruct": [291, 298], "detach": [137, 143, 224, 237, 355, 361], "detail": [3, 5, 6, 7, 9, 10, 11, 13, 14, 16, 17, 20, 22, 24, 25, 35, 39, 43, 45, 53, 56, 66, 70, 86, 87, 88, 89, 96, 98, 110, 116, 118, 121, 128, 133, 137, 139, 147, 150, 166, 167, 168, 181, 187, 198, 204, 206, 208, 218, 220, 223, 224, 225, 230, 234, 238, 239, 240, 253, 255, 259, 261, 263, 284, 291, 292, 298, 300, 315, 316, 324, 327, 372], "detailsbr": [291, 298], "detect": [8, 118, 121, 159, 163, 191, 195, 245, 250, 268, 275, 276, 277, 288, 329, 334, 362, 369], "determin": [10, 101, 102, 104, 206, 211, 291, 296], "determinist": [349, 351], "determint": [10, 206, 215], "dev": [28, 30, 43, 45, 53, 56, 126, 127, 336, 337], "deval": [349, 352], "develop": [2, 4, 8, 9, 12, 16, 28, 29, 30, 35, 37, 43, 44, 45, 53, 55, 56, 66, 68, 80, 81, 86, 87, 88, 89, 90, 91, 92, 93, 96, 98, 110, 111, 128, 130, 134, 155, 157, 164, 167, 171, 173, 175, 177, 191, 195, 196, 197, 198, 204, 219, 220, 268, 289, 299, 305, 362, 363, 372], "deviat": [362, 365], "devic": [2, 5, 6, 7, 10, 11, 13, 24, 27, 51, 52, 64, 65, 101, 102, 108, 128, 131, 136, 137, 139, 140, 141, 146, 147, 149, 150, 175, 177, 206, 213, 218, 222, 223, 224, 227, 228, 230, 231, 232, 234, 235, 237, 238, 240, 241, 253, 256, 257, 259, 262, 263, 267, 268, 280, 285, 289, 299, 301, 304, 329, 332, 333, 335, 336, 337, 340, 341, 355, 359, 361, 362, 363, 366, 367, 371], "devop": [79, 100], "df": [4, 6, 9, 171, 174, 198, 201, 202, 203, 204, 238, 242, 259, 262, 329, 333, 336, 340, 342, 344, 346, 348, 349, 351, 353, 354, 355, 357, 359, 362, 365, 369], "di": [3, 181, 185, 291, 297], "diagnos": [155, 157], "diagnost": [355, 359], "diagon": [349, 353], "diagram": [4, 6, 7, 8, 13, 14, 17, 21, 24, 26, 100, 101, 102, 107, 171, 174, 191, 195, 197, 224, 225, 234, 253, 257, 259, 262, 263, 299, 301], "diari": [268, 289], "dict": [3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 101, 102, 108, 110, 113, 116, 118, 121, 123, 137, 143, 159, 163, 171, 174, 181, 184, 206, 212, 213, 215, 218, 222, 224, 228, 234, 238, 240, 241, 243, 244, 245, 247, 248, 252, 253, 257, 259, 262, 263, 268, 271, 275, 276, 277, 286, 288, 299, 302, 304, 329, 331, 336, 338, 355, 361], "dictat": [291, 298], "dictionari": [3, 4, 7, 14, 171, 174, 181, 187, 224, 229, 233, 238, 240, 241, 253, 257, 299, 304, 342, 346], "did": [3, 181, 185, 268, 289, 291, 297, 298, 329, 333, 335, 336, 340, 341], "didn": [268, 289], "diego": [268, 289], "differ": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 20, 22, 24, 26, 66, 71, 79, 82, 83, 86, 87, 101, 102, 104, 105, 106, 112, 118, 120, 121, 126, 127, 128, 134, 135, 137, 144, 147, 151, 155, 158, 159, 162, 164, 165, 167, 169, 170, 171, 174, 175, 180, 191, 194, 198, 204, 206, 212, 215, 218, 221, 224, 225, 228, 230, 238, 239, 240, 241, 253, 257, 259, 261, 267, 268, 285, 289, 291, 294, 299, 304, 305, 329, 335, 336, 341, 349, 352, 355, 359, 362, 363, 371], "differenti": [349, 350], "difficult": [8, 159, 163, 191, 195], "diffus": [5, 13, 260, 263, 264, 333, 338, 340], "diffusionpolici": [340, 341], "digit": [7, 11, 14, 218, 222, 227, 238, 242, 253, 255, 256], "dii": [101, 102, 106], "dilat": 13, "dim": [137, 140, 143, 224, 237, 329, 332, 336, 337, 339, 342, 345, 362, 367, 371], "dimens": [224, 237, 342, 343, 346, 355, 357], "dimension": [349, 350, 351], "dinger": [268, 289], "dir": [84, 85, 86, 87, 88, 89, 147, 153, 224, 226, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364, 367, 371], "direct": [7, 14, 17, 22, 24, 26, 118, 124, 253, 257, 291, 298, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "directli": [3, 8, 17, 22, 24, 26, 27, 82, 83, 88, 89, 96, 98, 118, 122, 181, 183, 184, 191, 192, 238, 239, 329, 330, 333, 336, 337, 341, 342, 343, 344, 346, 349, 351, 352, 355, 356, 362, 363, 369], "director": [291, 297, 298], "directori": [0, 5, 9, 43, 46, 53, 58, 82, 83, 86, 87, 92, 93, 128, 135, 168, 198, 203, 224, 226, 234, 237, 245, 251, 329, 335, 342, 344, 348, 349, 350, 354, 355, 357, 361, 362, 367, 371], "dirpath": [329, 333, 336, 340], "disabl": [6, 13, 14, 16, 128, 133, 137, 139, 147, 150, 224, 237, 259, 263, 299, 306], "disaggreg": [10, 206, 217], "disappoint": [268, 287, 289], "discern": [291, 294], "disconnect": [342, 346], "discontinu": [336, 337], "discount": [342, 348], "discov": [291, 298], "discret": [8, 128, 135, 191, 195], "discuss": [7, 12, 14, 253, 256, 257], "disengag": [9, 198, 201], "disjoint": [224, 228, 291, 297, 298], "disk": [5, 10, 15, 86, 87, 88, 89, 155, 157, 159, 161, 164, 166, 167, 206, 208, 214, 224, 226, 238, 242, 245, 248, 329, 335, 342, 348, 349, 354, 355, 361, 362, 371], "dismiss": [268, 289], "displai": [1, 5, 13, 17, 22, 90, 91, 110, 113, 164, 167, 169, 170, 224, 226, 236, 237, 245, 250, 291, 294, 298, 333, 362, 369], "display_top_match": [128, 136], "disrupt": [291, 297], "dist": [238, 244], "dist_val_acc": [362, 367], "distanc": [4, 7, 9, 12, 14, 171, 174, 198, 201, 204, 253, 257, 329, 335, 349, 354], "distil": [101, 102, 105, 355, 361], "distilbert": [313, 314, 315, 316, 327], "distinct": [24, 26, 101, 102, 104, 159, 162, 224, 227, 342, 344], "distract": [291, 298], "distribut": [1, 2, 3, 7, 9, 10, 12, 14, 15, 17, 22, 84, 85, 88, 89, 100, 101, 102, 106, 107, 110, 116, 117, 126, 127, 128, 129, 130, 132, 143, 147, 149, 151, 168, 169, 170, 172, 173, 175, 177, 181, 182, 183, 188, 192, 196, 197, 198, 199, 200, 201, 203, 205, 206, 207, 208, 210, 211, 214, 226, 227, 228, 230, 232, 235, 236, 238, 239, 242, 243, 244, 245, 252, 253, 256, 260, 261, 268, 283, 290, 291, 293, 295, 298, 302, 304, 305, 306, 309, 316, 325, 331, 335, 337, 338, 341, 344, 348, 351, 354, 357, 359, 364, 367, 371], "distributeddataparallel": [5, 6, 13, 224, 225, 228, 231, 234, 235, 259, 263, 342, 345], "distributedsampl": [5, 13, 224, 228, 232, 234, 362, 366, 367], "div_term": [355, 358], "dive": [100, 101, 102, 109, 118, 119, 120, 125, 355, 357], "divers": [8, 191, 192, 195], "divid": [8, 17, 22, 191, 195], "dl_dw": [7, 14, 253, 257], "dmatrix": [4, 12, 171, 174, 349, 351, 352, 353], "dn": [24, 26], "do": [3, 4, 7, 9, 12, 13, 14, 35, 37, 66, 68, 82, 83, 86, 87, 118, 121, 123, 137, 139, 147, 149, 171, 172, 181, 184, 187, 198, 202, 203, 204, 253, 256, 267, 268, 280, 285, 289, 291, 294, 297, 298], "doc": [1, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 22, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 88, 89, 96, 98, 101, 102, 108, 118, 121, 164, 167, 169, 170, 198, 201, 203, 204, 206, 215, 216, 218, 223, 224, 230, 236, 259, 263, 268, 274, 287, 299, 306, 309, 315, 316, 325, 327], "docker": [24, 26], "dockerfil": [101, 102, 108, 110, 115], "document": [7, 8, 14, 16, 17, 22, 24, 25, 82, 83, 90, 91, 101, 102, 105, 109, 110, 117, 118, 122, 124, 125, 126, 127, 164, 167, 191, 192, 196, 197, 253, 257], "documentari": [291, 294], "doe": [3, 4, 6, 7, 8, 9, 10, 14, 24, 27, 88, 89, 90, 91, 101, 102, 105, 106, 137, 139, 147, 154, 155, 158, 159, 163, 171, 174, 181, 187, 188, 191, 196, 198, 201, 206, 212, 238, 243, 253, 257, 259, 262, 268, 289, 290, 291, 294, 298, 329, 335, 336, 341, 342, 348], "doesn": [3, 5, 9, 10, 15, 24, 26, 86, 87, 101, 102, 106, 128, 130, 137, 138, 181, 184, 198, 204, 206, 215, 224, 226, 268, 289, 291, 294, 297, 298, 355, 357], "doesnt": [291, 298], "dog": [126, 127, 147, 150, 362, 363], "doggo": [126, 127, 128, 129, 130, 133, 135, 136, 137, 138, 139, 142, 143, 144, 146, 147, 148, 150, 153], "dogma": [291, 298], "dolocationid": [9, 198, 201], "domain": [349, 353, 354], "domin": [8, 191, 195, 349, 353], "don": [1, 2, 3, 5, 9, 10, 15, 100, 128, 135, 137, 139, 145, 147, 153, 169, 170, 175, 180, 181, 186, 198, 202, 206, 212, 224, 225, 226, 230, 268, 289, 291, 294, 298, 329, 331, 335, 342, 348, 362, 363, 366], "donald": [268, 289], "done": [5, 9, 13, 28, 33, 35, 42, 66, 78, 82, 83, 88, 89, 90, 91, 92, 93, 101, 102, 108, 110, 114, 115, 128, 129, 137, 138, 139, 147, 148, 164, 166, 198, 204, 224, 228, 237, 291, 297, 298, 316, 323, 328, 342, 344, 349, 350], "donkei": [137, 144], "dont": [291, 298], "dool": [268, 289], "dorset": [268, 289], "dot": [3, 181, 187, 268, 287, 289, 329, 330, 336, 337, 342, 343, 345, 348, 349, 350, 362, 363], "dot_product": [342, 345], "dotenv": [137, 138], "doubl": [268, 287, 289, 291, 294], "down": [5, 6, 13, 16, 24, 26, 27, 53, 63, 90, 91, 101, 102, 106, 116, 147, 151, 224, 234, 259, 264, 267, 268, 285, 289, 291, 293, 298, 299, 301, 336, 337], "down_block_typ": [6, 259, 262], "downblock2d": [6, 259, 262], "download": [1, 5, 7, 13, 14, 15, 16, 66, 69, 82, 83, 86, 87, 88, 89, 92, 93, 100, 118, 121, 169, 170, 232, 245, 251, 253, 255, 257, 312, 316, 326, 342, 344, 355, 356, 357], "downsampl": 13, "downsample_pad": [6, 259, 262], "downscal": [16, 128, 133, 136, 137, 146, 316, 323, 328], "downscale_delay_": 16, "downstream": [10, 15, 118, 120, 122, 206, 214, 291, 293, 342, 348], "downtim": [92, 93, 126, 127, 147, 151, 153, 164, 167], "dp": [137, 140, 143], "draft": [268, 287, 289, 290], "drag": [268, 289], "dragon": [291, 298], "drama": [291, 294], "drastic": [126, 127], "draw": [291, 298], "dread": [268, 289], "dream": [291, 297, 298], "dreambr": [291, 298], "dreamnightmar": [291, 298], "dress": [268, 289], "drill": [8, 191, 193], "drive": [291, 297], "driver": [3, 10, 24, 27, 128, 134, 159, 161, 164, 167, 181, 184, 188, 206, 208, 213, 268, 289, 342, 344, 349, 353, 355, 361, 362, 371], "driver_artifact": [12, 13, 299, 306], "drop": [101, 102, 107, 224, 232, 237, 268, 289, 329, 331, 349, 352, 355, 357, 362, 371], "drop_column": [12, 128, 131, 137, 139, 329, 331], "drop_last": [5, 7, 13, 14, 224, 232, 253, 255, 257, 355, 357], "dropdown": [82, 83, 110, 116, 164, 166], "dropna": [342, 346, 355, 359, 362, 369], "dropout": [137, 140, 355, 358], "dropout_p": [137, 140, 143, 144], "ds_adjust": [9, 198, 202, 203], "ds_block_based_shuffl": [9, 198, 204], "ds_file_shuffl": [9, 198, 204], "ds_gener": [137, 143], "ds_iter": [349, 352], "ds_label": [10, 206, 212], "ds_limit": [9, 198, 203], "ds_meta": [291, 295, 297], "ds_normal": [10, 15, 206, 212, 213], "ds_pred": [10, 15, 206, 213, 214, 215, 216], "ds_randomized_block": [10, 15, 206, 215], "ds_randomized_row": [10, 15, 206, 215], "ds_review": [291, 295, 296], "ds_row_based_shuffl": [9, 198, 204], "ds_tip": [9, 198, 202], "ds_tmp": [362, 371], "dsl": [8, 191, 194], "dtest": [4, 171, 174], "dtrain": [4, 171, 174, 349, 352], "dtype": [6, 12, 16, 128, 130, 137, 141, 144, 146, 159, 163, 238, 243, 259, 262, 268, 289, 290, 329, 331, 336, 338, 341, 342, 346, 355, 357, 358], "due": [3, 5, 6, 8, 10, 11, 101, 102, 105, 110, 116, 128, 133, 136, 137, 146, 159, 163, 181, 185, 191, 195, 206, 208, 212, 218, 220, 224, 225, 259, 261, 268, 274, 287, 291, 297, 298, 316, 320, 328], "dummi": [101, 102, 108, 118, 123], "dummy_data_1000_500": [86, 87], "dummy_data_1000_720": [86, 87], "dummy_data_xxl": [86, 87], "dummy_kei": [102, 108], "dump": [11, 16, 118, 123, 137, 139, 140, 143, 164, 167, 218, 222], "duplic": [224, 225, 234, 245, 248, 355, 357, 362, 363], "durabl": [8, 191, 192], "durat": [88, 89, 164, 167, 168], "dure": [0, 3, 5, 6, 13, 28, 30, 43, 45, 53, 56, 86, 87, 101, 102, 105, 106, 128, 130, 164, 167, 181, 187, 224, 225, 233, 236, 238, 244, 259, 263, 268, 283, 290, 291, 294, 298, 299, 301, 304, 329, 330, 331, 333, 342, 346, 348, 349, 351, 352, 355, 356, 358, 361, 362, 364, 369], "dustin": [268, 289], "dvd": [291, 297], "dynam": [8, 9, 11, 16, 17, 22, 24, 25, 118, 121, 137, 144, 147, 151, 191, 197, 198, 204, 218, 220, 355, 356], "dynamic_lora_loading_path": [118, 121], "e": [0, 2, 3, 5, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 22, 24, 26, 27, 28, 29, 30, 35, 36, 43, 44, 45, 49, 50, 51, 53, 54, 56, 61, 62, 64, 66, 67, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 101, 102, 105, 110, 116, 126, 127, 128, 129, 137, 138, 147, 148, 155, 157, 159, 161, 164, 166, 175, 177, 181, 184, 185, 187, 191, 192, 195, 198, 202, 203, 206, 208, 210, 212, 218, 220, 221, 223, 224, 228, 229, 234, 235, 236, 238, 239, 241, 245, 246, 249, 252, 253, 257, 267, 268, 285, 329, 330, 336, 337, 342, 343, 346, 349, 352, 355, 361, 362, 364], "each": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 22, 43, 46, 53, 58, 79, 84, 85, 86, 87, 88, 89, 92, 93, 96, 98, 101, 102, 104, 105, 110, 113, 116, 118, 120, 121, 125, 126, 127, 128, 130, 131, 137, 139, 143, 146, 147, 149, 151, 159, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 174, 175, 180, 181, 183, 191, 197, 198, 201, 202, 203, 206, 210, 212, 213, 218, 221, 224, 225, 226, 227, 228, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 248, 253, 255, 257, 259, 262, 263, 267, 268, 272, 273, 274, 277, 280, 285, 287, 288, 289, 291, 295, 298, 299, 301, 304, 305, 309, 315, 316, 325, 327, 329, 330, 331, 336, 337, 340, 342, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 355, 356, 357, 359, 361, 362, 363, 364, 365, 367, 369, 371], "earli": [7, 14, 253, 257, 268, 289, 329, 335, 349, 354, 355, 357, 361, 362, 371], "earlier": [82, 83, 84, 85, 88, 89, 128, 132, 291, 297, 329, 331, 362, 370], "early_stopping_round": [349, 354], "earn": [268, 289], "earth": [291, 297], "eas": [8, 191, 194], "easi": [0, 8, 9, 16, 100, 101, 102, 106, 110, 116, 118, 122, 123, 125, 128, 132, 147, 149, 159, 163, 164, 167, 191, 197, 198, 200, 224, 236, 238, 239, 299, 304, 305, 309, 316, 325, 329, 330], "easier": [6, 8, 24, 26, 128, 133, 134, 191, 197, 259, 262, 291, 298, 299, 301, 342, 348], "easili": [2, 3, 4, 5, 6, 7, 9, 10, 11, 92, 93, 101, 102, 103, 107, 110, 111, 118, 119, 128, 135, 137, 142, 145, 147, 153, 159, 163, 171, 172, 175, 176, 181, 182, 198, 199, 206, 207, 218, 219, 253, 254, 259, 260, 267, 268, 285, 291, 293, 299, 301, 305, 309, 316, 325, 336, 337], "eastern": [291, 297, 298], "eat": [291, 297], "ec2": [18, 20, 22, 23, 24, 26, 27, 30, 31, 34, 43, 45, 53, 56, 88, 89, 372], "echo": [66, 69, 137, 138, 268, 289], "eclips": [268, 289], "ecolog": [349, 354], "ecosystem": [4, 5, 6, 8, 12, 126, 127, 171, 173, 191, 195, 196, 224, 225, 259, 261], "ed": [268, 289], "eddi": [268, 289, 291, 297, 298], "edg": [268, 289, 355, 361], "edgecolor": [342, 344], "edit": [82, 83, 84, 85, 126, 127, 268, 289], "editor": [35, 37, 66, 68, 82, 83, 84, 85, 90, 91], "educ": [362, 363], "ef": [15, 16, 21, 23, 24, 26, 28, 30, 34, 53, 57], "effect": [3, 4, 17, 22, 86, 87, 110, 112, 118, 121, 126, 127, 171, 174, 181, 184, 224, 229, 291, 297, 298, 342, 345, 346, 355, 361], "effici": [1, 8, 10, 11, 84, 85, 96, 99, 101, 102, 104, 110, 112, 118, 121, 126, 127, 128, 130, 132, 133, 169, 170, 191, 192, 195, 196, 206, 208, 218, 220, 221, 224, 225, 237, 238, 239, 242, 245, 252, 267, 268, 283, 285, 290, 291, 293, 298, 299, 301, 304, 305, 306, 329, 330, 331, 342, 343, 344, 346, 348, 355, 356, 357, 361, 362, 363, 365, 371], "efs_id": [17, 23, 28, 30], "egress": [24, 26], "eid": [268, 289], "eight": [336, 340, 362, 368], "eip": [28, 30, 43, 45, 53, 56], "eipalloc": [28, 30, 43, 45, 53, 56], "either": [9, 10, 15, 90, 91, 101, 102, 106, 164, 167, 198, 202, 206, 209, 224, 237, 268, 280, 289, 291, 298, 342, 344], "eject": [268, 289], "ek": [17, 18, 20, 22, 24, 27, 45, 46, 47, 52, 55, 56, 58, 59, 65, 126, 127, 372], "eks_cluster_nam": [43, 45, 46, 53, 56, 57, 58], "elam": [291, 297], "elaps": 12, "elast": [17, 22, 28, 30, 43, 45, 53, 56, 137, 144], "element": [16, 291, 298], "elev": [349, 350, 353], "elif": [299, 304, 329, 335, 355, 357], "elimin": [8, 101, 102, 105, 191, 192, 349, 351], "ellipsi": [94, 95], "els": [5, 7, 10, 92, 93, 118, 123, 137, 140, 146, 206, 213, 224, 237, 245, 251, 253, 256, 268, 280, 289, 299, 304, 313, 314, 315, 316, 327, 329, 335, 336, 341, 342, 344, 346, 348, 349, 352, 355, 357, 359, 361, 362, 367, 371], "elsewher": [126, 127], "elt": [8, 191, 192], "email": [35, 39, 66, 70, 96, 98, 164, 167, 168], "emb": [128, 135, 136, 137, 139, 268, 275, 276, 277, 280, 288, 289, 342, 343, 348], "embd": [147, 149], "embed": [0, 90, 91, 126, 127, 133, 135, 136, 137, 139, 140, 141, 146, 147, 149, 267, 268, 277, 280, 284, 285, 288, 289, 290, 292, 300, 324, 329, 335, 336, 341, 344, 345, 348], "embedd": 0, "embedding_dim": [137, 140, 143, 144, 224, 229, 342, 345, 346, 348], "embedding_gener": [128, 136], "embeddings_d": [128, 131, 133, 136], "embeddings_path": [128, 133, 136], "embedimag": [128, 131, 136, 137, 139], "emit": [101, 102, 105, 329, 333], "emmanuel": [268, 289], "emotion": [291, 297, 298], "emploi": [8, 191, 192], "empti": [28, 30, 33, 35, 42, 43, 45, 51, 53, 64, 66, 78, 92, 93], "emption": [355, 356], "en": [1, 14, 101, 102, 108, 169, 170, 268, 274, 287, 291, 298, 299, 306, 309, 315, 316, 325, 327], "enabl": [2, 3, 4, 8, 9, 10, 11, 12, 17, 21, 22, 24, 25, 27, 37, 68, 78, 84, 85, 86, 87, 88, 89, 92, 93, 100, 117, 118, 120, 121, 123, 125, 126, 127, 137, 138, 147, 151, 155, 158, 159, 163, 164, 167, 168, 171, 173, 175, 177, 181, 182, 187, 191, 192, 195, 197, 198, 203, 205, 206, 208, 218, 221, 238, 239, 246, 248, 249, 252, 267, 268, 283, 285, 290, 291, 293, 295, 298, 299, 305, 329, 330, 331, 335, 336, 337, 341, 342, 343, 346, 349, 350, 355, 356, 357, 361, 362, 363, 368, 371], "enable_access_log": [164, 167], "enable_auto_tool_choic": [118, 123], "enable_checkpoint": [6, 259, 263], "enable_filestor": [35, 39], "enable_lora": [118, 121], "enable_progress_bar": [329, 333, 336, 340], "encapsul": [224, 235, 362, 363], "encod": [101, 102, 104, 164, 167, 268, 275, 276, 277, 280, 288, 289, 291, 293, 335, 336, 337, 343, 348, 355, 357, 358], "encode_batch": [342, 344], "encount": [3, 5, 6, 66, 71, 181, 185, 259, 260, 316, 320, 328], "encourag": [329, 330, 342, 343], "end": [3, 8, 16, 24, 26, 79, 88, 89, 101, 102, 104, 105, 106, 108, 110, 114, 115, 117, 118, 121, 124, 137, 143, 164, 167, 172, 173, 181, 187, 191, 195, 196, 238, 244, 245, 250, 252, 268, 289, 291, 297, 298, 316, 320, 328, 329, 330, 335, 336, 337, 341, 342, 348, 349, 350, 354, 355, 356, 357, 361, 362, 363, 371], "end_run": [137, 143], "end_tim": [137, 144], "endpoint": [11, 16, 24, 26, 92, 93, 101, 102, 106, 108, 110, 114, 115, 117, 218, 222, 313, 314, 315, 316, 327, 329, 335, 342, 348, 349, 354], "enforc": [0, 3, 8, 10, 84, 85, 118, 122, 181, 187, 191, 192, 206, 212], "engag": [9, 118, 121, 198, 201], "engin": [2, 5, 6, 9, 10, 11, 16, 17, 20, 24, 27, 39, 69, 70, 78, 79, 94, 95, 108, 110, 116, 118, 121, 128, 130, 147, 151, 164, 165, 175, 177, 192, 194, 195, 196, 198, 205, 206, 208, 218, 220, 224, 225, 259, 261, 299, 301, 306, 349, 353, 354, 355, 356, 357], "engine_arg": [101, 102, 108], "engine_kwarg": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123], "english": [313, 314, 315, 316, 327], "enhanc": [8, 118, 120, 123, 155, 157, 159, 162, 163, 191, 192, 196], "enjoi": [268, 289, 291, 298], "enough": [3, 10, 15, 84, 85, 181, 190, 206, 215, 224, 234, 329, 331, 342, 344, 362, 364], "ensembl": [147, 151, 172, 349, 350], "ensu": [291, 298], "ensur": [1, 3, 5, 6, 8, 9, 13, 28, 29, 31, 35, 37, 40, 43, 44, 47, 53, 55, 59, 63, 66, 68, 75, 80, 81, 84, 85, 118, 122, 126, 127, 128, 132, 137, 141, 155, 158, 164, 165, 169, 170, 181, 187, 190, 191, 192, 198, 203, 224, 225, 228, 231, 234, 237, 238, 239, 241, 243, 245, 248, 259, 261, 268, 280, 284, 289, 292, 299, 300, 304, 324, 329, 333, 336, 341, 342, 344, 346, 349, 351, 352, 354, 355, 357, 362, 367], "enter": [82, 83, 90, 91, 92, 93, 168, 291, 294], "enterpris": [92, 93, 101, 102, 107, 110, 115, 117, 118, 125, 126, 127], "enthus": [137, 144], "entir": [3, 5, 6, 8, 9, 10, 15, 24, 25, 66, 67, 94, 95, 101, 102, 105, 126, 127, 128, 130, 132, 137, 139, 146, 181, 187, 191, 195, 198, 201, 206, 211, 214, 215, 224, 225, 228, 259, 262, 267, 285, 291, 298, 299, 304, 329, 335, 342, 344, 348, 349, 352, 355, 357, 362, 363], "entiti": [94, 95], "entri": [168, 238, 243, 355, 361], "entropi": [362, 363], "entrypoint": [16, 90, 91], "enum": [118, 122, 123], "enumer": [7, 14, 137, 139, 143, 146, 253, 255, 291, 295, 342, 344, 348], "env": [3, 84, 85, 110, 115, 128, 129, 137, 138, 147, 148, 181, 186, 224, 226, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "env_var": [3, 110, 113, 116, 118, 121, 122, 123, 137, 138, 181, 186, 187], "environ": [0, 4, 5, 6, 8, 9, 10, 11, 17, 19, 22, 28, 29, 34, 35, 36, 43, 44, 52, 53, 54, 65, 66, 67, 69, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 100, 101, 102, 104, 110, 113, 116, 118, 121, 123, 126, 127, 128, 129, 130, 137, 138, 147, 148, 151, 155, 156, 158, 159, 163, 164, 165, 166, 167, 171, 174, 182, 187, 191, 195, 198, 203, 206, 213, 218, 221, 222, 259, 262, 267, 268, 285, 299, 301, 302, 305, 309, 316, 325, 329, 330, 331, 333, 338, 340, 341, 342, 344, 349, 350, 351, 355, 357, 362, 363, 364], "environment": [84, 85], "eot": [28, 30, 35, 39, 43, 45, 53, 56, 66, 70], "ep": [13, 137, 140], "ephemer": [224, 226], "epic": [268, 289], "episod": [336, 341], "epoch": [5, 7, 13, 14, 128, 130, 137, 143, 224, 228, 233, 234, 236, 238, 239, 240, 245, 246, 247, 248, 250, 252, 253, 256, 257, 299, 304, 305, 306, 329, 332, 333, 334, 335, 336, 337, 339, 340, 341, 342, 343, 346, 347, 348, 355, 356, 359, 361, 362, 363, 367, 368, 369, 370, 371], "epoch_loss": 5, "equival": [9, 198, 201, 203], "ergonom": [336, 338], "ernst": [291, 298], "ernsthugo": [291, 298], "erotica": [291, 294], "errno": [9, 198, 203], "error": [3, 5, 6, 8, 9, 10, 13, 14, 43, 51, 53, 64, 66, 71, 84, 85, 118, 122, 137, 144, 155, 157, 159, 163, 164, 167, 181, 185, 191, 195, 198, 203, 206, 208, 212, 259, 260, 267, 285, 316, 320, 328, 329, 330, 342, 343, 348, 349, 350], "erupt": [291, 298], "escap": [291, 298], "especi": [3, 5, 9, 15, 16, 84, 85, 128, 130, 132, 181, 184, 187, 198, 205, 224, 232, 238, 239, 267, 268, 285, 291, 297, 299, 304, 329, 331, 355, 359, 362, 364], "essenti": [8, 17, 21, 101, 102, 109, 110, 117, 137, 146, 191, 195, 245, 246, 299, 304], "establish": [7, 14, 17, 19, 118, 124, 253, 256], "estim": [5, 88, 89, 224, 228], "estimate_pi": [88, 89], "eta": [4, 171, 174, 329, 330, 336, 337, 349, 352, 354], "etc": [2, 3, 4, 5, 6, 9, 10, 12, 17, 18, 19, 22, 23, 24, 27, 110, 113, 126, 127, 128, 130, 132, 133, 134, 135, 136, 137, 139, 144, 146, 147, 151, 153, 154, 164, 166, 167, 171, 174, 175, 177, 181, 190, 198, 201, 203, 206, 210, 224, 225, 229, 245, 250, 259, 261, 349, 352], "ether": [291, 298], "etl": [8, 15, 128, 130, 191, 192, 195], "euler": [329, 335], "europ": [291, 297, 298], "europa": [268, 289, 291, 298], "ev": [268, 289], "eval": [4, 5, 6, 10, 11, 13, 15, 16, 137, 138, 143, 146, 171, 174, 206, 213, 218, 222, 224, 237, 259, 263, 299, 304, 329, 335, 336, 341, 342, 346, 348, 349, 352, 355, 359, 361, 362, 367, 371], "eval_arrow": [349, 352], "eval_dataset": [299, 304], "eval_epoch": [137, 143], "eval_metr": [4, 171, 174, 349, 352], "eval_pr": [299, 303], "evals_result": [4, 171, 174, 349, 352], "evalu": [3, 7, 14, 16, 17, 20, 79, 118, 124, 138, 181, 187, 224, 237, 253, 256, 299, 301, 302, 303, 304, 329, 330, 335, 336, 337, 341, 342, 343, 348, 350, 351, 354, 362, 363, 367, 371], "evan": [268, 289], "even": [3, 16, 88, 89, 101, 102, 105, 118, 125, 128, 132, 137, 144, 147, 149, 151, 159, 163, 181, 187, 224, 234, 268, 289, 291, 294, 298, 315, 316, 317, 319, 320, 327, 328, 355, 361, 362, 367], "evenli": [224, 228, 229], "event": [5, 6, 8, 88, 89, 147, 154, 155, 157, 159, 161, 162, 191, 195, 224, 225, 259, 261], "eventu": [224, 237], "ever": [4, 12, 171, 173, 291, 294, 297], "everi": [0, 2, 86, 87, 90, 91, 94, 95, 175, 180, 224, 228, 233, 234, 238, 243, 244, 268, 289, 291, 295, 329, 332, 336, 337, 340, 342, 346, 349, 350, 351, 352, 362, 363, 364, 365, 369, 371], "every_n_epoch": [329, 333, 336, 340], "everyon": [291, 298], "everyth": [24, 26, 92, 93, 224, 235, 291, 294, 298, 329, 335, 336, 341, 342, 344, 348, 355, 359, 362, 364], "evil": [291, 297, 298], "evolut": [8, 191, 192], "evolv": [342, 346, 349, 350], "ex": [329, 331], "exact": [1, 3, 10, 80, 81, 110, 115, 118, 122, 169, 170, 181, 187, 206, 212], "exactli": [17, 22, 329, 331, 342, 347, 362, 363, 364, 365, 371], "examin": [43, 50, 53, 62], "exampl": [3, 5, 7, 8, 9, 10, 11, 15, 16, 18, 20, 22, 26, 28, 30, 31, 35, 38, 43, 44, 45, 53, 54, 56, 66, 67, 69, 71, 82, 83, 90, 91, 92, 93, 101, 102, 105, 106, 114, 116, 119, 120, 124, 125, 128, 130, 133, 137, 144, 155, 157, 158, 160, 164, 165, 166, 167, 181, 183, 186, 187, 188, 190, 191, 192, 198, 202, 203, 206, 208, 210, 211, 213, 218, 222, 223, 224, 226, 229, 234, 236, 238, 239, 245, 252, 253, 257, 267, 268, 277, 280, 285, 288, 289, 291, 293, 296, 299, 304, 315, 316, 327, 329, 330, 335, 336, 337, 342, 343, 348, 349, 350, 352, 353, 354, 355, 361, 362, 363, 364, 369, 371], "exce": [313, 314, 315, 316, 327], "excel": [110, 112, 118, 124], "except": [3, 6, 16, 128, 132, 181, 185, 245, 247, 259, 263, 268, 289, 329, 331, 362, 364], "excess": [362, 363], "excit": [268, 289], "exclus": [5, 101, 102, 106], "exdb": [13, 14], "execut": [3, 4, 5, 6, 7, 8, 14, 17, 18, 22, 43, 50, 53, 62, 66, 69, 82, 83, 88, 89, 90, 91, 96, 98, 101, 102, 107, 118, 120, 123, 128, 129, 130, 132, 135, 137, 138, 147, 148, 155, 157, 164, 166, 171, 174, 176, 178, 180, 181, 183, 184, 186, 188, 189, 191, 192, 196, 197, 201, 205, 207, 208, 212, 213, 214, 224, 225, 226, 227, 228, 238, 243, 253, 257, 259, 262, 263, 268, 280, 283, 289, 290, 299, 301, 304, 329, 331, 336, 338, 342, 344, 346, 349, 350, 351, 352, 354, 355, 356, 357, 359, 362, 363, 364, 367, 371], "execute_notebook": 0, "exercis": 100, "exhaust": [101, 102, 106, 245, 246, 349, 352], "exhibit": [291, 298, 355, 357], "exisitng": [24, 27], "exist": [4, 9, 24, 25, 26, 27, 28, 29, 30, 43, 44, 45, 52, 55, 56, 65, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 118, 123, 126, 127, 128, 129, 133, 137, 138, 139, 147, 148, 153, 164, 166, 167, 171, 174, 198, 203, 245, 251, 268, 274, 287, 291, 294, 299, 305, 309, 316, 325, 329, 333, 335, 336, 340, 341, 342, 344, 346, 348, 349, 354, 355, 357, 359, 361, 362, 363, 367, 369, 371, 372], "exist_ok": [5, 13, 137, 140, 142, 329, 331, 333, 336, 340, 342, 344, 349, 351, 355, 357, 362, 364], "existing_vpc_id": [17, 22], "exit": [329, 334], "exogen": [355, 361], "exp": [355, 358], "expand": [28, 29, 43, 44, 53, 55, 63, 66, 68, 88, 89, 164, 166, 224, 237, 329, 332], "expect": [5, 7, 14, 16, 82, 83, 101, 102, 106, 224, 227, 234, 253, 257, 329, 333, 342, 344, 349, 351, 355, 361, 362, 364], "expens": [4, 7, 10, 11, 12, 14, 15, 110, 112, 118, 121, 171, 174, 206, 213, 218, 220, 253, 256, 268, 277, 288, 291, 297, 329, 331], "expensive_comput": [3, 181, 189], "expensive_squar": [2, 3, 175, 180, 181, 184, 188, 189], "experi": [2, 5, 7, 79, 80, 81, 82, 83, 84, 85, 92, 93, 100, 110, 117, 118, 121, 125, 126, 127, 137, 138, 142, 143, 144, 155, 157, 159, 163, 164, 165, 175, 177, 224, 226, 228, 245, 252, 253, 257, 291, 294, 316, 317, 319, 320, 328, 336, 341, 349, 354, 362, 363, 370, 371], "experiment": [11, 218, 223, 342, 347], "experiment_id": [137, 144], "experiment_nam": [6, 137, 143, 144, 147, 150, 245, 248, 259, 263], "expert": [8, 191, 196, 291, 298, 329, 330, 336, 337, 362, 364], "expertli": [291, 298], "explain": [17, 21, 24, 26, 96, 97, 164, 167, 268, 289, 291, 297], "explan": [118, 121, 164, 166, 315, 316, 327], "explicit": [118, 124, 291, 294, 342, 343, 349, 352], "explicitli": [6, 7, 9, 10, 92, 93, 198, 202, 206, 211, 253, 257, 259, 262, 267, 268, 274, 285, 287], "explor": [8, 82, 83, 86, 87, 92, 93, 100, 101, 102, 103, 106, 110, 116, 117, 118, 119, 120, 125, 164, 167, 191, 192, 245, 252, 291, 294, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "explos": [159, 163], "export": [66, 69, 110, 113, 114, 118, 121, 155, 158, 168, 336, 341], "expos": [24, 27, 268, 289], "exposehead": [17, 22, 53, 63], "exposur": [17, 22], "expr": [291, 296], "express": [5, 88, 89, 224, 226, 291, 296, 329, 335], "expresswai": [268, 289], "expwrk_1dp3fa7w5hu3i83ldsi7lqvp9t": [128, 129, 137, 138, 147, 148], "extend": [3, 17, 22, 137, 143, 181, 190, 238, 239, 245, 247, 248, 252, 329, 330, 335, 336, 341, 342, 348, 349, 352, 354, 355, 361, 362, 365, 371], "extens": [10, 82, 83, 86, 87, 118, 123, 128, 130, 134, 137, 139, 143, 206, 210], "extern": [8, 9, 17, 22, 27, 53, 63, 66, 73, 118, 120, 123, 125, 159, 161, 191, 195, 198, 200, 245, 252], "extra": [101, 102, 106, 268, 289, 342, 343, 344, 346, 349, 351, 355, 359, 361, 362, 369], "extra_st": [245, 247, 248], "extract": [6, 8, 10, 13, 14, 53, 57, 191, 192, 206, 212, 259, 262, 342, 344, 348, 362, 369], "extract_dir": [342, 344], "extractal": [342, 344], "extrem": [16, 86, 87, 101, 102, 105, 128, 132, 159, 163, 316, 317, 319, 320, 328, 349, 350], "f": [3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 28, 30, 84, 85, 86, 87, 88, 89, 92, 93, 101, 102, 108, 110, 115, 118, 121, 122, 123, 126, 127, 128, 135, 137, 139, 140, 143, 144, 145, 146, 147, 150, 153, 159, 163, 164, 166, 168, 171, 174, 181, 183, 186, 187, 189, 198, 201, 203, 206, 213, 216, 218, 222, 224, 228, 233, 237, 245, 251, 253, 256, 259, 260, 262, 299, 304, 305, 329, 331, 333, 335, 336, 340, 341, 342, 344, 346, 348, 349, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 364, 367, 371], "f1": [137, 146, 268, 289, 362, 371], "f_": [329, 330, 336, 337, 349, 350, 355, 356, 362, 363], "face": [5, 6, 16, 17, 22, 100, 101, 102, 106, 110, 113, 114, 118, 121, 123, 137, 143, 224, 225, 238, 239, 259, 261, 267, 268, 283, 285, 287, 289, 290, 291, 294, 298, 302, 303, 306, 312, 316, 323, 326, 328, 362, 363, 364, 371], "facial": [291, 298], "facilit": [8, 86, 87, 191, 192, 196], "fact": [268, 289, 291, 294, 297, 298], "factor": [137, 143, 346, 348], "fahrenheit": [3, 118, 123, 181, 190], "fail": [3, 5, 6, 10, 13, 14, 128, 132, 137, 144, 147, 153, 159, 163, 181, 185, 206, 208, 212, 224, 225, 245, 246, 248, 249, 259, 261, 342, 346, 349, 352, 355, 360], "failur": [5, 6, 8, 10, 90, 91, 92, 93, 110, 115, 128, 132, 147, 151, 155, 157, 159, 161, 163, 182, 185, 191, 195, 206, 208, 212, 224, 225, 245, 246, 248, 249, 250, 259, 261, 336, 340, 349, 350, 354, 355, 356, 359, 361, 362, 363, 367, 371], "failure_config": [245, 248, 250, 329, 333, 336, 340, 342, 346, 349, 352, 355, 359, 362, 368], "failureconfig": [246, 249, 252, 329, 330, 331, 333, 336, 337, 338, 340, 342, 343, 344, 346, 349, 350, 351, 352, 355, 356, 357, 359, 362, 364, 367, 368, 371], "fair": [268, 289], "fake": [291, 295], "fake_kei": [101, 110, 114, 118, 121, 122, 123], "fall": [342, 344], "fallback": [128, 132, 329, 335, 336, 341], "fals": [4, 5, 6, 7, 11, 12, 13, 14, 16, 137, 140, 146, 164, 167, 171, 174, 218, 222, 224, 227, 230, 253, 256, 257, 259, 262, 263, 329, 332, 333, 335, 336, 339, 340, 341, 342, 344, 346, 349, 351, 352, 355, 357, 359, 361, 362, 367, 369, 371], "famili": [268, 289], "familiar": [79, 126, 127, 164, 165, 238, 241], "fan": [268, 289, 291, 294], "fanatic": [291, 298], "fanchant": [268, 289], "fantasi": [291, 297], "fantast": [291, 298], "far": [291, 294, 297, 349, 352], "fare": [291, 298], "fare_amount": [4, 12, 171, 174], "fashion": [9, 15, 128, 130, 198, 205], "fast": [5, 9, 10, 24, 26, 100, 101, 102, 106, 107, 118, 124, 126, 127, 147, 151, 198, 200, 206, 212, 224, 225, 226, 336, 337, 349, 350, 354, 362, 364, 367], "fastapi": [4, 8, 92, 93, 147, 148, 150, 151, 168, 171, 172, 174, 191, 197, 323, 326, 328], "fastapideploy": [92, 93], "faster": [7, 110, 116, 117, 128, 132, 134, 147, 151, 224, 237, 253, 257, 291, 296, 299, 305, 329, 335], "fastest": [82, 83], "fate": [291, 298], "father": [291, 298], "fault": [5, 6, 8, 9, 10, 17, 21, 22, 110, 115, 126, 127, 137, 144, 147, 153, 191, 195, 198, 200, 205, 206, 208, 224, 225, 247, 248, 250, 259, 261, 329, 330, 331, 333, 335, 336, 337, 340, 341, 342, 343, 346, 347, 348, 349, 350, 351, 352, 354, 355, 356, 360, 361, 363, 367, 368, 371, 372], "fc": 13, "fc1": [137, 140], "fc2": [137, 140], "fcb9ef8c96f844f08bcd0185601f3dbd": [137, 144], "feasibl": [336, 341], "featur": [4, 8, 17, 20, 24, 26, 82, 83, 101, 102, 106, 107, 110, 115, 116, 117, 125, 126, 127, 128, 130, 147, 151, 155, 158, 159, 160, 164, 167, 171, 174, 191, 192, 195, 196, 199, 201, 202, 291, 294, 309, 316, 325, 329, 331, 342, 348, 350, 351, 352, 354, 355, 361, 362, 364, 371], "feature_col": [349, 352, 353], "feature_column": [349, 351, 352, 353, 354], "feature_nam": [349, 352], "feb": [268, 289], "fed": [268, 289], "feder": [17, 22], "fee": [4, 12, 171, 174], "feed": [9, 198, 200, 202, 238, 239, 268, 289, 355, 356, 359, 362, 365], "feedback": [82, 83], "feel": [92, 93, 238, 241, 268, 289, 291, 294, 297, 298, 329, 335, 336, 341, 342, 348, 349, 354, 362, 371], "femal": [291, 294], "fenc": [268, 289], "fend": [291, 297], "ferrari": [268, 289], "ferri": [268, 289], "fetch": [5, 6, 128, 132, 182, 183, 184, 224, 225, 226, 237, 238, 241, 259, 261, 299, 304, 349, 351, 355, 357], "fetch_covtyp": [349, 351], "few": [66, 73, 80, 81, 82, 83, 84, 85, 118, 121, 164, 165, 268, 290, 291, 293, 294, 297, 298, 329, 330, 333, 335, 336, 341, 342, 343, 344, 348, 349, 354, 355, 361, 362, 371], "fewer": [8, 35, 39, 137, 144, 147, 151, 191, 195], "ff": [268, 289], "fiat": [268, 289], "fid": [329, 335], "field": [9, 92, 93, 118, 122, 198, 201, 238, 243, 336, 338], "fifo": [12, 13, 14, 299, 306], "fifoschedul": [7, 14, 253, 257], "fig": [7, 13, 14, 253, 255, 329, 331, 335, 362, 364], "figsiz": [5, 7, 13, 14, 224, 226, 237, 253, 255, 329, 331, 333, 335, 336, 340, 342, 344, 346, 349, 351, 355, 357, 359, 361, 362, 364, 369], "figur": [5, 9, 15, 198, 201, 224, 226, 237, 329, 333, 336, 340, 342, 344, 346, 355, 357, 359, 361, 362, 369], "file": [0, 1, 4, 5, 6, 7, 8, 11, 12, 13, 17, 22, 24, 26, 28, 30, 34, 35, 38, 39, 43, 45, 46, 53, 56, 58, 66, 69, 70, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 100, 118, 121, 126, 127, 128, 130, 133, 135, 137, 139, 143, 144, 145, 147, 150, 153, 159, 163, 164, 167, 169, 170, 171, 174, 191, 192, 201, 203, 205, 209, 210, 213, 217, 218, 223, 224, 226, 234, 238, 242, 245, 248, 251, 252, 253, 258, 259, 262, 264, 268, 284, 289, 292, 300, 324, 329, 331, 335, 336, 337, 341, 342, 343, 344, 348, 355, 357, 362, 363, 364, 365, 367, 371], "file_nam": [118, 121], "filenam": [82, 83, 128, 130, 224, 226, 329, 333, 336, 340], "filenotfounderror": [9, 198, 203, 329, 335, 362, 371], "filestor": [24, 26, 35, 36, 39, 66, 70], "filestore_capacity_gb": [35, 39], "filestore_instance_nam": [35, 39, 66, 70], "filestore_loc": [35, 39, 66, 70], "filestore_ti": [35, 39], "filesystem": [6, 13, 14, 164, 166, 224, 226, 259, 262, 299, 306, 355, 356], "fill": [268, 289, 291, 298], "film": [268, 289, 291, 294, 297, 298], "filmbr": [291, 298], "filmmak": [291, 294], "filter": [9, 28, 30, 35, 38, 43, 45, 53, 56, 66, 69, 71, 84, 85, 88, 89, 128, 130, 133, 137, 139, 164, 167, 198, 205, 224, 227, 293, 297, 298, 342, 343, 345, 348], "filterwarn": [329, 333, 336, 340], "final": [2, 3, 7, 10, 14, 15, 66, 69, 88, 89, 100, 118, 123, 175, 180, 181, 188, 206, 216, 224, 227, 232, 236, 237, 238, 244, 245, 249, 250, 251, 252, 253, 257, 291, 297, 298, 299, 306, 329, 332, 336, 341, 342, 344, 348, 349, 352, 354, 355, 359, 361, 362, 367, 368, 370, 371], "find": [3, 4, 5, 6, 10, 17, 22, 28, 30, 35, 42, 43, 45, 53, 56, 57, 63, 66, 69, 78, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 164, 166, 171, 174, 181, 186, 206, 212, 224, 225, 245, 252, 259, 261, 291, 294, 297, 298], "fine": [7, 24, 26, 90, 91, 118, 120, 121, 128, 135, 245, 252, 253, 256, 291, 297, 329, 333, 335, 349, 354, 362, 363, 367], "finer": [3, 181, 187], "finest": [268, 287, 289], "finetun": [5, 6, 7, 13, 14, 253, 257, 259, 264, 313, 314, 315, 316, 327], "finish": [3, 101, 102, 105, 137, 144, 181, 189, 224, 236, 336, 341, 349, 352, 362, 370], "fiorentina": [268, 289], "fiorina": [268, 289], "fir": [268, 289, 349, 350], "fire": [268, 289], "firewal": [17, 22, 35, 39, 42, 66, 70], "firewall_policy_nam": [35, 39, 66, 70], "first": [1, 2, 3, 4, 6, 7, 11, 12, 13, 14, 16, 28, 31, 35, 38, 40, 43, 47, 53, 57, 59, 66, 69, 75, 86, 87, 88, 89, 101, 102, 105, 106, 110, 113, 118, 121, 128, 130, 147, 149, 159, 163, 169, 170, 171, 173, 175, 177, 178, 180, 181, 184, 186, 218, 220, 222, 224, 227, 237, 253, 257, 259, 263, 268, 272, 273, 274, 287, 289, 291, 294, 297, 298, 329, 331, 333, 335, 342, 344, 348, 355, 357, 362, 367], "fit": [4, 5, 7, 10, 12, 13, 14, 15, 101, 102, 105, 137, 139, 143, 171, 174, 206, 215, 236, 238, 242, 244, 245, 249, 250, 253, 256, 257, 262, 291, 293, 299, 305, 329, 333, 334, 336, 340, 342, 346, 347, 349, 350, 352, 354, 355, 359, 360, 361, 362, 363, 368, 370], "fit_model": [7, 253, 256], "five": [291, 297, 298, 329, 333, 336, 340, 362, 368], "fix": [224, 228, 299, 304, 329, 330, 342, 344, 355, 357], "flag": [88, 89], "flap": [291, 294], "flashi": [291, 298], "flatten": [329, 331, 362, 364], "flavor": [84, 85], "flawless": [268, 289], "fleet": [126, 127, 291, 294, 298, 355, 356], "flew": [268, 289], "flexibl": [3, 8, 9, 11, 15, 16, 96, 99, 101, 102, 107, 118, 119, 120, 128, 132, 147, 151, 181, 188, 191, 192, 194, 195, 198, 201, 218, 221, 267, 268, 285, 291, 293, 299, 305], "flexibli": [16, 137, 144], "flink": [8, 128, 130, 191, 195], "flip": [362, 363], "flip_sin_to_co": [6, 259, 262], "flippen": [291, 297], "float": [3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 137, 143, 146, 171, 174, 181, 185, 198, 201, 206, 213, 218, 222, 224, 234, 245, 248, 253, 256, 257, 259, 262, 268, 290, 329, 332, 336, 339, 342, 346, 349, 353, 355, 359], "float16": [6, 259, 262], "float32": [11, 12, 137, 141, 146, 218, 222, 268, 289, 290, 329, 331, 336, 338, 341, 342, 346, 355, 357, 358, 361], "floral": [28, 30, 43, 45, 53, 56], "flore": [268, 289], "flow": [17, 22, 96, 99, 118, 123, 155, 157, 195, 336, 337], "flush": [3, 101, 102, 108, 110, 114, 115, 118, 121, 181, 185], "fly": [238, 239], "fmt": [349, 353], "fn": [137, 146], "fn_arg": [118, 123], "fn_call": [118, 123], "fn_callabl": [118, 123], "fn_constructor_arg": [349, 353, 354, 355, 361, 362, 371], "fn_constructor_kwarg": [10, 15, 16, 128, 131, 137, 139, 206, 213], "fn_kwarg": [15, 128, 131, 137, 139], "fname": [355, 357], "foam": [268, 289], "focu": [100, 118, 120, 126, 127, 159, 162, 238, 239, 291, 294, 355, 356], "focus": [0, 8, 24, 26, 101, 102, 104, 126, 127, 155, 158, 191, 192, 194, 268, 289, 355, 361], "folder": [4, 5, 9, 10, 11, 82, 83, 86, 87, 90, 91, 118, 121, 168, 171, 174, 198, 203, 206, 213, 218, 222, 284, 292, 300, 324, 349, 354, 355, 361, 362, 371], "follow": [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 22, 24, 26, 28, 29, 35, 37, 42, 43, 44, 46, 51, 53, 55, 58, 63, 64, 66, 68, 71, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 100, 101, 102, 108, 110, 112, 118, 121, 124, 128, 129, 137, 138, 147, 153, 155, 157, 158, 159, 163, 164, 167, 168, 169, 170, 175, 180, 181, 183, 191, 194, 197, 198, 201, 206, 208, 213, 218, 220, 224, 225, 227, 238, 239, 240, 253, 257, 259, 261, 262, 263, 291, 298, 299, 305, 336, 341, 342, 343, 348, 349, 350, 355, 356, 361, 362, 363, 365], "followup": [291, 298], "fontsiz": [329, 331, 362, 364], "foo": [128, 131], "food": [330, 335, 371], "food101": [329, 331, 362, 363, 364, 371], "food101_diffusion_ft": [329, 333], "food101_diffusion_result": [329, 333], "food101_ft_resum": [362, 368, 371], "food101_ft_run": [362, 371], "food101_lit": [329, 331, 362, 364, 365, 367, 368, 371], "food101_single_run": [362, 371], "food101dataset": [363, 366, 371], "footag": [268, 289], "footbal": [268, 287, 289], "footer": 0, "footprint": [128, 130], "forbidden": [13, 14], "forc": [137, 139, 224, 237, 356, 358, 361, 362, 371], "forcibli": [291, 297], "ford": [291, 294], "forecast": 361, "foreground": [291, 298], "foregroundbr": [291, 298], "forest": [351, 353], "forg": [1, 169, 170], "forget": [268, 289, 291, 298], "forgotten": [291, 298], "forgottenbr": [291, 298], "fork": 0, "form": [3, 12, 128, 131, 181, 183, 291, 298, 342, 344], "format": [7, 10, 14, 100, 118, 120, 121, 122, 123, 125, 128, 130, 132, 168, 206, 210, 212, 238, 239, 242, 253, 255, 299, 304, 329, 331, 342, 344, 355, 357, 362, 363, 364], "fort": [291, 297], "forum": [110, 117, 118, 125], "forward": [6, 13, 137, 139, 140, 143, 224, 225, 228, 238, 240, 245, 247, 259, 262, 268, 289, 299, 304, 332, 333, 336, 337, 339, 340, 342, 343, 345, 349, 351, 355, 358, 361], "found": [7, 14, 17, 22, 224, 230, 245, 247, 250, 251, 253, 257, 291, 298, 329, 335, 336, 341, 342, 343, 349, 352, 362, 371], "foundat": [7, 8, 14, 109, 118, 125, 191, 192, 253, 256, 342, 348], "four": [5, 6, 101, 102, 105, 224, 227, 259, 263, 342, 344], "fourth": [268, 289, 291, 297, 298], "fox": [268, 289], "foxx": [268, 289], "fp": [137, 139, 140, 143, 146], "fp16": [101, 102, 106, 110, 112, 116], "fp8": [110, 116], "frac": [362, 365], "fraction": [11, 137, 144, 147, 149, 151, 153, 182, 190, 218, 220, 315, 316, 327], "fragment": [147, 151], "fragrant": [268, 289], "frame": [349, 351, 352], "framework": [4, 5, 6, 9, 11, 12, 15, 16, 100, 125, 171, 173, 174, 192, 197, 198, 200, 218, 219, 221, 224, 225, 259, 261, 291, 293, 309, 316, 325], "franc": [101, 102, 108, 118, 121], "francisco": [118, 123], "frank": [268, 289], "fraud": [8, 191, 195], "freak": [291, 297], "free": [3, 88, 89, 90, 91, 92, 93, 100, 110, 116, 126, 127, 128, 136, 137, 146, 147, 154, 181, 186, 224, 237, 267, 268, 285, 289, 299, 301, 349, 354, 355, 361, 362, 371], "freed": [224, 237], "freeli": [9, 15, 198, 201], "freez": [126, 127, 137, 139], "freq_shift": [6, 259, 262], "frequenc": [6, 259, 262, 342, 344, 349, 351], "frequent": [8, 191, 192, 195], "fresh": [224, 237], "fri": [362, 363], "friction": [2, 175, 177], "fridai": [268, 289], "friend": [268, 289], "friendli": [90, 91, 110, 112, 159, 163, 329, 331, 342, 348, 362, 363, 364], "frighten": [291, 297, 298], "frill": [291, 298], "from": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 24, 27, 28, 29, 30, 32, 35, 36, 39, 41, 42, 43, 44, 45, 47, 48, 50, 53, 54, 57, 59, 60, 62, 63, 66, 67, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 101, 102, 104, 105, 106, 108, 110, 111, 113, 114, 115, 116, 118, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 153, 154, 155, 158, 159, 161, 163, 164, 166, 167, 169, 170, 171, 172, 173, 174, 181, 183, 186, 188, 190, 191, 195, 198, 200, 202, 203, 205, 206, 207, 208, 210, 212, 213, 218, 219, 220, 223, 224, 225, 226, 227, 228, 232, 233, 234, 235, 236, 237, 239, 240, 242, 243, 244, 246, 247, 248, 251, 252, 253, 254, 257, 259, 260, 261, 263, 267, 268, 271, 274, 283, 285, 286, 287, 289, 290, 291, 293, 294, 297, 298, 299, 302, 303, 304, 312, 316, 323, 326, 328, 330, 331, 333, 337, 338, 340, 343, 346, 348, 350, 351, 352, 356, 357, 359, 361, 362, 363, 364, 365, 367, 369, 370, 371], "from_artifacts_dir": [137, 146, 147, 149], "from_directori": [5, 13, 224, 234, 245, 248, 342, 346, 355, 359, 361, 362, 367, 371], "from_huggingfac": [268, 274, 287], "from_item": [16, 159, 163, 291, 295, 329, 331, 336, 338, 355, 361], "from_numpi": [355, 361, 362, 371], "from_pretrain": [6, 128, 131, 147, 149, 259, 262, 299, 304], "from_pydict": [362, 364], "from_pylist": [355, 357], "from_torch": [10, 206, 210], "fromarrai": [128, 131, 147, 149, 238, 243], "front": [349, 351, 362, 364], "frontal": [291, 294], "fr\u00e9chet": [329, 335], "fsdp": [5, 224, 226, 231, 245, 252], "fspd": [137, 144], "ft": [268, 287, 289], "fuck": [268, 289], "full": [4, 6, 9, 10, 15, 17, 20, 22, 24, 26, 53, 63, 90, 91, 118, 121, 126, 127, 159, 163, 164, 167, 171, 174, 198, 201, 206, 208, 214, 224, 229, 236, 246, 252, 259, 262, 291, 297, 298, 329, 330, 336, 337, 342, 344, 346, 348, 349, 352, 355, 359, 362, 364, 365, 369, 371], "full_path": [362, 365], "fulli": [3, 5, 6, 16, 17, 18, 80, 81, 96, 98, 100, 181, 183, 224, 225, 227, 238, 239, 244, 259, 261, 329, 330, 333, 336, 337, 342, 343, 344, 346, 349, 350, 352, 362, 363, 367], "fullyshardeddataparallel": [5, 224, 231], "function": [3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 53, 63, 86, 87, 94, 95, 110, 113, 118, 120, 123, 125, 128, 130, 131, 132, 137, 140, 147, 151, 171, 174, 176, 180, 181, 185, 190, 192, 196, 198, 201, 202, 203, 204, 206, 210, 212, 215, 216, 218, 221, 224, 227, 228, 231, 233, 234, 238, 243, 253, 257, 259, 260, 262, 263, 268, 280, 289, 291, 296, 298, 301, 303, 306, 315, 316, 327, 329, 331, 336, 337, 340, 341, 342, 343, 344, 346, 349, 350, 354, 355, 356, 362, 363, 367], "fundament": [3, 11, 16, 24, 26, 79, 101, 102, 103, 104, 108, 126, 127, 164, 165, 181, 182, 218, 221], "further": [10, 84, 85, 128, 134, 206, 210, 213, 224, 236, 291, 298, 315, 316, 327, 362, 365], "fuse": [10, 206, 212], "futur": [2, 9, 128, 130, 164, 167, 175, 179, 198, 204, 291, 294, 297, 298, 336, 337, 355, 356, 357, 358, 359], "future_tru": [355, 361], "g": [0, 2, 3, 7, 8, 9, 10, 11, 12, 14, 15, 17, 22, 24, 26, 27, 28, 29, 35, 36, 42, 43, 44, 53, 54, 66, 67, 78, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 110, 116, 155, 157, 159, 161, 164, 166, 175, 177, 181, 184, 185, 187, 191, 192, 195, 198, 203, 206, 208, 210, 212, 218, 220, 221, 224, 228, 229, 234, 235, 236, 238, 239, 241, 245, 246, 249, 252, 253, 257, 267, 268, 285, 287, 289, 342, 346, 349, 352, 355, 361], "g4dn": [128, 133, 136, 137, 139, 143, 146], "g54aiirwj1": [86, 87], "g54aiirwj1s8t9ktgzikqur41k": [86, 87], "gain": [126, 127, 342, 348, 349, 353], "galaxi": [268, 289], "galleri": [268, 289], "gallo": [291, 294], "gambl": [291, 297], "game": [268, 289], "gan": [329, 330], "gandhi": [268, 289], "gang": [291, 297], "gannon": [291, 297], "gap": [8, 16, 118, 121, 126, 127, 191, 192, 355, 357], "gape": [291, 294], "garbag": [2, 175, 177, 224, 226, 237], "garden": [268, 289], "gate": [110, 113, 114, 118, 122, 123], "gatewai": [17, 22, 28, 30, 43, 45, 53, 56], "gather": [4, 88, 89, 171, 174, 362, 364], "gaussian": [329, 330, 335, 336, 338, 355, 361], "gb": [3, 86, 87, 110, 112, 116, 181, 183], "gc": [9, 24, 26, 35, 36, 42, 66, 78, 118, 121, 198, 203, 224, 226, 234, 237, 245, 252, 342, 348, 362, 371], "gca": [349, 353], "gce": [17, 18, 24, 27, 40, 66, 69, 372], "gcloud": [35, 37, 38, 66, 68, 69, 71, 72, 78], "gcp": [17, 18, 22, 24, 26, 27, 37, 38, 39, 41, 42, 66, 68, 69, 70, 74, 79, 94, 95, 96, 98], "gcp_if_": [35, 39], "gcp_project_id": [35, 38, 39, 66, 69, 70, 72, 78], "gcp_region": [35, 38, 39, 66, 69, 70, 71, 72, 76, 78], "gcs_bucket_nam": [35, 39, 42, 66, 70, 78], "gear": [268, 289], "gee": [268, 289], "gener": [1, 2, 9, 10, 11, 13, 15, 16, 35, 39, 86, 87, 88, 89, 90, 91, 105, 109, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 135, 147, 148, 149, 159, 163, 164, 166, 167, 168, 169, 170, 175, 177, 198, 202, 204, 206, 211, 213, 215, 218, 222, 224, 226, 237, 245, 252, 262, 267, 268, 277, 284, 285, 288, 291, 292, 294, 300, 324, 333, 337, 341, 342, 343, 348, 355, 356, 361], "generate_embed": [126, 127, 128, 135], "generate_synthetic_imag": [159, 163], "generated_bi": [159, 163], "generative_cv": [329, 333, 335], "genit": [291, 294], "geniu": [291, 298], "genr": [342, 348], "geo": [349, 350], "german": [291, 298], "german_shepherd": [147, 150], "germani": [268, 289, 291, 298], "get": [1, 4, 5, 6, 8, 11, 12, 16, 17, 22, 28, 29, 30, 31, 35, 39, 40, 43, 44, 45, 47, 49, 50, 51, 53, 55, 57, 59, 61, 62, 63, 64, 66, 72, 73, 75, 82, 83, 86, 87, 88, 89, 90, 91, 103, 109, 110, 113, 115, 116, 119, 121, 123, 126, 127, 128, 131, 136, 137, 138, 147, 150, 151, 153, 159, 162, 164, 166, 167, 169, 170, 171, 172, 174, 182, 183, 184, 185, 186, 187, 188, 190, 191, 195, 218, 222, 224, 227, 228, 237, 256, 259, 263, 268, 289, 291, 297, 298, 299, 304, 315, 316, 320, 327, 328, 329, 330, 333, 335, 336, 340, 341, 342, 344, 346, 349, 352, 355, 357, 359, 361, 362, 364, 367, 371], "get_best_result": [4, 7, 12, 14, 171, 174, 253, 257], "get_checkpoint": [245, 246, 247, 252, 329, 331, 333, 336, 338, 340, 342, 344, 346, 349, 351, 352, 354, 355, 357, 359, 362, 363, 364, 367], "get_config_dict": [6, 259, 262], "get_context": [4, 5, 13, 137, 143, 171, 174, 224, 228, 233, 234, 238, 240, 245, 247, 248, 329, 331, 333, 336, 338, 340, 342, 344, 346, 349, 351, 352, 355, 357, 359, 362, 364, 367], "get_current_temperatur": [118, 123], "get_dataset_shard": [6, 137, 143, 238, 241, 259, 263, 329, 333, 336, 340, 342, 344, 346, 349, 351, 352], "get_devic": [6, 137, 141, 259, 260, 262, 299, 304], "get_image_featur": [128, 131, 147, 149], "get_linear_schedule_with_warmup": [6, 259, 260, 262], "get_model": [349, 352, 353], "get_prob": [147, 149, 150], "get_scor": [349, 353, 354], "get_temperature_d": [118, 123], "get_top_match": [128, 136], "get_us": 168, "get_user_profil": 168, "get_world_rank": [4, 5, 13, 137, 143, 171, 174, 224, 233, 234, 245, 248, 329, 333, 336, 340, 342, 346, 349, 352, 355, 359, 362, 367], "get_world_s": [5, 13, 224, 228, 238, 240, 245, 247], "getbucketloc": [17, 22], "getenv": [84, 85, 86, 87], "getlogg": [164, 167], "getobject": [17, 22], "getsizeof": [3, 181, 183], "gettempdir": [329, 333, 336, 340], "gettingstart": 100, "getvalu": [329, 331, 362, 364], "gh": 0, "ghetto": [268, 289], "giant": [291, 298], "gib": [12, 14], "gift": [268, 289], "girl": [268, 289], "git": [0, 92, 93, 94, 95, 100], "github": [0, 1, 17, 22, 43, 46, 48, 53, 58, 60, 66, 73, 76, 92, 93, 100, 169, 170, 355, 357], "githubusercont": [355, 357], "give": [7, 14, 53, 57, 80, 81, 84, 85, 92, 93, 94, 95, 101, 102, 107, 224, 227, 245, 248, 253, 256, 291, 297, 342, 344, 348, 349, 351], "given": [2, 4, 5, 7, 8, 10, 13, 14, 15, 94, 95, 118, 123, 137, 139, 147, 149, 164, 167, 171, 174, 175, 178, 191, 195, 196, 197, 206, 215, 253, 257, 313, 314, 315, 316, 327, 329, 332, 336, 337, 339, 349, 350, 355, 356], "gke": [17, 18, 20, 24, 27, 69, 70, 72, 75, 126, 127, 372], "glanc": [80, 81], "glass": [268, 289], "glob": [329, 335, 336, 338, 341], "global": [28, 30, 35, 39, 43, 45, 53, 56, 66, 69, 70, 137, 144, 224, 228, 329, 332, 342, 344, 362, 365, 367], "global_batch_s": [5, 13, 224, 228, 229, 235, 238, 240, 244, 245, 247, 248, 250, 299, 305], "gloriou": [291, 297], "gloss": [291, 298], "gm": [268, 287, 289], "go": [3, 17, 22, 80, 81, 101, 102, 107, 110, 116, 118, 121, 137, 144, 181, 184, 267, 268, 285, 289, 291, 297, 298, 362, 364], "goal": [4, 100, 171, 174, 268, 289, 299, 301, 336, 337, 362, 363], "goaldotcom": [268, 289], "god": [268, 289], "goe": [88, 89, 94, 95, 268, 287, 289, 291, 294], "gold": [291, 297], "golions2012": [268, 289], "gone": [291, 298], "gonna": [268, 289], "good": [7, 8, 14, 28, 30, 43, 45, 53, 56, 110, 112, 137, 138, 159, 163, 191, 197, 224, 226, 237, 238, 242, 253, 256, 268, 289, 291, 294, 297, 298, 349, 353, 355, 357], "googl": [8, 17, 19, 20, 24, 27, 37, 39, 68, 191, 192, 268, 289], "google_cloud": [66, 69], "google_project_id": [35, 39, 66, 70, 78], "google_region": [35, 39, 66, 70, 78], "googleapi": [35, 38, 66, 69], "gore": [291, 298], "got": [3, 181, 189, 268, 289, 291, 298], "gotrib": [268, 289], "gotten": [291, 298], "govern": [126, 127], "gpu": [1, 3, 4, 7, 8, 9, 10, 11, 12, 14, 15, 24, 27, 28, 34, 43, 46, 53, 58, 80, 81, 84, 85, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 113, 114, 116, 117, 118, 119, 124, 125, 126, 127, 128, 129, 132, 133, 136, 137, 139, 143, 144, 146, 155, 157, 159, 161, 169, 170, 171, 172, 174, 181, 187, 190, 191, 196, 198, 205, 206, 208, 212, 213, 217, 218, 220, 221, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 237, 238, 239, 244, 245, 252, 253, 254, 256, 257, 260, 261, 267, 268, 283, 285, 289, 290, 299, 301, 304, 305, 306, 315, 316, 327, 329, 330, 333, 335, 336, 337, 340, 341, 342, 343, 346, 348, 349, 352, 356, 362, 363, 366, 368, 371], "grab": [329, 331, 362, 371], "gracefulli": [92, 93, 316, 320, 328], "grad": [224, 228], "grade": [101, 102, 103, 107, 110, 117, 118, 125, 137, 145, 329, 335], "grader": [268, 289], "gradient": [4, 5, 6, 12, 13, 137, 143, 171, 174, 224, 225, 228, 231, 234, 259, 263, 299, 304, 349, 350, 351, 355, 357], "gradual": [159, 163, 329, 330, 349, 350], "grafana": [88, 89, 92, 93, 110, 116, 157, 164, 166, 167], "grai": [5, 7, 10, 13, 14, 16, 206, 211, 224, 226, 237, 253, 255], "grain": [3, 11, 24, 26, 181, 187, 218, 221], "grand": [268, 289], "grant": [17, 22, 24, 26, 291, 294], "granular": [94, 95, 164, 167], "granularli": 16, "graph": [2, 3, 137, 144, 155, 158, 159, 163, 175, 177, 181, 184], "grass": [291, 298], "grayscal": [5, 7, 13, 14, 224, 226, 227, 237, 253, 255], "great": [94, 95, 110, 115, 268, 289, 291, 298, 316, 317, 319, 320, 328], "greater": [12, 86, 87, 88, 89], "greec": [268, 289], "green": [329, 330, 362, 363], "grei": [268, 289], "grep": [28, 30, 43, 45, 49, 50, 51, 53, 56, 61, 62, 64, 126, 127], "grid": [7, 14, 224, 226, 237, 253, 257, 329, 333, 336, 340, 341, 342, 346, 355, 357, 359, 361, 362, 369], "grim": [291, 297, 298], "grimnoir": [291, 298], "ground": [10, 15, 206, 212, 215, 224, 226, 336, 338, 355, 356, 359, 361, 362, 371], "ground_truth_label": [10, 15, 206, 215], "group": [14, 21, 23, 24, 26, 28, 30, 34, 43, 45, 53, 56, 57, 63, 79, 80, 81, 88, 89, 94, 95, 96, 98, 199, 207, 224, 225, 235, 268, 289, 329, 330, 342, 348, 355, 356, 362, 363, 364, 365, 369], "groupbi": [8, 191, 196, 205, 342, 344, 346, 355, 359, 362, 369], "grouplen": [342, 344], "grow": [2, 4, 12, 100, 171, 173, 175, 177, 291, 294], "grpc": [8, 11, 16, 147, 151, 191, 197, 218, 220, 222], "gserviceaccount": [35, 39, 66, 70], "gsutil": [35, 42, 66, 78], "gt": [291, 298], "guarante": [8, 118, 122, 191, 196, 224, 234], "guard": [342, 346, 362, 369], "gucci": [268, 289], "gui": [268, 289], "guid": [1, 5, 6, 10, 13, 14, 24, 26, 28, 29, 43, 44, 53, 54, 66, 67, 79, 92, 93, 100, 110, 117, 118, 120, 121, 122, 123, 125, 137, 139, 143, 155, 156, 158, 164, 165, 167, 169, 170, 206, 210, 224, 234, 259, 263, 268, 289, 299, 306, 315, 316, 327], "gunman": [291, 297], "gym": [336, 337, 338, 341], "gymnasium": [336, 337, 338], "gz": [13, 14], "h": [5, 11, 13, 137, 144, 147, 153, 168, 218, 222, 224, 237, 329, 330, 331, 332, 355, 357, 362, 371], "ha": [1, 3, 5, 6, 8, 16, 17, 20, 35, 42, 53, 63, 66, 78, 80, 81, 84, 85, 86, 87, 88, 89, 94, 95, 96, 98, 110, 112, 118, 121, 125, 128, 130, 132, 135, 147, 151, 159, 163, 164, 167, 169, 170, 181, 183, 191, 195, 196, 197, 224, 225, 227, 238, 242, 243, 259, 261, 268, 272, 273, 274, 280, 283, 287, 289, 290, 291, 294, 297, 298, 299, 304, 316, 317, 319, 320, 328, 342, 344], "had": [9, 198, 204, 268, 289, 291, 294], "hadoop": [8, 191, 195], "hahha": [268, 289], "hail": [291, 298, 355, 356], "half": [268, 289, 291, 297, 298, 355, 356, 357], "halfstarv": [291, 298], "halloween": [268, 287, 289], "halv": [7, 14, 253, 257], "ham": [268, 289], "hamburg": [362, 363], "hand": [1, 79, 100, 101, 102, 109, 118, 120, 126, 127, 169, 170, 268, 289, 291, 297, 342, 344, 362, 363], "handheld": [291, 298], "handl": [3, 4, 8, 10, 11, 12, 16, 17, 22, 24, 25, 26, 66, 73, 80, 81, 90, 91, 100, 101, 102, 104, 107, 110, 113, 115, 118, 122, 126, 127, 137, 144, 155, 157, 159, 163, 168, 171, 174, 181, 185, 187, 190, 191, 192, 195, 196, 197, 206, 208, 218, 220, 221, 224, 225, 226, 227, 228, 230, 232, 237, 238, 239, 240, 244, 245, 246, 248, 251, 268, 283, 290, 291, 293, 299, 301, 304, 309, 316, 325, 329, 330, 333, 335, 336, 337, 338, 342, 343, 344, 346, 348, 349, 351, 352, 355, 356, 362, 363, 364, 366, 367, 369], "handwritten": [7, 11, 14, 218, 222, 224, 226, 253, 255, 256], "hang": [137, 144, 155, 157, 291, 297, 298, 336, 337], "happen": [2, 7, 8, 14, 159, 163, 175, 179, 191, 197, 245, 248, 253, 257, 291, 297, 298], "happi": [268, 289], "happybirthdayremuslupin": [268, 287, 289, 290], "hard": [291, 297, 298], "hardli": [291, 294], "hardwar": [5, 6, 10, 11, 84, 85, 88, 89, 100, 101, 102, 106, 107, 112, 113, 114, 125, 147, 151, 206, 208, 212, 213, 218, 220, 221, 224, 225, 238, 239, 245, 248, 259, 261, 299, 301, 304, 305, 306, 349, 350], "hark": [291, 297], "harm": 176, "harri": [268, 289], "hasattr": [362, 367], "hasek": [268, 289], "hash": [9, 198, 204], "hashicorp": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68], "hashtag": [268, 289], "hat": [342, 343, 362, 363], "hater": [268, 289], "have": [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 28, 29, 30, 34, 35, 37, 43, 44, 45, 46, 52, 53, 55, 56, 58, 65, 66, 68, 82, 83, 84, 85, 86, 87, 94, 95, 100, 101, 102, 105, 110, 117, 128, 131, 133, 137, 139, 141, 144, 147, 151, 159, 163, 164, 165, 169, 170, 171, 172, 181, 185, 188, 189, 191, 194, 195, 198, 203, 206, 208, 212, 213, 215, 218, 220, 222, 224, 234, 238, 244, 253, 257, 259, 263, 268, 280, 289, 291, 294, 295, 296, 297, 298, 299, 304, 309, 316, 325, 342, 343, 349, 351, 352, 362, 363, 371], "hdf": [8, 9, 191, 195, 198, 203], "he": [268, 289, 291, 297, 298], "head": [3, 9, 14, 17, 21, 22, 24, 26, 43, 50, 53, 62, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 128, 132, 147, 153, 159, 162, 163, 181, 183, 187, 198, 201, 203, 291, 298, 329, 333, 336, 340, 349, 351, 355, 357, 361], "head_nod": [43, 50, 53, 62], "header": [0, 5, 13, 342, 348], "headlei": [268, 287, 289], "headless": [24, 26], "headnodeconfig": [43, 50, 53, 62], "health": [24, 25, 101, 102, 106, 147, 152, 168, 329, 333], "healthi": [84, 85, 92, 93, 355, 359], "heap": [3, 181, 189], "hear": [268, 289], "heard": [291, 294], "heart": [291, 297, 355, 359], "heat": [316, 317, 319, 320, 328], "heatmap": [349, 353], "heaven": [291, 297], "heavi": [5, 9, 10, 15, 80, 81, 198, 204, 206, 215, 238, 239], "heavili": [291, 298], "heavli": [224, 232], "hebdo": [268, 289], "hei": [17, 22, 268, 289], "height": [10, 15, 16, 159, 163, 206, 212, 329, 331, 349, 354], "held": [268, 289], "hell": [268, 289], "hello": [82, 83, 86, 87, 90, 91, 101, 164, 167, 268, 289], "hello_world": [1, 82, 83, 90, 91, 169, 170], "helm": [24, 25, 43, 44, 45, 46, 48, 51, 53, 55, 56, 58, 60, 64, 66, 68, 73, 76, 78], "helm_upgrade_command": [43, 45, 53, 56], "help": [1, 7, 8, 10, 14, 16, 17, 20, 24, 26, 82, 83, 101, 102, 105, 147, 154, 155, 157, 164, 166, 169, 170, 191, 195, 206, 212, 224, 226, 253, 256, 291, 298, 349, 351, 353, 355, 356, 357], "helper": [118, 123, 224, 226, 231, 232, 233, 234, 342, 344, 349, 351, 357, 363, 371], "helper_tool_map": [118, 123], "her": [268, 289, 291, 294, 297, 298], "here": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 28, 30, 43, 45, 53, 63, 100, 101, 102, 103, 104, 107, 110, 111, 117, 118, 119, 121, 122, 124, 125, 126, 127, 128, 129, 137, 138, 147, 148, 159, 163, 164, 165, 166, 167, 171, 172, 174, 175, 176, 179, 180, 181, 182, 190, 191, 192, 193, 194, 195, 197, 198, 199, 202, 203, 204, 205, 206, 207, 210, 212, 215, 218, 219, 221, 222, 223, 224, 226, 228, 229, 236, 237, 238, 240, 245, 248, 252, 253, 254, 256, 257, 258, 259, 260, 262, 263, 264, 267, 268, 285, 289, 291, 293, 295, 297, 298, 299, 301, 309, 315, 316, 325, 327, 329, 330, 331, 336, 337, 342, 343], "herm": [118, 123], "hero": [268, 289, 291, 298], "herself": [291, 298], "heteregen": [10, 206, 208], "heterogen": [4, 8, 9, 10, 15, 126, 127, 128, 129, 132, 137, 138, 144, 147, 151, 171, 174, 191, 196, 198, 205, 206, 208, 217], "hf": [110, 113], "hf_d": [329, 331], "hf_dataset": [268, 274, 287], "hf_token": [110, 113, 114, 115, 116, 118, 121, 122, 123], "hi": [268, 287, 289, 291, 294, 297, 298], "hidden": [0, 291, 298, 336, 341], "hidden_dim": [137, 140, 143, 144], "hide": [291, 297], "hierarch": [164, 167], "hierarchi": [96, 99], "high": [3, 5, 6, 7, 8, 10, 11, 13, 14, 16, 17, 22, 92, 93, 101, 102, 104, 105, 106, 107, 118, 124, 128, 132, 147, 151, 181, 187, 191, 192, 195, 196, 206, 212, 218, 220, 221, 224, 227, 235, 253, 256, 259, 263, 268, 283, 289, 290, 291, 298, 309, 316, 323, 325, 328, 336, 341, 342, 343, 344, 346, 362, 363], "higher": [8, 15, 86, 87, 101, 102, 105, 110, 116, 117, 118, 124, 128, 130, 147, 151, 191, 196, 238, 239, 342, 343, 344], "highest": [291, 297, 329, 333, 342, 348], "highli": [17, 22, 24, 26, 126, 127, 147, 151, 159, 163, 349, 351], "highlight": [3, 6, 8, 181, 183, 191, 195, 224, 234, 259, 263, 291, 298, 342, 344, 349, 353], "hike": [268, 289], "him": [268, 289, 291, 297, 298], "himself": [268, 289, 291, 297], "hindu": [268, 289], "hint": [2, 3, 5, 6, 7, 10, 13, 14, 175, 180, 181, 190, 206, 212, 253, 257, 259, 263, 299, 302], "hire": [291, 297], "hist": [4, 171, 174, 342, 344, 349, 352], "histor": [8, 191, 192, 195, 342, 343, 355, 356], "histori": [118, 123, 224, 236, 342, 346, 355, 357, 359, 361, 362, 369, 371], "hit": [28, 30, 43, 45, 53, 56, 159, 163, 268, 289, 342, 348], "hitchhik": [268, 289], "hiya": [268, 289], "hmu": [268, 289], "hmw": [268, 289], "hoc": [336, 337, 349, 353], "hogan": [268, 289], "hogwart": [268, 287, 289, 290], "hold": [9, 10, 15, 198, 201, 206, 210, 224, 228, 299, 304], "hole": [268, 289], "holidai": [355, 361], "hollywood": [268, 289, 291, 297, 298], "home": [14, 86, 87, 126, 127, 128, 129, 135, 137, 138, 144, 145, 147, 148, 153, 268, 289], "homebrew": [28, 29, 43, 44, 53, 55, 66, 68, 155, 158], "homecom": [268, 289], "homepath": [155, 158], "hong": [291, 297], "hood": [3, 10, 126, 127, 137, 144, 181, 184, 206, 210, 224, 230], "hook": [291, 297, 298, 336, 341], "hop": [268, 289], "hope": [268, 289, 291, 297], "horizon": [355, 357, 358, 359, 361], "horizont": [110, 113, 116, 147, 149, 151], "horribl": [291, 298], "horror": [291, 298], "hospit": [291, 298], "host": [5, 17, 20, 22, 24, 27, 80, 81, 82, 83, 96, 97, 98, 224, 232, 329, 330], "hostfil": [137, 144], "hostnam": 13, "hot": [11, 137, 143, 218, 223, 349, 353, 362, 363], "hotwif": [268, 289], "hour": [12, 101, 102, 106, 159, 163, 268, 289, 329, 331, 355, 356, 357, 361, 362, 364], "hourli": [356, 361], "hous": [291, 297], "housekeep": [224, 226], "hoverboard": [268, 289], "how": [3, 4, 5, 6, 7, 8, 13, 14, 15, 16, 18, 28, 30, 35, 36, 39, 43, 45, 53, 56, 63, 80, 81, 86, 87, 92, 93, 96, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 115, 119, 120, 123, 125, 126, 127, 128, 135, 137, 143, 147, 149, 159, 163, 164, 165, 167, 168, 171, 174, 181, 185, 191, 193, 199, 201, 202, 203, 204, 207, 213, 227, 228, 229, 230, 234, 238, 239, 240, 245, 246, 248, 253, 257, 258, 259, 260, 262, 263, 267, 268, 280, 283, 284, 285, 287, 289, 290, 291, 292, 293, 296, 297, 298, 299, 300, 301, 306, 316, 320, 324, 328, 344, 346, 348, 352, 357, 371], "howev": [3, 8, 9, 10, 15, 24, 26, 137, 139, 181, 187, 191, 195, 198, 203, 204, 206, 215, 224, 234, 291, 297], "html": [0, 1, 14, 28, 29, 43, 44, 53, 55, 101, 102, 108, 169, 170, 268, 274, 287, 299, 306, 309, 315, 316, 325, 327], "http": [0, 1, 4, 8, 11, 12, 13, 14, 16, 17, 22, 24, 27, 28, 29, 35, 37, 43, 44, 46, 48, 53, 55, 58, 60, 63, 66, 68, 73, 76, 88, 89, 92, 93, 100, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 126, 127, 128, 129, 136, 137, 138, 147, 148, 150, 151, 153, 155, 158, 164, 167, 168, 169, 170, 171, 174, 191, 197, 218, 222, 268, 274, 287, 299, 306, 309, 315, 316, 320, 325, 327, 328, 342, 344, 355, 357], "huddleston": [268, 289], "hudi": [8, 191, 192], "hug": [110, 113, 114, 118, 121, 123, 137, 143, 267, 268, 283, 285, 287, 290, 291, 294, 302, 303, 306, 312, 316, 323, 326, 328, 362, 363, 364, 371], "huge": [291, 298], "huggingfac": [5, 6, 101, 102, 108, 110, 113, 114, 115, 118, 121, 122, 259, 261, 268, 271, 286], "huggingface_hub": [118, 121], "hugo": [291, 298], "hulk": [268, 289], "human": [9, 198, 201, 224, 234, 268, 289], "humbl": [291, 297], "humor": [291, 297, 298], "hundr": [349, 350], "hunt": [268, 289], "hurt": [268, 289], "hustl": [268, 289], "hvar": [268, 289], "hxwxc": [159, 163], "hybrid": [118, 124, 342, 348], "hydrologi": [349, 350], "hyper": [137, 146], "hyperband": [7, 14, 253, 257], "hyperparam": [355, 361], "hyperparamet": [5, 6, 8, 12, 13, 137, 138, 140, 143, 172, 191, 195, 224, 228, 229, 235, 238, 244, 245, 248, 250, 254, 256, 258, 259, 263, 329, 335, 336, 341, 342, 348, 349, 352, 354, 355, 361, 362, 371], "hypnot": [291, 298], "i": [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 22, 24, 25, 27, 28, 32, 33, 35, 39, 41, 43, 44, 45, 49, 50, 51, 53, 54, 56, 61, 62, 63, 64, 66, 67, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 103, 105, 106, 107, 111, 114, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 130, 131, 132, 133, 136, 137, 138, 139, 143, 146, 147, 151, 155, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 192, 193, 195, 199, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 237, 238, 239, 241, 242, 243, 245, 246, 247, 248, 250, 253, 254, 255, 256, 257, 259, 260, 262, 263, 267, 268, 277, 280, 283, 285, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 301, 304, 305, 306, 315, 320, 323, 327, 328, 329, 330, 331, 333, 335, 336, 337, 341, 342, 343, 344, 345, 346, 348, 349, 350, 351, 352, 355, 356, 357, 358, 359, 360, 362, 363, 364, 365, 367, 368, 369, 371], "iam": [20, 21, 23, 24, 26, 27, 28, 29, 30, 34, 35, 36, 38, 39, 43, 44, 45, 55, 56, 63, 66, 69, 70], "ic": [268, 289], "iceberg": [8, 10, 191, 192, 206, 210], "ichiro": [268, 289], "icon": [82, 83, 118, 122, 291, 298], "id": [23, 28, 30, 35, 38, 39, 43, 45, 47, 48, 53, 56, 59, 60, 66, 69, 70, 75, 76, 80, 81, 88, 89, 118, 121, 123, 126, 127, 147, 153, 159, 163, 164, 167, 168, 176, 224, 235, 291, 295, 296, 297, 298, 299, 304, 343, 362, 364], "idea": [28, 30, 43, 45, 53, 56, 291, 298], "ideal": [4, 8, 82, 83, 110, 112, 117, 128, 133, 171, 174, 191, 192, 195, 291, 298, 349, 351, 355, 361], "ident": [17, 22, 24, 27, 35, 39, 43, 45, 53, 56, 66, 70, 94, 95, 126, 127, 224, 225, 234, 238, 240], "identifi": [28, 30, 43, 45, 53, 56, 110, 113, 118, 121, 159, 163, 224, 233, 234], "idiot": [268, 289, 291, 298], "idl": [28, 30, 43, 45, 53, 56, 101, 102, 105, 106, 126, 127, 128, 130, 132, 133, 136, 137, 146], "idx": [6, 224, 237, 259, 262, 342, 348, 355, 357, 362, 365, 371], "idx1": [13, 14], "idx2item": [342, 348], "idx3": [13, 14], "ig": [268, 289], "ignor": [329, 333, 336, 340], "iid": [342, 344, 348], "ill": [268, 289], "illustr": [10, 24, 26, 100, 206, 210, 268, 289, 329, 335], "iloc": [6, 137, 144, 147, 150, 259, 262, 342, 348, 355, 357, 361, 362, 365], "im": [268, 289, 291, 298], "imag": [3, 5, 6, 7, 8, 10, 13, 14, 15, 16, 24, 26, 80, 81, 86, 87, 92, 93, 101, 102, 108, 110, 115, 126, 127, 130, 131, 135, 137, 139, 145, 147, 148, 149, 153, 159, 163, 164, 167, 181, 186, 191, 192, 206, 210, 211, 212, 213, 219, 224, 226, 227, 228, 232, 237, 240, 241, 242, 245, 247, 253, 255, 256, 257, 259, 262, 263, 291, 298, 332, 335, 371], "image_arr": [238, 243], "image_arrai": [159, 163], "image_batch": 16, "image_byt": [329, 331, 362, 364, 365, 371], "image_bytes_raw": [329, 331], "image_classifi": 16, "image_classifier_ingress": 16, "image_height": [159, 163], "image_id": [10, 159, 163, 206, 212], "image_latents_256": [6, 259, 262], "image_uri": [101, 102, 108, 110, 115, 159, 163], "image_width": [159, 163], "imagebatchpredictor": [362, 371], "imagenet": [224, 227, 362, 364, 365], "imagenet_mean": [362, 365, 371], "imagenet_std": [362, 365, 371], "imageri": [291, 298], "imageserviceingress": 16, "imagin": [291, 297, 298], "imbalanc": [349, 351], "imdb": [291, 293, 294, 295, 297, 298], "img": [5, 10, 128, 131, 206, 211, 224, 226, 237, 329, 331, 335, 362, 364, 365, 371], "immatur": [8, 191, 196], "immedi": [2, 3, 8, 10, 92, 93, 101, 102, 105, 175, 179, 181, 187, 191, 195, 206, 211, 245, 250, 329, 334, 362, 365], "immut": [3, 181, 183], "impact": [8, 86, 87, 101, 102, 105, 106, 118, 124, 191, 196, 291, 298], "implement": [2, 5, 6, 7, 8, 9, 10, 14, 15, 17, 21, 92, 93, 126, 127, 164, 167, 168, 175, 180, 191, 192, 196, 197, 198, 204, 206, 212, 213, 219, 220, 224, 227, 228, 238, 239, 253, 256, 259, 262, 263, 267, 268, 277, 285, 288, 362, 363], "implementaiton": 13, "impli": [291, 294], "implicit": [329, 335], "import": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 108, 110, 113, 114, 115, 116, 118, 121, 122, 123, 128, 129, 131, 133, 136, 137, 138, 139, 140, 141, 142, 143, 146, 147, 148, 159, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 180, 181, 182, 187, 198, 199, 206, 207, 218, 219, 223, 233, 253, 254, 259, 260, 267, 285, 293, 298, 301, 309, 320, 325, 328, 333, 335, 340, 346, 350, 354, 359, 361, 367, 371], "import_path": [92, 93, 101, 102, 108, 110, 115, 118, 122], "importance_typ": [349, 353], "imposs": [291, 298], "impress": [291, 298], "improv": [2, 9, 10, 11, 118, 121, 122, 126, 127, 128, 130, 132, 137, 144, 147, 151, 155, 157, 159, 162, 175, 180, 198, 204, 205, 206, 208, 217, 218, 220, 238, 239, 329, 335, 349, 350, 354, 355, 359, 361], "imshow": [5, 7, 10, 13, 14, 16, 206, 211, 224, 226, 237, 253, 255, 329, 331, 335, 362, 364, 371], "in_channel": [5, 6, 13, 224, 227, 259, 262], "in_count": [88, 89], "in_featur": [13, 137, 140], "in_proj": [355, 358], "inaccess": [159, 163], "incept": [329, 335], "includ": [0, 3, 4, 5, 8, 9, 10, 11, 12, 13, 17, 20, 24, 26, 28, 34, 43, 52, 53, 65, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 106, 118, 121, 128, 134, 135, 147, 151, 153, 155, 157, 158, 164, 166, 171, 174, 181, 186, 187, 191, 192, 195, 196, 198, 201, 206, 215, 218, 221, 224, 236, 245, 248, 251, 299, 301, 304, 305, 309, 316, 325, 342, 344, 346, 355, 359, 362, 369], "include_path": [10, 15, 16, 128, 130, 137, 139, 146, 206, 210, 212], "incom": [16, 147, 148, 362, 371], "incomplet": [224, 232], "incorpor": [8, 191, 192, 362, 363], "incorrect_squar": [3, 181, 185], "increas": [101, 102, 105, 110, 116, 128, 130, 133, 136, 137, 139, 143, 146, 159, 163, 245, 252, 316, 323, 328, 329, 335, 342, 346], "increasingli": [1, 169, 170], "increment": [8, 92, 93, 147, 151, 191, 192, 362, 363], "incur": [8, 164, 167, 191, 196], "indent": [137, 140, 143], "independ": [9, 10, 11, 86, 87, 110, 116, 147, 149, 198, 205, 206, 210, 218, 221, 224, 230, 238, 239, 243, 355, 356, 359, 362, 365], "index": [0, 4, 8, 128, 133, 164, 166, 171, 174, 191, 192, 224, 226, 237, 309, 316, 325, 342, 344, 349, 351, 352, 355, 357, 362, 365, 371], "index_col": [349, 352], "indi": [291, 294], "indian": [268, 289], "indic": [1, 3, 169, 170, 181, 187, 224, 237, 342, 343, 344, 348, 349, 353, 362, 365], "individu": [2, 4, 12, 137, 144, 159, 162, 171, 173, 175, 180], "indonesiasayshbdforjustinbieb": [268, 289], "industri": 372, "ineffect": [291, 297], "ineffici": [101, 102, 105, 147, 151, 267, 268, 285], "inf": [137, 143], "infer": [3, 8, 9, 10, 11, 12, 15, 16, 84, 85, 90, 91, 106, 111, 117, 118, 121, 124, 126, 127, 130, 135, 137, 138, 139, 146, 147, 148, 151, 172, 181, 187, 190, 191, 195, 196, 197, 198, 201, 205, 206, 208, 213, 217, 218, 222, 225, 226, 234, 236, 245, 252, 284, 289, 292, 300, 315, 316, 324, 327, 329, 330, 335, 336, 337, 341, 344, 350, 352, 356, 357, 363], "inference_01": 372, "inference_02": 372, "inference_03": 372, "inference_04": 372, "inference_05": 372, "inference_06": 372, "inference_07": 372, "inference_08": 372, "inference_mod": [128, 131, 137, 140, 143, 147, 149, 224, 237], "inference_row": [362, 371], "inferf": [342, 343], "infinit": [10, 206, 212], "influenc": [291, 298, 336, 337], "info": [1, 12, 14, 110, 116, 164, 167, 169, 170, 268, 289], "inform": [5, 13, 43, 44, 53, 54, 66, 67, 86, 87, 118, 123, 155, 158, 159, 163, 164, 166, 168, 224, 228, 268, 289, 309, 316, 325, 362, 369], "infrastructur": [1, 5, 6, 18, 19, 25, 26, 35, 36, 79, 82, 83, 92, 93, 96, 97, 98, 100, 106, 110, 115, 117, 118, 121, 125, 159, 162, 163, 169, 170, 224, 225, 245, 249, 259, 261, 336, 337, 342, 348, 362, 363], "ingest": [4, 12, 15, 126, 127, 137, 139, 171, 174, 238, 239, 244, 245, 252, 349, 350, 351, 354, 362, 363], "ingmar": [291, 294], "ingress": [4, 16, 24, 26, 27, 51, 52, 64, 65, 78, 147, 150, 171, 174, 313, 314, 315, 316, 327], "ingress_from_cidr_map": [17, 22], "ingress_with_self": [17, 22], "inher": 100, "inherit": [4, 12, 92, 93, 171, 173], "ini": [155, 158], "init": [1, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 88, 89, 126, 127, 128, 129, 137, 138, 147, 148, 169, 170, 224, 226, 268, 274, 287, 291, 294, 299, 305], "initi": [1, 3, 6, 8, 10, 13, 15, 24, 26, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 92, 93, 118, 121, 128, 131, 137, 140, 147, 149, 153, 169, 170, 181, 184, 191, 196, 206, 213, 224, 225, 235, 237, 259, 262, 263, 268, 274, 277, 284, 287, 288, 292, 293, 299, 300, 301, 304, 305, 324, 342, 343, 355, 356], "initial_replica": 16, "inject": [164, 167, 224, 229, 232, 329, 330, 336, 337, 338, 339, 362, 366], "inlin": [128, 135, 137, 145, 147, 153, 315, 316, 327], "inner": [291, 297], "inning": [268, 287, 289], "innings": [268, 287, 289], "innoc": [291, 297, 298], "inplac": [13, 137, 140, 349, 351], "input": [6, 9, 10, 11, 12, 15, 16, 86, 87, 101, 102, 104, 128, 130, 131, 136, 137, 139, 147, 149, 164, 166, 198, 201, 203, 204, 206, 208, 212, 213, 214, 215, 216, 218, 222, 224, 226, 227, 228, 238, 239, 245, 252, 259, 262, 268, 277, 288, 291, 296, 299, 304, 315, 316, 327, 332, 349, 350, 355, 356, 357, 358, 359, 361], "input_window": [355, 357, 358, 359, 361], "inputdatabuff": [10, 206, 214], "inscrut": [8, 191, 195], "insert": [101, 102, 105], "insid": [3, 9, 86, 87, 88, 89, 96, 98, 128, 131, 133, 164, 166, 167, 181, 188, 190, 198, 205, 224, 226, 229, 238, 244, 291, 294, 329, 333, 336, 337, 349, 352, 354, 362, 363, 365, 371], "insight": [8, 137, 144, 159, 161, 163, 191, 195], "inspect": [3, 4, 5, 6, 13, 86, 87, 92, 93, 171, 174, 181, 187, 225, 226, 245, 252, 259, 262, 267, 268, 285, 329, 331, 364], "inspir": [268, 289], "instal": [3, 4, 24, 25, 27, 28, 29, 30, 37, 44, 45, 51, 52, 54, 55, 56, 64, 65, 67, 68, 72, 74, 79, 82, 83, 84, 85, 86, 87, 110, 114, 126, 127, 128, 129, 137, 138, 147, 148, 171, 172, 181, 186, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "instanc": [1, 3, 4, 5, 9, 10, 12, 15, 21, 23, 24, 26, 30, 34, 39, 43, 50, 53, 62, 66, 70, 80, 81, 84, 85, 88, 89, 110, 116, 126, 127, 128, 131, 132, 133, 134, 136, 137, 139, 143, 144, 146, 159, 163, 164, 167, 168, 169, 170, 171, 174, 181, 187, 190, 198, 204, 206, 208, 215, 224, 226, 232, 237, 267, 268, 285, 291, 298, 315, 320, 327], "instance_iam_role_arn": [17, 23], "instance_profile_arn": [28, 30], "instance_typ": [43, 50, 53, 62, 159, 163], "instanceid": [28, 30, 43, 45, 53, 56], "instant": [82, 83], "instanti": [3, 6, 16, 181, 190, 259, 262, 362, 363, 368], "instead": [3, 4, 5, 7, 9, 13, 14, 86, 87, 88, 89, 92, 93, 128, 129, 130, 131, 137, 138, 147, 148, 171, 174, 181, 184, 185, 189, 198, 201, 203, 204, 224, 225, 230, 231, 238, 239, 240, 241, 245, 247, 253, 257, 267, 268, 277, 285, 288, 291, 297, 298, 329, 330, 342, 343, 344, 349, 350, 352, 353, 355, 356, 362, 364, 367], "instruct": [24, 25, 84, 85, 100, 101, 102, 108, 110, 112, 113, 116, 118, 121, 122, 124, 126, 127], "instrument": [168, 268, 289], "insubordin": [291, 297], "int": [3, 4, 5, 6, 7, 10, 13, 14, 15, 159, 163, 171, 174, 181, 183, 185, 187, 206, 212, 215, 224, 232, 233, 237, 238, 241, 245, 248, 253, 255, 256, 257, 259, 262, 329, 331, 332, 336, 338, 339, 342, 345, 349, 353, 354, 355, 357, 359, 362, 364, 366, 367, 371], "int32": [349, 353, 355, 357], "int4": [110, 116], "int64": [137, 141, 268, 287, 290, 291, 295, 296, 336, 338, 342, 344], "intact": [224, 228], "integ": [12, 137, 139, 238, 243, 342, 343, 344, 362, 363], "integr": [4, 5, 6, 8, 9, 12, 13, 17, 21, 24, 27, 79, 80, 81, 92, 93, 101, 102, 107, 110, 116, 118, 120, 122, 123, 125, 126, 127, 147, 151, 154, 155, 157, 158, 159, 163, 164, 167, 171, 173, 191, 192, 195, 196, 197, 198, 205, 224, 225, 232, 240, 241, 244, 259, 261, 267, 268, 285, 291, 293, 299, 305, 309, 316, 325, 330, 336, 337, 340, 341, 342, 343, 349, 354, 355, 357, 362, 363, 371, 372], "intellig": [8, 118, 123, 191, 192], "intend": [86, 87, 92, 93, 291, 294, 299, 305], "intens": [1, 3, 4, 7, 8, 14, 101, 102, 106, 110, 112, 169, 170, 171, 174, 181, 187, 191, 196, 238, 239, 253, 255, 291, 298], "intent": [291, 297, 298, 329, 330, 349, 350], "interact": [8, 16, 24, 26, 82, 83, 96, 98, 100, 101, 102, 106, 107, 118, 123, 125, 191, 195, 291, 298, 342, 343, 344, 349, 354], "interconnect": [110, 116], "interest": [268, 289, 291, 294], "interfac": [4, 5, 8, 155, 158, 171, 173, 191, 194, 224, 226, 238, 239, 309, 316, 325, 342, 346], "intermedi": [3, 5, 10, 13, 101, 102, 104, 128, 130, 181, 184, 206, 208, 224, 234, 349, 354, 355, 361, 362, 371], "intern": [7, 17, 22, 159, 161, 238, 240, 253, 258, 342, 346, 348, 362, 366], "internet": [17, 22], "interoper": [8, 191, 192], "interpret": [342, 348, 349, 354, 355, 361], "interrupt": [84, 85, 147, 151, 245, 248, 250, 355, 360, 362, 363], "interv": [6, 8, 16, 191, 197, 259, 262, 355, 356, 357], "intervent": [245, 248, 342, 347, 355, 360, 362, 363], "interview": [9, 15, 198, 205, 268, 289], "intrigu": [291, 297, 298], "intro": [11, 12, 43, 44, 53, 55, 66, 68, 218, 223, 372], "introduc": [2, 8, 11, 16, 82, 83, 126, 127, 155, 156, 159, 162, 175, 177, 191, 195, 218, 219], "introduct": [15, 16, 238, 239, 299, 301], "introductori": 100, "intuit": [8, 191, 197], "invari": [291, 294], "invent": [291, 297, 298, 355, 356], "invert_yaxi": [349, 353], "invest": [268, 287, 289], "investig": [137, 144], "invit": [96, 98], "invoc": [2, 175, 178, 180], "invok": [2, 3, 175, 179, 180, 181, 188, 349, 354], "involv": [9, 10, 15, 101, 102, 108, 198, 204, 206, 215, 268, 289, 291, 297, 298, 309, 316, 325], "io": [0, 1, 10, 14, 43, 44, 46, 48, 53, 55, 58, 60, 66, 68, 73, 76, 128, 136, 169, 170, 182, 206, 209, 224, 226, 268, 274, 287, 299, 306, 309, 315, 316, 325, 327, 329, 331, 355, 357, 362, 364, 365, 371], "iot": [8, 191, 195], "ip": [13, 14, 17, 20, 22, 28, 30, 43, 45, 53, 56, 66, 73, 128, 133, 136, 137, 146, 224, 235, 362, 363], "ipykernel": [1, 169, 170], "ipynb": [82, 83, 86, 87, 100, 126, 127, 372], "ipywidget": [128, 129, 137, 138, 147, 148], "iran": [268, 289], "iron": [268, 289], "irrupt": [268, 289], "is_avail": [5, 7, 253, 256, 299, 304, 329, 335, 336, 341, 355, 361, 362, 371], "is_big_tip": 12, "isdir": [137, 142, 245, 251, 362, 371], "isfil": [362, 371], "isinst": [3, 181, 184], "islam": [268, 289], "isn": [5, 13, 53, 63, 101, 102, 106, 128, 130, 137, 143, 146, 147, 151, 224, 232, 268, 289, 291, 294, 329, 335, 362, 363], "isol": [8, 11, 17, 19, 22, 191, 192, 218, 221], "issu": [2, 5, 6, 8, 53, 63, 66, 71, 88, 89, 118, 125, 155, 157, 159, 163, 175, 177, 191, 195, 224, 225, 237, 245, 248, 259, 261, 291, 294, 362, 363], "itali": [268, 289], "itbr": [291, 298], "item": [2, 3, 5, 6, 7, 10, 13, 14, 118, 121, 137, 139, 143, 147, 150, 155, 158, 159, 163, 175, 180, 181, 189, 206, 212, 224, 233, 253, 255, 257, 259, 262, 291, 298, 299, 304, 329, 331, 336, 338, 345, 346, 349, 353, 355, 359, 361, 362, 367, 371], "item2idx": [342, 344, 348], "item_col": [342, 344], "item_embed": [342, 345, 348], "item_id": [342, 344, 348], "item_idx": [342, 343, 344, 345, 346], "item_metadata": [342, 348], "item_vec": [342, 345], "item_vector": [342, 348], "iter": [9, 10, 11, 12, 14, 80, 81, 82, 83, 101, 102, 105, 118, 124, 126, 127, 128, 130, 198, 202, 206, 209, 218, 220, 224, 228, 238, 241, 299, 304, 329, 330, 333, 335, 336, 340, 341, 342, 344, 346, 347, 349, 354, 355, 357], "iter_torch_batch": [137, 143, 238, 239, 241, 245, 252, 329, 333, 336, 340, 342, 343, 346, 348], "iterations_since_restor": 13, "iterrow": [342, 348], "its": [2, 3, 5, 7, 8, 9, 10, 11, 14, 15, 53, 63, 86, 87, 92, 93, 96, 98, 101, 102, 105, 107, 110, 116, 159, 162, 163, 164, 166, 167, 175, 177, 181, 183, 187, 191, 192, 198, 203, 205, 206, 210, 218, 221, 224, 225, 226, 228, 235, 236, 237, 253, 257, 267, 268, 285, 289, 291, 298, 309, 316, 317, 319, 320, 325, 328, 329, 330, 331, 342, 343, 346, 349, 350, 351, 352, 353, 355, 356, 362, 365, 367], "itself": [9, 159, 163, 198, 201], "iv": [291, 298], "j": [0, 2, 137, 143, 175, 180, 268, 289, 342, 344, 348], "jack": [291, 297], "jackson": [268, 289], "jadwal": [268, 289], "jai": [268, 287, 289, 291, 297], "jail": [268, 289], "jame": [268, 289, 291, 297], "jami": [268, 289], "jan": [268, 289], "janet": [268, 289], "januari": [268, 287, 289], "java": [8, 191, 195], "jean": [291, 298], "jeanmarc": [291, 298], "jeff": [291, 297], "jgz99": [110, 115], "jit": [10, 11, 15, 16, 206, 213, 218, 222], "job": [4, 6, 7, 14, 17, 22, 24, 26, 27, 28, 32, 34, 35, 41, 43, 50, 53, 62, 63, 66, 77, 84, 85, 86, 87, 88, 89, 94, 95, 96, 98, 118, 121, 126, 127, 132, 147, 154, 159, 161, 163, 164, 166, 171, 174, 224, 225, 227, 228, 230, 234, 235, 237, 238, 244, 245, 246, 248, 249, 252, 253, 257, 259, 261, 263, 268, 287, 289, 290, 291, 297, 298, 329, 335, 336, 341, 342, 348, 349, 350, 354, 355, 361, 362, 363, 368, 371, 372], "job_config": [90, 91], "job_descript": [118, 121], "job_id": [90, 91], "jobconfig": [90, 91], "joe": [268, 289], "john": [268, 287, 289, 291, 294, 297], "johnson": [268, 289, 291, 294], "joi": [268, 289], "join": [5, 6, 8, 9, 13, 17, 22, 86, 87, 118, 121, 125, 128, 133, 137, 139, 143, 146, 159, 163, 168, 191, 196, 198, 205, 224, 234, 237, 245, 247, 248, 259, 263, 268, 289, 293, 295, 298, 329, 333, 335, 336, 340, 341, 344, 346, 349, 351, 352, 355, 357, 359, 361, 362, 364, 367, 371], "join_typ": [291, 297], "joint": [329, 330, 355, 356, 362, 364], "jointli": [137, 144], "joke": [110, 114], "journei": [168, 291, 298], "jpeg": [8, 191, 192, 329, 330, 331, 335, 362, 364], "jpg": [128, 130, 137, 146], "jq": [28, 30, 43, 45, 53, 56], "json": [4, 8, 10, 11, 12, 16, 102, 108, 119, 120, 121, 123, 125, 137, 139, 140, 143, 146, 147, 150, 153, 164, 167, 168, 171, 174, 191, 192, 206, 210, 218, 219, 222, 316, 320, 328, 329, 331, 336, 338, 342, 344, 349, 351, 362, 364, 367], "json_method1": [118, 122], "json_request": [11, 16, 164, 167, 218, 222], "json_schema": [118, 122], "juda": [268, 289], "judg": [291, 297, 349, 351], "juggl": [11, 218, 220], "jule": [268, 289], "jump": [11, 16, 218, 222, 291, 298, 336, 337], "jun": [268, 289], "june": [4, 12, 66, 69, 100, 171, 174], "jupyt": [84, 85, 128, 130], "jupyter_execute_notebook": 0, "jupyterlab": [82, 83], "just": [7, 9, 14, 15, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 94, 95, 101, 102, 106, 118, 123, 126, 127, 128, 133, 135, 136, 137, 139, 140, 198, 201, 203, 238, 239, 245, 248, 253, 256, 257, 268, 280, 289, 291, 294, 295, 297, 298, 329, 330, 331, 342, 343, 362, 364, 371], "justifi": [268, 289, 291, 297], "justin": [268, 289], "j\u00e4reg\u00e5rd": [291, 298], "k": [101, 102, 105, 126, 127, 137, 139, 147, 153, 268, 287, 289, 299, 304, 336, 337, 341, 342, 348, 349, 350, 355, 361, 362, 371], "k8": [17, 18, 20, 27, 43, 45, 46, 50, 53, 56, 58, 62, 66, 69, 77, 79, 155, 158, 372], "kafka": [8, 128, 130, 191, 195], "katharina": [291, 298], "ke": [268, 289], "keep": [6, 84, 85, 94, 95, 101, 102, 108, 224, 225, 227, 228, 234, 237, 238, 239, 240, 245, 248, 251, 259, 262, 268, 283, 289, 290, 291, 296, 329, 331, 333, 336, 337, 340, 341, 342, 343, 344, 346, 349, 351, 352, 355, 359, 361, 362, 364, 367, 368, 369, 371], "keeper": [291, 297], "kei": [7, 8, 9, 10, 11, 14, 15, 20, 22, 24, 26, 28, 30, 43, 45, 53, 56, 86, 87, 100, 103, 104, 108, 116, 137, 141, 147, 150, 164, 166, 191, 192, 198, 204, 206, 207, 215, 218, 221, 224, 226, 227, 228, 234, 238, 240, 245, 247, 253, 257, 299, 301, 342, 348, 349, 352, 353, 362, 367], "kenni": [268, 289], "kept": [268, 289, 342, 344], "kernel": [1, 24, 27, 101, 102, 107, 128, 129, 137, 138, 147, 148, 169, 170, 224, 227], "kernel_s": [5, 7, 13, 14, 224, 227, 253, 256, 257], "kerri": [268, 287, 289], "kessler": [291, 298], "keylogg": [118, 121], "kick": [342, 346, 362, 368], "kiddo": [268, 289], "kill": [7, 14, 224, 237, 253, 257, 291, 294, 297, 298, 329, 331], "kim": [268, 289], "kind": [1, 169, 170, 349, 351], "kingdom": [291, 298], "kitchen": [268, 289], "klaviyo": 16, "klondik": [291, 297], "km": [17, 22], "know": [3, 8, 118, 121, 159, 163, 181, 189, 191, 196, 245, 246, 248, 291, 297, 298, 355, 356], "knowledg": [79, 110, 117, 164, 165, 349, 353, 354], "known": [329, 330], "kong": [291, 297], "kpop": [268, 289], "kri": [268, 289], "kserv": [147, 151], "kube": [43, 46, 51, 53, 58, 63, 64], "kubeconfig": [43, 46, 53, 58], "kubectl": [43, 44, 46, 49, 50, 51, 53, 55, 58, 61, 62, 63, 64, 68, 73, 78], "kuberai": [155, 158], "kubernet": [11, 17, 19, 20, 44, 45, 47, 52, 55, 56, 59, 65, 68, 69, 73, 75, 78, 79, 126, 127, 218, 220], "kucinich": [268, 289], "kueue": [24, 26], "kurt": [268, 289], "kv": [104, 106, 109, 110, 116, 349, 353], "kv_cache_util": [110, 116], "kvedzwag2qa8i5bj": [110, 115, 147, 153], "kwarg": [128, 131], "l": [4, 5, 9, 10, 13, 15, 53, 63, 171, 174, 198, 201, 203, 206, 210, 329, 330, 336, 337, 342, 343, 362, 363], "l4": [101, 102, 108, 118, 121, 122], "l40": [110, 113, 114, 116, 118, 123], "l6": [268, 277, 288], "lab": [7, 14, 253, 257], "label": [4, 5, 7, 10, 11, 12, 13, 14, 15, 128, 130, 137, 139, 141, 143, 146, 164, 165, 171, 174, 206, 212, 213, 215, 218, 222, 224, 226, 228, 237, 238, 240, 241, 242, 243, 245, 247, 253, 255, 256, 257, 268, 272, 273, 274, 287, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 303, 304, 316, 320, 328, 329, 331, 333, 335, 336, 340, 342, 346, 349, 351, 352, 353, 354, 355, 359, 361, 364, 365, 369, 371], "label_col": [349, 352], "label_column": [4, 12, 171, 174, 349, 352], "label_nam": [329, 331, 362, 364, 371], "label_to_class": [137, 139, 146], "labeled_batch": [10, 206, 212], "labia": [291, 294], "labour": [268, 289], "lack": [8, 191, 192, 196], "lag": [355, 361], "lai": [268, 289], "lakehous": [10, 206, 210, 238, 239, 245, 252], "lambda": [6, 86, 87, 147, 150, 159, 163, 259, 262, 291, 296, 349, 353, 354], "land": [0, 349, 350], "landscap": [195, 316, 317, 319, 320, 328, 372], "languag": [5, 6, 8, 13, 104, 191, 192, 194, 195, 259, 264, 342, 348], "laptop": [1, 126, 127, 169, 170, 291, 293], "lar": [291, 298], "larami": [291, 297], "larg": [0, 4, 5, 6, 8, 9, 10, 13, 15, 86, 87, 104, 105, 106, 110, 112, 117, 118, 124, 128, 132, 137, 139, 147, 151, 159, 163, 171, 174, 182, 183, 184, 190, 191, 192, 195, 198, 205, 206, 208, 245, 246, 259, 262, 263, 267, 268, 277, 283, 285, 288, 290, 291, 293, 296, 298, 299, 301, 304, 305, 306, 309, 316, 325, 329, 330, 331, 335, 342, 344, 348, 349, 350, 351, 354, 355, 356, 361, 362, 364], "large_mat_from_object_stor": [3, 181, 183], "large_matrix": [3, 181, 183], "larger": [8, 16, 86, 87, 128, 136, 147, 154, 191, 196, 245, 252, 291, 296, 329, 335, 336, 341, 342, 343, 362, 371], "largest": [118, 121], "last": [3, 5, 6, 8, 15, 53, 57, 92, 93, 101, 102, 105, 118, 123, 137, 139, 144, 181, 189, 191, 195, 196, 224, 225, 232, 236, 246, 247, 248, 259, 261, 268, 287, 289, 290, 299, 306, 342, 346, 347, 349, 352, 354, 355, 357, 359, 362, 365, 369], "last_login": 168, "lastmodifi": [86, 87], "latenc": [8, 11, 86, 87, 105, 109, 110, 116, 118, 124, 128, 130, 164, 167, 191, 195, 197, 218, 220, 336, 341, 362, 371], "latent": [6, 259, 262, 342, 343], "later": [7, 13, 14, 16, 82, 83, 101, 102, 107, 128, 130, 137, 139, 168, 224, 226, 230, 234, 236, 238, 242, 243, 253, 256, 291, 298, 329, 331, 332, 336, 339, 342, 343, 344, 346, 349, 351, 362, 365], "latest": [1, 5, 13, 28, 29, 43, 44, 53, 55, 66, 74, 82, 83, 169, 170, 224, 236, 245, 246, 247, 248, 249, 250, 268, 274, 287, 291, 297, 309, 315, 316, 325, 327, 333, 336, 337, 340, 341, 342, 343, 346, 347, 348, 355, 356, 359, 360, 361, 362, 363, 370], "latin": [342, 348], "laugh": [268, 289], "laughter": [268, 289], "launch": [2, 3, 4, 6, 7, 9, 10, 11, 17, 19, 84, 85, 92, 93, 94, 95, 96, 98, 101, 102, 103, 111, 118, 119, 128, 133, 135, 136, 137, 139, 143, 145, 146, 147, 148, 153, 165, 171, 172, 175, 176, 181, 182, 189, 198, 199, 206, 207, 218, 219, 228, 230, 237, 253, 254, 259, 260, 267, 268, 285, 291, 293, 296, 299, 301, 309, 316, 325, 330, 337, 349, 350, 352, 354, 356, 361, 363], "layer": [4, 17, 19, 110, 113, 116, 137, 139, 140, 171, 173, 224, 227, 329, 335, 342, 344, 355, 356], "layer1": 13, "layer2": 13, "layer3": 13, "layer4": 13, "layers_per_block": [6, 259, 262], "layout": [342, 344], "lazi": [9, 15, 128, 130, 198, 201, 202, 207, 268, 280, 283, 289, 290, 342, 344, 349, 351], "lbc": [43, 46, 53, 58], "lbl": [329, 331], "le": [268, 289], "lead": [3, 8, 181, 187, 189, 191, 196, 268, 289, 291, 298], "leader": [268, 289], "leagu": [268, 289], "leakag": [355, 357], "lean": [362, 363], "learn": [3, 4, 5, 6, 7, 9, 10, 13, 14, 15, 28, 34, 101, 102, 103, 110, 117, 125, 126, 127, 128, 135, 159, 160, 161, 171, 173, 181, 185, 187, 192, 195, 198, 205, 206, 217, 226, 228, 236, 253, 257, 259, 262, 267, 268, 277, 283, 284, 285, 288, 290, 291, 292, 293, 294, 298, 299, 300, 301, 302, 304, 305, 306, 309, 316, 324, 325, 335, 341, 344, 345, 346, 348, 351, 357, 359, 361, 364, 371], "learning_r": [224, 229], "least": [3, 17, 22, 24, 26, 27, 101, 102, 108, 181, 185, 187], "leav": [5, 28, 30, 43, 45, 80, 81, 84, 85, 92, 93, 291, 297, 329, 333, 349, 351, 354, 355, 361, 362, 371], "lecun": [13, 14], "left": [2, 8, 82, 83, 84, 85, 101, 102, 105, 175, 180, 191, 197, 291, 298, 342, 347, 348, 349, 354, 355, 360], "leftarrow": [329, 330, 336, 337], "leftov": [245, 251], "legend": [329, 333, 336, 340, 342, 346, 355, 359, 361, 362, 369], "leigh": [268, 289], "lemieux": [268, 289], "len": [3, 5, 6, 88, 89, 137, 140, 143, 181, 189, 224, 226, 237, 259, 262, 291, 295, 313, 314, 315, 316, 327, 342, 344, 346, 348, 349, 351, 352, 353, 354, 355, 357, 359, 362, 364, 365], "lena": [291, 294], "length": [10, 28, 30, 43, 45, 53, 56, 101, 102, 104, 105, 106, 110, 113, 118, 124, 206, 208, 313, 314, 315, 316, 327, 355, 357], "leo": [291, 298], "leopold": [291, 298], "lesnar": [268, 289], "less": [118, 121, 291, 294, 329, 335], "lesson": [12, 372], "let": [2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 35, 39, 43, 45, 46, 53, 58, 82, 83, 84, 85, 86, 87, 92, 93, 94, 95, 101, 102, 106, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 125, 164, 166, 171, 172, 174, 175, 180, 181, 183, 184, 185, 187, 189, 190, 198, 201, 202, 203, 204, 206, 210, 211, 213, 215, 216, 218, 222, 224, 226, 227, 228, 232, 234, 253, 255, 256, 257, 259, 262, 263, 329, 330, 331, 332, 333, 336, 337, 342, 343, 349, 350, 351, 355, 359, 362, 364, 366, 371], "level": [0, 5, 6, 7, 8, 11, 13, 14, 16, 17, 22, 24, 27, 88, 89, 94, 95, 101, 102, 105, 107, 128, 133, 155, 157, 159, 162, 164, 167, 182, 185, 191, 195, 218, 221, 224, 227, 235, 253, 256, 259, 263, 291, 294, 342, 344, 346, 355, 356, 361, 362, 363], "leverag": [8, 10, 11, 24, 25, 126, 127, 164, 167, 191, 192, 206, 208, 217, 218, 220, 267, 268, 283, 285, 290, 291, 293, 299, 301, 305, 329, 335, 336, 341, 349, 354], "lexu": [118, 122, 268, 289], "lgbm": [5, 6, 224, 225, 259, 261], "lh": [9, 198, 203], "li": [291, 298], "liar": [268, 289], "lib": [86, 87, 137, 144, 268, 289], "libomp": [4, 171, 172], "librari": [2, 5, 7, 8, 9, 10, 11, 14, 15, 16, 80, 81, 86, 87, 88, 89, 147, 151, 159, 163, 175, 177, 191, 195, 196, 198, 200, 201, 206, 210, 212, 213, 215, 218, 221, 222, 224, 226, 253, 256, 257, 267, 284, 285, 292, 293, 300, 301, 303, 305, 309, 320, 324, 325, 328, 329, 331, 336, 338, 342, 344, 355, 357, 362, 364], "licens": [1, 169, 170], "lie": [268, 289, 336, 338], "life": [268, 289, 291, 294, 298], "lifecycl": [8, 11, 13, 24, 25, 26, 86, 87, 90, 91, 100, 191, 197, 218, 221, 225], "lift": [80, 81], "light": [0, 10, 206, 212, 268, 289], "lightli": 100, "lightn": [5, 137, 143, 261, 330, 331, 332, 335, 336, 337, 338, 340, 341], "lightning_training_loop": [6, 259, 262], "lightningmodul": [6, 259, 262, 330, 337], "lightweight": [11, 88, 89, 218, 220, 268, 277, 288, 342, 343, 344, 348, 349, 351, 355, 357, 361, 362, 363], "lik": [291, 298], "like": [3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 23, 24, 26, 27, 28, 30, 31, 35, 38, 39, 40, 41, 43, 45, 47, 53, 56, 59, 62, 66, 75, 82, 83, 84, 85, 101, 102, 107, 110, 113, 114, 118, 122, 126, 127, 128, 130, 132, 133, 134, 137, 139, 144, 147, 154, 171, 174, 181, 184, 185, 186, 187, 191, 192, 195, 196, 197, 198, 201, 203, 205, 206, 213, 224, 225, 228, 229, 233, 235, 238, 241, 253, 257, 259, 263, 268, 289, 291, 293, 294, 297, 298, 299, 305, 309, 316, 320, 325, 328, 329, 335, 336, 341, 342, 343, 344, 348, 349, 350, 352], "likeeeeeeeee": [268, 289], "likelihood": [362, 363], "limit": [7, 8, 9, 24, 27, 28, 30, 43, 45, 53, 56, 84, 85, 86, 87, 101, 102, 104, 118, 121, 191, 192, 198, 203, 213, 253, 257], "limousin": [4, 9, 12, 171, 174, 198, 201], "line": [82, 83, 92, 93, 128, 129, 137, 138, 147, 148, 155, 158, 291, 298, 329, 330, 355, 356, 362, 363], "linear": [3, 7, 13, 14, 137, 140, 181, 190, 253, 257, 336, 339, 355, 358], "linearmodel": [3, 181, 190], "lineup": [268, 287, 289, 290], "link": [0, 24, 26, 92, 93, 118, 120, 125, 224, 230], "linux": [155, 158], "list": [3, 4, 9, 10, 15, 28, 30, 32, 33, 35, 38, 41, 43, 45, 49, 50, 51, 53, 56, 57, 61, 62, 64, 66, 69, 71, 77, 78, 82, 83, 86, 87, 90, 91, 96, 98, 101, 102, 106, 118, 121, 128, 130, 137, 143, 159, 163, 171, 174, 181, 184, 198, 201, 203, 206, 210, 216, 224, 236, 237, 238, 242, 291, 297, 329, 331, 336, 338, 342, 344, 348, 349, 351, 352, 355, 361], "list_": [355, 357], "list_objects_v2": [86, 87, 118, 121], "listbucket": [17, 22], "listbucketmultipartupload": [17, 22], "listdir": [362, 371], "listfil": [10, 206, 214], "listmultipartuploadpart": [17, 22], "lit": [268, 287, 289], "lite": [329, 330, 331, 364], "liter": [268, 289], "littl": [268, 289, 291, 297, 298], "live": [268, 289, 291, 297, 316, 317, 319, 320, 328, 342, 348, 355, 356], "ll": [1, 5, 6, 7, 14, 17, 18, 35, 37, 43, 47, 50, 53, 59, 62, 66, 68, 70, 75, 80, 81, 82, 83, 88, 89, 100, 101, 102, 103, 109, 110, 111, 112, 113, 115, 119, 121, 159, 160, 169, 170, 226, 227, 229, 230, 235, 252, 253, 256, 259, 262, 268, 289, 291, 297, 355, 356], "llama": [101, 102, 108, 113, 114, 115, 116, 118, 121, 124], "llamafactoryai": [118, 121], "llm": [10, 105, 109, 112, 114, 115, 117, 120, 122, 123, 125, 128, 132, 147, 151, 206, 208], "llm_config": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123], "llmconfig": [101, 102, 108, 110, 113, 116, 118, 121, 123], "lo": [1, 169, 170], "load": [3, 4, 8, 11, 12, 16, 17, 22, 24, 27, 28, 34, 51, 52, 64, 65, 82, 83, 84, 85, 92, 93, 101, 102, 107, 108, 110, 113, 115, 118, 121, 123, 128, 130, 131, 137, 138, 139, 140, 143, 146, 147, 151, 171, 174, 181, 187, 190, 191, 192, 197, 199, 205, 207, 208, 213, 218, 220, 222, 225, 226, 232, 234, 239, 246, 248, 250, 252, 254, 262, 267, 277, 283, 285, 288, 290, 293, 298, 299, 301, 302, 303, 304, 305, 306, 309, 315, 316, 325, 327, 330, 334, 335, 336, 341, 343, 346, 348, 350, 353, 354, 356, 359, 361, 363, 365, 367, 371], "load_data": [4, 171, 174], "load_dataset": [268, 271, 274, 286, 287, 291, 294, 299, 302, 304, 329, 331, 355, 357, 362, 364, 371], "load_dotenv": [137, 138], "load_ext": [128, 129, 137, 138, 147, 148], "load_from_checkpoint": [6, 259, 263], "load_model": [4, 12, 128, 131, 171, 174, 349, 352], "load_model_ray_train": [5, 13, 224, 228, 231, 238, 240, 245, 247], "load_model_torch": [5, 13], "load_state_dict": [5, 13, 137, 140, 224, 237, 245, 247, 329, 335, 336, 341, 342, 346, 348, 355, 359, 361, 362, 367, 371], "loadbalanc": [24, 27], "loaded_df": [6, 259, 262], "loaded_model": [5, 13], "loaded_model_ray_train": [5, 6, 13, 259, 263], "loader": [5, 238, 240, 241, 243, 245, 247, 299, 301, 329, 333, 342, 343, 355, 357, 362, 363, 365, 366, 371], "loan": [268, 289], "loc": [12, 14], "local": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 22, 28, 31, 35, 40, 43, 47, 51, 53, 59, 64, 66, 75, 92, 93, 100, 101, 102, 103, 108, 111, 117, 118, 119, 126, 127, 128, 130, 137, 144, 147, 150, 153, 156, 157, 164, 165, 166, 167, 171, 172, 174, 175, 176, 181, 182, 188, 191, 195, 198, 199, 203, 206, 207, 213, 216, 218, 219, 220, 222, 223, 224, 225, 226, 234, 237, 238, 239, 253, 254, 259, 260, 267, 268, 274, 285, 287, 289, 291, 293, 299, 301, 305, 306, 309, 316, 325, 329, 330, 331, 333, 336, 337, 340, 342, 343, 349, 350, 352, 355, 356, 362, 367, 371], "local_fil": [86, 87], "local_file_path": [118, 121], "local_idx": [362, 365], "local_path": [5, 10, 11, 13, 15, 16, 118, 121, 164, 167, 206, 213, 218, 222], "local_pred_fold": [10, 206, 216], "local_storag": [6, 13, 259, 262], "local_zip": [342, 344], "localhost": [0, 4, 11, 12, 16, 101, 102, 108, 110, 114, 118, 121, 122, 123, 126, 127, 164, 167, 171, 174, 218, 222, 316, 320, 328], "locat": [1, 5, 35, 39, 66, 70, 86, 87, 94, 95, 118, 123, 128, 132, 136, 137, 144, 146, 147, 154, 164, 167, 169, 170, 224, 226, 234, 236, 349, 350], "lock": [1, 88, 89, 169, 170, 284, 291, 292, 297, 300, 324], "lockfil": [126, 127], "lodg": [349, 350], "lofton": [268, 289], "log": [5, 6, 8, 10, 12, 13, 14, 17, 21, 22, 28, 31, 32, 35, 36, 40, 41, 43, 47, 50, 53, 59, 62, 63, 66, 75, 77, 82, 83, 90, 91, 92, 93, 100, 101, 102, 107, 110, 116, 128, 134, 147, 153, 155, 157, 159, 161, 162, 163, 165, 168, 191, 192, 195, 206, 214, 224, 225, 226, 228, 233, 234, 235, 236, 238, 240, 244, 245, 250, 252, 259, 261, 262, 299, 306, 329, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 346, 349, 350, 352, 354, 355, 358, 359, 361, 362, 363, 367, 372], "log_artifact": [137, 143], "log_engine_metr": [110, 113, 116], "log_every_n_step": [6, 259, 262], "log_level": [164, 167], "log_metr": [137, 143], "log_param": [137, 143], "log_result": [88, 89], "logdir": [12, 13, 299, 306], "logger": [164, 167], "logging_config": [164, 167], "logic": [3, 5, 6, 8, 10, 11, 12, 13, 14, 16, 17, 19, 22, 118, 124, 137, 138, 147, 149, 151, 181, 187, 191, 196, 197, 206, 212, 218, 221, 224, 227, 238, 240, 245, 247, 259, 263, 299, 306, 315, 316, 317, 319, 320, 327, 328, 329, 330, 333, 335, 336, 337, 342, 343, 346, 355, 357, 362, 363, 367], "login": [28, 31, 35, 38, 40, 43, 47, 53, 59, 66, 69, 75, 82, 83], "logist": 12, "logit": [10, 11, 15, 16, 206, 213, 218, 222, 224, 227, 237, 299, 303, 304, 362, 367, 371], "logloss": 12, "loguniform": [7, 14, 253, 257], "loki": [268, 289], "lol": [268, 289], "london": [268, 289], "long": [3, 4, 5, 6, 7, 12, 13, 14, 17, 21, 96, 98, 101, 102, 105, 106, 118, 124, 171, 174, 181, 187, 224, 225, 245, 246, 253, 256, 259, 261, 263, 268, 289, 291, 298, 342, 344, 346, 355, 356], "longer": [5, 8, 191, 197, 224, 228, 237, 238, 240, 245, 251, 252, 329, 335], "longrightarrow": [336, 337, 355, 356], "look": [3, 4, 5, 7, 8, 9, 10, 13, 15, 16, 28, 30, 31, 35, 40, 43, 45, 47, 53, 56, 57, 59, 66, 75, 110, 117, 118, 121, 125, 137, 139, 141, 164, 166, 171, 174, 181, 190, 191, 197, 198, 204, 206, 211, 215, 224, 226, 228, 238, 241, 253, 257, 268, 287, 289, 291, 297, 298, 329, 335, 349, 351], "look_back_period_": 16, "lookup": [342, 343], "loop": [118, 123, 137, 143, 176, 225, 226, 227, 229, 230, 234, 235, 236, 239, 243, 244, 246, 248, 249, 250, 252, 261, 263, 299, 301, 304, 329, 330, 333, 335, 341, 343, 350, 354, 356, 362, 363, 367, 371], "lora": [101, 102, 105, 109, 119, 120, 125, 147, 151], "lora_checkpoint": [118, 121], "lora_config": [118, 121], "lose": [245, 246, 249], "loss": [5, 6, 7, 12, 13, 14, 137, 143, 224, 226, 227, 228, 233, 234, 236, 238, 240, 245, 247, 253, 256, 257, 259, 262, 291, 298, 299, 304, 305, 306, 330, 332, 337, 339, 341, 349, 350, 352, 354, 361, 363, 364, 367, 368], "loss_fn": [6, 137, 143, 259, 262, 329, 332, 336, 339, 355, 359], "loss_funct": 13, "loss_ms": [6, 259, 262], "lot": [5, 6, 128, 134, 224, 225, 259, 261], "louboutin": [268, 287, 289], "loung": [268, 289], "love": [126, 127, 268, 289, 291, 298, 316, 320, 328], "low": [8, 101, 102, 105, 107, 118, 121, 124, 191, 195, 197, 336, 339, 341, 355, 361, 362, 363, 371], "lower": [86, 87, 88, 89, 110, 112, 117, 118, 124, 268, 289, 313, 314, 315, 316, 327], "lowercas": [313, 314, 315, 316, 327], "lowest": [362, 368], "lr": [5, 6, 7, 13, 14, 137, 143, 144, 224, 228, 238, 240, 245, 247, 253, 256, 257, 259, 262, 263, 299, 304, 305, 329, 332, 336, 339, 342, 346, 355, 359, 361, 362, 367, 368], "lr_factor": [137, 143, 144], "lr_patienc": [137, 143, 144], "lr_schedul": [6, 137, 143, 259, 262], "lssf": [1, 169, 170], "lstm": [355, 361], "lstrip": [86, 87], "lt": [291, 298], "luck": [268, 289, 291, 297], "lucki": [268, 289], "lupin": [268, 287, 289, 290], "lustr": [8, 191, 192], "ly": [291, 298], "m": [0, 1, 3, 5, 13, 169, 170, 181, 189, 268, 289, 291, 297, 298, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "m1": [1, 169, 170], "m2": [1, 169, 170], "m3": [1, 169, 170], "m5": [88, 89, 128, 136, 137, 139, 159, 163], "mac": [268, 289, 299, 304], "machin": [1, 3, 4, 9, 16, 17, 20, 27, 80, 81, 82, 83, 84, 85, 90, 91, 137, 144, 155, 156, 159, 161, 169, 170, 171, 173, 181, 185, 192, 195, 198, 205, 267, 268, 277, 280, 284, 285, 288, 289, 290, 291, 292, 293, 298, 299, 300, 301, 305, 306, 309, 316, 324, 325, 349, 350], "maco": [1, 4, 155, 158, 169, 170, 171, 172], "macosx": [1, 169, 170], "maddon": [268, 289], "made": [6, 11, 218, 221, 259, 263, 291, 294, 297, 298, 342, 346], "madison": [268, 289], "magnitud": [355, 357], "mai": [3, 5, 6, 28, 30, 35, 39, 42, 43, 45, 53, 56, 63, 66, 70, 71, 73, 78, 84, 85, 86, 87, 88, 89, 94, 95, 128, 129, 137, 138, 139, 147, 148, 155, 158, 181, 187, 189, 190, 238, 239, 259, 260, 268, 289, 291, 297, 298, 309, 316, 325, 349, 353, 362, 371], "maiden": [268, 289], "main": [0, 3, 5, 6, 9, 11, 15, 28, 30, 35, 39, 43, 45, 53, 56, 90, 91, 92, 93, 101, 102, 108, 110, 113, 128, 135, 137, 143, 159, 163, 164, 167, 168, 181, 188, 198, 204, 218, 223, 224, 228, 259, 262, 268, 289, 291, 298, 301, 342, 344, 355, 357], "maintain": [3, 8, 11, 17, 19, 86, 87, 101, 102, 105, 126, 127, 181, 190, 191, 192, 197, 218, 221, 291, 298, 336, 337], "mainten": [24, 25], "major": [291, 294], "make": [0, 1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 66, 78, 82, 83, 86, 87, 92, 93, 94, 95, 100, 101, 102, 104, 118, 120, 121, 128, 130, 132, 133, 134, 147, 149, 159, 163, 169, 170, 171, 174, 181, 184, 188, 191, 197, 198, 201, 204, 205, 218, 223, 224, 226, 232, 236, 238, 239, 242, 243, 244, 245, 246, 248, 249, 253, 257, 258, 259, 262, 263, 267, 268, 285, 289, 291, 293, 294, 297, 298, 299, 301, 305, 309, 313, 314, 315, 316, 325, 327, 329, 330, 331, 336, 338, 342, 348, 349, 350, 351, 355, 356, 357, 360, 361, 362, 363, 365], "make_pendulum_dataset": [336, 338], "makedir": [137, 142, 329, 331, 333, 336, 340, 342, 344, 349, 351, 355, 357, 362, 364], "malaga": [268, 289], "male": [291, 294, 298], "mall": [268, 289], "malloc": [155, 158], "mamba": [284, 292, 300, 324], "man": [268, 289, 291, 294, 297, 298], "manag": [2, 5, 6, 8, 12, 17, 19, 20, 21, 22, 25, 27, 35, 37, 39, 43, 47, 51, 53, 59, 64, 66, 68, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 107, 109, 110, 115, 126, 127, 128, 135, 147, 153, 155, 158, 159, 162, 164, 167, 175, 177, 182, 191, 192, 224, 225, 238, 240, 259, 261, 268, 289, 299, 305, 309, 316, 325, 329, 330, 333, 335, 336, 337, 340, 343, 348, 349, 350, 355, 356, 362, 363, 366, 367, 371], "mani": [2, 4, 9, 10, 11, 28, 30, 43, 45, 53, 56, 88, 89, 92, 93, 101, 102, 105, 110, 117, 118, 122, 124, 128, 134, 135, 171, 174, 175, 177, 182, 186, 187, 198, 201, 206, 212, 218, 220, 224, 227, 229, 230, 232, 268, 277, 280, 287, 288, 289, 291, 294, 298, 316, 323, 328, 342, 343, 344, 349, 352, 355, 361, 362, 363], "manipul": [291, 297], "mann": [291, 297, 298], "manner": [4, 9, 10, 15, 171, 174, 198, 199, 201, 203, 206, 207, 291, 293, 299, 305, 362, 363], "mansbridg": [268, 289], "manual": [5, 35, 42, 66, 78, 137, 144, 224, 225, 226, 228, 230, 231, 234, 246, 248, 252, 268, 289, 299, 304, 329, 330, 335, 336, 337, 342, 343, 347, 349, 350, 352, 354, 355, 356, 360, 362, 363, 366, 367, 369], "manual_se": [355, 359], "map": [6, 12, 16, 118, 121, 123, 128, 130, 131, 137, 139, 146, 159, 163, 238, 243, 259, 262, 299, 304, 336, 337, 342, 344, 348, 349, 350, 351, 355, 361, 362, 363, 364, 365, 367, 371], "map_batch": [4, 9, 10, 12, 15, 16, 128, 131, 137, 139, 146, 164, 166, 171, 174, 198, 202, 206, 212, 213, 215, 267, 268, 277, 280, 283, 285, 288, 289, 290, 329, 331, 336, 338, 342, 344, 349, 353, 354, 355, 361, 362, 371], "map_group": [9, 10, 15, 198, 204, 205, 206, 215], "map_loc": [5, 6, 13, 137, 140, 224, 237, 259, 263, 329, 335, 336, 341, 342, 346, 348, 355, 359, 361, 362, 367, 371], "mapbatch": [10, 206, 214, 268, 290], "mapreduc": [8, 191, 195], "mar": [316, 317, 319, 320, 328], "marathon": [268, 289], "marc": [291, 298], "march": [268, 289], "mario": [268, 289], "mark": [155, 158, 291, 298], "markdown": 0, "marker": [329, 333, 336, 340, 342, 346, 355, 359, 361, 362, 369], "market_typ": [159, 163], "marlei": [268, 289], "marri": [291, 294], "martial": [291, 297], "martin": [268, 289], "mask": [299, 304], "mass": [268, 289], "massiv": [10, 206, 208], "master": [1, 14, 118, 120, 169, 170, 291, 297, 299, 306, 355, 357], "mat1_ref": [3, 181, 183], "mat2_ref": [3, 181, 183], "match": [3, 10, 43, 51, 53, 64, 88, 89, 92, 93, 110, 113, 118, 121, 122, 124, 128, 136, 181, 187, 206, 213, 245, 250, 268, 289, 349, 351], "matching_analysi": [118, 121], "materi": [4, 9, 11, 12, 16, 128, 130, 137, 139, 159, 163, 164, 166, 171, 174, 198, 201, 202, 204, 207, 208, 210, 211, 212, 213, 215, 218, 222, 267, 268, 285, 290, 342, 344, 349, 351, 352, 354, 362, 371], "materialized_d": [268, 283, 290], "materializeddataset": [268, 290, 291, 295], "math": [2, 175, 180, 336, 338, 355, 357, 358], "mathbb": [329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363], "mathcal": [329, 330, 336, 337, 342, 343, 362, 363], "mathemat": [118, 124], "matmul": [3, 181, 183, 342, 348], "matplotlib": [5, 7, 10, 13, 14, 16, 128, 129, 137, 138, 147, 148, 206, 207, 224, 226, 253, 254, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "matric": [362, 371], "matrix": [137, 146, 346, 348], "matrixfactorizationmodel": [342, 345, 346, 348], "matt": [268, 289], "matter": [101, 102, 106, 268, 289, 291, 294, 362, 365], "matur": [8, 92, 93, 191, 196], "max": [9, 10, 15, 16, 86, 87, 110, 113, 198, 204, 206, 212, 215, 291, 298, 329, 333, 336, 340, 342, 346, 355, 357, 359, 362, 367], "max_": 15, "max_depth": [4, 12, 171, 174, 349, 352, 354], "max_epoch": [6, 259, 262, 263, 329, 333, 335, 336, 340], "max_failur": [245, 248, 252, 329, 333, 336, 340, 342, 346, 349, 352, 355, 359, 362, 363, 368], "max_len": [355, 358], "max_length": [299, 304], "max_lora": [118, 121], "max_lora_rank": [118, 121], "max_model_len": [101, 102, 108, 113, 118, 121, 122, 123], "max_nod": [43, 50, 53, 62, 159, 163], "max_num_adapters_per_replica": [118, 121], "max_ongoing_request": [8, 191, 197], "max_replica": [16, 101, 102, 108, 110, 113, 116, 118, 123], "max_retri": [3, 181, 185], "max_siz": [10, 206, 213], "max_step": [6, 259, 263], "max_t": [329, 332, 336, 339], "maxim": [8, 9, 101, 102, 104, 106, 107, 191, 192, 197, 198, 204, 267, 268, 285], "maximum": [16, 43, 50, 53, 62, 80, 81, 84, 85, 101, 102, 104, 105, 110, 112, 116, 117, 118, 121, 124, 245, 250], "maxpool": 13, "maxpool2d": 13, "maxpumperla": [0, 299, 306], "mayb": [291, 297], "mb": [355, 357], "mcintir": [291, 297], "mcm": [137, 146], "md": [0, 164, 167, 284, 292, 300, 324], "mdmad": [268, 289], "me": [17, 22, 110, 114, 115, 268, 289, 291, 294, 297, 298], "mean": [1, 3, 7, 9, 10, 14, 15, 66, 69, 101, 102, 105, 128, 130, 169, 170, 181, 183, 187, 198, 201, 202, 204, 206, 211, 215, 224, 231, 237, 253, 257, 329, 330, 336, 337, 342, 343, 348, 355, 357, 361, 362, 365, 366, 371], "meant": [8, 191, 192, 194, 342, 348], "meantim": [291, 297], "measur": [342, 348], "meat": [291, 294], "mechan": [8, 10, 155, 157, 191, 192, 206, 208, 362, 363, 367], "medium": [101, 102, 106, 109, 114, 117, 118, 124], "meet": [1, 10, 11, 16, 101, 102, 105, 126, 127, 169, 170, 206, 208, 218, 220, 268, 289], "megatron": [137, 144], "melodrama": [291, 298], "member": [94, 95], "memori": [2, 3, 5, 6, 9, 10, 12, 14, 15, 88, 89, 104, 105, 107, 109, 110, 112, 114, 116, 118, 120, 121, 124, 128, 130, 131, 134, 137, 139, 155, 157, 159, 161, 163, 175, 177, 181, 183, 187, 189, 195, 196, 198, 200, 201, 206, 208, 210, 211, 212, 214, 215, 224, 226, 232, 237, 238, 243, 259, 260, 262, 267, 280, 285, 289, 291, 293, 298, 329, 331, 335, 342, 344, 351, 355, 361, 362, 364, 365, 371], "memory_usag": [9, 198, 201], "memorydb": 21, "men": [291, 294, 297], "mental": [291, 294], "mention": [291, 298, 355, 361], "menu": [128, 136, 137, 146, 147, 154], "merg": [9, 198, 205, 342, 348], "merlin": [268, 289], "messag": [1, 3, 8, 13, 14, 16, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 128, 133, 137, 139, 147, 150, 159, 161, 168, 169, 170, 181, 190, 191, 195, 329, 333, 336, 340], "messages_cv": [118, 121], "messages_nemoguard": [118, 121], "messages_yara": [118, 121], "messi": [268, 287, 289], "meta": [101, 102, 108, 110, 112, 113, 116, 118, 121, 342, 346, 355, 359, 362, 367], "meta_path": [362, 367], "metadata": [8, 9, 88, 89, 128, 132, 159, 163, 191, 197, 198, 205, 245, 248, 268, 274, 287, 290, 291, 293, 295, 297, 336, 340, 342, 346, 348, 349, 354, 362, 365], "metadataprint": [272, 273], "method": [3, 5, 6, 7, 9, 10, 11, 14, 15, 16, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 128, 131, 155, 158, 181, 183, 190, 198, 202, 206, 211, 213, 214, 215, 218, 222, 224, 225, 253, 257, 259, 261, 262, 267, 268, 277, 285, 288, 291, 298, 299, 304, 329, 335, 342, 343, 348], "method_nam": [3, 181, 190], "metric": [4, 6, 7, 12, 14, 16, 17, 22, 82, 83, 92, 93, 110, 113, 116, 118, 125, 137, 143, 144, 146, 147, 150, 152, 155, 157, 158, 159, 161, 162, 163, 165, 171, 174, 225, 226, 227, 228, 235, 238, 240, 244, 245, 247, 248, 250, 252, 253, 256, 257, 259, 261, 263, 301, 305, 306, 329, 333, 334, 335, 336, 337, 340, 341, 343, 344, 348, 349, 351, 352, 354, 355, 356, 359, 360, 361, 362, 363, 367, 368, 369, 370, 371, 372], "metrics_d": [137, 146], "metrics_datafram": [5, 6, 13, 224, 236, 259, 263, 329, 333, 336, 340, 342, 346, 355, 359, 362, 369], "metrics_interval_": 16, "mf_ray_train": [342, 346], "miami": [268, 289], "mic": [268, 287, 289], "michael": [268, 289, 291, 298], "micro": [362, 367], "microservic": [96, 98, 101, 102, 106, 147, 151, 168], "mid": [245, 250, 349, 350, 362, 370], "mid_block_scale_factor": [6, 259, 262], "middl": [268, 289], "midwai": [268, 289], "might": [4, 5, 6, 7, 10, 12, 24, 26, 28, 30, 43, 45, 53, 56, 66, 69, 101, 102, 105, 171, 172, 174, 206, 212, 224, 225, 253, 257, 259, 261, 291, 294, 329, 335, 349, 352, 354, 362, 371], "migrat": [11, 16, 218, 222, 224, 227, 262, 336, 337], "milan": [268, 289], "mile": [4, 8, 9, 12, 15, 137, 139, 171, 174, 191, 195, 196, 198, 201], "million": [4, 12, 171, 174, 268, 287, 289, 291, 298], "min": [4, 7, 9, 10, 12, 14, 15, 16, 110, 113, 137, 143, 171, 174, 198, 204, 206, 211, 212, 215, 253, 257, 349, 352, 359, 361, 362, 368], "min_": 15, "min_nod": [43, 50, 53, 62], "min_replica": [16, 101, 102, 108, 110, 113, 116, 118, 123], "min_siz": [10, 206, 213], "mind": [268, 289, 291, 294, 298], "mine": [268, 289], "minecraft": [268, 287, 289, 290], "miner": [291, 297], "mini": [5, 6, 224, 225, 259, 263, 329, 330, 336, 337], "miniconda": [1, 169, 170], "miniforge3": [1, 169, 170], "minilm": [268, 277, 288], "minim": [5, 6, 7, 8, 12, 14, 17, 20, 22, 24, 26, 128, 130, 132, 137, 143, 191, 192, 195, 224, 225, 253, 257, 259, 261, 262, 299, 305, 329, 332, 336, 337, 341, 342, 343, 349, 350, 362, 363, 371], "minimalist": 0, "minimum": [16, 17, 22, 43, 50, 53, 62, 80, 81, 84, 85], "minu": 12, "minut": [5, 9, 35, 39, 43, 45, 53, 56, 66, 70, 73, 80, 81, 84, 85, 198, 201, 268, 289, 355, 356, 357], "mirror": [355, 356, 362, 363], "mise": [291, 298], "miseenscen": [291, 298], "misfortun": [291, 297], "mismatch": [355, 359], "miss": [268, 289, 291, 297, 298, 329, 335, 336, 340], "missouri": [268, 289], "mistak": [291, 297], "mistral": [118, 124], "mitch": [268, 289], "mix": [6, 259, 262, 263, 329, 335, 336, 341, 362, 371], "mkdir": [5, 13, 137, 140], "ml": [1, 4, 8, 9, 11, 12, 15, 16, 84, 85, 86, 87, 90, 91, 94, 95, 100, 126, 127, 128, 130, 147, 149, 154, 169, 170, 171, 173, 174, 191, 194, 196, 197, 198, 205, 218, 219, 220, 221, 222, 267, 268, 283, 285, 290, 309, 312, 315, 316, 325, 326, 327, 342, 344, 348, 362, 363], "mlbcentral": [268, 289], "mlflow": [137, 142, 143, 144, 147, 148, 150, 245, 252, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "mlogloss": [349, 352], "mlop": [4, 12, 118, 121, 126, 127, 137, 138, 171, 173, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "mlp": [336, 339, 341, 342, 348], "mm": [3, 181, 187], "mmlu": [118, 124], "mnist": [5, 7, 10, 11, 14, 15, 16, 164, 167, 206, 210, 212, 213, 215, 218, 222, 225, 228, 231, 232, 234, 236, 237, 238, 242, 245, 251, 252, 253, 254, 255, 256, 257], "mnist_app": [11, 16, 164, 167, 218, 222, 223], "mnist_app_handl": [11, 218, 222], "mnist_classifi": [11, 16, 218, 222], "mnist_classifier_arg": [10, 206, 213], "mnist_deploy": 16, "mnist_deployment_handl": 16, "mnist_pr": [10, 15, 206, 216, 217], "mnist_preprocessor": 16, "mnistclassifi": [10, 11, 15, 16, 206, 213, 214, 218, 222], "mnt": [4, 5, 6, 9, 10, 11, 12, 13, 15, 16, 86, 87, 128, 133, 137, 139, 142, 144, 147, 150, 164, 166, 171, 174, 198, 203, 206, 213, 218, 222, 224, 226, 227, 232, 234, 236, 238, 242, 245, 251, 259, 262, 263, 264, 329, 331, 333, 335, 336, 340, 341, 342, 344, 346, 348, 349, 351, 352, 354, 355, 356, 357, 362, 363, 364, 365, 367, 368, 371], "mock": [291, 297], "modal": [9, 10, 15, 198, 205, 206, 217], "modano": [268, 289], "mode": [0, 4, 7, 11, 12, 14, 118, 122, 128, 130, 137, 143, 171, 174, 207, 218, 223, 224, 228, 237, 253, 257, 299, 304], "model": [3, 8, 9, 10, 11, 12, 15, 16, 84, 85, 86, 87, 90, 91, 104, 106, 107, 108, 111, 114, 115, 117, 119, 120, 121, 122, 123, 125, 126, 127, 128, 131, 135, 138, 139, 143, 144, 145, 146, 147, 148, 149, 150, 151, 181, 187, 190, 191, 192, 193, 196, 197, 198, 202, 205, 206, 208, 213, 215, 217, 218, 220, 221, 222, 223, 225, 226, 228, 234, 235, 236, 237, 238, 240, 245, 246, 247, 248, 250, 252, 254, 256, 260, 264, 267, 277, 283, 284, 285, 288, 290, 291, 292, 298, 299, 300, 301, 302, 303, 304, 305, 306, 323, 324, 326, 328, 329, 330, 332, 333, 335, 336, 337, 338, 340, 341, 344, 346, 347, 348, 350, 351, 353, 354, 356, 357, 359, 361, 364, 367, 371], "model1": [4, 171, 174], "model1_predict": [4, 171, 174], "model2": [4, 171, 174], "model2_predict": [4, 171, 174], "model_config": [6, 259, 262], "model_dump": [4, 118, 123, 171, 174], "model_id": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123, 128, 131, 136, 137, 139, 147, 149, 150], "model_json_schema": [118, 122], "model_kwarg": [355, 361], "model_loading_config": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123], "model_nam": [6, 84, 85, 259, 262, 263, 268, 275, 276, 277, 288], "model_path": [4, 5, 11, 13, 171, 174, 218, 222, 224, 237, 349, 352], "model_predict": [4, 171, 174], "model_ref": [128, 131], "model_registri": [137, 142, 143, 144, 147, 150], "model_select": [4, 171, 172, 349, 351], "model_sourc": [101, 102, 108, 110, 113, 116, 118, 121, 122, 123], "model_state_dict": [245, 247], "modelcheckpoint": [329, 333, 335, 336, 340], "modelclass": [275, 276], "modelwork": [224, 237], "moder": [101, 102, 105, 118, 121, 124], "modern": [8, 128, 130, 191, 192, 195, 267, 268, 283, 285, 290, 291, 293, 299, 301, 306], "modif": [92, 93], "modifi": [3, 4, 5, 9, 10, 11, 28, 30, 35, 39, 43, 45, 53, 56, 66, 71, 86, 87, 88, 89, 92, 93, 164, 166, 167, 171, 174, 181, 189, 198, 203, 206, 213, 218, 222, 246, 329, 330, 362, 363], "modul": [5, 13, 17, 21, 22, 35, 39, 53, 57, 101, 102, 103, 108, 109, 110, 115, 117, 118, 120, 125, 137, 140, 143, 164, 167, 224, 231, 234, 238, 239, 246, 247, 248, 342, 345, 348, 355, 358, 361, 362, 363, 371], "modular": [17, 21, 22, 342, 343], "mofo": [268, 289], "mom": [268, 289], "momentum": [13, 137, 140, 299, 304], "mondai": [268, 289], "monei": [291, 294, 297], "mongodb": [8, 191, 192], "monitor": [5, 6, 8, 13, 24, 25, 82, 83, 90, 91, 92, 93, 100, 101, 111, 113, 115, 117, 118, 124, 125, 137, 144, 147, 153, 155, 157, 158, 159, 161, 162, 163, 164, 165, 166, 191, 195, 224, 225, 233, 234, 259, 261, 349, 354, 355, 361], "monro": [268, 289], "month": [4, 9, 118, 123, 171, 174, 198, 201, 268, 289], "moon": [268, 289], "more": [2, 3, 7, 8, 9, 10, 11, 13, 14, 16, 17, 22, 26, 28, 30, 43, 44, 45, 53, 54, 56, 66, 67, 88, 89, 96, 98, 101, 102, 105, 107, 108, 112, 117, 120, 126, 127, 128, 132, 135, 137, 139, 144, 147, 151, 155, 158, 159, 161, 163, 164, 166, 175, 180, 181, 184, 185, 187, 188, 189, 191, 194, 196, 197, 198, 204, 205, 206, 208, 212, 215, 217, 218, 220, 221, 223, 224, 225, 226, 230, 234, 236, 245, 252, 253, 256, 257, 261, 264, 284, 291, 292, 297, 298, 299, 300, 304, 306, 309, 315, 316, 323, 324, 325, 327, 328, 329, 335, 342, 348, 349, 353, 354, 355, 359, 362, 371], "morn": [268, 289], "moron": [291, 297], "morri": [291, 297], "morti": [268, 289], "mosh": [268, 289], "most": [3, 8, 9, 10, 15, 24, 26, 80, 81, 84, 85, 86, 87, 96, 98, 118, 121, 122, 181, 184, 190, 191, 194, 198, 202, 206, 209, 211, 224, 236, 245, 250, 291, 297, 298, 316, 320, 328, 329, 333, 336, 340, 342, 344, 347, 349, 353, 362, 364, 367, 369], "most_rec": [86, 87], "mostli": [101, 102, 106], "motion": [336, 337], "motiv": [291, 294], "mount": [17, 22, 86, 87], "mountaincar": [336, 341], "mouth": [291, 297], "move": [5, 6, 7, 8, 13, 14, 100, 101, 102, 109, 137, 139, 191, 192, 224, 227, 228, 231, 232, 234, 235, 237, 238, 240, 241, 253, 256, 257, 259, 262, 267, 268, 285, 291, 298, 299, 304, 329, 335, 336, 337, 341, 349, 351, 355, 357, 362, 363], "movement": [6, 259, 262], "movi": [268, 289, 291, 294, 297, 298, 343, 344], "movielen": [343, 348], "mp": [268, 275, 276, 277, 283, 288, 289, 290, 299, 301, 304, 305], "mse": [329, 330, 333, 336, 340, 342, 343, 346], "mse_loss": [6, 259, 262, 342, 346], "mseloss": [329, 332, 336, 339], "mta": [268, 289], "mtv": [291, 298], "mtvstar": [268, 289], "mtvstarsof2015": [268, 289], "much": [8, 9, 101, 102, 106, 118, 121, 128, 130, 137, 143, 159, 163, 191, 196, 198, 201, 203, 268, 289, 291, 294, 297, 298, 329, 335, 342, 343, 349, 351], "muck": [291, 298], "muddi": [291, 297], "multi": [9, 10, 11, 15, 16, 17, 19, 84, 85, 101, 102, 105, 106, 107, 110, 112, 116, 117, 118, 121, 124, 125, 137, 144, 147, 151, 198, 203, 205, 206, 217, 218, 220, 224, 225, 226, 235, 238, 239, 244, 245, 252, 329, 330, 333, 335, 336, 337, 342, 343, 344, 348, 349, 350, 352, 362, 363, 371], "multi_actor_tracing_ray_serve_exampl": 168, "multiclass": 13, "multiclassaccuraci": [362, 367], "multilabel_confusion_matrix": [137, 146], "multimod": [126, 127, 329, 330], "multipl": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 22, 24, 26, 79, 84, 85, 96, 98, 101, 102, 105, 106, 107, 110, 111, 112, 113, 116, 117, 118, 120, 121, 124, 125, 137, 144, 147, 149, 153, 159, 161, 164, 167, 168, 171, 173, 175, 180, 181, 183, 187, 188, 191, 192, 195, 198, 203, 205, 206, 212, 213, 218, 220, 221, 224, 225, 232, 234, 238, 239, 242, 253, 257, 259, 261, 263, 267, 268, 277, 285, 288, 291, 293, 296, 299, 301, 304, 305, 306, 309, 316, 325, 329, 330, 331, 335, 336, 337, 341, 342, 344, 346, 348, 349, 350, 354, 355, 359, 361, 362, 364, 369, 371, 372], "multiplex": [10, 11, 16, 206, 208, 218, 220], "multipli": [3, 181, 190, 355, 361], "multiprocess": [3, 181, 187, 362, 365], "multithread": [3, 10, 181, 187, 206, 212], "multivari": [355, 361], "mum": [268, 287, 289], "muslim": [268, 289], "must": [118, 121, 245, 250, 329, 330, 336, 337], "mutabl": [137, 139], "mutat": [3, 181, 190, 268, 290], "mutual": [101, 102, 106], "my": [0, 3, 17, 22, 35, 38, 39, 82, 83, 101, 102, 108, 110, 113, 114, 115, 116, 118, 121, 122, 123, 168, 181, 186, 268, 287, 289, 291, 294, 298], "my_custom_env": [3, 181, 186], "my_simple_model": [7, 14, 253, 257], "my_xgboost_func": [4, 171, 174], "myself": [268, 289, 291, 294, 298], "mysentimentmodel": [315, 316, 320, 327], "mysql": [8, 191, 192], "n": [1, 3, 5, 24, 26, 53, 63, 86, 87, 110, 115, 118, 121, 123, 128, 136, 169, 170, 181, 187, 189, 224, 228, 268, 283, 284, 287, 289, 290, 291, 292, 298, 300, 324, 329, 330, 332, 333, 336, 337, 341, 343, 349, 353, 354, 362, 371], "n_step": [336, 338, 341], "nab": [355, 357], "naiv": [10, 11, 206, 208, 218, 220, 329, 335], "naiveti": [291, 297, 298], "nake": [291, 297], "nam": [53, 57], "name": [1, 5, 6, 7, 8, 11, 12, 13, 14, 16, 17, 21, 22, 23, 28, 30, 31, 32, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 53, 56, 57, 58, 59, 62, 66, 69, 70, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 101, 102, 108, 110, 115, 118, 121, 122, 123, 137, 144, 147, 153, 159, 163, 164, 166, 167, 168, 169, 170, 191, 197, 218, 222, 223, 224, 228, 234, 235, 237, 238, 244, 245, 248, 250, 253, 257, 259, 263, 291, 294, 297, 329, 331, 333, 335, 336, 340, 341, 342, 344, 346, 348, 349, 352, 355, 359, 362, 364, 368, 371], "namespac": [43, 45, 46, 48, 49, 50, 51, 53, 56, 58, 60, 61, 62, 64, 66, 73, 76, 78, 224, 234], "nandito": [268, 289], "narrat": [291, 298], "naruto": [268, 289], "nash": [268, 289], "nashnewvideo": [268, 289], "nat": [17, 22, 24, 26, 28, 30, 43, 45, 53, 56], "natgatewai": [28, 30, 43, 45, 53, 56], "nation": [268, 289, 291, 297, 316, 317, 319, 320, 328], "nativ": [2, 4, 8, 24, 25, 27, 100, 101, 102, 106, 107, 159, 162, 163, 171, 173, 175, 179, 191, 192, 194, 195, 196, 291, 297, 298, 329, 333, 335, 336, 337, 340, 341, 355, 356, 362, 363], "nativesbr": [291, 298], "natur": [118, 123, 316, 317, 319, 320, 328], "navig": [82, 83, 90, 91, 92, 93, 94, 95, 155, 158, 164, 166, 168], "nbsp": [80, 81, 86, 87, 126, 127, 128, 129, 137, 138, 147, 148], "nc": [291, 294], "nccl": [299, 305, 362, 363], "ndarrai": [7, 10, 11, 14, 15, 16, 206, 212, 213, 215, 218, 222, 253, 257, 268, 275, 276, 277, 288, 290, 355, 361], "ndcg": [342, 348], "ndim": [224, 237], "nearli": [126, 127], "necessari": [5, 9, 13, 16, 28, 29, 30, 34, 35, 38, 39, 43, 44, 45, 52, 53, 54, 56, 63, 65, 66, 67, 69, 198, 202, 224, 232, 268, 274, 287, 299, 301, 302, 313, 314, 315, 316, 327, 349, 351, 362, 363], "need": [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 27, 28, 29, 30, 31, 35, 36, 37, 39, 40, 42, 43, 44, 45, 47, 53, 55, 56, 57, 59, 63, 66, 68, 70, 71, 75, 78, 79, 80, 81, 82, 83, 84, 85, 92, 93, 101, 102, 106, 110, 111, 112, 113, 115, 117, 118, 119, 121, 122, 123, 124, 128, 129, 130, 137, 138, 139, 143, 147, 148, 155, 158, 159, 163, 169, 170, 175, 180, 181, 183, 184, 186, 191, 192, 195, 198, 201, 203, 205, 206, 208, 213, 216, 218, 220, 222, 224, 225, 226, 227, 230, 232, 234, 238, 239, 240, 243, 245, 251, 252, 253, 257, 259, 261, 263, 268, 280, 289, 291, 297, 298, 299, 305, 309, 316, 325, 329, 331, 335, 336, 337, 342, 344, 346, 348, 349, 351, 352, 353, 362, 363, 364, 365, 366, 367, 369], "neg": [8, 137, 146, 191, 196, 291, 294, 316, 317, 319, 320, 328], "nemoguard": [118, 121], "nephew": [268, 289], "nest": [182, 184], "net": [6, 137, 140, 259, 262, 329, 332, 336, 339, 362, 367], "netflix": [9, 10, 15, 198, 205, 206, 217], "network": [5, 6, 7, 9, 10, 14, 15, 17, 20, 22, 24, 26, 27, 53, 63, 66, 78, 79, 86, 87, 101, 102, 106, 147, 151, 155, 157, 159, 161, 163, 198, 204, 206, 208, 215, 224, 225, 253, 256, 259, 261, 329, 330, 332, 362, 363], "networkinterfaceid": [28, 30, 43, 45, 53, 56], "neural": [7, 14, 137, 140, 253, 256, 342, 348, 355, 358, 362, 363], "never": [0, 147, 151, 268, 289, 291, 297, 298], "new": [3, 4, 8, 9, 13, 16, 24, 27, 28, 30, 46, 50, 53, 58, 62, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 101, 102, 105, 110, 115, 118, 123, 128, 136, 137, 143, 171, 174, 181, 189, 190, 191, 193, 198, 201, 245, 250, 268, 284, 289, 291, 292, 294, 297, 298, 300, 324, 333, 336, 341, 349, 350, 354, 355, 356, 362, 370, 372], "newaxi": [349, 353], "newli": [66, 72], "newsha": [268, 289], "next": [3, 7, 9, 13, 14, 16, 17, 20, 35, 37, 43, 47, 53, 59, 66, 68, 75, 80, 81, 84, 85, 90, 91, 92, 93, 104, 119, 121, 124, 128, 132, 157, 164, 166, 181, 189, 198, 203, 224, 226, 231, 253, 257, 268, 289, 331, 344, 352, 356, 357, 364, 367], "nf": [86, 87], "nfl": [268, 287, 289], "nginx": [24, 27, 51, 52, 64, 65, 78], "nhead": [355, 358, 359, 361], "nhl": [268, 287, 289, 290], "nia": [268, 289], "niall": [268, 289], "nice": [245, 252], "nicer": [8, 191, 196], "nick": [268, 289], "nicki": [268, 289], "nigga": [268, 289], "night": [268, 287, 289, 290, 291, 297, 298], "nightli": [336, 341, 355, 361], "nightmar": [291, 297, 298], "nightmarish": [291, 298], "nine": [329, 331, 362, 364], "nirvana": [268, 289], "nlb": [24, 27], "nlp": [245, 252], "nn": [5, 6, 7, 13, 14, 137, 140, 143, 224, 226, 227, 231, 234, 245, 248, 253, 254, 256, 257, 259, 260, 299, 302, 329, 331, 332, 336, 338, 339, 342, 344, 345, 355, 357, 358, 359, 362, 364, 367], "no_grad": [5, 10, 11, 13, 15, 16, 206, 213, 218, 222, 329, 335, 336, 341, 342, 346, 348, 355, 359, 362, 367], "no_restart": [224, 237], "node": [1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 14, 16, 17, 20, 21, 22, 24, 26, 27, 28, 30, 35, 36, 39, 43, 45, 46, 50, 56, 58, 62, 63, 66, 70, 72, 80, 81, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 105, 106, 110, 112, 113, 115, 116, 117, 118, 124, 126, 127, 128, 129, 132, 133, 136, 137, 138, 139, 143, 144, 146, 147, 148, 151, 153, 154, 157, 159, 161, 162, 163, 169, 170, 175, 177, 181, 183, 185, 187, 198, 201, 203, 206, 212, 218, 220, 224, 225, 226, 228, 234, 235, 238, 239, 243, 245, 248, 249, 252, 253, 256, 259, 261, 268, 280, 289, 291, 293, 295, 329, 330, 335, 336, 337, 342, 343, 348, 349, 350, 351, 352, 357, 360, 361, 362, 363, 367, 371], "node_ip": 13, "nodegroup": [53, 57], "noderol": [53, 57], "nofril": [291, 298], "noir": [291, 297, 298], "noirlik": [291, 298], "nois": [6, 259, 262, 332, 335, 336, 337, 338, 339, 341, 355, 357], "noise_schedul": [6, 259, 262], "noised_lat": [6, 259, 262], "noiser": [329, 335], "noisi": [329, 332, 336, 337, 339], "noisy_act": [336, 338, 339], "noisy_img": [329, 332], "non": [5, 11, 101, 102, 106, 107, 108, 110, 114, 116, 118, 121, 122, 123, 128, 130, 164, 167, 218, 223, 224, 225, 226, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364, 367], "non_block": [224, 237], "none": [5, 6, 7, 10, 13, 14, 128, 129, 137, 138, 139, 147, 148, 206, 213, 224, 233, 234, 237, 245, 248, 253, 257, 259, 262, 299, 306, 329, 333, 335, 336, 340, 341, 342, 346, 348, 349, 352, 355, 358, 359, 362, 365, 371], "norm": [224, 237, 355, 357, 361], "norm_ep": [6, 259, 262], "norm_num_group": [6, 259, 262], "normal": [5, 7, 10, 13, 14, 15, 16, 206, 207, 212, 214, 224, 226, 232, 237, 238, 239, 243, 253, 254, 255, 257, 329, 330, 335, 337, 339, 341, 342, 343, 348, 349, 351, 353, 356, 361, 362, 363, 365, 371], "normalci": [291, 298], "normalis": [355, 357, 362, 365], "normalize_cpu": [224, 237], "normalized_batch": [10, 15, 16, 206, 212], "normalized_img": 5, "north": [268, 289, 291, 297], "not_ready_ref": [3, 181, 189], "note": [1, 7, 9, 11, 13, 14, 15, 16, 17, 20, 24, 27, 28, 30, 35, 39, 43, 45, 47, 53, 56, 59, 66, 70, 75, 101, 102, 107, 126, 127, 128, 129, 130, 133, 135, 136, 137, 138, 145, 146, 147, 148, 153, 154, 155, 157, 158, 164, 167, 169, 170, 176, 182, 184, 185, 198, 202, 203, 204, 211, 212, 215, 218, 222, 223, 226, 238, 239, 241, 243, 253, 257, 262, 268, 289, 290, 291, 296, 297, 298, 316, 320, 328, 342, 348, 349, 351, 362, 363, 367], "notebook": [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 28, 29, 35, 36, 43, 44, 53, 54, 80, 81, 84, 85, 86, 87, 96, 97, 100, 101, 102, 103, 110, 111, 118, 119, 128, 129, 130, 136, 137, 138, 139, 146, 147, 148, 154, 155, 156, 159, 160, 164, 165, 171, 172, 175, 176, 181, 182, 184, 198, 199, 206, 207, 218, 219, 224, 225, 226, 253, 254, 259, 260, 267, 268, 284, 285, 290, 292, 298, 299, 300, 301, 306, 309, 316, 320, 323, 324, 325, 328, 329, 330, 336, 337, 341, 342, 343, 344, 349, 354, 355, 356, 362, 363, 364, 371, 372], "noth": [137, 140, 224, 237, 268, 289, 291, 298], "notic": [5, 9, 137, 140, 143, 198, 203, 224, 228, 268, 289, 291, 298], "notif": [164, 167, 168], "notificationservic": 168, "nov": [13, 268, 289], "novelti": [342, 348], "now": [1, 3, 4, 5, 6, 10, 11, 13, 15, 16, 17, 22, 28, 30, 34, 35, 39, 43, 45, 51, 52, 53, 63, 64, 65, 66, 70, 82, 83, 88, 89, 90, 91, 94, 95, 101, 102, 108, 110, 114, 115, 116, 117, 118, 120, 121, 122, 123, 125, 128, 129, 131, 137, 138, 147, 148, 164, 166, 167, 169, 170, 171, 174, 181, 184, 206, 213, 218, 222, 224, 227, 232, 234, 235, 237, 238, 240, 241, 242, 243, 244, 245, 247, 248, 259, 263, 268, 289, 291, 297, 298, 329, 330, 331, 333, 335, 336, 341, 342, 344, 346, 348, 349, 351, 354, 362, 371], "nowher": [291, 294], "np": [2, 3, 5, 6, 7, 10, 11, 14, 15, 16, 128, 131, 136, 137, 139, 143, 147, 148, 149, 159, 163, 164, 167, 175, 176, 181, 182, 183, 187, 189, 206, 207, 212, 213, 215, 218, 219, 222, 224, 226, 237, 238, 243, 253, 254, 257, 259, 260, 262, 268, 271, 275, 276, 277, 286, 288, 289, 299, 302, 303, 329, 331, 336, 338, 341, 342, 344, 349, 351, 352, 353, 355, 357, 361, 362, 364, 371], "nthread": [349, 352], "ntop": [342, 348], "nude": [291, 294], "nuditi": [291, 294], "nuge": [268, 289], "nugent": [268, 289], "null": [28, 30, 43, 45, 53, 56], "num": [13, 155, 158], "num_actor": [362, 371], "num_block": [268, 290, 291, 295], "num_boost_round": [4, 171, 174, 349, 352], "num_class": [5, 13, 137, 140, 143, 144, 224, 227, 349, 352, 362, 367, 371], "num_cpu": [3, 9, 10, 128, 130, 181, 187, 198, 204, 206, 212, 329, 331, 349, 353, 354], "num_decoder_lay": [355, 358], "num_encoder_lay": [355, 358], "num_epoch": [5, 7, 13, 14, 137, 143, 144, 224, 228, 229, 235, 238, 240, 244, 245, 247, 248, 250, 253, 256, 257], "num_gpu": [3, 7, 10, 15, 16, 128, 130, 131, 137, 139, 146, 147, 149, 181, 187, 206, 212, 213, 224, 237, 253, 256, 268, 280, 289, 355, 361, 362, 371], "num_imag": [159, 163], "num_item": [342, 344, 345, 346, 348], "num_label": [299, 304], "num_lay": [355, 358, 359, 361], "num_parquet_shard": [342, 344], "num_partit": [291, 297], "num_replica": [16, 147, 149, 151, 153, 313, 314, 315, 316, 323, 327, 328], "num_return": [3, 181, 189], "num_row": [268, 287, 290, 291, 294, 295, 296, 362, 365], "num_row_group": [362, 365], "num_sampl": [4, 7, 12, 14, 171, 174, 253, 257], "num_to_keep": [329, 333, 336, 340, 342, 346, 349, 352, 355, 359, 362, 368], "num_training_step": [6, 259, 262], "num_us": [342, 344, 345, 346, 348], "num_warmup_step": [6, 259, 262, 263], "num_work": [4, 5, 6, 12, 13, 137, 143, 171, 174, 224, 225, 230, 245, 252, 259, 262, 263, 299, 305, 306, 329, 333, 336, 340, 342, 346, 349, 352, 355, 357, 359, 362, 363, 365, 366, 368, 371], "number": [3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 24, 27, 80, 81, 84, 85, 88, 89, 110, 113, 116, 118, 122, 137, 144, 159, 161, 164, 166, 167, 171, 174, 181, 187, 189, 198, 201, 203, 206, 210, 212, 213, 224, 227, 228, 229, 235, 238, 244, 245, 250, 253, 255, 257, 259, 263, 267, 268, 277, 285, 288, 291, 294, 296, 299, 301, 304, 305, 306, 315, 316, 323, 327, 328, 336, 338, 342, 344, 346, 362, 371], "numenta": [355, 357], "numer": [9, 118, 121, 198, 201, 224, 226, 329, 335, 349, 350, 362, 364], "numpi": [2, 3, 5, 6, 7, 9, 10, 11, 14, 15, 16, 128, 131, 136, 137, 139, 140, 143, 147, 148, 149, 159, 163, 164, 167, 175, 176, 181, 182, 198, 202, 206, 207, 210, 213, 218, 219, 222, 224, 226, 237, 238, 243, 253, 254, 259, 260, 268, 271, 286, 290, 299, 302, 329, 331, 335, 336, 338, 342, 344, 349, 351, 352, 355, 357, 361, 362, 364, 371], "nuremburg": [268, 289], "nutshel": 12, "nvdp": [43, 46, 51, 53, 58, 64], "nvidia": [24, 27, 51, 52, 64, 65, 118, 121, 268, 280, 289], "nvlink": [110, 116], "nvme": [5, 224, 225, 226], "nyc": [4, 9, 164, 166, 171, 174, 198, 201, 204, 361], "nyc_taxi": [355, 357], "nyc_taxi_2021": 12, "nyc_taxi_t": [355, 357], "nyc_taxi_transform": [355, 359], "o": [2, 3, 5, 6, 11, 13, 84, 85, 86, 87, 101, 110, 113, 116, 118, 121, 123, 128, 129, 133, 137, 138, 139, 142, 143, 146, 147, 148, 155, 157, 159, 163, 175, 176, 181, 182, 186, 187, 218, 223, 224, 226, 234, 237, 245, 247, 248, 251, 259, 260, 262, 263, 268, 284, 287, 289, 292, 299, 300, 302, 324, 329, 331, 333, 335, 336, 338, 340, 341, 342, 344, 346, 348, 349, 351, 352, 354, 355, 357, 359, 361, 362, 364, 367, 369, 371], "ob": [336, 338, 339, 341], "obj": [86, 87, 118, 121], "obj_ref": [3, 181, 183], "object": [2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 22, 35, 42, 66, 78, 88, 89, 92, 93, 101, 102, 105, 110, 113, 118, 121, 123, 128, 130, 131, 137, 144, 171, 174, 175, 179, 180, 184, 190, 191, 196, 197, 198, 201, 206, 214, 215, 218, 220, 222, 224, 234, 235, 236, 238, 243, 245, 250, 252, 253, 256, 257, 267, 268, 283, 285, 289, 290, 344, 348, 349, 352, 354, 362, 364, 369], "object_ref": [3, 181, 189], "objectref": [2, 3, 175, 179, 181, 183, 184, 189], "oblig": [291, 298], "oblivi": [291, 298], "obs_dim": [336, 339, 341], "obs_sampl": [336, 341], "observ": [5, 6, 14, 92, 93, 101, 118, 125, 126, 127, 128, 134, 135, 137, 138, 163, 224, 225, 259, 261, 299, 306, 336, 337, 338, 339, 349, 351, 355, 358, 362, 365], "observed_data": [164, 166], "obtain": [86, 87, 299, 304], "obtus": [291, 294], "obviou": [291, 294], "occupi": [245, 251], "occur": [8, 159, 163, 191, 195, 238, 243, 245, 249, 342, 344], "ocean": [268, 289], "oct": [268, 287, 289], "octob": [268, 289], "off": [0, 5, 7, 10, 13, 14, 110, 116, 118, 121, 124, 128, 129, 137, 138, 147, 148, 206, 211, 224, 226, 237, 253, 255, 268, 287, 289, 291, 298, 329, 331, 335, 342, 346, 347, 349, 353, 354, 355, 360, 362, 364, 365, 368, 371], "offenc": [268, 289], "offend": [268, 289], "offer": [4, 8, 11, 24, 25, 26, 90, 91, 96, 99, 110, 112, 116, 128, 133, 134, 137, 144, 147, 153, 171, 173, 191, 192, 194, 195, 196, 218, 220, 221, 291, 294, 298, 329, 331], "offici": [17, 22, 35, 39, 82, 83, 84, 85, 126, 127, 155, 158, 164, 167], "offlin": [12, 88, 89, 126, 127, 336, 337, 338, 342, 344, 349, 350], "offlinemnistclassifi": 16, "offlinepredictor": [4, 12, 171, 174], "offload": [86, 87, 362, 363], "often": [8, 100, 137, 139, 191, 192, 195, 196, 224, 237, 342, 344, 349, 353], "oh": [268, 289], "olap": [8, 191, 192], "old": [268, 289, 291, 294, 298, 349, 354, 355, 361, 362, 371], "older": [164, 167, 329, 335], "oltp": [8, 191, 192], "olympics2012": [268, 289], "omp_num_thread": [3, 181, 187], "on_demand": [159, 163], "on_epoch": [329, 332, 336, 339], "on_fit_start": [6, 259, 262], "on_step": [6, 259, 262], "onc": [2, 10, 12, 15, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 84, 85, 86, 87, 90, 91, 101, 102, 105, 108, 110, 114, 115, 118, 121, 128, 130, 131, 137, 138, 147, 153, 175, 180, 182, 184, 188, 190, 206, 212, 213, 224, 225, 226, 234, 237, 267, 268, 277, 285, 288, 299, 304, 329, 333, 342, 343, 349, 353, 355, 356, 361, 362, 363, 365, 371], "one": [2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 16, 17, 22, 24, 26, 27, 28, 30, 43, 45, 53, 56, 84, 85, 86, 87, 88, 89, 96, 98, 101, 102, 104, 105, 106, 107, 118, 120, 121, 124, 137, 143, 144, 147, 151, 155, 158, 164, 166, 167, 171, 174, 175, 180, 181, 185, 187, 188, 189, 191, 193, 197, 198, 203, 205, 206, 208, 218, 220, 221, 224, 225, 227, 230, 232, 236, 238, 239, 245, 252, 259, 261, 267, 268, 285, 289, 291, 294, 297, 298, 329, 333, 349, 351, 352, 356, 359, 360, 362, 363, 365, 367, 368], "one_hot": [137, 143], "onehellofanighttour": [268, 289], "ones": [3, 8, 28, 30, 43, 45, 53, 56, 82, 83, 101, 102, 105, 110, 112, 181, 189, 191, 194, 268, 289, 309, 316, 325], "ongo": [16, 24, 25, 79], "onli": [2, 3, 5, 9, 10, 13, 15, 16, 17, 20, 24, 26, 28, 31, 35, 40, 43, 47, 53, 59, 66, 75, 84, 85, 86, 87, 88, 89, 94, 95, 96, 98, 100, 101, 102, 105, 107, 118, 121, 128, 130, 132, 137, 143, 144, 155, 158, 159, 162, 163, 166, 175, 180, 181, 190, 198, 201, 203, 206, 211, 212, 214, 215, 225, 227, 228, 233, 245, 248, 251, 252, 268, 289, 291, 293, 294, 296, 297, 298, 299, 304, 305, 329, 335, 336, 337, 341, 342, 344, 346, 348, 349, 350, 352, 355, 357, 359, 361, 362, 363, 365, 367, 369], "onlin": [8, 11, 12, 16, 88, 89, 126, 127, 128, 130, 151, 153, 191, 192, 218, 222, 238, 239, 268, 284, 289, 292, 300, 315, 324, 327, 355, 361, 362, 371], "onlinemnistclassifi": [11, 16, 218, 222], "onlinemnistpreprocessor": 16, "onlinepredictor": 12, "onto": [3, 7, 10, 14, 181, 187, 206, 211, 224, 237, 253, 256], "onu": [8, 191, 195], "oom": [5, 6, 10, 128, 130, 159, 163, 206, 212, 259, 260], "op": [126, 127, 362, 364], "open": [0, 1, 2, 5, 8, 13, 35, 38, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 100, 118, 123, 124, 128, 136, 137, 139, 140, 143, 144, 146, 155, 158, 159, 162, 164, 166, 168, 169, 170, 175, 177, 191, 192, 268, 289, 291, 297, 329, 331, 362, 364, 365, 371], "openai": [101, 102, 107, 108, 110, 113, 114, 115, 118, 121, 122, 123, 128, 131, 136, 137, 139, 147, 150], "openapi": 16, "opentelemetri": [155, 157, 168], "oper": [8, 12, 17, 19, 26, 35, 39, 44, 45, 46, 49, 51, 52, 54, 56, 58, 61, 63, 64, 65, 67, 70, 78, 79, 96, 98, 101, 102, 104, 106, 126, 127, 128, 130, 131, 137, 139, 144, 155, 157, 158, 164, 165, 166, 168, 191, 192, 193, 196, 199, 205, 207, 208, 211, 212, 268, 283, 290, 291, 293, 295, 296, 297, 299, 302, 329, 330, 331, 362, 363], "opinion": [17, 21, 291, 294], "oppos": [291, 298], "opt": [155, 158], "opt_path": [362, 367], "opt_state_path": [355, 359], "optim": [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 22, 24, 26, 103, 107, 109, 111, 117, 118, 124, 125, 126, 127, 128, 132, 137, 143, 144, 146, 147, 153, 191, 192, 196, 198, 202, 204, 205, 206, 212, 218, 220, 224, 226, 228, 238, 240, 245, 246, 247, 248, 250, 252, 253, 254, 256, 257, 259, 262, 267, 268, 277, 285, 288, 299, 304, 329, 332, 336, 339, 342, 346, 348, 355, 357, 359, 362, 363, 364, 367, 371], "optimizerlrschedul": [6, 259, 260, 262], "option": [3, 4, 5, 9, 10, 11, 12, 14, 15, 16, 20, 21, 26, 28, 30, 34, 35, 36, 39, 42, 45, 51, 52, 56, 64, 65, 78, 84, 85, 101, 102, 105, 108, 118, 121, 122, 128, 133, 159, 163, 164, 167, 171, 172, 173, 181, 185, 187, 198, 204, 206, 210, 213, 215, 218, 221, 223, 224, 226, 237, 238, 241, 267, 268, 280, 285, 289, 299, 306, 315, 316, 323, 327, 328, 329, 331, 333, 336, 340, 342, 346, 352, 354, 355, 358, 359, 361, 372], "optuna": [7, 14, 253, 254, 257], "optunasearch": [7, 14, 253, 257], "orang": [3, 181, 183], "orc": [8, 191, 192], "orchestr": [24, 25, 26, 106, 110, 113, 147, 154, 168, 192, 224, 225, 226, 230, 235, 329, 330, 335, 336, 337, 342, 343, 346, 348, 349, 354, 355, 356, 357, 359, 361, 362, 363, 364, 367], "order": [2, 7, 14, 80, 81, 128, 130, 137, 146, 159, 161, 175, 177, 253, 257, 268, 289, 291, 297, 349, 351, 352], "order_bi": [137, 144, 147, 150], "ordinari": [291, 294], "oregon": [268, 289], "org": [96, 98, 99, 342, 344, 372], "org_967t9ah1lbk1yqf1zau6a1v247": [86, 87], "org_xxxxxxx": [35, 39], "organ": [8, 9, 17, 19, 22, 35, 39, 79, 88, 89, 94, 95, 99, 100, 137, 139, 191, 192, 198, 201, 224, 234], "organiz": [24, 26, 100], "orient": 12, "origin": [5, 13, 53, 63, 224, 227, 268, 287, 289, 290, 291, 298, 329, 331, 342, 347, 348, 355, 357, 362, 363], "original_user_id": [342, 348], "oscar": [268, 289], "oss": [128, 132, 134, 137, 142, 144, 147, 153, 159, 162, 163], "ossci": [13, 14], "other": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 17, 22, 24, 25, 66, 78, 82, 83, 86, 87, 88, 89, 118, 120, 128, 129, 133, 137, 138, 142, 147, 148, 155, 157, 169, 170, 171, 173, 174, 175, 177, 181, 188, 190, 191, 192, 194, 196, 198, 201, 218, 221, 224, 225, 226, 229, 237, 238, 239, 253, 257, 259, 261, 263, 268, 289, 290, 291, 297, 298, 299, 301, 309, 316, 325, 329, 333, 335, 336, 341, 342, 344, 346, 355, 359, 362, 363], "otherwis": [5, 8, 118, 121, 191, 197, 342, 344], "otlp": 168, "our": [3, 5, 6, 7, 9, 10, 11, 14, 15, 16, 88, 89, 90, 91, 113, 114, 115, 118, 121, 122, 123, 128, 131, 164, 166, 168, 181, 188, 198, 201, 206, 210, 216, 218, 222, 224, 234, 253, 255, 256, 257, 259, 262, 263, 268, 289, 291, 298, 299, 303], "out": [3, 4, 5, 6, 8, 9, 10, 16, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 128, 134, 137, 143, 155, 157, 159, 163, 171, 174, 181, 185, 189, 191, 196, 198, 205, 206, 209, 224, 225, 254, 257, 259, 260, 267, 285, 287, 289, 291, 294, 297, 298, 329, 330, 331, 336, 338, 342, 344, 348, 349, 354, 355, 361, 362, 365, 371], "out_channel": [5, 6, 13, 224, 227, 259, 262], "out_featur": [13, 137, 140], "out_img_byt": [329, 331], "out_label": [329, 331], "out_proj": [355, 358], "out_ref": [3, 181, 184], "outbound": [17, 22], "outbr": [291, 298], "outdoor": [268, 289], "outhous": [268, 289], "outlier": [11, 218, 220], "outlook": 111, "output": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 28, 30, 31, 35, 39, 40, 42, 43, 45, 47, 53, 56, 57, 59, 66, 69, 70, 73, 75, 78, 88, 89, 92, 93, 101, 102, 104, 106, 109, 110, 113, 119, 120, 121, 123, 125, 128, 130, 147, 149, 164, 166, 167, 169, 170, 171, 174, 175, 180, 181, 185, 198, 200, 201, 203, 206, 210, 212, 214, 216, 218, 222, 224, 225, 226, 227, 228, 234, 236, 238, 240, 245, 247, 251, 253, 256, 257, 259, 262, 267, 268, 285, 299, 304, 329, 332, 335, 342, 344, 348, 355, 356, 358, 361, 372], "output_column": [291, 298], "output_csv": [342, 344], "output_dir": [329, 331, 362, 364], "output_path": [159, 163], "output_s": 13, "outsid": [5, 6, 13, 24, 27, 137, 139, 259, 263], "outstand": [8, 191, 197], "over": [3, 4, 7, 9, 10, 12, 14, 24, 26, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 128, 130, 171, 174, 181, 190, 198, 202, 205, 206, 208, 209, 224, 228, 238, 240, 245, 246, 253, 257, 268, 289, 291, 297, 298, 299, 304, 329, 335, 342, 346, 348, 349, 350, 354, 359, 361, 362, 363, 371], "overal": [15, 299, 301, 349, 352], "overcom": [268, 289], "overfit": [362, 369], "overhead": [2, 5, 6, 8, 82, 83, 100, 101, 102, 105, 110, 116, 175, 177, 191, 195, 224, 225, 259, 261, 336, 341], "overlap": [159, 162, 224, 225, 355, 356], "overload": [159, 163], "overr": [291, 298], "overrid": [28, 30, 35, 39, 43, 45, 53, 56, 92, 93, 224, 227], "overriden": [90, 91], "overse": 13, "oversubscrib": [3, 181, 187], "overview": [7, 8, 9, 10, 14, 15, 86, 87, 111, 117, 119, 137, 144, 156, 158, 159, 160, 164, 166, 172, 176, 191, 192, 198, 199, 206, 207, 219, 224, 225, 253, 256, 263, 372], "overwhelm": [159, 163], "overwrit": [82, 83], "own": [3, 5, 9, 11, 12, 24, 26, 35, 39, 80, 81, 86, 87, 92, 93, 96, 98, 110, 115, 118, 125, 181, 183, 198, 203, 218, 221, 224, 226, 268, 287, 289, 291, 298, 309, 316, 317, 319, 320, 325, 328, 329, 330, 331, 333, 336, 337, 340, 342, 343, 346, 349, 350, 351, 352, 355, 356, 362, 365, 367, 371], "owner": [35, 37, 66, 68, 96, 98], "ownership": [291, 297], "ox": [268, 289], "p": [137, 140, 144, 291, 298, 329, 330, 355, 361], "p50": [164, 167], "p90": [164, 167], "p99": [164, 167], "pa": [329, 331, 349, 351, 352, 355, 357, 362, 364], "pack": [268, 289, 329, 332], "packag": [1, 84, 85, 86, 87, 128, 129, 137, 138, 144, 147, 148, 169, 170, 245, 248, 284, 292, 300, 324, 329, 335, 336, 338, 342, 348, 349, 354], "pad": [5, 7, 13, 14, 128, 131, 147, 149, 224, 227, 253, 256, 257, 299, 304, 329, 332], "page": [0, 9, 10, 15, 101, 102, 109, 110, 116, 147, 153, 164, 166, 198, 204, 206, 215], "pagedattent": [101, 102, 107], "pagerduti": [164, 167], "pai": [101, 102, 107, 291, 298], "paid": [4, 9, 171, 174, 198, 201], "pain": [8, 191, 195], "pair": [5, 342, 343, 362, 365], "pal": [268, 289], "pale": [291, 298], "pan": [291, 297], "pancak": [362, 363], "panda": [4, 5, 6, 9, 12, 13, 171, 172, 198, 199, 201, 202, 203, 204, 224, 226, 236, 238, 242, 259, 260, 293, 294, 329, 331, 336, 338, 342, 344, 349, 351, 352, 353, 354, 355, 357, 361, 362, 364, 371], "panel": [2, 3, 175, 180, 181, 189], "pant": [268, 289], "paper": [268, 289], "par": [291, 298], "parallel": [1, 7, 8, 9, 10, 14, 15, 104, 106, 113, 117, 128, 130, 137, 144, 169, 170, 176, 191, 195, 198, 200, 202, 204, 206, 208, 210, 228, 230, 238, 239, 242, 243, 244, 253, 257, 262, 267, 268, 274, 280, 283, 285, 287, 289, 290, 291, 293, 295, 296, 299, 304, 309, 316, 325, 329, 330, 331, 335, 336, 337, 338, 340, 342, 343, 344, 348, 349, 350, 351, 353, 355, 356, 361, 362, 363, 364, 371], "parallel_strategi": [5, 224, 231, 245, 252], "parallel_strategy_kwarg": [5, 224, 231], "param": [4, 12, 137, 144, 171, 174, 316, 320, 328, 342, 346, 349, 352], "param_group": [137, 143], "param_nam": [224, 229], "param_spac": [4, 7, 12, 14, 171, 174, 253, 257], "paramet": [2, 4, 5, 6, 7, 9, 10, 13, 14, 15, 28, 31, 35, 40, 43, 47, 53, 59, 66, 75, 84, 85, 92, 93, 101, 102, 106, 110, 112, 117, 118, 121, 123, 125, 137, 143, 144, 171, 174, 175, 180, 198, 202, 204, 206, 212, 213, 215, 224, 228, 229, 231, 238, 239, 240, 244, 245, 247, 252, 253, 256, 257, 259, 262, 268, 280, 289, 291, 296, 299, 301, 304, 315, 316, 327, 329, 332, 336, 339, 342, 346, 349, 352, 355, 359, 362, 363, 367], "parameter": [11, 218, 223, 245, 247], "paramor": [268, 289], "parcel": [291, 298], "parent": [5, 13, 137, 140], "parish": [268, 289], "pariti": [24, 26], "park": [268, 289, 316, 317, 319, 320, 328], "parquet": [4, 6, 8, 9, 10, 12, 15, 86, 87, 159, 163, 164, 166, 171, 174, 191, 192, 198, 201, 203, 204, 205, 206, 210, 216, 238, 239, 242, 245, 251, 252, 259, 262, 330, 335, 336, 341, 343, 350, 352, 354, 356, 359, 363, 367, 371], "parquet_256": [329, 331, 362, 364, 365], "parquet_dir": [342, 344, 349, 351, 355, 357, 359], "parquet_fil": [362, 365], "parquet_path": [329, 331, 355, 357, 362, 365, 366, 371], "parquetdataset": [6, 259, 262], "parquetfil": [362, 365], "pars": [8, 11, 16, 88, 89, 102, 108, 110, 114, 118, 122, 137, 146, 147, 148, 191, 197, 218, 222, 355, 357], "parseabl": [118, 120, 122], "part": [0, 7, 9, 13, 14, 15, 16, 53, 57, 82, 83, 110, 113, 137, 139, 198, 199, 201, 253, 254, 291, 298, 299, 301, 305, 342, 344], "parti": [155, 157, 268, 289], "particular": [10, 206, 212, 291, 294], "particularli": [3, 181, 188, 268, 289], "partit": [10, 206, 208, 267, 268, 274, 285, 287, 329, 330, 336, 337, 349, 351, 362, 365], "partner": [268, 289, 291, 297], "pass": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 92, 93, 137, 139, 143, 171, 174, 182, 187, 190, 191, 196, 198, 201, 203, 206, 208, 213, 218, 222, 224, 225, 228, 229, 230, 231, 235, 238, 239, 240, 242, 243, 244, 245, 247, 248, 250, 252, 253, 257, 259, 262, 263, 268, 289, 299, 304, 342, 343, 349, 352, 362, 371], "passeng": [4, 9, 12, 171, 174, 198, 201, 204, 291, 298, 361], "passenger_count": [4, 9, 12, 171, 174, 198, 201], "passrol": [17, 22], "past": [82, 83, 84, 85, 88, 89, 90, 91, 268, 289, 355, 356, 357, 358, 359, 361], "past_list": [355, 361], "past_norm": [355, 361], "patch": [8, 24, 26, 191, 196], "patch32": [128, 131, 136, 137, 139, 147, 150], "path": [4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 24, 27, 66, 69, 86, 87, 110, 113, 118, 121, 128, 129, 130, 133, 137, 138, 139, 140, 142, 143, 146, 147, 148, 150, 153, 159, 163, 164, 166, 167, 171, 174, 191, 197, 198, 203, 206, 211, 212, 213, 215, 216, 218, 222, 223, 224, 226, 234, 235, 236, 237, 238, 244, 245, 247, 248, 250, 251, 259, 262, 263, 299, 306, 329, 331, 333, 334, 335, 336, 340, 341, 342, 344, 346, 348, 349, 351, 352, 354, 355, 357, 359, 361, 362, 364, 367, 371], "pathlib": [4, 5, 13, 137, 140, 171, 174, 224, 226, 355, 357], "paths_to_delet": [245, 251, 362, 371], "patienc": [137, 143], "patient": [268, 289], "pattern": [8, 11, 176, 182, 184, 187, 191, 192, 218, 223, 224, 234, 238, 240, 245, 252, 335, 344, 348, 354, 371], "payload": [4, 12, 171, 174], "payment_typ": [9, 198, 201, 204], "pb": [159, 163], "pc": [268, 287, 289, 290], "pd": [4, 5, 6, 9, 12, 13, 171, 172, 174, 198, 199, 201, 202, 224, 226, 238, 242, 259, 260, 262, 291, 294, 329, 331, 336, 338, 342, 344, 348, 349, 351, 353, 354, 355, 357, 361, 362, 364], "pdf": [342, 344], "pe": [355, 358], "peac": [268, 289], "peak": [101, 102, 106], "peer": [24, 26], "penalti": [164, 167], "pend": [12, 13, 299, 306], "pendulum": [340, 341], "pendulum_diffus": [336, 340, 341], "pendulum_diffusion_ft": [336, 340], "pendulum_diffusion_result": [336, 340], "peopl": [268, 289, 291, 294, 298], "per": [9, 10, 15, 16, 86, 87, 101, 102, 106, 107, 110, 113, 116, 118, 121, 128, 130, 137, 144, 147, 151, 155, 157, 164, 166, 167, 198, 201, 204, 206, 210, 211, 227, 229, 230, 234, 235, 238, 244, 301, 305, 329, 332, 333, 336, 339, 340, 344, 346, 349, 350, 352, 353, 354, 355, 356, 359, 361, 362, 363, 367, 369, 371], "per_worker_batch": [224, 228], "percentag": [9, 198, 202], "percentil": [355, 361], "perceptu": [329, 335], "perfect": [110, 112, 268, 289, 291, 298, 329, 331], "perform": [2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 84, 85, 86, 87, 88, 89, 92, 93, 101, 102, 103, 104, 110, 112, 116, 117, 118, 122, 123, 124, 125, 126, 127, 137, 139, 144, 155, 157, 159, 162, 164, 167, 171, 172, 173, 174, 175, 180, 181, 187, 191, 192, 195, 196, 197, 198, 199, 202, 204, 205, 206, 207, 208, 212, 213, 215, 218, 219, 220, 222, 224, 234, 236, 238, 239, 241, 245, 246, 252, 253, 254, 257, 259, 262, 263, 268, 277, 280, 283, 288, 289, 290, 291, 296, 297, 298, 299, 301, 306, 329, 331, 335, 336, 337, 341, 342, 348, 349, 351, 353, 354, 355, 356, 359, 361, 362, 363, 367, 371], "performantli": [1, 169, 170], "perhap": [7, 8, 14, 191, 196, 253, 257, 291, 297, 298], "period": [16, 80, 81, 84, 85, 101, 102, 106, 355, 361], "permiss": [17, 22, 24, 26, 28, 29, 35, 36, 43, 44, 53, 55, 63, 86, 87, 94, 95], "permut": [329, 335], "persi": [268, 289], "persis": [224, 226], "persist": [5, 6, 17, 21, 22, 24, 26, 86, 87, 88, 89, 128, 130, 155, 157, 159, 162, 207, 208, 225, 226, 227, 228, 232, 238, 242, 245, 246, 248, 251, 259, 263, 336, 340, 342, 344, 347, 349, 350, 351, 363, 367], "person": [291, 298, 342, 343, 348], "perspect": [291, 294, 298], "pertain": [88, 89], "petabyt": [128, 132], "phase": [109, 299, 304], "philip": [268, 289], "philosop": [291, 298], "philosophi": [291, 297], "photo": [268, 289, 362, 363], "photograph": [291, 298, 329, 330, 362, 364], "physic": [3, 10, 159, 161, 181, 187, 206, 212], "pi": [88, 89, 336, 337, 338, 341], "pi4_sampl": [88, 89], "pi_": [336, 337], "pic": [268, 289], "pick": [53, 57, 101, 102, 108, 245, 250, 252, 268, 289, 291, 298, 342, 347, 355, 360, 362, 363, 370, 371], "pickup": [355, 356], "pid": [13, 14, 299, 306], "piec": [164, 166, 268, 289], "pil": [5, 128, 131, 136, 137, 139, 147, 148, 224, 226, 232, 237, 238, 243, 329, 331, 362, 364], "pile": [291, 294], "pin": [224, 237, 329, 331, 362, 363, 366], "pine": [349, 350], "pinecon": [8, 191, 192], "pink": [291, 294], "pinterest": [10, 206, 217], "pioneer": [8, 191, 195], "pip": [0, 1, 66, 74, 82, 83, 84, 85, 86, 87, 110, 114, 126, 127, 128, 129, 137, 138, 147, 148, 153, 155, 158, 168, 169, 170, 182, 284, 292, 300, 324, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "pipelin": [4, 6, 8, 9, 10, 15, 84, 85, 101, 102, 105, 106, 117, 118, 125, 128, 130, 147, 151, 159, 163, 165, 171, 174, 182, 191, 192, 195, 198, 203, 205, 206, 208, 212, 213, 217, 224, 226, 238, 239, 241, 242, 244, 245, 252, 259, 262, 291, 293, 298, 312, 313, 314, 315, 316, 326, 327, 329, 330, 331, 336, 337, 341, 342, 343, 349, 350, 354, 355, 356, 361, 362, 363, 365, 371], "pipeline_parallel_s": [110, 116], "pitch": [268, 289], "pivot": [336, 337], "pixel": [7, 10, 14, 206, 212, 224, 226, 232, 238, 242, 243, 253, 255, 330, 331, 333, 336, 338, 362, 364], "pixeldiffus": [329, 332, 333, 335], "pizza": [362, 363], "pl": [6, 259, 260, 262, 263, 329, 331, 332, 333, 336, 338, 339, 340], "pl_ckpt": [329, 335], "place": [0, 8, 101, 102, 105, 191, 197, 224, 226, 245, 247, 249, 268, 289, 291, 298, 316, 317, 319, 320, 328], "placehold": [28, 29, 35, 36, 43, 44, 51, 53, 54, 64, 66, 67, 82, 83, 101, 102, 108, 110, 114], "placement": [224, 228, 231, 232, 235, 336, 337, 355, 361, 362, 363, 366, 367, 371], "plai": [82, 83, 268, 289, 291, 297, 362, 371], "plain": [329, 331, 362, 363], "plan": [10, 17, 22, 24, 26, 27, 28, 30, 35, 39, 43, 45, 51, 53, 56, 64, 66, 70, 79, 118, 121, 164, 167, 206, 211, 212, 214, 268, 287, 289], "plane": [19, 21], "planner": [355, 356], "plate": [268, 289], "plateau": [355, 361], "platform": [8, 16, 17, 20, 24, 26, 79, 88, 89, 100, 101, 102, 106, 128, 130, 147, 154, 155, 157, 158, 159, 162, 164, 165, 166, 167, 191, 192, 194, 195, 224, 226, 268, 274, 287, 299, 301, 305], "plausibl": [336, 341], "pleas": [17, 22, 28, 31, 35, 40, 43, 44, 47, 51, 53, 54, 59, 64, 66, 67, 75, 78, 100, 118, 121, 164, 166, 167, 284, 292, 300, 324, 342, 348, 362, 363], "plenti": [128, 133], "plot": [16, 224, 226, 236, 237, 291, 294, 297, 298, 331, 332, 338, 339, 344, 349, 351, 354, 357, 361, 364, 371], "plotlin": [291, 298], "plt": [5, 7, 10, 13, 14, 16, 206, 207, 211, 224, 226, 237, 253, 254, 255, 329, 331, 333, 335, 336, 338, 340, 342, 344, 346, 349, 351, 353, 355, 357, 359, 361, 362, 364, 369, 371], "plu": [17, 22, 224, 226, 336, 338, 362, 364], "plugin": [6, 24, 27, 51, 52, 64, 65, 66, 72, 259, 263, 329, 333, 336, 340], "pm": [336, 337], "pndm": [329, 335], "png": [10, 128, 136, 147, 150, 153, 206, 212], "poc": [24, 26], "pod": [24, 26, 27, 43, 49, 50, 51, 53, 61, 62, 64, 66, 73, 101, 102, 106], "point": [5, 10, 11, 17, 20, 24, 26, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 86, 87, 101, 102, 108, 110, 115, 128, 130, 137, 144, 168, 206, 212, 218, 223, 224, 228, 234, 245, 250, 267, 268, 285, 289, 291, 297, 298, 349, 351], "pointless": [291, 294], "pole": [349, 350], "polici": [17, 21, 22, 24, 26, 27, 35, 39, 42, 63, 66, 70, 332, 335, 338], "polish": [291, 298], "polit": [118, 121, 291, 294, 298], "politician": [291, 294], "poll": [268, 289], "pomeranian": [137, 144], "pont": [268, 289], "pool": [10, 128, 132, 206, 213], "poor": [11, 218, 220], "poorli": [7, 14, 253, 257], "pop": [128, 129, 137, 138, 147, 148], "popul": [9, 28, 30, 43, 45, 198, 201], "popular": [4, 8, 118, 121, 171, 173, 174, 191, 194, 195, 268, 275, 276, 277, 288, 342, 344], "porn": [291, 294], "porno": [291, 294], "pornograph": [291, 294], "port": [137, 144, 155, 158, 349, 350], "portion": [299, 304], "pos_enc": [355, 358], "posit": [8, 137, 146, 191, 196, 268, 287, 289, 291, 293, 294, 296, 298, 316, 320, 328, 355, 358], "posix": [17, 22], "possibl": [110, 117, 118, 123, 125, 147, 151, 291, 293], "possibli": [8, 191, 197], "post": [4, 5, 6, 7, 11, 12, 13, 16, 17, 22, 147, 150, 153, 164, 167, 168, 171, 174, 218, 222, 253, 258, 259, 264, 315, 316, 327, 329, 330, 342, 343, 348], "poster": [291, 298], "postgresql": [8, 191, 192], "postwar": [291, 297, 298], "potato": [291, 294], "potemkin": [291, 298], "potenti": [3, 159, 161, 181, 185], "potter": [268, 289], "power": [3, 118, 119, 120, 123, 126, 127, 181, 188, 267, 268, 285, 291, 293, 299, 301, 305, 349, 354], "powershel": [118, 121], "pq": [329, 331, 355, 357, 362, 364, 365], "practic": [1, 5, 17, 20, 35, 39, 79, 96, 97, 101, 119, 120, 121, 125, 128, 130, 155, 157, 169, 170, 224, 225, 234, 237, 245, 252, 268, 283, 290, 291, 295, 299, 301, 355, 357, 362, 363], "practition": [4, 12, 171, 173], "prayer": [268, 289], "pre": [10, 15, 35, 39, 82, 83, 84, 85, 86, 87, 126, 127, 128, 135, 137, 145, 147, 153, 206, 213, 299, 304, 313, 314, 315, 316, 326, 327, 329, 335, 342, 348, 355, 356, 362, 363], "preced": [137, 144, 291, 298], "precis": [6, 101, 102, 106, 110, 112, 137, 146, 259, 262, 263, 329, 335, 336, 341, 362, 371], "precomput": [101, 102, 104, 362, 365], "preconfigur": [224, 227], "pred": [5, 137, 146, 224, 237, 336, 339, 342, 346, 349, 353, 354, 355, 358, 359, 361, 362, 367, 371], "pred_d": [137, 146, 349, 353, 354, 355, 361, 362, 371], "pred_label": [349, 352, 353], "pred_nois": [329, 332, 335, 336, 341], "pred_norm": [355, 361], "pred_prob": [349, 352], "pred_row": [355, 361, 362, 371], "predefin": [84, 85], "predic": [9, 198, 205], "predict": [4, 7, 8, 10, 11, 13, 14, 15, 16, 118, 124, 126, 127, 137, 139, 140, 146, 147, 148, 150, 153, 171, 174, 191, 193, 206, 213, 216, 218, 222, 253, 257, 267, 268, 285, 299, 303, 304, 309, 315, 316, 320, 325, 327, 328, 329, 330, 332, 335, 336, 337, 339, 342, 343, 345, 348, 349, 350, 352, 353, 354, 355, 356, 357, 358, 361, 362, 364, 371], "predict_prob": [137, 140, 146, 147, 149], "predicted_label": [10, 11, 15, 16, 164, 167, 206, 213, 215, 218, 222, 362, 371], "predicted_prob": [12, 137, 146], "prediction_pipelin": [4, 171, 174], "predictor": [4, 12, 101, 102, 104, 137, 146, 147, 149, 171, 174, 362, 371], "preemption": [10, 88, 89, 206, 208, 245, 249], "prefect": [147, 154], "prefer": [24, 26, 28, 31, 35, 40, 43, 47, 53, 59, 66, 73, 75, 82, 83, 118, 124, 168, 291, 298, 329, 335], "prefer_spot": [159, 163], "prefetch": [362, 371], "prefetch_batch": [6, 238, 241, 259, 263], "prefil": 109, "prefix": [5, 9, 35, 39, 86, 87, 198, 203, 342, 348, 355, 361, 362, 371], "prefix_for_the_resources_ad": [35, 39], "preinstal": [80, 81], "prem": [80, 81], "premier": [268, 289], "premis": [17, 20, 94, 95], "prepar": [3, 6, 79, 92, 93, 118, 121, 128, 130, 181, 186, 224, 228, 231, 235, 239, 245, 252, 259, 263, 291, 293, 297, 298, 299, 301, 304, 336, 340, 359, 361, 363, 371], "prepare_data_load": [5, 13, 225, 227, 228, 234, 245, 252, 355, 357, 362, 363, 364, 366, 371], "prepare_model": [5, 13, 137, 143, 225, 227, 228, 234, 245, 252, 342, 343, 344, 346, 355, 357, 359, 362, 363, 364, 367, 371], "prepare_train": [329, 333, 336, 340], "preproc": [137, 146], "preprocess": [5, 6, 8, 9, 10, 12, 15, 16, 84, 85, 126, 127, 128, 129, 130, 131, 138, 143, 144, 191, 192, 198, 201, 205, 206, 212, 224, 226, 232, 237, 238, 239, 243, 244, 245, 252, 259, 262, 284, 292, 293, 299, 300, 301, 324, 329, 330, 331, 336, 337, 341, 342, 344, 348, 355, 356, 362, 363, 364, 365], "preprocess_imag": [329, 331], "preprocessed_data": [137, 139], "preprocessed_data_path": [137, 139], "preprocessed_df": [291, 298], "preprocessed_train": [137, 139], "preprocessed_train_d": [137, 143], "preprocessed_train_path": [137, 139, 143], "preprocessed_v": [137, 139], "preprocessed_val_d": [137, 143], "preprocessed_val_path": [137, 139, 143], "preprocessor": [9, 16, 137, 139, 140, 143, 146, 147, 149, 198, 205, 291, 294], "preprocessor_app": 16, "preprocessor_handl": 16, "prerequisit": [36, 54, 67], "presenc": [349, 354], "present": [5, 8, 101, 102, 106, 118, 122, 164, 165, 191, 195, 224, 226, 291, 297, 298, 342, 344, 349, 350, 352, 355, 357, 361, 362, 371], "preserv": [137, 146, 224, 225, 227, 349, 351, 355, 356], "press": [1, 169, 170], "pressur": [101, 102, 105, 128, 130, 159, 163, 316, 317, 319, 320, 328], "pretend": [291, 295], "pretenti": [291, 294], "pretrain": [6, 259, 262, 263, 264, 268, 275, 276, 277, 288], "pretrainedconfig": [6, 259, 262], "pretti": [291, 297, 362, 371], "prevent": [10, 206, 212, 355, 357], "preview": [0, 28, 30, 35, 39, 43, 45, 53, 56, 119, 126, 127], "previou": [43, 48, 53, 60, 66, 76, 82, 83, 101, 102, 104, 105, 118, 123, 128, 131, 137, 138, 139, 164, 165, 238, 239, 240, 245, 247, 250, 316, 320, 328, 342, 348, 349, 350, 362, 363], "previous": [5, 245, 247], "price": [4, 12, 137, 144, 171, 174, 268, 287, 289, 355, 356], "priest": [268, 289], "primari": [8, 155, 158, 191, 195], "primarili": [6, 10, 15, 16, 24, 26, 206, 214, 259, 262, 355, 359], "prime": [268, 289], "primit": [362, 364], "princip": [17, 22], "print": [1, 3, 4, 5, 6, 7, 10, 13, 14, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 137, 140, 146, 164, 166, 169, 170, 171, 174, 181, 183, 185, 187, 189, 206, 212, 224, 228, 233, 245, 251, 253, 256, 257, 259, 262, 268, 274, 283, 287, 290, 291, 294, 295, 296, 297, 298, 299, 304, 305, 316, 320, 328, 329, 331, 333, 334, 335, 336, 338, 340, 341, 342, 344, 346, 348, 349, 351, 352, 353, 354, 355, 357, 359, 360, 361, 362, 364, 365, 367, 368, 370, 371], "print_metrics_ray_train": [5, 13, 224, 228, 233, 238, 240, 245, 247], "printout": [362, 371], "prior": [8, 82, 83, 137, 138, 139, 191, 192, 349, 352], "priorit": [7, 14, 253, 257], "prioriti": [126, 127, 137, 146], "privat": [17, 20, 22, 24, 26, 28, 30, 31, 35, 40, 43, 45, 47, 53, 56, 59, 66, 69, 75, 86, 87, 94, 95], "private_subnet": [17, 22], "privileg": [17, 22, 24, 26], "prj_cz951f43jjdybtzkx1s5sjgz99": [128, 129, 137, 138, 147, 148], "pro": [118, 124], "prob": [3, 137, 146, 181, 185, 349, 353], "probabilist": [355, 361], "probabl": [137, 140, 146, 147, 149, 150, 268, 289, 362, 363], "problem": [9, 88, 89, 101, 102, 107, 118, 124, 155, 157, 198, 201], "proce": [6, 13, 16, 259, 262], "process": [2, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 20, 22, 66, 67, 84, 85, 88, 89, 92, 93, 105, 107, 108, 109, 110, 111, 122, 123, 128, 130, 132, 134, 136, 137, 139, 144, 146, 159, 161, 164, 166, 171, 174, 175, 178, 179, 180, 182, 184, 186, 188, 192, 197, 198, 200, 203, 205, 206, 208, 209, 210, 212, 213, 218, 221, 224, 225, 227, 228, 230, 232, 234, 235, 237, 238, 239, 242, 253, 256, 257, 259, 262, 263, 267, 268, 274, 277, 280, 283, 285, 287, 288, 289, 290, 298, 299, 301, 304, 305, 306, 309, 316, 325, 331, 336, 337, 341, 342, 343, 344, 349, 350, 353, 355, 356, 357, 361, 362, 365], "processed_d": [329, 331], "processor": [128, 131, 147, 149], "prod": [3, 126, 127, 181, 186], "produc": [4, 5, 6, 8, 9, 10, 13, 15, 171, 174, 191, 193, 198, 203, 206, 210, 224, 234, 259, 263, 291, 297, 329, 330, 342, 348, 355, 361], "product": [1, 3, 8, 17, 20, 79, 84, 85, 86, 87, 101, 102, 103, 104, 106, 107, 110, 111, 112, 115, 116, 117, 118, 119, 120, 122, 124, 125, 164, 167, 169, 170, 181, 187, 191, 196, 199, 207, 224, 234, 245, 252, 254, 260, 267, 268, 285, 291, 298, 309, 316, 325, 329, 335, 342, 343, 344, 345, 348, 349, 350, 352, 362, 363], "production": [90, 91, 100, 147, 154, 245, 252], "profession": 79, "profil": [24, 26, 28, 30, 34, 137, 144, 155, 157, 336, 341, 362, 371], "profile_data": 168, "prog_bar": [6, 259, 262, 329, 332, 336, 339], "program": [3, 118, 124, 164, 167, 181, 189], "programm": 16, "programmat": [16, 90, 91, 164, 167], "progress": [5, 13, 88, 89, 224, 225, 233, 236, 245, 246, 248, 249, 342, 344, 346, 362, 363, 364], "project": [0, 10, 11, 17, 19, 22, 35, 37, 38, 39, 66, 68, 69, 70, 72, 78, 86, 87, 97, 99, 168, 206, 208, 218, 220, 268, 289, 291, 298], "project_numb": [35, 39], "prometheu": 157, "promot": [355, 361], "promote_opt": [349, 352], "prompt": [1, 80, 81, 82, 83, 101, 102, 104, 105, 118, 121, 169, 170], "promptli": [291, 297], "proof": [24, 26, 268, 289], "propag": [126, 127], "proper": [28, 29, 35, 36, 43, 44, 53, 55, 137, 141, 224, 228], "properli": [1, 8, 118, 123, 155, 158, 169, 170, 191, 195, 284, 292, 299, 300, 301, 316, 320, 324, 328, 362, 364], "properti": [8, 118, 123, 191, 192], "proport": [9, 10, 15, 198, 203, 206, 210, 213, 342, 344], "proprietari": [8, 126, 127, 191, 192], "prosper": [268, 289], "protect": [349, 350], "protocol": [8, 11, 168, 191, 192, 197, 218, 220], "prototyp": [82, 83, 110, 112, 117], "prove": [16, 329, 334, 362, 370], "provid": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 24, 27, 28, 30, 35, 39, 41, 45, 50, 53, 56, 66, 70, 79, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 100, 101, 102, 103, 105, 107, 110, 112, 113, 115, 116, 117, 118, 120, 121, 122, 123, 126, 127, 128, 130, 135, 147, 151, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 173, 175, 176, 177, 181, 182, 191, 192, 195, 197, 198, 199, 200, 206, 207, 215, 218, 220, 221, 223, 224, 225, 226, 229, 238, 239, 244, 245, 250, 259, 261, 291, 293, 299, 301, 305, 309, 316, 325, 329, 333, 342, 344, 346, 349, 353, 355, 357, 359, 362, 363], "provis": [5, 6, 10, 12, 24, 25, 26, 27, 79, 96, 98, 126, 127, 206, 208, 259, 261, 329, 330, 349, 350, 362, 363], "proxi": [8, 147, 151, 164, 167, 168, 191, 197], "proxim": [349, 350], "proxy_http_request": [164, 167, 168], "proxy_route_to_replica": [164, 167, 168], "prune": [329, 335, 336, 341, 349, 354], "pseudo": [329, 335], "pt": [5, 10, 11, 13, 15, 16, 128, 131, 137, 140, 146, 147, 149, 206, 213, 217, 218, 222, 223, 224, 234, 237, 245, 247, 248, 329, 335, 342, 346, 348, 355, 359, 361, 362, 367, 371], "public": [4, 9, 10, 11, 15, 16, 17, 20, 22, 110, 113, 128, 130, 137, 138, 164, 166, 171, 174, 198, 201, 204, 206, 210, 212, 213, 215, 218, 222, 267, 268, 283, 285, 289, 290, 291, 293, 294, 297, 298], "public_subnet": [17, 22], "publicli": [94, 95, 118, 121], "publish": [92, 93], "pull": [82, 83, 268, 289, 329, 331, 342, 346, 349, 352, 355, 359, 362, 363, 364, 365, 369], "pulocationid": [9, 198, 201], "pumpkin": [268, 289], "pun": [291, 294], "punchestown": [268, 289], "punctuat": [291, 298], "pure": [329, 330, 342, 343], "purpl": [268, 289], "purpos": [1, 2, 15, 22, 137, 144, 147, 154, 169, 170, 175, 177, 192, 291, 294, 295, 362, 363], "push": [82, 83, 268, 289, 291, 298, 362, 363], "pushdown": [9, 198, 205], "put": [3, 17, 22, 128, 131, 176, 181, 183, 268, 289, 291, 297, 298], "putobject": [17, 22], "pwd": 5, "py": [0, 1, 10, 11, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 108, 110, 113, 116, 118, 121, 122, 123, 128, 129, 135, 137, 138, 147, 148, 155, 158, 159, 163, 164, 166, 169, 170, 206, 211, 215, 218, 223], "py311": [101, 102, 108, 110, 115], "py312": [159, 163], "py_execut": [128, 129, 137, 138, 147, 148], "pyarrow": [8, 10, 164, 166, 191, 192, 206, 210, 329, 331, 336, 338, 342, 344, 349, 351, 352, 355, 357, 362, 363, 364], "pydant": [4, 16, 118, 122, 171, 172], "pydata": [355, 357], "pyflink": [8, 191, 195], "pypi": [86, 87], "pyplot": [5, 7, 10, 13, 14, 16, 206, 207, 224, 226, 253, 254, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "pyproj": [84, 85], "pyproject": [126, 127], "pyspark": [8, 191, 195], "python": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 16, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 100, 101, 102, 106, 108, 110, 113, 115, 118, 121, 123, 126, 127, 128, 129, 135, 137, 138, 147, 148, 155, 158, 164, 166, 167, 168, 169, 170, 171, 173, 175, 177, 178, 179, 181, 184, 185, 190, 191, 192, 194, 195, 196, 198, 200, 218, 220, 224, 227, 237, 253, 257, 259, 263, 284, 292, 299, 300, 301, 306, 309, 316, 324, 325, 329, 331, 336, 338, 342, 343, 344, 349, 351, 355, 357, 362, 364], "python3": [0, 86, 87, 137, 144], "pythonmalloc": [155, 158], "pytorch": [10, 137, 138, 140, 143, 206, 210, 224, 225, 226, 227, 228, 232, 234, 238, 239, 240, 241, 243, 245, 252, 254, 261, 302, 305, 306, 309, 316, 325, 329, 330, 331, 333, 335, 336, 337, 338, 340, 342, 343, 344, 345, 346, 348, 356, 358, 362, 363, 364, 365, 371], "pyyaml": 0, "q": [101, 102, 105, 118, 124, 126, 127, 128, 129, 137, 138, 147, 148, 342, 344], "q2": [268, 289], "q_q": [268, 289], "qp": [147, 151, 164, 167], "qt": [268, 287, 289, 290], "qtr": [268, 289], "quad": [329, 330, 336, 337, 342, 343], "qualif": [118, 121], "qualit": [362, 371], "qualiti": [8, 110, 112, 191, 192, 329, 335, 342, 348], "quantiz": [101, 102, 105, 117], "queri": [8, 28, 30, 43, 45, 53, 56, 57, 88, 89, 92, 93, 110, 114, 118, 121, 147, 153, 159, 162, 163, 164, 167, 168, 191, 192, 316, 328], "question": [268, 289], "queu": [24, 26], "queue": [2, 8, 11, 16, 24, 26, 128, 135, 164, 167, 175, 177, 191, 197, 218, 221], "quick": [2, 5, 12, 17, 20, 24, 26, 88, 89, 118, 121, 172, 175, 176, 224, 226, 234, 235, 329, 331, 333, 342, 344, 349, 351, 352, 362, 364, 371], "quickli": [1, 9, 82, 83, 94, 95, 101, 102, 106, 169, 170, 198, 201, 291, 298, 329, 331, 349, 351, 355, 359, 362, 364, 369], "quickstart": [28, 29, 35, 37, 43, 44, 53, 55, 66, 68], "quit": [268, 289, 291, 298], "quot": [268, 289], "quota": [17, 19, 101, 102, 106, 126, 127], "qwen": [118, 122, 123, 124], "qwen2": [118, 122], "qwen3": [118, 123], "r": [0, 1, 8, 13, 35, 42, 66, 78, 126, 127, 128, 129, 137, 138, 140, 146, 147, 148, 168, 169, 170, 191, 192, 268, 284, 289, 291, 292, 294, 300, 324, 336, 337, 342, 343, 344, 349, 350, 352, 355, 356, 362, 363, 364], "r1": [118, 124], "r2": [8, 191, 192], "race": [268, 289, 291, 294, 297], "radio": [268, 289], "rafe": [268, 289], "ragnarok": [268, 289], "rahul": [268, 289], "rai": [17, 19, 20, 22, 25, 27, 34, 42, 49, 50, 51, 52, 61, 62, 64, 65, 79, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 96, 98, 99, 106, 109, 115, 116, 117, 120, 122, 123, 125, 126, 127, 129, 130, 131, 133, 134, 135, 136, 138, 139, 141, 142, 143, 145, 146, 148, 149, 150, 152, 153, 156, 157, 162, 163, 177, 178, 179, 183, 184, 185, 186, 187, 188, 195, 201, 202, 203, 204, 210, 211, 212, 214, 215, 216, 222, 223, 226, 227, 229, 230, 231, 232, 233, 235, 236, 247, 248, 249, 250, 251, 256, 262, 271, 275, 276, 277, 286, 287, 288, 289, 302, 304, 305, 315, 320, 326, 327, 332, 335, 338, 341, 345, 347, 348, 354, 360, 364, 367, 368, 369, 370], "railwai": [291, 298], "rais": [3, 181, 185, 329, 333, 335, 336, 340, 362, 371], "ram": [10, 206, 212], "ramen": [362, 363], "rammstein": [268, 289], "rand": [3, 11, 16, 164, 167, 181, 183, 187, 218, 222], "randint": [3, 5, 6, 7, 12, 14, 118, 123, 159, 163, 181, 189, 224, 226, 237, 253, 257, 259, 262, 329, 332, 336, 338], "randn": [329, 335, 336, 338, 341], "randn_lik": [6, 259, 262, 329, 332], "random": [2, 3, 5, 7, 9, 10, 11, 12, 14, 15, 16, 35, 41, 53, 62, 88, 89, 118, 123, 159, 163, 164, 167, 175, 176, 181, 182, 183, 185, 187, 189, 198, 204, 206, 215, 218, 222, 224, 226, 237, 253, 257, 329, 331, 335, 336, 337, 338, 341, 342, 344, 348, 349, 351, 362, 363, 364], "random_shuffl": [9, 10, 15, 198, 204, 206, 215, 329, 331, 336, 338, 349, 351], "random_st": [4, 171, 174, 349, 351, 362, 365], "randomize_block_ord": [9, 10, 15, 198, 204, 206, 215, 342, 344], "randomli": [5, 9, 10, 15, 128, 130, 198, 204, 206, 215, 224, 226, 342, 348], "rang": [2, 3, 5, 7, 10, 12, 13, 14, 16, 17, 22, 88, 89, 128, 130, 137, 143, 146, 159, 163, 175, 180, 181, 183, 185, 189, 206, 212, 224, 226, 228, 237, 238, 240, 243, 245, 247, 253, 256, 257, 291, 295, 299, 304, 309, 316, 325, 329, 335, 336, 338, 341, 342, 346, 349, 353, 355, 356, 357, 359, 362, 365, 367], "rank": [4, 5, 6, 13, 118, 121, 171, 174, 225, 227, 228, 233, 235, 245, 248, 252, 259, 263, 329, 333, 346, 348, 349, 352, 353, 354, 355, 359, 362, 366, 367], "rap": [268, 289], "rapid": [82, 83], "rapidli": [80, 81], "rate": [6, 7, 14, 24, 27, 101, 102, 105, 164, 167, 224, 228, 253, 257, 259, 262, 268, 289, 291, 294, 299, 304, 305, 329, 335, 336, 341, 345, 346, 348, 355, 361, 362, 371], "rather": [10, 101, 102, 104, 206, 211, 291, 294, 298, 342, 348, 349, 351, 355, 356], "ratings_d": [342, 344], "ratings_parquet": [342, 344], "ratings_parquet_uri": [342, 344], "ratio": [137, 144, 168, 349, 351, 353], "rattl": [291, 298], "rattler": [268, 289], "ravenstein": [291, 298], "raw": [8, 12, 13, 14, 53, 57, 159, 162, 163, 191, 192, 238, 242, 243, 268, 289, 299, 304, 329, 331, 335, 342, 344, 348, 349, 350, 351, 353, 355, 357, 362, 363, 364, 371], "raw_path": [342, 344], "ray_actor_opt": [16, 147, 149], "ray_address": [137, 138], "ray_data_synthet": [159, 163], "ray_dedup_log": [14, 299, 306], "ray_enable_windows_or_osx_clust": [155, 158], "ray_pl_ckpt": [329, 333, 336, 340], "ray_result": [14, 299, 306], "ray_runtime_env_hook": [128, 129, 137, 138, 147, 148], "ray_scheduler_ev": [13, 14, 16, 128, 133, 137, 139, 147, 150], "ray_train_v2_en": [137, 138, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "ray_wheel_url": [126, 127], "rayddp": [329, 333], "rayddpstrategi": [6, 259, 260, 263, 329, 333, 336, 340], "raylightningenviron": [6, 259, 260, 263, 329, 331, 333, 336, 338, 340], "rayproject": [84, 85], "rayserv": [315, 316, 327], "raytaskerror": [3, 181, 185], "raytrainreportcallback": [6, 259, 260, 263, 329, 330, 333, 336, 337, 340, 341, 349, 350, 351, 352, 353, 354], "raytrainwork": [13, 299, 306], "raytrainxgboosttrain": [4, 171, 172, 174], "rayturbo": [10, 11, 126, 127, 128, 132, 137, 144, 147, 151, 206, 208, 218, 220], "rbac": [96, 98, 99], "rbi": [268, 287, 289], "rd": [17, 22, 268, 289, 349, 351], "rdata": [355, 357, 361, 362, 371], "re": [1, 3, 7, 10, 11, 15, 28, 31, 33, 35, 40, 42, 43, 46, 47, 53, 58, 59, 66, 75, 78, 79, 82, 83, 88, 89, 90, 91, 92, 93, 110, 114, 115, 118, 125, 128, 130, 131, 134, 137, 139, 144, 147, 154, 169, 170, 181, 185, 206, 215, 218, 223, 224, 231, 232, 237, 245, 251, 252, 253, 256, 268, 289, 291, 294, 329, 330, 331, 335, 336, 337, 341, 342, 343, 348, 349, 350, 351, 354, 355, 361, 362, 363, 364], "reach": [24, 26, 101, 102, 104, 245, 250, 291, 297], "read": [2, 3, 5, 6, 8, 11, 12, 13, 17, 22, 96, 98, 128, 129, 130, 132, 175, 180, 181, 184, 188, 189, 191, 192, 201, 203, 207, 210, 218, 223, 224, 226, 237, 238, 239, 242, 259, 264, 291, 297, 298, 329, 331, 342, 344, 349, 351, 355, 357, 362, 363, 364, 365, 371], "read_csv": [5, 9, 13, 198, 201, 342, 344, 348, 355, 357], "read_databricks_t": [10, 206, 210], "read_imag": [10, 15, 16, 128, 130, 137, 139, 146, 206, 210, 212, 215], "read_json": [9, 198, 201], "read_parquet": [4, 6, 9, 10, 12, 128, 136, 137, 143, 164, 166, 171, 174, 198, 201, 204, 206, 210, 238, 242, 259, 262, 329, 331, 342, 344, 349, 351, 362, 371], "read_row_group": [362, 365], "read_tabl": [355, 357, 362, 365], "readabl": [9, 198, 201, 224, 234], "readfil": [10, 206, 214], "readi": [1, 3, 28, 31, 35, 40, 43, 46, 47, 53, 58, 59, 66, 75, 79, 80, 81, 82, 83, 92, 93, 101, 102, 109, 110, 112, 117, 118, 121, 122, 125, 128, 131, 132, 169, 170, 181, 189, 224, 232, 238, 243, 245, 252, 268, 289, 299, 304, 336, 338, 341, 342, 343, 344, 355, 357, 362, 363, 365], "readm": [164, 167, 284, 292, 300, 324, 372], "readme_01": 372, "ready_ref": [3, 181, 189], "real": [8, 101, 102, 104, 118, 123, 125, 128, 130, 191, 195, 268, 289, 291, 295, 299, 304, 337, 341, 342, 343, 348, 349, 354, 355, 361, 362, 363], "realist": [291, 298, 329, 330, 342, 344, 362, 371], "realiti": [291, 294], "realknowncaus": [355, 357], "realli": [268, 289, 291, 294, 297, 298], "reason": [86, 87, 101, 102, 105, 107, 110, 112, 118, 124, 159, 161, 336, 341], "reasoning_pars": [118, 123], "reassur": [362, 365], "rebuild": [0, 224, 237, 342, 348], "rec": [329, 331, 355, 357, 362, 364], "rec_sys_tutori": [342, 344, 346, 348], "recal": [16, 137, 146], "recalcul": [101, 102, 105], "recap": [7, 14, 253, 257], "receiv": [5, 6, 8, 24, 26, 118, 121, 123, 191, 197, 224, 225, 259, 263, 309, 316, 325, 342, 343, 344, 346, 355, 357], "recent": [86, 87, 224, 236, 245, 250, 291, 298, 329, 333, 336, 340, 342, 347, 355, 361, 362, 367, 369], "recent_kei": [86, 87], "recent_nam": [86, 87], "recip": [5, 268, 289], "recipi": 168, "reclaim": [329, 335, 342, 348], "recommend": [0, 4, 5, 6, 7, 8, 9, 10, 11, 17, 22, 24, 26, 84, 85, 101, 102, 103, 110, 111, 119, 121, 122, 164, 165, 171, 172, 191, 195, 198, 199, 202, 206, 207, 217, 218, 219, 223, 224, 226, 234, 245, 252, 253, 254, 259, 260, 262, 345], "recomput": [342, 348], "record": [4, 9, 12, 137, 139, 155, 157, 171, 174, 198, 201, 329, 331, 355, 357, 362, 364, 369], "recov": [10, 128, 132, 147, 153, 206, 208, 245, 246, 329, 330, 349, 354, 355, 356, 361], "recoveri": [224, 234, 245, 246, 248, 252, 329, 330, 336, 341, 342, 343, 346, 355, 359, 362, 363, 371], "recreat": [16, 355, 361], "recurr": [355, 356], "recurs": [28, 33, 43, 51, 53, 64, 245, 251], "red": [101, 102, 105, 268, 289, 291, 298, 329, 330, 362, 363], "redefin": [4, 171, 174], "redeploi": [336, 337], "redi": [17, 21], "redshift": [8, 191, 192], "reduc": [4, 7, 8, 9, 10, 11, 84, 85, 101, 102, 103, 118, 120, 122, 126, 127, 128, 130, 147, 151, 171, 172, 191, 192, 198, 199, 206, 207, 211, 212, 218, 219, 253, 254, 268, 283, 290, 299, 305, 355, 361], "reducelronplateau": [137, 143], "reduct": [110, 116], "redund": [101, 102, 105, 147, 151, 224, 234, 362, 371], "ref": [2, 3, 128, 135, 137, 145, 147, 153, 175, 179, 180, 181, 183, 184, 187, 189, 190], "refer": [2, 3, 6, 9, 10, 13, 15, 17, 22, 28, 29, 35, 37, 43, 44, 53, 54, 55, 66, 67, 68, 82, 83, 86, 87, 96, 98, 101, 102, 104, 128, 131, 155, 157, 158, 164, 167, 175, 177, 179, 181, 183, 184, 185, 187, 198, 201, 206, 212, 216, 224, 236, 237, 259, 262, 299, 303], "reflect": [8, 191, 192, 362, 369], "refresh": [291, 297], "reg": [4, 171, 174], "regard": [268, 289], "regardless": [5, 6, 259, 262, 355, 359], "region": [12, 13, 14, 17, 23, 28, 30, 35, 38, 39, 43, 45, 46, 48, 53, 56, 58, 60, 66, 69, 70, 72, 76, 86, 87, 118, 121], "regist": [20, 22, 24, 26, 29, 30, 33, 34, 36, 39, 41, 44, 45, 49, 52, 54, 56, 61, 65, 67, 70, 84, 85, 96, 98, 100, 128, 129, 137, 138, 147, 148, 155, 157, 164, 165, 313, 314, 315, 316, 327, 329, 335, 342, 348, 362, 371], "register_buff": [355, 358], "register_us": 168, "registr": [35, 39, 42, 66, 78, 79], "registration_complet": 168, "registri": [126, 127, 138, 144, 147, 150, 362, 371], "regress": [7, 14, 253, 257, 349, 354], "regular": [2, 88, 89, 175, 178, 224, 227, 342, 348, 362, 365, 371], "reimplement": [238, 240], "reinforc": [8, 191, 195], "rel": [0, 5, 355, 361], "rel_path": [118, 121], "relat": [43, 51, 53, 57, 64, 88, 89, 168], "relationship": [7, 14, 17, 19, 22, 253, 257], "releas": [1, 28, 30, 43, 45, 51, 53, 56, 64, 137, 138, 169, 170, 224, 237, 268, 289, 291, 294, 336, 341], "relev": [5, 9, 10, 15, 118, 121, 198, 204, 205, 206, 215, 342, 348, 355, 359, 362, 365], "reli": [8, 11, 191, 192, 195, 197, 218, 220, 238, 239, 349, 351, 362, 364], "reliabl": [5, 6, 8, 10, 90, 91, 92, 93, 118, 120, 122, 125, 126, 127, 128, 132, 191, 195, 206, 208, 224, 225, 245, 246, 252, 259, 261, 362, 367], "religi": [268, 289], "religion": [268, 289], "reload": [11, 118, 121, 218, 223, 224, 237, 245, 248, 249, 342, 348], "relpath": [118, 121], "relu": [13, 137, 140, 329, 332, 336, 339], "remain": [110, 112, 159, 163, 224, 227, 234, 268, 287, 289, 290, 329, 331, 342, 344, 355, 359], "remaind": [342, 344], "remark": [268, 289], "remast": [268, 289], "remateri": [349, 351], "rememb": [88, 89, 90, 91, 92, 93, 110, 116, 224, 234, 291, 298, 329, 335], "remind": [268, 289, 291, 298], "remot": [1, 3, 4, 7, 10, 11, 15, 16, 82, 83, 88, 89, 90, 91, 126, 127, 147, 150, 168, 169, 170, 171, 174, 176, 180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 206, 210, 218, 222, 224, 237, 253, 256, 349, 350, 353, 354, 362, 371], "remote_add": [2, 3, 175, 178, 179, 181, 184, 187, 188], "remote_funct": [2, 175, 179], "remote_path": [10, 11, 206, 213, 218, 222], "remov": [1, 3, 6, 9, 43, 51, 53, 64, 101, 102, 107, 128, 131, 137, 138, 169, 170, 181, 190, 198, 203, 224, 226, 234, 245, 251, 259, 262, 342, 348, 362, 371], "remove_code_output": 0, "remu": [268, 287, 289, 290], "renam": [82, 83, 349, 351, 355, 357], "renew": [24, 27], "rent": [291, 294], "repackag": [336, 337], "repartit": [9, 159, 163, 198, 204, 267, 268, 274, 285, 287, 290], "repeat": [13, 14, 299, 306, 349, 353, 355, 358], "repeatedli": [137, 139, 268, 277, 288], "replac": [28, 29, 30, 31, 35, 36, 38, 40, 41, 43, 44, 45, 47, 48, 53, 54, 56, 57, 59, 60, 62, 66, 67, 69, 72, 75, 76, 82, 83, 86, 87, 92, 93, 101, 102, 105, 107, 118, 121, 159, 163, 238, 239, 240, 243, 268, 289, 291, 298, 329, 335, 336, 337, 341, 342, 348, 355, 361, 362, 371], "replic": [3, 5, 6, 181, 183, 224, 225, 234, 259, 263], "replica": [8, 16, 92, 93, 101, 102, 107, 108, 113, 118, 121, 147, 151, 152, 153, 164, 167, 168, 191, 197, 220, 224, 225, 313, 314, 315, 316, 323, 327, 328, 362, 371], "replica_handle_request": [164, 167, 168], "repo": [0, 43, 46, 48, 53, 58, 60, 66, 73, 76, 101, 102, 108, 118, 121, 126, 127], "repo_id": [118, 121], "report": [6, 7, 8, 12, 14, 16, 191, 193, 225, 226, 227, 228, 236, 238, 240, 244, 245, 247, 248, 252, 253, 256, 257, 259, 263, 268, 289, 329, 330, 332, 333, 335, 336, 337, 338, 342, 343, 344, 346, 349, 352, 355, 356, 359, 361, 362, 363, 367, 369, 371], "report_metrics_torch": [5, 13], "reportedli": [268, 289], "repositori": [0, 66, 73, 92, 93], "repres": [4, 7, 9, 12, 101, 102, 106, 171, 174, 198, 201, 253, 255, 291, 298, 342, 343, 345], "represent": [101, 102, 104, 106], "reproduc": [1, 169, 170, 284, 292, 300, 324, 329, 331, 342, 344, 349, 350, 351, 362, 365], "republican": [268, 289], "req": [82, 83], "request": [2, 4, 8, 10, 11, 12, 16, 24, 26, 27, 101, 102, 104, 105, 107, 113, 116, 118, 121, 122, 123, 128, 136, 147, 148, 150, 164, 167, 171, 172, 174, 175, 180, 182, 188, 191, 197, 206, 213, 218, 219, 220, 221, 222, 224, 237, 309, 315, 325, 327, 355, 357, 361, 362, 371], "request_data": 12, "requir": [0, 3, 5, 6, 8, 9, 10, 11, 15, 17, 19, 20, 21, 22, 24, 26, 27, 37, 39, 46, 52, 58, 65, 68, 74, 78, 79, 84, 85, 86, 87, 90, 91, 96, 98, 100, 105, 107, 110, 112, 113, 114, 117, 122, 123, 126, 127, 128, 129, 135, 137, 138, 144, 147, 148, 168, 181, 187, 188, 191, 192, 195, 198, 204, 206, 214, 215, 218, 220, 221, 224, 225, 231, 238, 239, 245, 248, 259, 260, 261, 268, 280, 284, 289, 292, 299, 300, 304, 305, 324, 329, 330, 336, 337, 342, 344, 355, 357, 362, 363, 367, 369], "rerun": [128, 130, 349, 354], "res18": [238, 244], "resampl": 356, "rescal": [329, 335], "research": [110, 112, 117, 118, 122, 267, 268, 285, 299, 305], "reserv": [2, 3, 4, 5, 6, 7, 8, 9, 10, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 101, 102, 103, 110, 111, 118, 119, 126, 127, 155, 156, 159, 160, 164, 165, 171, 172, 175, 176, 181, 182, 187, 191, 192, 198, 199, 206, 207, 253, 254, 259, 260, 267, 268, 285, 291, 293, 299, 301, 309, 316, 325], "reset": [13, 128, 136, 137, 143, 146, 147, 154, 224, 228, 336, 338], "reshap": [6, 10, 206, 208, 259, 262], "resid": [355, 361], "residu": [355, 356], "resili": [92, 93, 110, 115, 245, 246, 248, 252, 336, 337, 349, 350, 355, 356], "resiz": [128, 133, 136, 137, 146, 363], "resnet": [13, 225, 226, 231, 237, 329, 335, 362, 363], "resnet18": [5, 7, 13, 14, 224, 226, 227, 234, 236, 253, 254, 256, 257, 362, 364, 367, 371], "resolut": [6, 259, 262, 263], "resolv": [5, 53, 63], "resourc": [5, 6, 7, 8, 11, 12, 13, 14, 16, 18, 19, 22, 23, 24, 25, 26, 27, 33, 34, 36, 37, 42, 50, 51, 52, 57, 62, 63, 64, 65, 68, 78, 79, 80, 81, 84, 85, 88, 89, 90, 91, 92, 93, 96, 98, 99, 104, 105, 112, 124, 126, 127, 128, 130, 132, 137, 144, 147, 149, 151, 155, 157, 164, 166, 182, 188, 190, 191, 196, 197, 208, 218, 220, 221, 224, 227, 230, 235, 237, 238, 244, 245, 248, 250, 253, 257, 259, 263, 267, 268, 285, 291, 296, 299, 301, 305, 306, 336, 337, 341, 355, 356, 362, 363], "resources_per_work": [137, 143, 299, 305, 349, 352], "resp": [164, 167], "respect": [291, 297, 298], "respond": [118, 121], "respons": [11, 16, 24, 27, 84, 85, 86, 87, 101, 102, 104, 106, 108, 110, 114, 115, 118, 120, 121, 122, 123, 147, 150, 164, 167, 168, 218, 222, 309, 316, 320, 325, 328], "response_format": [118, 122], "rest": [43, 51, 53, 64, 224, 227, 238, 240, 245, 247, 291, 298, 309, 316, 325, 342, 344, 362, 364], "restart": [1, 84, 85, 86, 87, 118, 121, 128, 129, 136, 137, 138, 146, 147, 148, 153, 154, 169, 170, 224, 226, 245, 246, 248, 249, 316, 320, 328, 336, 340, 341, 342, 346, 349, 350], "restor": [224, 236, 246, 247, 248, 252, 291, 298, 329, 330, 335, 342, 347, 348], "restored_train": [245, 250], "restrict": [94, 95], "result": [1, 4, 7, 10, 12, 14, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 88, 89, 90, 91, 118, 123, 128, 130, 137, 143, 147, 154, 169, 170, 171, 174, 176, 180, 182, 183, 184, 190, 206, 208, 225, 226, 227, 234, 235, 237, 245, 250, 252, 253, 257, 267, 268, 285, 291, 297, 298, 299, 304, 305, 306, 313, 314, 315, 316, 320, 327, 328, 329, 333, 334, 336, 337, 340, 342, 344, 346, 347, 348, 349, 352, 354, 359, 360, 362, 364, 368, 369, 370, 371], "resum": [5, 6, 126, 127, 128, 132, 224, 225, 234, 246, 247, 248, 249, 252, 259, 261, 330, 333, 335, 336, 337, 340, 343, 346, 348, 349, 350, 352, 354, 359, 361, 362, 363, 367, 370], "resume_from_checkpoint": [342, 347], "retain": [88, 89, 159, 162, 349, 352, 355, 359], "retent": [291, 298, 362, 363], "rethink": [291, 298], "retrain": [336, 341, 342, 343, 349, 354, 355, 361, 362, 371], "retri": [5, 6, 10, 137, 144, 182, 206, 212, 224, 225, 246, 249, 250, 252, 259, 261, 329, 333, 336, 337, 342, 343, 346, 349, 352, 362, 363, 367, 368], "retriev": [2, 3, 128, 136, 175, 179, 181, 190, 224, 237, 238, 241, 342, 346, 349, 352], "retrigg": [147, 154], "retry_except": [3, 181, 185], "return": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 82, 83, 88, 89, 90, 91, 118, 122, 123, 128, 130, 131, 136, 137, 139, 140, 141, 143, 146, 147, 149, 150, 159, 163, 164, 166, 169, 170, 171, 174, 175, 178, 179, 180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 198, 202, 206, 212, 213, 215, 218, 222, 223, 224, 227, 228, 231, 232, 233, 236, 237, 238, 241, 243, 245, 250, 253, 255, 257, 259, 262, 268, 275, 276, 277, 280, 288, 289, 291, 298, 299, 303, 304, 313, 314, 315, 316, 327, 329, 331, 332, 335, 336, 338, 339, 341, 342, 343, 344, 345, 349, 351, 352, 353, 354, 355, 357, 358, 361, 362, 365, 366, 371], "return_tensor": [128, 131, 147, 149], "reus": [3, 10, 15, 17, 22, 101, 102, 105, 181, 183, 206, 213, 224, 226, 237, 268, 277, 288, 349, 351, 353, 354, 355, 361], "reusabl": [355, 356], "reveal": [342, 344], "reveng": [291, 297], "revers": [147, 150, 349, 353, 355, 357, 362, 371], "review": [10, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 86, 87, 206, 210, 291, 293, 296, 297, 298, 299, 301, 306], "rewrit": [5, 6, 224, 225, 259, 261, 342, 348, 362, 363], "rf": [4, 5, 6, 7, 9, 10, 12, 13, 15, 16, 171, 174, 198, 203, 205, 206, 217, 253, 258, 259, 264], "rg_idx": [362, 365], "rg_meta": [362, 365], "rgb": [128, 131, 136, 147, 149, 224, 227, 237, 329, 330, 331, 362, 363, 365, 371], "rice": [362, 363], "rich": [8, 82, 83, 191, 195], "richer": [336, 341], "rick": [268, 289], "ricki": [268, 289], "ride": [4, 12, 171, 174, 291, 297, 355, 356, 361], "ridicul": [291, 298], "ridlei": [291, 298], "rifl": [291, 297], "riget": [291, 298], "right": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 16, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 82, 83, 101, 102, 103, 105, 110, 111, 118, 119, 124, 125, 128, 129, 137, 138, 147, 148, 155, 156, 159, 160, 164, 165, 171, 172, 175, 176, 180, 181, 182, 191, 192, 198, 199, 206, 207, 211, 218, 222, 224, 227, 234, 238, 241, 253, 254, 259, 260, 267, 268, 285, 289, 291, 293, 297, 298, 299, 301, 309, 316, 325, 336, 337], "rightarrow": [349, 350], "rigid": [8, 191, 194], "rip": [291, 297], "rise": [362, 369], "risibl": [291, 294], "risk": [291, 298], "riskbr": [291, 298], "river": [268, 289, 291, 298], "riverboat": [291, 297], "rkn": [159, 163], "rllib": [8, 191, 195], "rm": [4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 16, 28, 33, 35, 42, 43, 51, 53, 64, 66, 78, 171, 174, 198, 203, 205, 206, 217, 218, 223, 253, 258, 259, 264], "rmse": [4, 7, 14, 171, 174, 253, 257, 342, 348], "rmtree": [128, 133, 137, 139, 142, 245, 251, 329, 335, 336, 341, 342, 344, 348, 349, 354, 355, 361, 362, 371], "road": [349, 350], "roadmap": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 101, 102, 103, 110, 111, 118, 119, 164, 165, 171, 172, 175, 176, 181, 182, 191, 192, 198, 199, 206, 207, 218, 219, 224, 227, 253, 254, 259, 260], "roar": [291, 297], "robert": [291, 297], "robin": [24, 27, 268, 289], "robot": [336, 341], "robust": [0, 8, 118, 122, 191, 192, 245, 249, 252, 336, 337, 355, 361, 362, 363], "rock": [268, 287, 289], "role": [21, 23, 24, 26, 27, 28, 30, 34, 35, 37, 43, 45, 56, 63, 66, 68, 86, 87, 101, 102, 108, 110, 114, 115, 118, 121, 122, 123, 159, 162, 291, 298], "roll": [92, 93, 110, 115, 164, 167, 268, 289, 336, 338], "rollin": [268, 289], "rollout": [11, 147, 151, 153, 164, 167, 218, 220, 336, 337], "roma": [268, 289], "roman": [291, 297], "ronda": [291, 297], "roof": [268, 289], "root": [2, 5, 6, 7, 13, 14, 86, 87, 118, 121, 155, 158, 164, 167, 168, 175, 180, 224, 225, 226, 232, 253, 255, 257, 259, 261, 342, 348], "roughli": [329, 331, 336, 338, 362, 364], "round": [24, 27, 268, 289, 291, 298, 342, 344, 349, 350, 352, 354], "rout": [3, 11, 16, 17, 22, 24, 26, 27, 101, 102, 106, 168, 181, 190, 218, 220, 315, 316, 327], "route_prefix": [4, 16, 92, 93, 118, 122, 147, 150, 171, 174], "row": [5, 6, 128, 130, 131, 137, 139, 146, 164, 166, 201, 210, 212, 224, 226, 236, 237, 238, 242, 243, 259, 262, 267, 268, 272, 273, 274, 285, 287, 289, 290, 291, 293, 294, 295, 296, 297, 298, 329, 331, 333, 342, 343, 344, 346, 348, 349, 350, 351, 353, 355, 357, 361, 362, 363, 365, 369, 371], "row_group": [362, 365], "row_group_idx": [362, 365], "row_group_map": [362, 365], "royal": [268, 289], "rpc": [3, 8, 181, 190, 191, 192], "rsplit": [128, 130, 137, 139], "rstrip": [86, 87], "rubbish": [291, 298], "rubbl": [291, 297, 298], "rube": [291, 297], "ruin": [291, 298], "rule": [8, 17, 22, 84, 85, 118, 121, 191, 197, 268, 289], "rumor": [268, 289], "run": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 19, 21, 25, 27, 28, 30, 31, 32, 33, 34, 40, 41, 42, 43, 45, 46, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 61, 62, 64, 65, 66, 69, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 96, 98, 100, 101, 102, 103, 106, 108, 111, 112, 114, 116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 132, 134, 135, 137, 138, 144, 147, 148, 150, 151, 153, 155, 157, 158, 159, 161, 162, 163, 165, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 181, 182, 184, 186, 190, 191, 196, 198, 199, 200, 201, 203, 205, 206, 207, 208, 211, 213, 217, 218, 219, 220, 221, 222, 223, 225, 226, 227, 228, 229, 230, 232, 234, 235, 236, 238, 239, 243, 244, 245, 246, 249, 250, 251, 252, 253, 254, 256, 257, 258, 260, 262, 264, 267, 277, 285, 288, 289, 291, 293, 297, 298, 299, 301, 304, 305, 306, 309, 316, 320, 323, 325, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 340, 341, 342, 343, 346, 347, 348, 349, 350, 352, 354, 356, 359, 360, 363, 365, 367, 368, 369, 370], "run_command": [82, 83], "run_config": [4, 5, 6, 12, 13, 171, 174, 224, 234, 235, 238, 244, 245, 248, 250, 259, 263, 329, 333, 336, 340, 342, 346, 349, 352, 355, 359, 362, 368], "run_id": [137, 144], "runawai": [268, 289], "runconfig": [4, 5, 6, 12, 13, 171, 172, 174, 225, 226, 238, 244, 245, 248, 250, 252, 259, 263, 329, 331, 333, 336, 338, 340, 342, 344, 346, 349, 350, 351, 352, 355, 356, 357, 359, 362, 363, 364, 367, 368, 369, 371], "runnabl": [164, 165], "runnam": [137, 144], "runnng": [88, 89], "runtim": [11, 80, 81, 84, 85, 118, 120, 121, 126, 127, 128, 129, 137, 138, 147, 148, 164, 167, 182, 218, 221, 299, 302, 316, 320, 327, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364, 372], "runtime_env": [3, 110, 113, 116, 118, 121, 122, 123, 128, 129, 137, 138, 147, 148, 181, 186, 187, 299, 302], "runtimeenv": [299, 302], "runwai": [9, 15, 198, 205], "ruse": [291, 297], "rust": [8, 191, 192], "ruth": [291, 297], "rw": 13, "ryan": [268, 289], "s3": [4, 8, 9, 10, 11, 12, 13, 14, 15, 16, 21, 23, 24, 26, 28, 30, 33, 34, 43, 45, 51, 53, 56, 57, 63, 64, 86, 87, 118, 121, 128, 130, 136, 137, 139, 146, 147, 150, 153, 164, 166, 171, 174, 191, 192, 198, 201, 203, 204, 206, 210, 212, 213, 215, 218, 222, 224, 234, 245, 252, 329, 335, 342, 348, 362, 371], "s3_bucket_id": [17, 23, 28, 30], "s3_f": [164, 166], "s3_kei": [86, 87, 118, 121], "s3_path": [6, 259, 262], "s3f": [6, 259, 260, 262], "s3filesystem": [6, 164, 166, 259, 262], "s5": [101, 102, 105], "s6": [101, 102, 105, 268, 289], "s7": [101, 102, 105], "s_": [336, 337], "s_k": [336, 337], "saatchi": [268, 289], "sacrif": [336, 337], "safe": [28, 30, 43, 45, 53, 56, 224, 226, 245, 246, 268, 289, 329, 331, 355, 359, 362, 367], "safetensor": [118, 121], "safeti": [118, 122], "sagemak": [128, 130, 147, 151], "sai": [3, 17, 22, 90, 91, 181, 184, 268, 289, 291, 297, 298], "said": [268, 289, 291, 294], "sake": [9, 198, 202], "salad": [362, 363], "sam": [268, 289], "samara": [128, 136, 147, 150, 153], "same": [1, 3, 4, 5, 6, 7, 9, 11, 12, 13, 14, 16, 24, 26, 28, 30, 43, 45, 53, 56, 86, 87, 90, 91, 101, 102, 108, 110, 115, 117, 118, 125, 126, 127, 128, 131, 137, 139, 146, 169, 170, 171, 174, 181, 186, 198, 202, 204, 218, 221, 222, 224, 227, 230, 231, 232, 234, 238, 240, 243, 244, 245, 247, 250, 252, 253, 257, 259, 262, 263, 268, 289, 290, 291, 294, 298, 329, 330, 331, 333, 336, 341, 342, 348, 349, 352, 354, 355, 361, 362, 365, 369, 371], "sampl": [4, 5, 6, 7, 12, 14, 15, 16, 17, 22, 28, 30, 34, 35, 39, 43, 45, 46, 53, 56, 58, 66, 70, 73, 86, 87, 137, 141, 155, 157, 171, 174, 225, 228, 229, 237, 253, 257, 259, 262, 291, 294, 298, 315, 316, 327, 331, 338, 342, 348, 349, 351, 355, 357, 362, 363, 364, 365, 371], "sample_act": [336, 341], "sample_batch": [12, 137, 141, 349, 353], "sample_count": [88, 89], "sample_idx": [5, 224, 226], "sample_imag": [329, 335], "sample_s": [6, 259, 262], "sampler": [5, 6, 13, 224, 225, 228, 238, 240, 259, 263, 362, 367], "samsara": 16, "samsung": [268, 289], "san": [118, 123, 268, 289], "sander": [268, 289], "saniti": [224, 226, 235, 333, 336, 340, 342, 344, 349, 351], "sat": [268, 289], "satisfi": [3, 181, 183], "satur": [159, 163], "saturdai": [268, 289], "save": [3, 5, 6, 53, 63, 84, 85, 86, 87, 88, 89, 101, 102, 106, 118, 121, 126, 127, 128, 130, 132, 133, 137, 138, 139, 140, 142, 143, 181, 187, 225, 227, 228, 236, 238, 240, 244, 246, 247, 250, 252, 259, 263, 291, 297, 329, 330, 331, 332, 335, 336, 337, 340, 341, 342, 346, 347, 348, 349, 350, 352, 354, 355, 356, 359, 362, 363, 364, 367, 371], "save_checkpoint_and_metrics_ray_train": [5, 13, 224, 228, 234, 238, 240], "save_checkpoint_and_metrics_ray_train_with_extra_st": [245, 247, 248], "save_checkpoint_and_metrics_torch": [5, 13], "save_hyperparamet": [6, 259, 262], "save_last": [329, 333, 336, 340], "save_model": [4, 171, 174], "save_top_k": [329, 333, 336, 340], "saw": [268, 289], "sayhellodebuglog": [164, 167], "sayhellodefaultlog": [164, 167], "scaffold": [11, 218, 223], "scala": [8, 191, 195], "scalabl": [4, 8, 9, 11, 12, 16, 17, 22, 86, 87, 90, 91, 92, 93, 96, 99, 100, 104, 109, 110, 117, 118, 125, 126, 127, 128, 130, 132, 147, 151, 153, 171, 173, 191, 192, 195, 196, 198, 200, 218, 219, 221, 238, 239, 245, 252, 267, 268, 283, 285, 290, 291, 293, 298, 299, 305, 306, 309, 316, 325, 329, 330, 336, 341, 342, 343, 344, 349, 350, 353, 354, 355, 356, 361, 362, 363, 365], "scalar": [3, 181, 190, 329, 332, 336, 337, 339], "scale": [1, 2, 3, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 22, 24, 25, 26, 27, 43, 50, 53, 62, 63, 79, 84, 85, 88, 89, 92, 93, 96, 99, 100, 101, 102, 103, 106, 107, 112, 113, 115, 118, 124, 126, 127, 128, 132, 137, 138, 143, 144, 147, 149, 151, 153, 159, 163, 169, 170, 175, 177, 181, 187, 191, 192, 195, 197, 198, 201, 205, 206, 212, 213, 218, 221, 225, 226, 227, 232, 235, 238, 239, 243, 245, 246, 248, 250, 252, 253, 257, 261, 262, 267, 283, 285, 290, 291, 293, 299, 301, 305, 309, 323, 325, 328, 329, 330, 331, 335, 338, 342, 343, 344, 345, 348, 349, 350, 351, 354, 355, 356, 357, 361, 362, 363, 364, 365, 371], "scaling_config": [4, 5, 6, 12, 13, 137, 143, 171, 174, 224, 230, 235, 238, 244, 245, 248, 250, 259, 263, 299, 305, 329, 333, 336, 340, 342, 346, 349, 352, 355, 359, 362, 368], "scalingconfig": [4, 5, 6, 12, 13, 137, 143, 171, 174, 225, 226, 245, 252, 259, 263, 299, 302, 305, 329, 330, 331, 333, 336, 337, 338, 340, 342, 343, 344, 346, 349, 350, 351, 352, 355, 356, 357, 359, 362, 364, 368], "scan": [291, 297], "scari": [291, 298], "scenario": [3, 8, 16, 17, 22, 28, 30, 43, 45, 53, 56, 110, 112, 117, 181, 187, 189, 191, 192, 299, 304, 362, 363], "scene": [291, 294, 298, 342, 343], "schedul": [2, 3, 4, 5, 6, 7, 9, 10, 12, 14, 24, 26, 88, 89, 101, 102, 107, 128, 132, 135, 137, 143, 147, 151, 171, 174, 175, 177, 179, 180, 181, 187, 188, 190, 198, 203, 206, 212, 213, 253, 257, 259, 262, 268, 289, 299, 306, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 363, 371], "schema": [8, 9, 118, 122, 123, 191, 192, 198, 201, 268, 287, 290, 291, 295, 296, 329, 331, 342, 344, 355, 357], "schemat": [5, 6, 224, 225, 259, 263], "schlong": [291, 294], "school": [268, 289], "schumer": [268, 289], "scienc": [8, 191, 195], "scientif": [8, 118, 124, 191, 195, 336, 338, 349, 351], "scikit": [309, 316, 325, 349, 351], "scipt": [164, 167], "scope": [17, 19, 86, 87, 96, 98, 159, 162], "score": [118, 121, 268, 289, 316, 320, 328, 329, 333, 335, 342, 343, 344, 348, 349, 352, 354, 355, 357, 359], "scoreless": [268, 287, 289], "scott": [291, 298], "scotu": [268, 289], "scratch": [7, 14, 28, 30, 43, 45, 53, 56, 128, 130, 253, 257, 329, 333, 336, 340, 362, 363, 371], "screen": [80, 81, 82, 83, 94, 95, 291, 298], "script": [0, 82, 83, 88, 89, 92, 93, 96, 98, 118, 121, 159, 163, 164, 167, 329, 335, 336, 341, 349, 350, 362, 363], "scroll": [53, 63], "scrumptiou": [28, 30, 43, 45, 53, 56], "sdk": [35, 37, 66, 68, 69, 90, 91, 126, 127, 128, 135], "sea": [268, 289], "seaborn": [349, 351], "seal": [291, 298], "seamless": [8, 86, 87, 101, 102, 107, 118, 123, 191, 192, 195, 196, 291, 293, 336, 337, 341, 355, 360, 361], "seamlessli": [1, 8, 9, 100, 126, 127, 169, 170, 191, 196, 198, 205, 238, 240, 245, 248, 267, 268, 285, 329, 330, 336, 337, 342, 348, 349, 350, 354, 355, 356, 357, 362, 363], "search": [4, 7, 12, 14, 17, 22, 88, 89, 126, 127, 128, 133, 171, 174, 253, 254, 257, 329, 335, 336, 341, 349, 354, 355, 361, 362, 371], "search_alg": [7, 14, 253, 257], "search_run": [137, 144, 147, 150], "season": [355, 357], "seattl": [291, 297], "second": [0, 2, 3, 7, 12, 14, 53, 57, 66, 69, 110, 113, 147, 151, 164, 166, 167, 175, 180, 181, 184, 253, 257, 268, 280, 289, 293, 349, 354, 362, 363, 370], "secondarili": [10, 15, 206, 214], "secret": [17, 22, 268, 289, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "section": [0, 7, 14, 17, 20, 21, 24, 26, 82, 83, 84, 85, 88, 89, 128, 131, 155, 157, 158, 164, 166, 167, 224, 226, 253, 256], "secur": [19, 21, 23, 24, 26, 28, 30, 34, 35, 39, 53, 63, 79, 96, 99, 101, 102, 106, 107, 110, 115, 118, 125], "security_group_descript": [17, 22], "security_group_id": [17, 23, 28, 30], "security_group_nam": [17, 22], "securitygroup": [17, 21, 22], "sedan": [118, 122], "see": [1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 22, 24, 25, 28, 30, 32, 35, 39, 41, 43, 45, 50, 53, 56, 62, 63, 66, 69, 77, 86, 87, 92, 93, 94, 95, 96, 98, 101, 102, 106, 108, 110, 113, 115, 118, 121, 122, 123, 128, 134, 137, 138, 139, 143, 144, 147, 152, 155, 158, 159, 163, 164, 166, 167, 169, 170, 175, 180, 181, 184, 190, 198, 203, 204, 205, 206, 215, 217, 218, 223, 224, 228, 230, 235, 236, 245, 246, 253, 256, 258, 259, 262, 263, 268, 284, 287, 289, 291, 292, 294, 297, 298, 299, 300, 306, 315, 316, 320, 324, 327, 328, 329, 330, 335, 336, 337, 355, 359, 361, 362, 363], "seed": [329, 335, 336, 338, 342, 344, 362, 365], "seek": [329, 331], "seem": [291, 298], "seen": [110, 117, 268, 289, 291, 294, 298], "segment": [15, 17, 22, 355, 361], "seiz": [291, 294], "select": [1, 5, 28, 30, 43, 45, 53, 56, 80, 81, 82, 83, 92, 93, 94, 95, 110, 116, 123, 125, 128, 129, 137, 138, 147, 148, 164, 166, 169, 170, 224, 226, 299, 301, 304, 342, 348, 349, 352, 355, 361, 362, 369], "select_column": [4, 171, 174], "selector": [94, 95], "self": [3, 4, 6, 10, 11, 12, 15, 16, 128, 131, 137, 139, 140, 146, 147, 149, 150, 164, 167, 171, 174, 181, 190, 206, 213, 218, 222, 224, 237, 259, 262, 268, 275, 276, 277, 288, 313, 314, 315, 316, 327, 329, 332, 333, 336, 339, 340, 342, 345, 349, 353, 355, 356, 357, 358, 361, 362, 363, 365, 371], "sell": [291, 297], "semant": [3, 181, 188, 190], "semi": [8, 191, 192], "send": [3, 12, 16, 118, 122, 123, 147, 150, 159, 163, 164, 167, 168, 181, 190, 309, 315, 325, 327], "send_welcome_email": 168, "sens": [4, 7, 12, 14, 171, 172, 253, 257, 291, 297, 298], "sent": [8, 164, 167, 168, 191, 197, 224, 237, 291, 297], "sentenc": [118, 121, 124, 224, 230, 267, 268, 271, 277, 280, 285, 286, 288, 289], "sentence_transform": [268, 271, 286], "sentencetransform": [267, 268, 271, 275, 276, 277, 280, 283, 285, 286, 288, 289, 290], "sentiment": [268, 274, 287, 309, 313, 314, 315, 316, 320, 323, 325, 327, 328], "sep": [342, 344, 348], "separ": [2, 3, 5, 7, 8, 11, 14, 86, 87, 96, 99, 147, 151, 155, 158, 175, 179, 181, 188, 191, 192, 218, 221, 224, 232, 253, 257, 342, 343, 349, 351], "sept": [268, 289], "sequenc": [9, 101, 102, 105, 198, 201, 291, 298, 299, 301, 302, 304, 306, 349, 350, 357, 358], "sequenti": [7, 13, 14, 101, 102, 104, 253, 256, 329, 332, 336, 339, 355, 356], "sequoia": [155, 158], "seri": [79, 147, 152, 357, 361], "serial": [2, 8, 175, 177, 191, 192, 195, 349, 352, 362, 363], "serializ": [329, 331], "series_id": [355, 357], "serious": [291, 294, 298], "serv": [12, 17, 19, 20, 28, 29, 43, 44, 53, 54, 92, 93, 94, 95, 105, 109, 112, 115, 116, 117, 120, 122, 123, 125, 126, 127, 149, 150, 152, 153, 154, 155, 158, 159, 162, 165, 195, 196, 222, 223, 224, 237, 245, 252, 284, 292, 300, 315, 320, 324, 326, 327, 329, 335, 336, 341, 342, 344, 345, 348, 349, 354, 355, 361, 362, 371], "serve_llama": [101, 102, 108], "serve_llama_3_1_70b": [110, 113, 114, 115, 116], "serve_my_lora_app": [118, 121], "serve_my_qwen": [118, 122], "serve_my_qwen3": [118, 123], "serveclass": [313, 314], "server": [0, 101, 137, 144, 155, 158], "serverless": [17, 20, 126, 127], "servic": [7, 8, 17, 21, 22, 24, 26, 27, 28, 30, 35, 36, 38, 39, 43, 45, 53, 56, 63, 66, 69, 70, 73, 78, 84, 85, 86, 87, 88, 89, 94, 95, 96, 98, 101, 102, 105, 108, 111, 114, 116, 117, 126, 127, 148, 150, 151, 152, 154, 164, 167, 168, 191, 197, 219, 220, 253, 258, 284, 292, 300, 309, 316, 324, 325, 372], "service2_6hxismeqf1fkd2h7pfmljmncvm": [147, 153], "serving_01": 372, "serving_02": 372, "serving_03": 372, "serving_04": 372, "serving_05": 372, "serving_06": 372, "serving_07": 372, "session": [86, 87, 96, 98, 126, 127, 168], "session_2024": [12, 13], "session_2025": [299, 306], "session_latest": [155, 158, 164, 167, 168], "set": [0, 2, 3, 5, 6, 7, 10, 11, 12, 13, 16, 17, 18, 21, 24, 27, 28, 30, 34, 35, 38, 39, 43, 45, 46, 48, 52, 53, 56, 58, 60, 65, 66, 69, 70, 76, 80, 81, 84, 85, 92, 93, 96, 97, 111, 116, 117, 119, 121, 122, 126, 127, 128, 133, 135, 137, 139, 142, 144, 147, 150, 151, 153, 156, 164, 167, 175, 180, 181, 186, 187, 206, 212, 218, 222, 223, 224, 225, 226, 227, 230, 232, 235, 237, 245, 246, 253, 255, 257, 259, 261, 262, 263, 268, 289, 291, 296, 297, 298, 299, 301, 304, 305, 306, 315, 316, 327, 329, 331, 342, 344, 346, 349, 352, 355, 356, 359, 362, 363, 365], "set_epoch": [5, 13, 224, 228, 238, 240, 362, 367], "set_experi": [137, 143], "set_float32_matmul_precis": [336, 341], "set_grad_en": [299, 304, 355, 361, 362, 371], "set_index": [355, 357], "set_titl": [7, 13, 14, 253, 255, 329, 331, 362, 364], "set_tracking_uri": [137, 143, 144, 147, 150], "seth": [268, 289], "setup": [6, 8, 12, 17, 18, 20, 24, 25, 26, 43, 51, 53, 64, 79, 84, 85, 94, 95, 98, 99, 100, 110, 112, 116, 117, 118, 123, 126, 127, 137, 144, 157, 164, 165, 167, 191, 195, 224, 227, 235, 238, 244, 245, 247, 248, 250, 259, 262, 263, 284, 292, 300, 301, 306, 324, 333, 337, 344, 346, 351, 354, 357, 364, 372], "seven": [101, 102, 105], "sever": [8, 17, 21, 88, 89, 101, 102, 105, 106, 107, 191, 195, 291, 297, 309, 316, 325], "sevigni": [291, 294], "sex": [268, 289, 291, 294], "sexist": [268, 289], "sg": [17, 22, 28, 30], "sgd": [299, 304], "sh": [1, 43, 44, 53, 55, 66, 68, 169, 170, 329, 331], "shallow": [349, 350], "shame": [291, 298], "shape": [3, 6, 9, 10, 15, 16, 24, 26, 84, 85, 86, 87, 128, 136, 137, 146, 159, 163, 181, 187, 198, 203, 206, 212, 224, 237, 259, 262, 268, 289, 290, 329, 332, 336, 338, 341, 349, 351, 355, 357, 358, 361, 362, 365], "shard": [4, 5, 6, 13, 101, 102, 106, 171, 174, 224, 225, 227, 228, 230, 232, 234, 238, 239, 240, 241, 242, 244, 245, 248, 259, 263, 329, 330, 331, 333, 335, 336, 337, 338, 340, 341, 342, 343, 344, 346, 348, 349, 350, 352, 354, 355, 356, 357, 362, 371], "shard_0": [362, 364, 365], "share": [0, 3, 5, 8, 9, 11, 17, 21, 22, 24, 26, 28, 34, 35, 36, 53, 63, 94, 95, 100, 118, 121, 125, 128, 130, 131, 133, 137, 139, 155, 158, 181, 183, 191, 192, 198, 203, 218, 220, 224, 226, 234, 245, 251, 268, 289, 332, 336, 339, 349, 350, 351, 355, 356, 357, 361, 362, 363], "shared_path": [86, 87], "shared_storag": [86, 87], "sharetea": [268, 289], "she": [268, 289, 291, 294, 297, 298], "sheeran": [268, 289], "shell": [10, 11, 66, 69, 206, 213, 218, 222], "sheriff": [291, 297], "shift": [82, 83, 90, 91, 92, 93, 349, 351, 355, 356, 359], "shine": [268, 289, 291, 297, 298], "shippuden": [268, 289], "shit": [268, 289], "shock": [291, 294], "shoe": [291, 298], "shoot": [268, 289, 291, 297], "shootout": [291, 297], "short": [291, 294, 349, 352, 355, 356], "shorter": [110, 116], "shot": [291, 294, 298], "should": [1, 3, 5, 6, 9, 10, 11, 13, 24, 26, 53, 63, 92, 93, 118, 121, 164, 166, 169, 170, 181, 189, 190, 198, 200, 204, 206, 214, 218, 222, 224, 227, 230, 259, 263, 268, 289, 291, 294, 297, 316, 320, 328, 329, 335, 336, 341, 342, 348, 349, 354, 362, 371], "should_checkpoint": 13, "shouldn": [137, 139], "show": [3, 4, 6, 7, 10, 14, 16, 28, 33, 35, 36, 43, 51, 53, 64, 88, 89, 101, 102, 105, 107, 118, 120, 126, 127, 164, 165, 168, 171, 174, 181, 187, 206, 211, 213, 214, 224, 225, 226, 234, 235, 236, 237, 238, 240, 253, 257, 259, 262, 263, 267, 268, 272, 273, 274, 280, 284, 285, 287, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 324, 329, 331, 333, 335, 336, 337, 340, 342, 344, 346, 349, 351, 353, 355, 357, 359, 361, 362, 363, 364, 369, 371], "showcas": [118, 119, 268, 280, 284, 289, 291, 292, 294, 298, 300, 309, 316, 324, 325], "shown": [164, 167, 224, 226, 291, 294], "shuffl": [5, 6, 7, 13, 14, 128, 130, 137, 139, 199, 207, 224, 228, 232, 238, 239, 240, 241, 245, 252, 253, 255, 257, 259, 262, 263, 299, 302, 304, 336, 338, 342, 344, 349, 354, 355, 357, 359, 362, 365, 366, 367], "shut": [116, 224, 234, 267, 268, 285, 291, 293, 299, 301], "shutdown": [12, 16, 110, 114, 116, 118, 121, 122, 123, 301, 309, 320, 325], "shutil": [128, 133, 137, 139, 142, 224, 226, 245, 251, 329, 331, 335, 336, 338, 341, 342, 344, 348, 349, 351, 354, 355, 357, 361, 362, 364, 371], "sick": [268, 289], "sid": [17, 22], "side": [8, 191, 196, 291, 297, 298, 329, 335, 342, 348, 362, 371], "sidebar": 0, "sidecar": [101, 102, 106], "sidestep": [329, 330], "sidewalk": [291, 297], "sight": [291, 294], "sign": [10, 11, 155, 157, 164, 165, 206, 213, 218, 222, 268, 289], "signal": [159, 161, 355, 357], "signatur": [6, 7, 14, 253, 257, 259, 263], "signifi": [9, 198, 201], "signific": [5, 6, 8, 101, 102, 106, 191, 196, 224, 225, 259, 261, 267, 268, 285], "significantli": [84, 85, 159, 162], "signup": 100, "silicon": [1, 169, 170, 267, 268, 280, 285, 289, 299, 301, 304], "silu": [6, 259, 262], "sim": [329, 330, 336, 337], "similar": [4, 12, 24, 27, 110, 113, 114, 155, 158, 164, 166, 171, 174, 329, 335], "similarli": [3, 8, 181, 190, 191, 193, 315, 316, 327, 342, 344], "simpl": [2, 4, 5, 6, 7, 8, 9, 11, 12, 14, 16, 82, 83, 86, 87, 92, 93, 94, 95, 101, 102, 105, 108, 109, 110, 112, 118, 124, 137, 139, 140, 165, 171, 174, 175, 177, 178, 191, 197, 198, 200, 202, 218, 222, 224, 226, 229, 235, 237, 253, 256, 257, 259, 262, 284, 291, 292, 297, 298, 300, 309, 313, 314, 315, 316, 324, 325, 327, 329, 330, 335, 342, 343, 344, 345, 348, 362, 363, 371], "simple_pipelin": [164, 166], "simpler": [110, 117, 118, 121], "simpli": [5, 147, 154, 309, 316, 325, 349, 354, 362, 370], "simplifi": [15, 35, 39, 80, 81, 100, 362, 363], "simul": [3, 168, 181, 185, 309, 325, 336, 337, 341, 342, 344, 362, 363], "simultan": [101, 102, 104, 159, 163], "sin": [336, 337, 338, 341, 355, 358], "sinc": [3, 9, 86, 87, 101, 102, 105, 137, 139, 159, 163, 164, 167, 181, 187, 198, 201, 224, 227, 234, 268, 289, 291, 298, 329, 331, 342, 347], "sing": [268, 289], "singl": [1, 3, 8, 9, 10, 12, 16, 101, 102, 104, 105, 106, 110, 112, 117, 118, 120, 121, 125, 147, 149, 153, 164, 167, 169, 170, 181, 187, 191, 195, 198, 201, 206, 211, 224, 225, 226, 228, 237, 245, 251, 260, 263, 268, 289, 291, 293, 298, 329, 330, 331, 336, 340, 342, 343, 349, 350, 351, 354, 355, 356, 362, 363, 364, 365], "single_gpu_mnist": 13, "sink": [4, 10, 171, 174, 206, 209], "sinusoid": [355, 358], "sisterlif": [268, 289], "sit": [268, 287, 289, 291, 294], "site": [0, 17, 22, 86, 87, 137, 144, 291, 294], "situat": [291, 298], "six": [101, 102, 105], "size": [5, 7, 9, 10, 13, 14, 80, 81, 84, 85, 88, 89, 101, 102, 105, 106, 109, 114, 116, 118, 121, 147, 151, 159, 163, 164, 167, 198, 202, 206, 208, 212, 224, 226, 227, 228, 229, 253, 255, 268, 274, 280, 287, 289, 291, 297, 299, 304, 305, 329, 332, 333, 335, 336, 338, 341, 342, 348, 352, 355, 357, 358, 361, 362, 364], "size_in_byt": [3, 181, 183], "sj": [268, 287, 289, 290], "skagwai": [291, 297], "skew": [10, 206, 208, 342, 344, 349, 351], "skill": [118, 121], "skip": [329, 331, 349, 350, 351, 362, 371], "sklearn": [4, 137, 146, 171, 172, 349, 351], "skylynn": [268, 289], "sla": [126, 127], "slack": [164, 167], "sleazi": [291, 297], "sleep": [2, 3, 164, 166, 175, 180, 181, 184, 189, 291, 298], "slice": [4, 171, 174, 329, 330, 342, 344, 349, 350, 352, 362, 363, 365], "slick": [291, 298], "slide": [268, 289, 356], "slim": [84, 85], "slip": [291, 297], "slo": [101, 102, 105], "slope": [349, 350], "slot": [224, 226, 355, 356], "slow": [11, 110, 112, 218, 220, 267, 268, 285], "slow_adjust_total_amount": [164, 166], "slower": [291, 296], "slowest": [9, 10, 15, 198, 204, 206, 215], "slowli": [291, 298], "slowyourrol": [268, 289], "sm": [268, 289], "small": [8, 9, 10, 15, 16, 88, 89, 101, 102, 106, 110, 112, 117, 118, 121, 124, 128, 130, 191, 192, 198, 201, 202, 206, 211, 215, 329, 330, 336, 341, 342, 344, 348, 355, 361, 362, 363], "small_siz": [299, 304], "small_unet_model_config": [6, 259, 262], "smaller": [0, 3, 17, 22, 118, 121, 124, 159, 163, 181, 187], "smallest": [80, 81], "smart": [101, 102, 107, 291, 297], "smith": [268, 287, 289, 290], "smoke": [245, 252], "smoothl1": [355, 359], "smoothl1loss": [355, 359], "smoothli": [291, 293], "sn": [349, 351, 353], "snake": [268, 289], "snap": [268, 287, 289], "snapshot": [86, 87, 329, 334, 349, 354], "snapshot_download": [118, 121], "snicker": [291, 298], "snippet": [5, 6, 13, 82, 83, 86, 87, 259, 263], "snowflak": [8, 10, 191, 192, 206, 210], "so": [0, 6, 7, 9, 13, 14, 80, 81, 82, 83, 84, 85, 88, 89, 92, 93, 118, 124, 126, 127, 128, 130, 135, 137, 139, 144, 146, 147, 151, 155, 157, 159, 163, 198, 202, 203, 224, 225, 226, 227, 232, 233, 234, 235, 237, 238, 239, 240, 242, 243, 245, 246, 247, 248, 250, 253, 257, 259, 262, 268, 287, 289, 291, 294, 297, 298, 309, 316, 325, 329, 332, 336, 339, 342, 344, 349, 351, 352, 355, 356, 357, 359, 362, 363, 365, 371], "socket": [8, 191, 195], "softbal": [268, 289], "softmax": [137, 140, 349, 350], "softprob": [349, 352], "softwar": [110, 114, 118, 121, 155, 157], "soil": [349, 350, 353], "sole": [17, 22, 291, 298], "solid": [342, 348], "solut": [2, 3, 5, 6, 7, 8, 10, 11, 13, 14, 15, 17, 21, 101, 102, 103, 107, 109, 118, 125, 126, 127, 128, 130, 175, 180, 181, 190, 191, 196, 206, 208, 212, 218, 220, 224, 225, 238, 239, 253, 257, 259, 261, 263, 299, 305, 355, 361], "solv": [101, 102, 106, 118, 124], "some": [4, 5, 6, 7, 8, 9, 10, 13, 15, 53, 63, 86, 87, 88, 89, 101, 102, 105, 110, 114, 118, 120, 122, 147, 154, 159, 162, 163, 164, 165, 171, 174, 191, 194, 195, 198, 204, 205, 206, 210, 211, 215, 253, 258, 259, 264, 268, 289, 291, 294, 295, 296, 297, 298], "someth": [3, 43, 47, 53, 59, 66, 75, 181, 184, 268, 289, 291, 297, 362, 363], "sometim": [3, 28, 30, 43, 45, 53, 56, 181, 187, 291, 298], "somewher": [3, 181, 190], "song": [268, 289], "sonnet": [118, 124], "soon": [3, 181, 189, 268, 289], "sophist": [7, 8, 14, 118, 120, 123, 125, 191, 196, 253, 257], "sorri": [268, 287, 289, 290], "sort": [8, 9, 86, 87, 137, 144, 147, 150, 191, 192, 198, 204, 291, 294, 342, 343, 344, 348, 349, 353, 362, 371], "sort_index": [342, 346, 355, 359, 362, 369], "sorted_prob": [147, 150], "sorted_run": [137, 144, 147, 150], "soul": [268, 287, 289], "sound": [268, 289, 291, 298], "soup": [268, 289], "sourc": [0, 1, 2, 10, 17, 22, 66, 69, 100, 128, 130, 132, 137, 144, 155, 158, 159, 162, 169, 170, 175, 177, 206, 210, 238, 239, 245, 252, 291, 295, 297, 298], "south": [268, 289], "sox": [268, 289], "space": [3, 4, 7, 8, 14, 17, 22, 171, 174, 181, 189, 191, 193, 245, 251, 253, 257, 291, 297, 329, 335, 336, 341, 342, 343, 348, 349, 354, 355, 361, 362, 371], "span": [168, 268, 289, 355, 357], "spark": [9, 128, 130, 195, 198, 205], "spatial": [349, 350], "spawn": [224, 235, 237], "speak": [291, 298], "speci": [349, 354], "special": [8, 9, 110, 117, 118, 120, 121, 124, 125, 191, 192, 198, 203, 268, 287, 289, 291, 297], "specif": [3, 6, 9, 15, 24, 27, 35, 36, 39, 66, 70, 84, 85, 86, 87, 88, 89, 94, 95, 101, 102, 107, 110, 113, 118, 124, 128, 134, 137, 144, 147, 149, 151, 152, 155, 157, 158, 159, 161, 164, 167, 176, 177, 181, 187, 190, 198, 204, 215, 224, 235, 237, 259, 262, 263, 329, 330, 335, 349, 353, 354, 362, 365], "specifi": [1, 3, 5, 6, 7, 9, 10, 11, 13, 14, 16, 24, 26, 80, 81, 82, 83, 84, 85, 92, 93, 94, 95, 118, 121, 122, 128, 135, 137, 143, 145, 147, 149, 153, 168, 169, 170, 181, 185, 187, 198, 201, 206, 212, 213, 218, 221, 223, 224, 231, 238, 244, 245, 250, 253, 257, 259, 263, 299, 301, 304, 305, 306, 313, 314, 315, 316, 327, 342, 346], "specific": [224, 227], "speed": [4, 5, 6, 7, 8, 11, 14, 16, 86, 87, 118, 124, 128, 130, 171, 174, 191, 192, 218, 220, 224, 225, 253, 256, 259, 261, 291, 293, 298, 329, 331, 362, 364, 371], "speedup": [267, 268, 285], "speific": [88, 89], "spend": [291, 294], "spike": [101, 102, 106, 107, 110, 115], "spiki": [80, 81, 84, 85], "spill": [10, 15, 206, 214], "spillov": [88, 89], "spin": [10, 15, 88, 89, 92, 93, 126, 127, 206, 213, 267, 268, 277, 285, 288, 349, 350], "split": [4, 10, 12, 15, 86, 87, 101, 102, 106, 110, 113, 137, 139, 171, 174, 206, 212, 215, 224, 225, 226, 228, 229, 238, 240, 268, 274, 287, 289, 291, 294, 298, 330, 352, 353, 355, 357, 364, 371], "split_at_indic": [329, 331, 336, 338], "split_idx": [336, 338], "split_notebook": 0, "split_proportion": [342, 344], "spoil": [291, 297, 298], "spoiler": [291, 298], "spoken": [291, 297], "spot": [10, 84, 85, 88, 89, 126, 127, 128, 132, 137, 144, 206, 208, 268, 289, 349, 353], "spotifi": [7, 9, 198, 205, 253, 258], "sprai": [291, 298], "sprang": [291, 298], "spruce": [349, 350], "spur": [291, 297], "spy": [155, 158], "sql": [8, 191, 192, 196], "sqrt": [2, 7, 14, 175, 180, 253, 257, 355, 358], "sqrt_add": [2, 175, 180], "squad": [268, 289], "squar": [2, 3, 175, 180, 181, 184, 224, 226, 268, 289, 329, 330, 342, 343, 348], "square_ref": [3, 181, 184], "square_ref_1": [3, 181, 188], "square_ref_2": [3, 181, 188], "square_valu": [3, 181, 184], "squarederror": [4, 171, 174], "squeez": [7, 14, 253, 255, 329, 335, 336, 341, 342, 348, 355, 358, 359], "src": [355, 358], "ssh": [17, 20, 22, 137, 144, 362, 363], "ssl": [24, 27], "sso": [101, 102, 107], "sst": [313, 314, 315, 316, 327], "st": [17, 22, 268, 289], "stabil": [329, 335, 342, 346, 355, 357], "stabilityai": [6, 259, 262, 263], "stabl": [5, 13, 101, 102, 108, 260, 263, 264], "stablediffus": [6, 259, 262, 263], "stack": [6, 28, 32, 35, 39, 41, 43, 45, 50, 53, 56, 62, 66, 70, 77, 100, 159, 162, 163, 259, 262, 336, 338, 349, 351, 355, 356, 361, 362, 371], "stadium": [268, 289], "stage": [3, 5, 6, 8, 10, 164, 166, 181, 187, 191, 194, 206, 208, 212, 213, 224, 226, 234, 259, 262, 268, 289], "stagnant": [355, 359], "stai": [224, 225, 238, 240, 268, 289, 329, 330], "stakehold": [355, 361], "standalon": [224, 227], "standard": [6, 8, 35, 39, 84, 85, 90, 91, 164, 167, 191, 192, 224, 226, 228, 232, 234, 238, 243, 259, 262, 263, 291, 294, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 358, 362, 363, 364, 365], "stander": [291, 298], "stapl": [291, 294], "star": [268, 289, 291, 298, 336, 337, 342, 344], "stare": [291, 294], "starlett": [11, 12, 16, 147, 148, 218, 219], "start": [1, 3, 4, 5, 6, 9, 10, 11, 12, 13, 16, 17, 20, 28, 29, 32, 35, 36, 41, 43, 44, 50, 53, 54, 55, 62, 66, 67, 69, 77, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 103, 105, 109, 110, 111, 114, 118, 119, 121, 124, 126, 127, 128, 130, 132, 157, 159, 163, 164, 165, 166, 167, 169, 170, 171, 172, 174, 181, 182, 186, 187, 198, 199, 206, 207, 210, 212, 213, 218, 219, 224, 225, 226, 227, 230, 231, 235, 238, 244, 245, 246, 247, 249, 254, 259, 260, 262, 263, 267, 268, 285, 291, 293, 294, 297, 301, 304, 305, 309, 316, 320, 325, 328, 329, 330, 335, 336, 337, 341, 342, 344, 355, 357, 359, 362, 363, 364, 370], "start_epoch": [245, 247, 342, 346, 355, 359, 362, 367], "start_run": [137, 143], "start_tim": [137, 144], "start_token": [355, 359], "starter": [92, 93], "startswith": [342, 348, 362, 371], "startup": [3, 24, 26, 110, 117, 128, 130, 155, 158, 181, 186], "starv": [291, 298], "state": [3, 4, 8, 11, 12, 17, 22, 28, 30, 43, 45, 53, 56, 82, 83, 101, 102, 107, 118, 123, 128, 130, 159, 161, 164, 166, 171, 174, 181, 190, 191, 195, 196, 207, 218, 220, 221, 246, 247, 250, 252, 268, 277, 288, 289, 291, 294, 298, 329, 335, 338, 339, 341, 342, 347, 349, 353, 355, 356, 361, 362, 371], "state_dict": [5, 13, 137, 140, 224, 234, 237, 245, 248, 329, 335, 336, 341, 342, 346, 348, 355, 359, 361, 362, 367, 371], "state_dict_fp": [137, 140, 146], "stateless": [2, 10, 175, 178, 206, 213, 329, 331], "statement": [5, 17, 22], "static": [8, 10, 101, 102, 105, 191, 197, 206, 208], "station": [291, 298], "statist": [9, 159, 161, 198, 205], "stats_d": [349, 353, 354], "statu": [12, 13, 14, 16, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 90, 91, 92, 93, 128, 133, 137, 139, 144, 147, 150, 159, 163, 164, 166, 168, 268, 289, 299, 306], "std": [9, 10, 15, 198, 204, 206, 215, 224, 237, 355, 357, 361, 362, 365, 371], "stderr": [164, 167], "steadi": [329, 333], "steadili": [224, 236, 355, 359], "steak": [362, 363], "steal": [291, 297], "steam": [291, 294], "step": [2, 3, 5, 6, 7, 8, 10, 13, 14, 28, 29, 30, 35, 37, 39, 42, 43, 44, 45, 47, 48, 53, 54, 55, 56, 59, 60, 66, 67, 68, 71, 75, 76, 78, 92, 93, 100, 105, 119, 124, 128, 130, 134, 135, 137, 139, 143, 159, 163, 164, 165, 175, 176, 178, 180, 181, 182, 191, 197, 206, 209, 212, 224, 225, 227, 228, 229, 232, 236, 237, 238, 240, 242, 243, 247, 253, 256, 257, 259, 262, 268, 289, 291, 293, 299, 304, 330, 337, 338, 343, 344, 346, 350, 351, 356, 357, 359, 363, 367], "step_size_hour": [355, 361], "sterl": [268, 289], "steven": [268, 289], "stewart": [291, 297], "still": [3, 5, 8, 13, 82, 83, 126, 127, 137, 138, 181, 184, 191, 196, 224, 234, 238, 240, 241, 245, 248, 250, 268, 289, 291, 297, 316, 323, 328, 329, 331, 355, 359, 362, 364], "stillkidrauhl": [268, 289], "stockholm": [291, 294], "stop": [1, 7, 14, 101, 102, 104, 169, 170, 224, 237, 253, 257, 291, 298, 316, 328, 329, 335, 349, 354, 355, 361, 362, 371], "storag": [6, 8, 9, 10, 15, 16, 17, 21, 22, 28, 34, 35, 36, 38, 39, 66, 69, 70, 80, 81, 100, 118, 121, 129, 130, 137, 139, 142, 147, 153, 159, 162, 163, 191, 192, 195, 198, 200, 202, 203, 206, 210, 216, 225, 226, 227, 232, 235, 236, 238, 244, 246, 248, 250, 252, 259, 262, 263, 336, 341, 344, 355, 356, 357, 362, 363, 364, 371, 372], "storage_fold": [4, 5, 9, 10, 11, 171, 174, 198, 203, 205, 206, 213, 216, 217, 218, 222, 223], "storage_path": [4, 5, 6, 12, 13, 171, 174, 224, 225, 234, 238, 244, 245, 248, 250, 259, 262, 263, 329, 333, 336, 340, 342, 346, 347, 349, 352, 355, 359, 362, 367, 368], "store": [6, 8, 10, 11, 12, 15, 17, 21, 88, 89, 126, 127, 128, 130, 131, 133, 137, 139, 144, 147, 154, 155, 158, 164, 166, 189, 190, 191, 192, 196, 197, 206, 214, 215, 218, 220, 224, 226, 227, 234, 235, 236, 238, 242, 243, 244, 245, 248, 252, 259, 262, 263, 267, 268, 283, 285, 289, 290, 291, 294, 298, 329, 330, 333, 335, 336, 341, 342, 344, 346, 348, 349, 350, 351, 354, 355, 357, 359, 362, 363, 364, 367, 369], "stori": [291, 294, 298], "storylin": [291, 297, 298], "str": [4, 5, 6, 7, 10, 11, 13, 14, 15, 16, 118, 122, 123, 159, 163, 171, 174, 206, 212, 213, 215, 218, 222, 224, 234, 245, 248, 253, 257, 259, 262, 268, 275, 276, 277, 288, 313, 314, 315, 316, 327, 349, 352, 355, 361, 362, 365, 366, 371], "strang": [291, 298], "stranger": [291, 297], "strategi": [6, 92, 93, 117, 128, 130, 137, 144, 259, 263, 329, 331, 333, 336, 340], "stratifi": [349, 351], "streak": [268, 287, 289], "stream": [4, 9, 10, 11, 15, 16, 101, 102, 106, 108, 110, 114, 115, 118, 121, 128, 130, 132, 147, 151, 164, 167, 171, 174, 197, 198, 205, 206, 208, 218, 220, 238, 239, 240, 241, 243, 244, 245, 252, 268, 287, 289, 290, 329, 330, 336, 337, 341, 342, 343, 344, 346, 348, 349, 350, 351, 352, 355, 357, 361], "streaming_split": [10, 206, 211], "streamlin": [17, 21, 24, 25, 90, 91, 137, 144], "street": [291, 297], "strength": [118, 121], "stretch": [268, 289], "strftime": [5, 13], "strict": [329, 335, 336, 341], "stride": [5, 7, 13, 14, 224, 227, 253, 256, 257, 355, 357], "strike": [291, 297, 298], "string": [35, 39, 43, 45, 48, 53, 56, 60, 66, 76, 88, 89, 118, 122, 123, 268, 287, 290, 291, 295, 296, 298, 362, 364], "strip": [342, 348, 355, 361, 362, 371], "strong": [101, 102, 106, 110, 112, 159, 163, 342, 345, 355, 357], "stronger": [110, 112], "structur": [10, 12, 97, 99, 102, 109, 119, 120, 121, 123, 125, 128, 130, 137, 139, 164, 167, 196, 205, 206, 210, 291, 294, 329, 330, 336, 337, 341, 342, 343, 355, 358, 359, 372], "stuck": [11, 159, 163, 218, 220], "student": [291, 294], "studi": [5, 6, 13, 259, 264, 291, 294], "studio": [82, 83, 268, 289, 291, 297], "stuff": [268, 289], "stun": [291, 297, 298], "stupid": [291, 298], "style": [0, 6, 128, 130, 238, 241, 245, 252, 259, 262, 291, 298, 329, 335, 342, 344, 346, 355, 356, 362, 363], "sub": [3, 181, 188], "subdirectori": 0, "subfold": [6, 259, 262], "subject": [118, 124], "submiss": [7, 14, 90, 91, 253, 257], "submit": [7, 14, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 82, 83, 88, 89, 126, 127, 128, 135, 137, 145, 159, 163, 253, 256, 268, 289], "subnet": [20, 23, 28, 30, 34, 35, 36, 39, 66, 70, 79], "subnet_id": [17, 23, 28, 30], "suboptim": [10, 206, 208], "subplot": [7, 13, 14, 224, 226, 237, 253, 255, 329, 331, 335, 342, 344, 362, 364], "subprocess": [10, 11, 206, 207, 213, 218, 219, 222, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "subraman": [268, 289], "subsampl": [349, 354], "subsequ": [101, 102, 106, 155, 157, 158, 164, 167, 291, 298], "subset": [5, 7, 9, 10, 15, 94, 95, 198, 202, 206, 210, 211, 253, 257, 299, 304, 329, 330, 331, 342, 344, 349, 351, 355, 357, 362, 363, 364], "substanc": [291, 298], "substanti": [355, 359], "subtract": [329, 335], "subword": [101, 102, 104], "success": [7, 14, 168, 253, 257], "successfulli": [28, 31, 34, 35, 40, 43, 47, 52, 53, 59, 65, 66, 75, 84, 85, 90, 91, 110, 117, 118, 121, 336, 341], "suck": [268, 289], "sudo": [155, 158], "suffer": [11, 218, 220], "suffici": [110, 114, 329, 331], "suffix": [2, 175, 179], "suggest": [10, 118, 121, 206, 213, 349, 353], "suit": [8, 128, 134, 147, 151, 191, 192, 224, 225, 342, 345], "suitabl": [0, 291, 298, 299, 301, 329, 335, 342, 344], "sum": [3, 9, 10, 15, 88, 89, 137, 146, 181, 183, 198, 201, 204, 206, 211, 215, 291, 294, 342, 345, 346, 349, 353, 354], "sum_ref": [3, 181, 184], "sum_valu": [3, 181, 184], "summar": [101, 102, 105, 110, 117, 118, 121, 124, 125, 268, 283, 290], "summari": [111, 118, 121, 125, 267, 285, 301], "summer": [268, 287, 289, 291, 298], "summerslam": [268, 287, 289], "summit": [9, 10, 13, 15, 198, 205, 206, 217], "sun": [268, 289, 291, 297, 298], "sunbeam": [28, 30], "sunda": [268, 289], "sundai": [268, 287, 289], "super": [6, 137, 140, 259, 262, 268, 284, 289, 292, 300, 324, 329, 332, 336, 339, 342, 345, 355, 358], "superior": [291, 298], "supervis": [336, 338, 355, 357, 362, 363], "suppli": [291, 297, 336, 341], "support": [1, 3, 5, 6, 8, 9, 10, 11, 15, 22, 24, 26, 27, 100, 101, 110, 116, 118, 121, 122, 125, 126, 127, 128, 130, 137, 139, 144, 147, 151, 154, 155, 157, 159, 162, 164, 165, 167, 169, 170, 181, 187, 190, 191, 192, 195, 196, 197, 198, 205, 206, 208, 210, 215, 216, 218, 220, 223, 224, 225, 226, 238, 241, 245, 247, 248, 250, 259, 261, 291, 293, 299, 305, 309, 316, 325, 336, 337, 342, 347, 348, 349, 351, 354, 362, 367], "suppos": [268, 289], "suptitl": [329, 331, 335, 362, 364], "sur": [268, 289], "sure": [0, 5, 66, 78, 82, 83, 118, 121, 245, 251, 268, 287, 289, 290, 329, 335, 342, 348, 362, 365], "surfac": [164, 167], "surg": [355, 361], "surpris": [291, 298], "surprisingli": [3, 181, 190, 268, 289], "surround": [291, 294], "surviv": [268, 287, 289, 290, 336, 341], "sushi": [362, 363], "suspens": [291, 298], "suv": [118, 122], "swai": [291, 298], "swap": [84, 85, 329, 335, 336, 341, 362, 371], "swede": [291, 294], "swedish": [291, 294], "sweep": [329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "swing": [268, 289, 291, 294, 336, 337], "switch": [118, 120, 121, 245, 252, 342, 348], "switcher": 0, "sy": [2, 3, 128, 129, 137, 138, 147, 148, 175, 176, 181, 182, 183, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "sydow": [291, 298], "symbol": [1, 169, 170, 336, 337], "sync": [82, 83, 224, 225, 268, 289, 362, 363, 367], "sync_dist": [6, 259, 262, 329, 332, 336, 339], "sync_on_comput": [362, 367], "synchron": [3, 5, 13, 181, 183, 224, 225, 230, 231, 234, 245, 248, 329, 333, 362, 367], "synthet": [159, 163, 336, 341], "synthetic_image_output": [159, 163], "system": [2, 3, 6, 8, 9, 10, 12, 14, 17, 22, 24, 26, 43, 46, 51, 53, 58, 63, 64, 66, 69, 86, 87, 92, 93, 100, 118, 120, 121, 122, 123, 128, 133, 137, 144, 147, 151, 155, 157, 158, 159, 161, 162, 164, 167, 175, 177, 181, 185, 191, 192, 195, 198, 203, 206, 212, 217, 245, 252, 259, 262, 348, 355, 360, 362, 363], "t": [1, 2, 3, 5, 8, 9, 10, 13, 15, 24, 26, 53, 63, 86, 87, 100, 101, 102, 106, 128, 130, 135, 137, 138, 139, 143, 145, 146, 147, 151, 153, 164, 166, 169, 170, 175, 180, 181, 184, 185, 186, 191, 196, 198, 202, 204, 206, 212, 215, 224, 225, 226, 230, 232, 237, 268, 287, 289, 291, 294, 297, 298, 329, 330, 331, 332, 335, 336, 337, 339, 341, 342, 344, 348, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 371], "t10k": [13, 14], "t4": [10, 12, 13, 14, 66, 71, 84, 85, 128, 131, 137, 139, 143, 146, 147, 149, 206, 213], "t_": [336, 337], "t_futur": [355, 361], "t_img": [329, 332], "t_past": [355, 361], "t_scale": [329, 332], "tab": [80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 128, 129, 137, 138, 144, 147, 148, 164, 166], "tabl": [7, 8, 10, 28, 30, 43, 45, 53, 56, 191, 192, 206, 210, 253, 257, 349, 352, 355, 357, 362, 364, 365], "tabular": [9, 15, 198, 201, 238, 242, 342, 343, 348, 351, 354, 362, 364], "tackl": [355, 356], "tag": [8, 28, 30, 43, 45, 53, 56, 137, 139, 144, 191, 196, 342, 348, 349, 354], "tail": [342, 344], "tailor": [224, 227], "take": [3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 23, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 73, 118, 120, 128, 130, 134, 137, 139, 141, 146, 164, 166, 171, 174, 181, 190, 198, 201, 202, 206, 211, 212, 218, 222, 226, 227, 234, 253, 256, 257, 259, 261, 263, 268, 280, 289, 291, 294, 297, 298, 299, 303, 331, 345, 351, 357, 364], "take_al": [342, 344], "take_batch": [9, 10, 12, 15, 16, 137, 141, 198, 202, 206, 211, 212, 213, 268, 280, 289, 349, 351], "takeawai": 103, "taken": [291, 294, 362, 371], "talent": [268, 289, 291, 298], "talk": [9, 10, 15, 118, 121, 198, 205, 206, 217, 268, 289, 291, 298], "taman": [268, 289], "tank": [268, 289], "target": [4, 5, 6, 10, 16, 17, 22, 118, 121, 137, 143, 171, 174, 206, 212, 224, 234, 238, 242, 259, 262, 284, 292, 300, 324, 336, 338, 342, 343, 344, 349, 351, 354, 355, 357, 359], "target_num_rows_per_block": [159, 163], "target_ongoing_request": 16, "target_path": [329, 335, 336, 341, 342, 348], "task": [2, 4, 7, 8, 9, 10, 11, 13, 14, 15, 43, 44, 53, 55, 66, 68, 82, 83, 84, 85, 88, 89, 101, 102, 106, 110, 112, 121, 128, 134, 164, 166, 171, 174, 175, 177, 178, 179, 180, 183, 189, 190, 191, 194, 195, 196, 198, 201, 203, 206, 208, 210, 212, 213, 218, 220, 253, 256, 291, 293, 298, 299, 302, 305, 329, 331, 335, 336, 341, 342, 345, 348, 349, 353, 354, 362, 363], "task_id": [88, 89], "taskpoolmapoper": [10, 206, 214], "tatum": [291, 297], "tavakolian": [268, 289], "tax": [4, 12, 171, 174, 268, 289], "taxi": [4, 9, 164, 166, 171, 174, 198, 201, 204, 361], "taximet": [9, 198, 201], "taxiwindowdataset": [355, 357], "tb": [10, 206, 208], "tbh": [268, 289], "tbl": [349, 352], "tc": [118, 123], "tcm": [291, 297], "tcp": [17, 22], "td3": [336, 341], "tea": [268, 289], "teach": [329, 330, 336, 337], "teacher": [291, 294, 356, 358], "team": [8, 94, 95, 96, 97, 99, 191, 195, 268, 289, 362, 363], "tear": [90, 91], "teardown": [24, 26], "tech": [268, 289], "technic": [2, 175, 177, 291, 298], "techniqu": [7, 14, 253, 256, 291, 298], "technologi": [8, 191, 192], "ted": [268, 289], "teen": [268, 289], "telemetri": [155, 157, 159, 163], "tell": [4, 6, 110, 114, 115, 171, 174, 224, 230, 234, 259, 263, 268, 289, 291, 294, 297, 362, 368], "temp": [28, 30, 43, 45, 53, 56, 224, 226, 234, 355, 359, 362, 364, 367], "temp_checkpoint_dir": [5, 13, 224, 234, 245, 248], "tempdir": [362, 367], "temperatur": [3, 118, 123, 181, 190], "tempfil": [5, 13, 137, 143, 224, 226, 234, 245, 248, 329, 331, 333, 336, 340, 342, 344, 346, 349, 351, 355, 359, 362, 364, 367], "templat": [24, 27, 92, 93, 101, 102, 109, 112, 126, 127, 245, 252], "tempor": [336, 341], "temporari": [101, 102, 106, 224, 234, 336, 341, 342, 346, 349, 354, 355, 361, 362, 367, 371], "temporarydirectori": [5, 13, 137, 143, 224, 234, 245, 248, 342, 346, 355, 359, 362, 367], "ten": [362, 363], "tenant": [118, 125, 342, 348], "tenni": [268, 289], "tensor": [5, 6, 10, 11, 13, 15, 16, 101, 102, 106, 110, 113, 117, 137, 141, 206, 213, 218, 222, 224, 232, 233, 237, 238, 241, 243, 259, 262, 299, 302, 331, 332, 335, 336, 341, 342, 348, 355, 357], "tensor_batch": [137, 141], "tensor_parallel_s": [110, 113, 116, 118, 123], "tensorflow": [309, 316, 325], "term": [3, 8, 17, 21, 181, 184, 191, 197, 291, 294, 355, 356], "termin": [1, 5, 12, 13, 24, 27, 28, 31, 32, 33, 35, 40, 42, 43, 47, 50, 51, 53, 59, 62, 64, 66, 69, 75, 78, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 102, 108, 110, 113, 115, 128, 133, 136, 137, 146, 147, 153, 164, 166, 168, 169, 170, 224, 226, 245, 250, 299, 306, 336, 338], "terminologi": [96, 97], "terraform": [17, 21, 22, 23, 29, 31, 33, 34, 37, 40, 42, 44, 47, 51, 52, 55, 57, 59, 64, 65, 68, 71, 75, 78, 79], "terrain": [349, 354], "terribl": [291, 298], "test": [4, 7, 11, 12, 14, 16, 30, 39, 45, 56, 70, 79, 82, 83, 92, 93, 110, 114, 117, 118, 122, 123, 124, 125, 137, 146, 155, 158, 171, 174, 218, 220, 222, 223, 245, 252, 253, 255, 268, 289, 299, 304, 305, 309, 325, 362, 365], "test_d": [137, 146], "test_job": [28, 32, 35, 41, 43, 50, 53, 62, 66, 77], "test_siz": [4, 12, 171, 174, 349, 351], "texan": [268, 289], "text": [6, 8, 53, 57, 86, 87, 105, 109, 118, 123, 159, 162, 191, 192, 259, 262, 267, 268, 272, 273, 274, 277, 280, 283, 285, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 301, 304, 313, 314, 315, 316, 320, 327, 328, 336, 337, 342, 343, 355, 356], "text_token": [291, 298], "textembedd": [268, 275, 276, 277, 280, 283, 288, 289, 290], "tf": [28, 30, 35, 39, 43, 45, 53, 56], "tfenv": [28, 29, 43, 44, 53, 55, 66, 68], "tfi": [268, 289], "tfvar": [28, 30, 43, 45, 53, 56], "tgt": [355, 358], "than": [5, 8, 10, 12, 82, 83, 101, 102, 104, 108, 110, 112, 118, 121, 159, 163, 191, 194, 196, 197, 206, 211, 212, 291, 294, 296, 297, 298, 342, 348, 349, 351, 353, 355, 356, 359], "thank": [118, 125, 168, 268, 289, 355, 359], "thats": [291, 298], "theater": [291, 294, 298], "thei": [1, 3, 8, 9, 10, 11, 15, 28, 30, 43, 45, 53, 56, 80, 81, 84, 85, 101, 102, 104, 106, 126, 127, 128, 130, 132, 147, 154, 159, 163, 169, 170, 181, 186, 187, 189, 191, 195, 196, 198, 202, 206, 211, 217, 218, 221, 224, 229, 238, 239, 245, 246, 268, 289, 291, 294, 297, 298, 309, 316, 325, 349, 351], "them": [0, 2, 3, 5, 6, 8, 10, 17, 20, 23, 24, 27, 53, 63, 82, 83, 86, 87, 88, 89, 94, 95, 110, 115, 118, 121, 126, 127, 147, 151, 159, 163, 175, 177, 179, 181, 186, 188, 189, 190, 191, 197, 206, 213, 224, 226, 237, 259, 262, 268, 280, 289, 291, 298, 329, 331, 332, 335, 336, 337, 340, 341, 342, 343, 346, 348, 362, 363, 364, 365, 369], "theme": [0, 168], "themselv": [291, 297], "theoret": [291, 298], "therefor": [90, 91, 291, 294], "theta": [329, 330, 336, 337, 341, 349, 350, 355, 356, 362, 363], "theta_": [336, 337], "theta_dot": [336, 341], "thfc": [268, 289], "thi": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 34, 35, 36, 37, 38, 39, 43, 44, 45, 47, 50, 52, 53, 54, 56, 57, 59, 62, 63, 65, 66, 67, 68, 69, 70, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 107, 109, 110, 111, 112, 116, 118, 119, 120, 121, 123, 126, 127, 128, 129, 130, 133, 135, 136, 137, 138, 139, 143, 145, 146, 147, 148, 149, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 171, 172, 174, 175, 176, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 251, 253, 254, 256, 257, 258, 259, 260, 262, 263, 264, 267, 268, 277, 280, 284, 285, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 309, 315, 316, 323, 324, 325, 327, 328, 331, 333, 340, 344, 345, 346, 347, 351, 352, 357, 358, 359, 360, 364, 365, 366, 367, 368, 369, 372], "thine": [268, 289], "thing": [3, 118, 123, 128, 133, 181, 186, 268, 289, 291, 297, 298], "think": [2, 3, 118, 124, 175, 176, 181, 182, 267, 268, 285, 289, 291, 293, 297, 298, 299, 301, 309, 316, 325], "third": [7, 14, 53, 57, 155, 157, 253, 257, 268, 289, 291, 297, 298], "tho": [268, 289], "thoma": [268, 289], "thor": [268, 289], "those": [8, 43, 50, 53, 62, 88, 89, 147, 154, 191, 196, 291, 294, 298, 349, 353], "though": [84, 85, 101, 102, 105, 268, 289], "thought": [291, 294], "three": [2, 6, 9, 10, 15, 101, 102, 107, 108, 109, 159, 162, 175, 180, 198, 201, 206, 209, 210, 245, 252, 259, 262, 291, 298, 329, 335, 362, 368], "thriller": [291, 298], "throb": [291, 294], "through": [3, 4, 5, 6, 7, 8, 13, 14, 17, 22, 23, 24, 27, 28, 29, 43, 44, 53, 54, 66, 67, 80, 81, 82, 83, 100, 101, 102, 104, 107, 109, 110, 111, 118, 120, 123, 147, 153, 159, 161, 162, 164, 167, 168, 171, 174, 181, 184, 191, 193, 196, 224, 226, 229, 245, 252, 253, 254, 259, 262, 267, 268, 283, 285, 289, 290, 291, 297, 298, 329, 330, 336, 341, 349, 350, 352, 355, 356, 362, 363], "throughout": [1, 169, 170, 362, 364], "throughput": [8, 10, 101, 102, 105, 128, 130, 137, 144, 146, 164, 166, 167, 191, 197, 206, 217, 238, 239, 245, 252, 267, 268, 283, 285, 290, 349, 353, 362, 371], "throw": [43, 51, 53, 64, 268, 283, 290], "thru": [291, 294], "thu": [5, 24, 27], "thumb": [8, 191, 197], "thursdai": [268, 287, 289, 290], "ti": [5, 6, 224, 227, 259, 263], "ticket": [268, 287, 289], "tidi": [349, 354, 355, 357, 361, 362, 371], "tie": [268, 289], "tiger": [268, 289, 291, 298], "tight": [268, 289], "tight_layout": [224, 237, 329, 331, 333, 335, 336, 340, 342, 344, 346, 355, 357, 359, 361, 362, 364, 369], "tightli": [17, 22], "tild": [336, 337], "tilt": [8, 191, 194], "timber": [268, 289], "time": [2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 24, 26, 84, 85, 88, 89, 96, 98, 101, 102, 104, 105, 106, 110, 113, 117, 118, 123, 126, 127, 128, 130, 132, 137, 139, 144, 147, 151, 152, 154, 159, 163, 164, 166, 167, 171, 174, 175, 176, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 195, 197, 206, 212, 218, 220, 224, 225, 226, 238, 239, 240, 245, 248, 253, 256, 259, 261, 263, 267, 268, 285, 289, 291, 294, 297, 298, 299, 306, 329, 330, 336, 337, 340, 341, 342, 343, 346, 348, 349, 352, 354, 357, 359, 361, 362, 363, 370], "time_since_restor": 13, "time_this_iter_": 13, "time_total_": 13, "timedelta": [355, 357], "timelin": [3, 181, 189], "timeseri": [164, 166], "timeseriesbatchpredictor": [355, 361], "timeseriestransform": [355, 358, 359, 361], "timestamp": [5, 13, 168, 224, 226, 342, 344, 348, 355, 357], "timestep": [6, 259, 262, 329, 330, 332, 335, 336, 337, 338, 339, 341], "tini": [329, 332, 335, 336, 339, 349, 351, 355, 361, 362, 365], "tint": [291, 298], "tip": [4, 9, 13, 14, 16, 128, 133, 137, 139, 147, 150, 171, 174, 198, 201, 202], "tip_amount": [4, 9, 164, 166, 171, 174, 198, 201, 202], "tip_percentag": [9, 198, 202], "titan": [268, 289], "titl": [5, 10, 66, 69, 147, 150, 206, 211, 224, 226, 237, 268, 287, 289, 329, 333, 336, 340, 344, 346, 349, 351, 353, 355, 357, 359, 361, 362, 369, 371], "tl": [24, 27], "tlc": [9, 198, 201], "tloss": [299, 304], "tmp": [12, 13, 88, 89, 155, 158, 164, 167, 168, 299, 306], "tmp_checkpoint": [362, 371], "tmpdir": [342, 346, 355, 359, 362, 367], "tn": [137, 146], "to_arrow_ref": [349, 352], "to_csv": [9, 198, 203, 342, 344, 355, 357], "to_datetim": [355, 357], "to_json": 12, "to_numpi": [349, 352, 353, 355, 357, 361], "to_panda": [9, 10, 15, 198, 204, 206, 215, 291, 298, 349, 352, 362, 365], "to_parquet": [9, 198, 203, 238, 242, 342, 344, 349, 351, 362, 365], "to_pylist": [355, 357], "to_tensor": [224, 237], "todai": [1, 169, 170, 268, 289], "todo": [164, 167], "togeth": [4, 5, 6, 10, 11, 16, 147, 151, 154, 171, 173, 176, 206, 212, 218, 221, 224, 227, 234, 235, 259, 263, 268, 289], "toggl": [128, 129, 137, 138, 147, 148], "toke": [110, 113], "token": [92, 93, 101, 102, 104, 105, 106, 107, 108, 110, 113, 114, 115, 116, 118, 121, 122, 123, 124, 147, 151, 168, 293, 294, 301, 302, 306], "tokenization_fn": [291, 298], "tokenize_funct": [299, 304], "toler": [5, 6, 8, 9, 10, 17, 21, 22, 92, 93, 110, 115, 126, 127, 137, 144, 147, 153, 191, 195, 198, 200, 205, 206, 208, 224, 225, 247, 248, 250, 259, 261, 329, 330, 331, 333, 335, 336, 337, 340, 341, 342, 343, 346, 347, 348, 349, 350, 351, 352, 354, 355, 356, 360, 361, 363, 367, 368, 371, 372], "tolist": [11, 16, 164, 167, 218, 222, 224, 237, 238, 242, 342, 348, 355, 357], "toll": [4, 9, 171, 174, 198, 201], "tolls_amount": [4, 9, 171, 174, 198, 201], "toml": [126, 127], "tomorrow": [118, 123, 268, 287, 289, 290], "ton": [291, 298], "tone": [118, 121], "tonight": [268, 287, 289, 290], "tonit": [268, 289], "too": [4, 9, 10, 28, 30, 43, 45, 53, 56, 110, 112, 137, 138, 171, 174, 182, 198, 203, 206, 212, 268, 283, 290, 291, 293, 297, 298, 355, 357], "took": [3, 181, 187], "tool": [4, 8, 12, 24, 25, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 82, 83, 88, 89, 100, 102, 109, 119, 120, 125, 126, 127, 155, 157, 158, 159, 161, 162, 163, 164, 166, 171, 173, 191, 192, 196, 197, 267, 268, 285, 291, 293, 299, 301, 305, 306, 336, 337, 342, 344, 349, 351, 355, 357], "tool_cal": [118, 123], "tool_call_cli": [118, 123], "tool_call_id": [118, 123], "tool_call_pars": [118, 123], "tool_choic": [118, 123], "top": [2, 4, 9, 12, 80, 81, 94, 95, 118, 121, 128, 129, 132, 136, 137, 138, 143, 147, 148, 151, 171, 173, 175, 177, 182, 186, 189, 198, 200, 268, 289, 309, 316, 325, 336, 341, 343, 349, 353, 355, 359, 362, 371], "top20": [268, 289], "top_item_id": [342, 348], "top_items_df": [342, 348], "top_match": [128, 136], "top_scor": [342, 348], "topic": [111, 117, 121, 268, 289], "topic_safety_output_restrict": [118, 121], "topk": [342, 348], "torch": [5, 7, 10, 11, 13, 14, 15, 16, 128, 131, 137, 139, 140, 141, 143, 144, 147, 148, 149, 206, 207, 213, 218, 219, 222, 224, 226, 227, 228, 231, 232, 233, 234, 237, 245, 247, 248, 253, 254, 255, 256, 257, 260, 261, 263, 268, 271, 286, 299, 302, 304, 329, 331, 332, 333, 335, 336, 338, 339, 340, 341, 342, 343, 344, 346, 348, 355, 357, 358, 359, 361, 362, 363, 364, 367, 371], "torch_": [5, 13], "torch_config": [299, 305], "torch_d": [10, 206, 210], "torchconfig": [299, 302, 305], "torchmetr": [13, 362, 367], "torchpredictor": [137, 146, 147, 148, 149], "torchrec": [342, 343], "torchscript": [336, 341], "torchtrain": [5, 137, 143, 226, 228, 229, 230, 239, 242, 245, 248, 250, 252, 260, 299, 301, 302, 305, 330, 331, 335, 337, 338, 342, 343, 344, 346, 347, 348, 355, 356, 357, 359, 361, 363, 364, 371], "torchtrainer_2025": [299, 306], "torchtrainer_4dd7a_00000": [299, 306], "torchtrainer_4dd7a_00000_0_2025": [299, 306], "torchtrainer_d89d0_00000_0_2024": 13, "torchvis": [5, 7, 10, 13, 14, 15, 16, 206, 207, 224, 226, 227, 238, 243, 253, 254, 329, 331, 362, 363, 364], "torqu": [336, 337, 341], "torranc": [268, 289], "total": [3, 4, 5, 9, 12, 13, 14, 101, 102, 106, 110, 116, 171, 174, 181, 190, 198, 201, 224, 228, 229, 299, 305, 329, 331, 336, 338], "total_amount": [9, 164, 166, 198, 201, 202], "total_amt": [7, 14, 253, 257], "totensor": [5, 7, 10, 13, 14, 15, 16, 206, 207, 212, 224, 226, 232, 237, 238, 243, 253, 254, 255, 257, 362, 365, 371], "touch": [291, 294, 342, 348, 349, 351], "tough": [316, 317, 319, 320, 328], "tougher": [316, 317, 319, 320, 328], "tour": [2, 175, 176, 268, 289], "tourism": [268, 289], "tourist": [118, 121], "toward": [8, 191, 194, 284, 292, 300, 316, 320, 324, 328, 336, 337, 342, 344], "tower": [342, 348], "town": [268, 289, 291, 297], "tp": [137, 146], "tpot": [101, 102, 106], "tpu": [3, 5, 6, 181, 187, 224, 227, 259, 263], "tqdm": [5, 299, 302, 329, 331, 342, 344, 362, 364], "tr_model": [355, 358], "trace": [5, 6, 8, 147, 153, 155, 157, 165, 191, 195, 224, 225, 259, 261, 291, 294], "traceback": [8, 191, 195], "track": [1, 5, 88, 89, 90, 91, 137, 143, 159, 162, 169, 170, 224, 225, 236, 245, 252, 284, 291, 292, 298, 300, 324, 329, 333, 336, 337, 341, 342, 343, 346, 348, 349, 354], "track_running_stat": [13, 137, 140], "tracker": [137, 142], "tractabl": [329, 330], "trade": [110, 116, 118, 124], "tradit": [101, 102, 106, 128, 130, 342, 343, 349, 350], "traffic": [17, 22, 53, 63, 66, 73, 92, 93, 101, 102, 106, 107, 110, 115, 147, 148, 151, 164, 167, 309, 316, 323, 325, 328, 355, 356], "trail": [268, 289, 291, 297], "train": [7, 8, 9, 10, 12, 14, 15, 84, 85, 86, 87, 90, 91, 101, 102, 104, 126, 127, 128, 130, 134, 139, 141, 145, 146, 147, 148, 149, 191, 193, 195, 196, 198, 200, 202, 205, 206, 209, 211, 213, 217, 226, 227, 229, 230, 231, 232, 237, 241, 242, 243, 248, 251, 253, 255, 256, 257, 258, 268, 274, 287, 289, 291, 294, 298, 302, 313, 314, 315, 316, 326, 327, 332, 334, 335, 338, 339, 345, 348, 353, 357, 358, 364, 367, 371], "train_arrow": [349, 352], "train_batch": [362, 367], "train_bert": [299, 301, 305, 306], "train_config": [299, 305, 342, 346, 348], "train_count": [329, 331], "train_ctx": [4, 171, 174], "train_d": [137, 139, 141, 143, 238, 242, 243, 244, 329, 331, 333, 336, 338, 340, 342, 344, 346, 349, 351, 352], "train_data": [5, 7, 13, 14, 224, 232, 253, 255, 257], "train_dataload": [6, 259, 262, 263, 329, 333, 336, 340], "train_dataset": [12, 299, 304], "train_df": [349, 351], "train_epoch": [137, 143], "train_frac": [342, 344], "train_func": [349, 350, 352, 354], "train_func_per_work": [299, 304, 305], "train_label": 13, "train_linear_model": [7, 14, 253, 257], "train_load": [5, 6, 13, 224, 232, 259, 262, 329, 333, 336, 340, 342, 346, 355, 359, 362, 367], "train_loop": [330, 335, 336, 337, 340], "train_loop_config": [4, 5, 6, 13, 137, 143, 171, 174, 235, 238, 244, 245, 248, 250, 259, 263, 299, 305, 342, 346, 349, 352, 355, 359, 362, 368], "train_loop_per_work": [6, 137, 143, 245, 248, 250, 259, 263, 299, 305, 329, 333, 342, 343, 346, 355, 356, 359, 363, 368], "train_loop_ray_train": [5, 6, 13, 224, 227, 228, 229, 230, 235, 259, 263], "train_loop_ray_train_ray_data": [238, 240, 244], "train_loop_ray_train_with_checkpoint_load": [245, 247, 248, 250], "train_loop_torch": [5, 7, 13, 14, 253, 256], "train_loss": [137, 143, 144, 329, 332, 333, 336, 339, 340, 342, 343, 346, 355, 359, 362, 367, 369], "train_loss_sum": [355, 359], "train_loss_tot": [362, 367], "train_model": [126, 127, 137, 145], "train_my_simple_model": [7, 14, 253, 257], "train_my_simple_model_2024": 14, "train_my_simple_model_3207e_00000_0_a": 14, "train_my_simple_model_3207e_00000terminated10": 14, "train_my_simple_model_3207e_00001terminated10": 14, "train_my_simple_model_3207e_00002terminated10": 14, "train_my_simple_model_3207e_00003terminated10": 14, "train_my_simple_model_3207e_00004terminated10": 14, "train_parquet": [349, 351], "train_pytorch": [7, 14, 253, 257], "train_pytorch_7cf0c_00000terminated10": 14, "train_pytorch_7cf0c_00001terminated10": 14, "train_record": [355, 357], "train_test_split": [4, 12, 171, 172, 174, 342, 344, 349, 351], "trainabl": [4, 7, 14, 171, 174, 253, 257], "trainbr": [291, 298], "traincontext": [5, 224, 228], "trainer": [4, 5, 6, 12, 13, 137, 143, 171, 174, 226, 227, 236, 238, 244, 245, 248, 249, 250, 252, 259, 262, 263, 299, 302, 305, 329, 333, 334, 336, 340, 342, 343, 346, 347, 354, 355, 359, 360, 362, 363, 368, 370, 371], "training_01": 372, "training_02": 372, "training_03": 372, "training_04": 372, "training_05": 372, "training_06": 372, "training_07": 372, "training_08": 372, "training_09": 372, "training_iter": 13, "training_step": [6, 259, 262, 329, 332, 336, 339], "trainingargu": [299, 302], "trajectori": [336, 341], "transact": [8, 191, 192], "transfer": [2, 3, 8, 9, 10, 15, 159, 163, 175, 177, 181, 184, 191, 192, 198, 204, 206, 212, 215, 362, 371], "transform": [5, 6, 7, 8, 11, 12, 13, 14, 16, 24, 27, 128, 130, 131, 132, 137, 139, 143, 146, 147, 148, 191, 192, 193, 195, 199, 200, 201, 204, 205, 207, 209, 211, 215, 218, 221, 222, 224, 226, 232, 239, 244, 253, 254, 255, 257, 259, 260, 268, 271, 277, 283, 286, 288, 290, 291, 293, 294, 298, 299, 301, 302, 304, 306, 312, 315, 316, 326, 327, 329, 330, 331, 336, 338, 341, 342, 344, 349, 351, 361, 363, 364, 366, 371], "transform_imag": [238, 243], "transient": [245, 248, 249, 362, 367], "transit": [86, 87, 126, 127, 329, 330, 336, 337, 342, 343, 349, 350, 355, 356], "transpar": [329, 333], "transpos": [329, 331], "travel": [118, 121, 268, 289], "treat": [291, 294, 297, 362, 363], "tree": [4, 5, 6, 12, 171, 174, 224, 225, 259, 261, 262, 268, 287, 289, 291, 294, 349, 350, 351], "tree_method": [4, 171, 174, 349, 352], "tremend": [291, 298], "trend": [268, 289], "tri": [291, 294, 349, 350], "trial": [4, 7, 12, 13, 14, 171, 174, 253, 257, 291, 298, 299, 306], "trial_id": 13, "tribul": [291, 298], "trier": [291, 298], "trigger": [10, 128, 130, 137, 139, 147, 154, 206, 211, 214, 268, 283, 290, 336, 338, 349, 351, 354], "trim": [329, 331, 362, 364], "trip": [4, 9, 12, 171, 174, 198, 201, 204, 291, 297, 355, 357, 361], "trip_amount": [4, 171, 174], "trip_dist": [4, 9, 12, 171, 174, 198, 201, 204], "trip_dur": 12, "trivial": [101, 102, 107], "trndnl": [268, 289], "troubleshoot": [82, 83, 88, 89, 118, 125, 155, 158], "truck": [118, 122], "true": [0, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 35, 39, 101, 102, 108, 110, 113, 114, 115, 116, 118, 121, 123, 128, 130, 131, 137, 138, 139, 140, 142, 143, 146, 147, 149, 150, 164, 166, 167, 168, 181, 185, 198, 201, 206, 210, 212, 213, 218, 222, 224, 225, 226, 230, 232, 237, 253, 255, 257, 259, 262, 263, 291, 294, 299, 304, 329, 331, 332, 333, 336, 339, 340, 342, 344, 346, 347, 349, 351, 353, 355, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 371], "truli": [268, 289, 291, 298], "trump": [268, 289], "truncat": [299, 304, 313, 314, 315, 316, 317, 319, 320, 327, 328, 329, 331, 336, 338], "trust": [17, 19, 22, 268, 289, 355, 361], "truth": [10, 15, 206, 212, 215, 224, 226, 336, 338, 355, 356, 359, 361, 362, 371], "try": [2, 3, 7, 8, 13, 14, 28, 29, 43, 44, 51, 53, 55, 64, 66, 68, 110, 117, 118, 125, 175, 180, 181, 185, 191, 196, 245, 252, 253, 257, 291, 297, 298, 329, 331, 335, 349, 354, 362, 364], "tryna": [268, 289], "tsui": [291, 297], "ttft": [101, 102, 106], "ttm": [291, 297, 298], "tuesdai": [268, 289], "tune": [8, 10, 12, 90, 91, 101, 102, 107, 110, 117, 118, 120, 121, 124, 125, 126, 127, 128, 135, 137, 138, 191, 195, 196, 206, 212, 238, 241, 245, 252, 256, 291, 298, 329, 335, 336, 341, 342, 348, 349, 353, 354, 355, 361, 362, 363, 371], "tune_config": [4, 7, 12, 14, 171, 174, 253, 257], "tuneconfig": [4, 7, 12, 14, 171, 174, 253, 257], "tuner": [4, 7, 12, 14, 171, 174, 253, 257], "tupl": [3, 6, 10, 181, 184, 206, 213, 238, 239, 240, 259, 262], "turn": [2, 7, 9, 128, 129, 137, 138, 147, 148, 175, 178, 198, 203, 253, 257, 268, 289, 291, 298, 349, 350], "tutori": [17, 20, 82, 83, 86, 87, 110, 112, 126, 127, 128, 129, 133, 137, 138, 146, 147, 148, 153, 224, 226, 251, 329, 330, 335, 336, 337, 342, 343, 344, 348, 349, 350, 355, 356, 357, 359, 361, 362, 363, 364], "tv": [268, 289, 291, 297, 298], "tweet": [268, 289], "tweet_ev": [268, 274, 287], "twilight": [268, 289], "twitter": [268, 289], "two": [2, 3, 17, 22, 24, 26, 84, 85, 90, 91, 105, 137, 140, 155, 158, 164, 166, 168, 175, 180, 181, 183, 184, 185, 224, 227, 236, 238, 242, 268, 280, 289, 295, 298, 342, 344, 346, 348, 349, 351, 355, 357, 362, 363, 369], "txt": [0, 1, 86, 87, 126, 127, 128, 129, 137, 138, 147, 148, 168, 169, 170, 284, 292, 300, 324], "type": [3, 5, 6, 7, 8, 9, 10, 11, 14, 16, 18, 22, 24, 26, 28, 32, 35, 41, 43, 50, 53, 62, 66, 71, 77, 80, 81, 84, 85, 96, 98, 101, 102, 106, 110, 113, 123, 124, 128, 134, 137, 141, 144, 147, 153, 159, 163, 164, 167, 168, 181, 185, 191, 192, 195, 198, 205, 206, 210, 212, 213, 218, 219, 253, 254, 259, 260, 262, 268, 271, 286, 291, 294, 299, 302, 350, 353, 354], "typic": [9, 24, 27, 100, 110, 112, 118, 121, 128, 130, 198, 200, 267, 268, 285, 291, 298, 329, 331, 342, 344, 362, 363], "u": [6, 7, 12, 13, 14, 15, 16, 28, 30, 35, 38, 39, 43, 45, 53, 56, 66, 69, 70, 71, 86, 87, 118, 121, 125, 128, 133, 136, 137, 139, 143, 146, 147, 150, 153, 155, 158, 164, 166, 224, 227, 253, 256, 259, 262, 263, 268, 289, 291, 294, 298, 336, 337, 342, 343, 344, 348], "u002c": [268, 289], "u002c000": [268, 289], "u2019": [268, 289], "u2019ll": [268, 289], "u2019m": [268, 289], "u2019r": [268, 289], "u2019t": [268, 289], "u2019v": [268, 289], "u_": [336, 337, 342, 343], "u_k": [336, 337], "uber": [7, 253, 258], "ubj": [4, 12, 171, 174], "ubyt": [13, 14], "udf": [8, 191, 195], "ui": [0, 16, 17, 22, 86, 87, 90, 91, 147, 153], "uid": [342, 344, 348], "uint8": [16, 128, 130, 131, 147, 149, 159, 163, 238, 243], "un": [291, 298], "unabl": [291, 298], "unassoci": [28, 30, 43, 45, 53, 56], "unattach": [28, 30, 43, 45, 53, 56], "unavail": [3, 181, 187], "unavoid": [291, 294], "unbound": [10, 206, 212], "uncaptur": [128, 132], "uncas": [313, 314, 315, 316, 327], "uncertainti": [355, 361], "unchang": [238, 243], "uncl": [291, 298], "uncom": [35, 39, 43, 46, 53, 58, 118, 122, 123, 299, 305], "uncondit": [329, 335], "unconnect": [291, 298], "unconnectedbr": [291, 298], "under": [3, 10, 66, 69, 84, 85, 92, 93, 126, 127, 137, 144, 164, 166, 181, 184, 206, 210, 224, 226, 227, 230, 234, 238, 242, 291, 297, 329, 331, 333, 335, 336, 340, 341, 342, 344, 348, 355, 356, 357, 359, 362, 364, 367], "underbrac": [355, 356], "underli": [3, 4, 5, 6, 10, 94, 95, 110, 113, 171, 174, 181, 183, 206, 211, 224, 225, 259, 261], "undersid": [291, 298], "understand": [7, 10, 14, 17, 18, 20, 79, 88, 89, 92, 93, 101, 102, 103, 104, 106, 108, 109, 110, 112, 113, 155, 157, 159, 163, 164, 167, 206, 212, 253, 256, 291, 297, 298, 299, 304, 342, 348, 349, 350, 362, 363], "understat": [291, 298], "understood": [110, 117], "underutil": [101, 102, 104, 105], "uneasy": [268, 289], "unet": [6, 259, 262], "unet2dconditionmodel": [6, 259, 260, 262], "unexpect": [5, 6, 155, 157, 224, 225, 259, 261, 268, 289, 355, 357], "ungat": [110, 113, 118, 121], "unifi": [1, 4, 8, 17, 23, 100, 101, 102, 107, 128, 134, 147, 153, 169, 170, 171, 173, 191, 192, 195], "uniform": [3, 4, 171, 174, 181, 188, 299, 304, 336, 337], "uniformli": [10, 206, 208], "uniniti": [10, 15, 206, 213], "uninstal": [43, 51, 53, 64, 66, 78], "uniqu": [9, 15, 86, 87, 101, 102, 106, 110, 113, 137, 139, 198, 201, 224, 232, 234, 291, 295, 298, 342, 348, 362, 364], "unique_item": [342, 348], "unique_us": [342, 348], "unit": [10, 11, 15, 101, 102, 106, 118, 123, 206, 215, 218, 221, 223, 291, 294, 362, 364], "univari": [355, 358], "univers": [291, 297], "unless": [9, 90, 91, 94, 95, 198, 202, 224, 237, 245, 247, 291, 294], "unlik": [90, 91, 224, 226, 238, 243, 268, 289], "unnecessari": [10, 11, 88, 89, 90, 91, 92, 93, 137, 139, 206, 212, 218, 220, 224, 234, 237, 268, 289, 336, 341, 349, 352], "unnot": [291, 298], "unpredict": [101, 102, 106], "unread": [329, 331], "unregist": [28, 33, 43, 51, 53, 64], "unreleas": [268, 289], "unrelentingli": [291, 297, 298], "unrival": [291, 298], "uns4": [268, 289], "unshuffl": [349, 351], "unsloth": [101, 102, 108, 110, 113, 118, 121], "unsqueez": [5, 13, 224, 237, 336, 341, 355, 357, 358, 359, 361], "unstabl": [329, 330], "unstructur": [8, 191, 192, 208, 372], "until": [3, 5, 8, 9, 13, 15, 101, 102, 105, 106, 159, 163, 181, 183, 191, 197, 198, 201, 202, 224, 235, 268, 280, 289, 291, 298, 349, 352], "untitl": [82, 83], "unus": [28, 30, 43, 45, 53, 56], "unveil": [268, 289], "unwrap": [5, 13, 224, 234, 245, 248], "up": [0, 3, 5, 6, 7, 8, 9, 10, 11, 15, 17, 18, 21, 24, 27, 28, 30, 33, 34, 35, 38, 39, 42, 45, 46, 50, 52, 56, 58, 62, 65, 66, 69, 70, 78, 84, 85, 96, 97, 101, 102, 106, 108, 111, 116, 117, 119, 121, 124, 126, 127, 128, 129, 130, 132, 133, 135, 136, 137, 138, 139, 142, 146, 147, 148, 151, 153, 154, 156, 157, 164, 165, 181, 185, 186, 191, 197, 198, 201, 206, 211, 213, 218, 221, 222, 225, 235, 248, 250, 253, 256, 257, 259, 261, 262, 267, 268, 277, 285, 288, 289, 291, 293, 294, 297, 298, 299, 301, 305, 331, 333, 337, 340, 346, 347, 350, 351, 352, 360, 363, 364, 368, 370], "up_block_typ": [6, 259, 262], "upblock2d": [6, 259, 262], "upcom": 199, "updat": [0, 1, 4, 5, 6, 7, 8, 11, 14, 16, 43, 46, 51, 53, 58, 64, 66, 73, 84, 85, 88, 89, 92, 93, 110, 115, 128, 129, 137, 138, 143, 147, 148, 153, 154, 169, 170, 171, 174, 191, 192, 218, 223, 224, 225, 228, 234, 245, 247, 253, 257, 259, 263, 284, 292, 299, 300, 304, 309, 316, 324, 325, 329, 335, 355, 361, 362, 367], "upgrad": [11, 43, 45, 46, 48, 53, 56, 58, 60, 73, 76, 82, 83, 92, 93, 147, 153, 218, 221, 329, 335, 336, 337, 341], "upload": [5, 13, 86, 87, 118, 121, 147, 153, 349, 354], "upload_fil": [86, 87, 118, 121], "upon": [88, 89], "upper": [82, 83], "upright": [336, 337], "upscal": [16, 128, 133, 136, 137, 139, 143, 146], "upscale_delay_": 16, "upset": [268, 289], "ur": [268, 289], "uri": [86, 87, 137, 144], "url": [8, 90, 91, 128, 136, 147, 149, 150, 153, 191, 197, 355, 357], "url_to_arrai": [128, 136, 147, 148, 149], "urljoin": [102, 108, 110, 114], "urllib": [102, 108, 110, 114, 137, 146, 147, 148], "urlpars": [137, 146, 147, 148, 150], "urmitz": [291, 298], "us": [2, 3, 11, 13, 20, 21, 23, 25, 27, 28, 29, 31, 34, 35, 36, 39, 40, 43, 44, 47, 52, 53, 54, 55, 59, 63, 65, 66, 67, 68, 69, 70, 71, 75, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 111, 112, 114, 115, 117, 120, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 142, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 157, 158, 159, 162, 163, 165, 166, 168, 173, 174, 175, 177, 178, 179, 181, 183, 184, 185, 186, 187, 188, 189, 190, 192, 193, 194, 195, 197, 199, 201, 202, 203, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 220, 222, 223, 226, 227, 228, 230, 231, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 250, 251, 252, 256, 258, 260, 262, 263, 264, 267, 268, 274, 277, 280, 283, 284, 285, 287, 288, 289, 290, 291, 292, 293, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 315, 320, 323, 324, 327, 328, 331, 333, 335, 338, 340, 341, 345, 346, 348, 352, 353, 354, 357, 359, 361, 364, 367, 369, 371], "usabl": [118, 122, 238, 243], "usag": [11, 12, 13, 14, 16, 88, 89, 101, 102, 105, 107, 118, 120, 121, 124, 155, 157, 159, 161, 163, 218, 220, 299, 306, 342, 346], "use_gpu": [4, 5, 6, 12, 13, 137, 143, 171, 174, 224, 225, 230, 259, 263, 329, 333, 336, 340, 342, 346, 349, 352, 355, 359, 362, 363, 368], "use_gpu_actor": [362, 371], "usecol": [342, 348], "user": [1, 3, 5, 8, 10, 13, 14, 17, 19, 24, 26, 66, 69, 80, 81, 86, 87, 88, 89, 90, 91, 94, 95, 97, 101, 102, 104, 105, 106, 108, 110, 114, 115, 118, 121, 122, 123, 124, 126, 127, 128, 133, 137, 142, 143, 144, 159, 161, 162, 163, 164, 167, 169, 170, 181, 183, 187, 191, 195, 196, 206, 212, 224, 228, 231, 232, 268, 287, 289, 290, 299, 306, 309, 316, 325, 345, 346], "user2idx": [342, 344, 348], "user_col": [342, 344], "user_embed": [342, 345, 348], "user_id": [168, 342, 344, 348], "user_idx": [342, 343, 344, 345, 346, 348], "user_nam": [66, 69], "user_storag": [86, 87], "user_vec": [342, 345], "user_vector": [342, 348], "userguid": [28, 29, 43, 44, 53, 55], "userservic": [164, 167, 168], "usual": [8, 9, 191, 194, 198, 202, 224, 237, 291, 297], "utc": 13, "util": [3, 5, 6, 7, 10, 11, 13, 14, 16, 88, 89, 92, 93, 101, 102, 104, 105, 106, 107, 126, 127, 128, 129, 130, 134, 136, 137, 146, 147, 148, 151, 153, 154, 164, 167, 181, 187, 206, 208, 212, 217, 218, 220, 221, 224, 225, 226, 232, 234, 238, 239, 245, 252, 253, 254, 255, 257, 259, 260, 262, 299, 301, 302, 304, 329, 331, 336, 338, 341, 342, 344, 355, 357, 362, 363, 364], "uuid": [336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "uv": [126, 127, 128, 129, 137, 138, 147, 148, 284, 292, 300, 324], "ux": [8, 191, 195], "v": [17, 20, 27, 43, 49, 50, 53, 61, 62, 82, 83, 90, 91, 101, 102, 105, 118, 124, 126, 127, 137, 139, 194, 195, 224, 225, 237, 245, 251, 268, 289, 299, 304, 342, 343, 348, 355, 359, 361, 362, 371, 372], "v1": [24, 27, 102, 108, 110, 114, 115, 118, 121, 122, 123, 336, 337, 338, 341], "v2": [137, 138, 224, 225, 268, 277, 288, 329, 330, 333, 336, 337, 342, 343, 349, 350, 355, 356], "v_": [342, 343], "val": [137, 139, 143, 333, 338, 342, 344, 346, 349, 351, 352, 355, 357, 359, 361, 362, 365, 367, 369, 371], "val_batch": [362, 367], "val_d": [137, 139, 143, 329, 331, 333, 336, 338, 340, 342, 344, 346, 349, 351, 352, 353, 354], "val_dataload": [329, 333, 336, 340], "val_df": [349, 351], "val_load": [329, 333, 336, 340, 342, 346, 355, 359, 362, 367], "val_loss": [137, 143, 144, 147, 150, 329, 332, 333, 336, 339, 340, 342, 343, 346, 355, 359, 362, 367, 368, 369], "val_loss_sum": [355, 359], "val_loss_tot": [362, 367], "val_parquet": [349, 351], "val_pd": [349, 352, 353], "val_record": [355, 357], "val_xb": [362, 367], "val_yb": [362, 367], "valid": [4, 12, 16, 79, 92, 93, 118, 122, 137, 143, 171, 174, 299, 304, 329, 332, 333, 336, 337, 338, 339, 340, 343, 352, 353, 354, 363, 367, 368, 371], "valid_dataset": 12, "valid_dataset_featur": 12, "validation_step": [329, 332, 336, 339], "valu": [3, 5, 7, 10, 13, 14, 28, 29, 30, 35, 36, 39, 43, 44, 45, 46, 51, 53, 54, 56, 58, 64, 66, 67, 72, 73, 92, 93, 104, 164, 166, 167, 181, 183, 184, 188, 206, 211, 212, 224, 229, 232, 233, 236, 238, 243, 253, 255, 291, 296, 329, 330, 333, 336, 337, 338, 342, 344, 348, 355, 356, 357, 358, 362, 367, 369], "valuabl": [291, 297], "value_count": [342, 344, 349, 351], "valueerror": [3, 181, 185, 329, 333, 336, 340], "values_nginx": [43, 46, 53, 58], "values_nginx_gke_priv": [66, 73], "values_nginx_gke_publ": [66, 73], "values_nvdp": [43, 46, 53, 58], "vamp": [268, 289], "vampett": [268, 289], "vampir": [268, 289], "van": [268, 289], "vanilla": [6, 101, 102, 105, 172, 254, 259, 263, 355, 356, 357], "var": [17, 22, 66, 78, 137, 138, 224, 226, 329, 331, 336, 338, 342, 344, 349, 351, 355, 357, 362, 364], "varepsilon": [329, 330], "varepsilon_": [336, 337], "varepsilon_k": [336, 337], "vari": [8, 9, 10, 15, 101, 102, 106, 191, 192, 198, 204, 206, 208, 213, 215], "variabl": [3, 28, 30, 35, 39, 43, 45, 53, 56, 66, 69, 84, 85, 86, 87, 88, 89, 128, 136, 137, 146, 147, 154, 181, 186, 187], "variat": [7, 14, 253, 257], "varieti": [10, 15, 206, 210, 215, 291, 293], "variou": [8, 84, 85, 86, 87, 110, 117, 191, 193, 238, 239, 299, 305], "vast": [8, 191, 192], "ve": [28, 34, 101, 102, 109, 110, 117, 118, 120, 125, 137, 139, 268, 289, 291, 294, 298, 362, 371], "vector": [8, 10, 101, 102, 104, 128, 133, 191, 192, 206, 212, 267, 268, 285, 336, 338, 341, 342, 343, 345, 349, 354], "veget": [349, 350], "veloc": [336, 337], "venu": [316, 317, 319, 320, 328], "venv": [0, 128, 129, 137, 138, 147, 148], "verbos": [164, 167], "veri": [5, 6, 7, 8, 9, 13, 14, 15, 110, 112, 191, 196, 198, 202, 224, 226, 253, 257, 259, 262, 263, 268, 289, 291, 298, 355, 361], "verif": 79, "verifi": [6, 10, 28, 33, 35, 38, 51, 63, 64, 66, 69, 72, 73, 74, 84, 85, 88, 89, 90, 91, 206, 212, 259, 262, 351, 355, 357], "vermaelen": [268, 289], "version": [1, 8, 15, 17, 22, 28, 29, 35, 37, 43, 44, 46, 53, 55, 58, 66, 68, 73, 74, 80, 81, 118, 121, 147, 150, 153, 155, 158, 159, 162, 164, 167, 169, 170, 191, 192, 238, 240, 268, 284, 289, 292, 300, 309, 316, 324, 325, 329, 331, 333, 355, 357], "versu": [137, 146, 329, 335, 349, 354, 362, 371], "vertex": [147, 151], "via": [0, 2, 3, 5, 6, 8, 10, 17, 20, 24, 26, 27, 28, 30, 35, 39, 43, 45, 53, 56, 82, 83, 94, 95, 96, 98, 126, 127, 155, 157, 164, 165, 167, 168, 175, 177, 181, 187, 191, 195, 196, 206, 208, 224, 225, 228, 229, 230, 237, 238, 243, 245, 252, 259, 261, 268, 289, 291, 297, 329, 332, 336, 340, 342, 344, 346, 362, 366, 367], "vicki": [268, 287, 289], "vid": [268, 289], "video": [8, 10, 191, 192, 206, 208, 268, 289, 291, 294, 298], "vietnam": [291, 294], "view": [1, 7, 9, 10, 11, 13, 14, 16, 17, 22, 28, 30, 32, 35, 39, 41, 43, 45, 50, 53, 56, 62, 63, 66, 77, 84, 85, 88, 89, 110, 116, 128, 133, 134, 137, 139, 144, 147, 150, 152, 153, 155, 157, 159, 163, 164, 166, 167, 169, 170, 198, 201, 202, 206, 212, 218, 223, 253, 257, 291, 294, 298, 329, 332, 336, 339], "viewer": [128, 134, 147, 153, 291, 294], "vincent": [291, 294], "violat": [118, 121], "viridi": [349, 353], "virtual": [0, 17, 20, 22, 27, 84, 85, 101, 102, 105, 128, 129, 137, 138, 147, 148, 155, 158], "virtuou": [291, 294], "visibl": [110, 116, 224, 233, 291, 294, 355, 361], "vision": [245, 252, 335, 371], "visit": [7, 14, 16, 155, 158, 253, 257], "visual": [7, 8, 9, 10, 15, 82, 83, 88, 89, 110, 116, 155, 157, 158, 159, 163, 191, 193, 198, 201, 206, 211, 253, 255, 291, 297, 298, 333, 335, 336, 337, 340, 346, 350, 359, 369], "visualis": [355, 357], "vit": [128, 131, 136, 137, 139, 147, 150, 245, 248, 250], "vit_b_16": [362, 371], "vit_l_32": [362, 371], "vllm": [108, 109, 110, 116, 117, 118, 122], "vm": [17, 18, 20, 27, 28, 32, 35, 39, 41, 66, 70, 79, 372], "vocal": [268, 289], "voic": [268, 289, 291, 298], "volatil": [5, 224, 226], "volleybal": [268, 289], "volum": [5, 8, 86, 87, 159, 163, 191, 192, 195, 224, 226, 349, 351, 362, 363, 371], "von": [291, 298], "vpc": [20, 21, 23, 24, 26, 28, 30, 34, 35, 36, 39, 43, 44, 45, 53, 55, 56, 66, 70, 78, 79, 101, 102, 107], "vpc_cidr": [17, 22], "vpc_id": [17, 22, 23, 28, 30], "vpcid": [28, 30, 43, 45, 53, 56], "vram": [110, 113, 268, 289], "vscode": [92, 93], "vstack": [137, 143], "vtripl": [118, 121], "vulva": [291, 294], "w": [7, 11, 14, 86, 87, 137, 139, 140, 143, 218, 222, 224, 237, 253, 257, 268, 287, 289, 291, 298, 329, 330, 331, 332, 362, 371], "w0": [3, 181, 190], "w1": [3, 181, 190], "wa": [6, 9, 12, 84, 85, 90, 91, 159, 163, 198, 201, 245, 250, 259, 262, 268, 289, 291, 294, 297, 298, 342, 347], "wai": [1, 3, 8, 9, 10, 16, 17, 20, 82, 83, 90, 91, 96, 99, 110, 116, 128, 130, 135, 136, 137, 146, 147, 153, 154, 169, 170, 181, 184, 191, 193, 194, 198, 200, 206, 211, 212, 224, 234, 268, 289, 291, 293, 297, 298, 309, 316, 317, 319, 320, 325, 328, 336, 341, 362, 363, 365, 371], "wait": [2, 66, 73, 84, 85, 101, 102, 105, 128, 130, 137, 138, 175, 179, 180, 182, 188, 268, 289], "wake": [268, 289, 291, 298], "walk": [5, 7, 13, 14, 28, 29, 43, 44, 53, 54, 66, 67, 82, 83, 86, 87, 110, 111, 118, 121, 253, 254, 268, 283, 289, 290, 329, 330, 349, 350, 355, 356], "walter": [291, 297], "wander": [291, 294], "wanna": [268, 289], "want": [0, 2, 3, 5, 6, 8, 9, 10, 11, 12, 15, 16, 17, 22, 35, 39, 42, 53, 56, 66, 69, 78, 82, 83, 88, 89, 101, 102, 106, 128, 130, 137, 143, 147, 149, 153, 175, 179, 181, 183, 184, 189, 190, 191, 193, 198, 201, 203, 204, 206, 212, 213, 215, 218, 222, 223, 224, 226, 232, 233, 237, 259, 262, 268, 277, 287, 288, 289, 291, 294, 297, 298, 299, 306, 355, 356, 359], "war": [268, 289, 291, 294], "warehous": [10, 206, 210], "warm": [6, 259, 262, 268, 289], "warmth": [268, 289], "warn": [268, 289, 291, 297, 298, 329, 333, 336, 340], "warner": [268, 287, 289], "wasn": [291, 297, 355, 357], "wast": [291, 298], "watch": [66, 73, 268, 289, 291, 298], "water": [291, 298], "wave": [291, 298], "wc": [9, 198, 201], "we": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 28, 29, 35, 37, 43, 44, 46, 50, 53, 55, 58, 62, 66, 68, 70, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 100, 101, 102, 103, 105, 108, 109, 111, 112, 113, 115, 119, 121, 122, 128, 131, 135, 136, 137, 146, 147, 154, 164, 166, 171, 174, 175, 179, 180, 181, 184, 185, 186, 188, 190, 191, 193, 194, 198, 201, 202, 203, 204, 206, 210, 212, 213, 214, 215, 218, 222, 224, 226, 227, 228, 229, 230, 231, 234, 235, 237, 238, 240, 241, 242, 243, 244, 245, 247, 248, 253, 256, 257, 259, 262, 263, 267, 268, 285, 289, 291, 294, 295, 297, 298, 299, 303, 305, 306, 309, 312, 316, 320, 323, 325, 326, 328, 342, 343, 346, 362, 369], "wealth": [291, 298], "weather": [355, 361], "weaviat": [8, 191, 192], "web": [82, 83, 88, 89, 155, 157, 165, 309, 316, 325, 336, 341], "webservic": [312, 326], "websit": [0, 155, 158], "webster": [291, 297], "wed": [268, 289], "wednesdai": [268, 287, 289], "week": [268, 289, 355, 356, 357], "weight": [3, 13, 101, 102, 106, 110, 113, 137, 139, 143, 181, 190, 224, 225, 228, 234, 237, 245, 248, 252, 329, 335, 336, 341, 342, 346, 348, 349, 351, 354, 355, 361, 362, 371], "weight_decai": [6, 259, 262, 263], "weights_onli": [5, 6, 13, 224, 237, 259, 263], "welbeck": [268, 289], "welcom": [12, 168], "well": [3, 8, 17, 22, 24, 26, 84, 85, 96, 99, 128, 133, 135, 137, 145, 147, 153, 181, 190, 191, 196, 268, 289, 291, 297, 298, 309, 316, 325, 342, 344, 345], "wellcraft": [291, 298], "welllll": [268, 289], "went": [291, 298], "were": [6, 245, 250, 259, 262, 268, 289, 291, 297, 298, 349, 351], "werewolf": [291, 298], "west": [12, 13, 14, 28, 30, 43, 45, 53, 56, 86, 87, 118, 121, 128, 133, 136, 137, 139, 143, 146, 147, 150, 153], "west2": [35, 38, 39, 66, 69, 70, 71], "western": [291, 297, 298], "wget": [342, 344], "what": [2, 4, 6, 7, 10, 12, 14, 18, 35, 36, 66, 70, 80, 81, 103, 108, 121, 123, 137, 139, 156, 159, 160, 162, 163, 171, 172, 174, 175, 179, 199, 206, 212, 230, 252, 253, 257, 259, 262, 268, 289, 291, 294, 298, 344], "whatev": [349, 354, 355, 361, 362, 371], "when": [1, 2, 3, 7, 14, 28, 30, 33, 35, 39, 42, 43, 45, 46, 50, 53, 56, 58, 62, 66, 70, 78, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 101, 102, 104, 105, 110, 114, 115, 116, 118, 123, 128, 130, 132, 135, 137, 145, 147, 153, 159, 162, 163, 164, 165, 167, 169, 170, 175, 179, 180, 181, 183, 185, 186, 187, 189, 190, 197, 199, 202, 203, 207, 209, 210, 211, 212, 213, 214, 215, 219, 232, 235, 236, 238, 239, 245, 248, 251, 253, 257, 260, 267, 268, 277, 285, 288, 289, 291, 294, 298, 316, 323, 328, 329, 331, 335, 336, 341, 342, 346, 348, 349, 351, 355, 359, 361, 362, 364, 367, 369], "where": [2, 5, 6, 7, 8, 9, 12, 13, 14, 53, 56, 66, 71, 86, 87, 94, 95, 96, 98, 100, 101, 102, 104, 110, 112, 128, 132, 135, 137, 146, 147, 149, 151, 155, 158, 159, 162, 175, 177, 191, 196, 197, 198, 201, 224, 234, 235, 236, 238, 244, 250, 253, 255, 259, 263, 268, 289, 291, 294, 297, 298, 299, 304, 309, 316, 325, 337, 343, 344, 347, 350, 356, 360, 363, 365], "wherea": [8, 191, 194, 197], "wherev": [268, 289], "whether": [1, 5, 6, 7, 12, 13, 14, 79, 86, 87, 94, 95, 169, 170, 224, 227, 230, 231, 232, 253, 257, 259, 263, 299, 301, 342, 346, 349, 351, 355, 359], "which": [2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 20, 27, 28, 30, 43, 45, 53, 56, 57, 63, 66, 69, 84, 85, 92, 93, 110, 112, 116, 118, 121, 123, 126, 127, 128, 130, 131, 147, 151, 155, 158, 159, 163, 164, 167, 171, 174, 175, 180, 181, 182, 183, 184, 188, 189, 190, 191, 193, 196, 198, 201, 202, 203, 204, 206, 211, 212, 213, 214, 224, 226, 228, 232, 233, 234, 236, 238, 240, 253, 257, 259, 262, 267, 268, 285, 291, 294, 297, 298, 299, 301, 304, 305, 309, 316, 325, 329, 330, 331, 349, 350, 353, 355, 359, 361, 362, 363, 364, 367, 371], "while": [3, 4, 8, 11, 82, 83, 84, 85, 88, 89, 100, 110, 112, 118, 121, 128, 134, 135, 137, 139, 145, 147, 153, 154, 159, 162, 163, 164, 167, 171, 173, 181, 188, 189, 191, 192, 196, 218, 221, 224, 225, 227, 228, 238, 239, 240, 241, 291, 294, 297, 298, 329, 331, 333, 342, 343, 344, 349, 351, 355, 356, 357, 362, 363, 364, 369], "whilst": [291, 298], "white": [7, 14, 253, 255, 268, 289, 291, 298], "whl": [126, 127], "who": [17, 22, 79, 94, 95, 101, 102, 106, 268, 289, 291, 294, 297, 298, 299, 306], "whole": [4, 12, 171, 174, 349, 352], "whose": [291, 297, 298], "why": [10, 111, 117, 206, 207, 224, 225, 245, 246, 268, 289, 291, 297], "whyyyyyyi": [268, 289], "wichita": [268, 289], "wide": [8, 66, 73, 128, 130, 191, 195, 268, 289, 291, 298, 309, 316, 325], "widescreen": [291, 297], "width": [10, 15, 16, 159, 163, 206, 212, 329, 331], "wife": [291, 298], "wildlif": [316, 317, 319, 320, 328], "wilki": [291, 297], "willam": [268, 287, 289], "william": [268, 289, 291, 297], "wilmer": [268, 289], "win": [8, 191, 196, 268, 287, 289], "wind": [291, 297], "window": [1, 35, 38, 82, 83, 110, 116, 155, 158, 169, 170, 245, 252, 291, 294, 356, 358, 361], "wire": [245, 252, 329, 333], "wise": [224, 237], "wish": [268, 289], "with_resourc": [7, 14, 253, 257], "within": [3, 8, 17, 19, 22, 24, 26, 82, 83, 86, 87, 90, 91, 94, 95, 96, 98, 128, 135, 155, 158, 181, 187, 191, 192, 336, 341, 342, 344], "without": [3, 8, 11, 17, 22, 82, 83, 84, 85, 100, 101, 102, 105, 118, 121, 122, 126, 127, 128, 130, 155, 158, 181, 183, 184, 187, 189, 191, 192, 218, 221, 224, 225, 245, 246, 248, 249, 291, 294, 298, 329, 330, 335, 336, 337, 342, 343, 344, 346, 347, 348, 349, 352, 353, 354, 355, 356, 357, 361, 362, 363, 370], "woman": [291, 297, 298], "women": [291, 294], "won": [3, 8, 128, 130, 181, 185, 191, 196, 291, 294], "wonder": [7, 24, 26, 253, 257, 268, 287, 289], "wood": [268, 289], "wooden": [291, 297], "word": [101, 102, 104, 291, 298], "work": [1, 2, 3, 5, 7, 9, 11, 13, 24, 26, 27, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 100, 101, 102, 104, 110, 113, 117, 118, 120, 126, 127, 128, 132, 135, 169, 170, 175, 180, 181, 183, 198, 203, 204, 207, 218, 221, 231, 238, 242, 245, 252, 253, 256, 267, 268, 277, 285, 288, 289, 291, 297, 298, 299, 305, 334, 336, 337, 342, 343, 344, 346, 349, 350, 351, 362, 363, 365, 370, 371], "worker": [1, 3, 4, 9, 10, 11, 13, 15, 17, 22, 24, 26, 43, 50, 53, 62, 80, 81, 92, 93, 126, 127, 128, 132, 134, 137, 143, 144, 147, 153, 159, 161, 163, 169, 170, 171, 174, 181, 183, 185, 186, 188, 190, 198, 200, 204, 206, 213, 215, 218, 221, 225, 227, 229, 230, 231, 232, 233, 234, 235, 237, 238, 241, 242, 243, 244, 245, 246, 248, 249, 268, 289, 291, 296, 301, 305, 306, 329, 330, 331, 332, 333, 335, 336, 337, 340, 341, 342, 343, 344, 346, 350, 351, 354, 355, 356, 357, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 371], "worker_devic": [268, 280, 289], "worker_nod": [43, 50, 53, 62, 159, 163], "worker_rank": [4, 171, 174], "workernodegroupconfig": [43, 50, 53, 62], "workflow": [1, 8, 24, 26, 80, 81, 82, 83, 88, 89, 90, 91, 100, 118, 123, 128, 130, 147, 154, 169, 170, 191, 192, 194, 195, 196, 219, 224, 225, 234, 237, 245, 252, 267, 268, 283, 284, 285, 290, 291, 292, 293, 298, 299, 300, 305, 306, 309, 316, 324, 325, 336, 337, 349, 350, 351, 355, 356, 361, 362, 363], "working_dir": [90, 91, 101, 102, 108, 110, 115, 128, 129, 137, 138, 147, 148], "workload": [1, 3, 4, 5, 6, 8, 9, 10, 12, 15, 17, 22, 25, 27, 28, 34, 43, 52, 53, 65, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 99, 100, 101, 102, 106, 110, 112, 117, 126, 127, 128, 129, 130, 132, 134, 135, 137, 138, 139, 143, 144, 145, 146, 147, 154, 155, 158, 159, 162, 163, 165, 169, 170, 171, 173, 174, 181, 187, 191, 192, 195, 196, 198, 205, 206, 208, 224, 232, 238, 239, 245, 252, 259, 262, 309, 316, 325, 335, 341, 348, 361, 371], "workload_identity_pool_provid": [35, 39, 66, 70], "workloadidentitypool": [35, 39, 66, 70], "workloadserviceaccountnam": [43, 45, 48, 53, 56, 60, 66, 76], "workshop": [80, 81], "workspac": [17, 21, 22, 24, 26, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 110, 112, 116, 117, 126, 127, 128, 129, 135, 137, 138, 145, 147, 148, 153, 336, 337, 349, 354, 355, 361, 362, 363, 371, 372], "workspace_v2": [82, 83], "world": [5, 82, 83, 90, 91, 118, 125, 164, 167, 224, 228, 268, 289, 291, 298, 299, 304, 329, 333, 342, 343, 348, 362, 363], "world_rank": [5, 224, 233, 235], "world_siz": [5, 224, 228, 238, 240], "worri": [126, 127, 362, 363], "worth": [268, 289], "would": [5, 6, 9, 13, 110, 112, 117, 128, 133, 159, 163, 198, 202, 203, 204, 224, 230, 259, 262, 263, 268, 289, 291, 293, 295, 297, 298, 299, 304], "wound": [291, 297], "wrangl": [224, 226], "wrap": [3, 5, 13, 92, 93, 100, 128, 135, 137, 145, 147, 151, 181, 184, 225, 226, 227, 228, 232, 234, 235, 238, 244, 330, 337, 338, 344, 352, 356, 357, 363, 366], "wright": [268, 289], "write": [2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15, 17, 22, 82, 83, 86, 87, 128, 129, 130, 137, 139, 159, 163, 168, 171, 174, 175, 177, 180, 181, 190, 191, 192, 199, 200, 202, 206, 207, 209, 211, 212, 216, 224, 234, 238, 242, 245, 248, 253, 257, 259, 263, 329, 331, 342, 343, 355, 357, 362, 363, 364, 365, 367], "write_csv": [9, 198, 203], "write_parquet": [4, 9, 10, 15, 128, 133, 137, 139, 159, 163, 164, 166, 171, 174, 198, 203, 206, 211, 216, 329, 331, 355, 357], "write_t": [355, 357, 362, 364], "writefil": [164, 166], "writer": [5, 13], "writerow": [5, 13], "written": [1, 8, 155, 158, 164, 167, 169, 170, 191, 194, 224, 226, 234, 291, 297, 329, 331, 349, 351, 355, 357, 362, 367], "wrong": [268, 289], "wrote": [9, 198, 203, 268, 289, 329, 331, 342, 344, 349, 351, 362, 364], "wt": [268, 287, 289], "ww2": [291, 298], "wwe": [268, 287, 289], "wyom": [291, 297], "x": [2, 3, 6, 7, 8, 14, 88, 89, 128, 129, 137, 138, 147, 148, 150, 153, 159, 163, 168, 175, 180, 181, 183, 184, 185, 189, 191, 193, 224, 237, 253, 257, 259, 262, 268, 289, 329, 330, 332, 336, 339, 341, 349, 352, 355, 358, 361, 362, 363, 371], "x_": [329, 330, 336, 337], "x_0": [329, 330, 332, 336, 337], "x_t": [329, 330, 332, 336, 337], "x_test": [4, 171, 174], "x_train": [4, 171, 174], "xb": [355, 357, 362, 367], "xgb": [5, 6, 224, 225, 259, 261, 349, 350, 351, 352, 353, 354], "xgb_model": [349, 352], "xgb_param": [349, 352], "xgboost": [7, 12, 253, 257, 351, 353, 354], "xgboost_predict": [4, 171, 174], "xgboosterror": [4, 171, 172], "xgboosttrain": [4, 12, 171, 172, 174, 349, 351, 352], "xgboosttrainer_2024": 12, "xgboosttrainer_81312_00000terminated10": 12, "xgboosttrainer_81312_00001terminated10": 12, "xgboosttrainer_81312_00002terminated10": 12, "xgbpredictor": [349, 353, 354], "xing": [35, 39, 66, 70], "xlabel": [329, 333, 336, 340, 342, 344, 346, 349, 353, 355, 359, 361, 362, 369], "xxx": [35, 39, 53, 56, 66, 69, 70], "xxxx": [35, 39, 66, 70], "xxxxx": [28, 30, 35, 39, 40, 43, 47, 53, 59, 66, 70, 75], "xxxxxx": [28, 30, 43, 45, 53, 56], "xxxxxxx": [43, 45, 53, 56], "xxxxxxxx": [28, 30, 43, 45, 53, 56], "xxxxxxxxx": [28, 30], "xxxxxxxxxx": [28, 30], "xxxxxxxxxxxx": [28, 30, 43, 45, 53, 56], "y": [1, 3, 5, 7, 13, 14, 88, 89, 102, 108, 110, 114, 116, 118, 121, 122, 123, 169, 170, 181, 183, 253, 257, 268, 289, 349, 352, 362, 363], "y_pred": [137, 140, 143, 146], "y_prob": [137, 140], "y_test": [4, 171, 174], "y_train": [4, 171, 174], "y_true": [137, 143], "ya": [268, 289], "yaml": [11, 43, 46, 53, 58, 66, 73, 92, 93, 101, 102, 108, 110, 115, 118, 122, 126, 127, 128, 135, 137, 145, 147, 153, 159, 163, 168, 218, 223], "yanke": [268, 287, 289], "yann": [13, 14], "yara": [118, 121], "yard": [268, 289], "yb": [355, 357, 362, 367], "ye": [17, 22, 268, 289], "year": [4, 118, 121, 123, 171, 174, 268, 289, 291, 294, 298], "yellow": [4, 12, 101, 102, 105, 171, 174, 291, 294], "yellow_tripdata_": [4, 171, 174], "yellow_tripdata_2011": [9, 164, 166, 198, 201, 204], "yellow_tripdata_2021": [4, 171, 174], "yelp": [299, 301, 306], "yelp_review_ful": [299, 304], "yepo": [268, 289], "yesterdai": [268, 289], "yet": [291, 298, 342, 343, 344, 362, 363], "yield": [3, 181, 188], "ylabel": [329, 333, 336, 340, 342, 344, 346, 349, 351, 353, 355, 357, 359, 361, 362, 369], "yml": 0, "york": [4, 9, 171, 174, 198, 201, 355, 356], "you": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 100, 101, 102, 103, 105, 106, 107, 108, 110, 111, 114, 115, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 130, 131, 133, 134, 135, 136, 137, 139, 142, 143, 144, 145, 146, 147, 149, 151, 152, 153, 154, 155, 158, 159, 160, 161, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 179, 181, 182, 184, 185, 186, 187, 188, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 211, 212, 213, 214, 215, 216, 218, 219, 223, 226, 229, 230, 231, 232, 233, 234, 235, 236, 237, 244, 250, 251, 252, 253, 254, 257, 259, 260, 261, 263, 267, 268, 274, 277, 280, 285, 287, 288, 289, 291, 293, 294, 295, 296, 297, 298, 299, 301, 304, 305, 306, 309, 316, 320, 325, 328, 331, 333, 338, 339, 340, 344, 346, 351, 352, 357, 358, 359, 364, 365, 366, 369], "young": [291, 294, 298], "your": [0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 22, 25, 26, 28, 29, 30, 31, 32, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 54, 56, 58, 59, 60, 62, 63, 64, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 98, 101, 102, 105, 106, 107, 108, 110, 113, 114, 115, 116, 118, 121, 122, 123, 124, 125, 126, 127, 128, 132, 135, 137, 139, 143, 147, 149, 150, 151, 154, 164, 166, 167, 168, 171, 174, 175, 178, 180, 181, 186, 188, 190, 198, 203, 206, 208, 209, 212, 213, 215, 216, 218, 220, 222, 224, 225, 226, 227, 228, 229, 232, 234, 235, 238, 239, 245, 246, 249, 251, 253, 257, 259, 263, 267, 268, 277, 285, 288, 289, 291, 293, 297, 298, 299, 301, 305, 306, 329, 330, 336, 337, 342, 343, 344, 346, 348, 349, 350, 352, 354, 355, 356, 357, 359, 361, 362, 363, 368, 369, 371], "your_anyscale_org_id": [35, 39], "your_gcp_project_nam": [66, 78], "your_project_id": [35, 38], "yourself": [3, 181, 186, 329, 330], "yr": [268, 289], "ytick": [349, 353], "yunikorn": [24, 26], "yy": [128, 129, 137, 138, 147, 148], "z": [137, 140, 143, 268, 289, 355, 357], "zentropa": [291, 297, 298], "zero": [8, 92, 93, 101, 102, 106, 107, 110, 117, 126, 127, 147, 151, 153, 164, 167, 191, 192, 299, 304, 316, 323, 328, 349, 351, 355, 358, 359, 361, 362, 367, 371], "zero_copy_onli": [349, 352], "zero_grad": [5, 7, 13, 14, 137, 143, 224, 228, 238, 240, 245, 247, 253, 256, 257, 299, 304, 342, 346, 355, 359, 362, 367], "zeros_lik": [355, 359], "zilliz": [8, 191, 192], "zip": [7, 14, 253, 255, 329, 331, 335, 342, 344, 348, 349, 353, 362, 364], "zip_ref": [342, 344], "zipfil": [342, 344], "zone": [9, 17, 22, 43, 45, 53, 56, 66, 71, 147, 151, 198, 201], "zprofil": [1, 169, 170], "zsh": [66, 69], "zshrc": [66, 69], "zuoma": [268, 289], "zz": [128, 129, 137, 138, 147, 148], "\u03b8": [336, 338, 341], "\u03c0": [88, 89, 336, 338], "\u03f5": [329, 332, 336, 339]}, "titles": ["Ray Enablement Content: Jupyter Book Publishing", "Introduction to Ray: Developer", "Introduction to Ray Core: Getting Started", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "Introduction to Ray Train + PyTorch", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "Introduction to Ray Tune", "Introduction to Ray Data: Industry Landscape", "Introduction to Ray Data: Ray Data + Structured Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "Introduction to Ray Serve with PyTorch", "Introduction to the Ray AI Libraries", "Introduction to Ray Train", "Intro to Ray Tune", "Intro to Ray Data", "Intro to Ray Serve", "Anyscale Administrator Overview", "Anyscale Administrator Overview", "1. What is an Anyscale Cloud?", "2. Cloud Deployment Types", "3. A Demonstrative Example of Resource Creation with AWS EC2", "3.1 IAM Role Definition", "4. Register Anyscale Cloud to Your Cloud Provider", "Deployment Options: Virtual Machines vs. Kubernetes", "Deployment Options: Virtual Machines vs. Kubernetes", "2. Virtual Machines (VM) vs. Kubernetes (K8s)", "3. (Optional) More Kubernetes Deployments Components", "Introduction: Deploy Anyscale Ray on AWS EC2 Instances", "Introduction: Deploy Anyscale Ray on AWS EC2 Instances", "1. Create Anyscale Resources with Terraform", "2. Register the Anyscale Cloud", "3. Test", "4. Cleanup", "5. Conclusion", "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)", "Introduction: Deploy Anyscale Ray on GCP Compute Engine Instances (GCE)", "Prerequisites", "1. Installation", "2. Create Anyscale Resources with Terraform", "3. Register the Anyscale Cloud", "4. Test", "5. Cleanup", "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster", "Introduction: Deploy Anyscale Ray on A New AWS EKS Cluster", "1. Create Anyscale Resources with Terraform", "2. Install Kubernetes Components", "3. Register the Anyscale Cloud", "4. Install the Anyscale Operator", "5. Verify the Installation", "6. Test", "7. Clean up", "8. Conclusion", "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster", "Introduction: Deploy Anyscale Ray on An Existing AWS EKS Cluster", "Prerequisites", "1. Create Anyscale Resources with Terraform", "2. Attach Required IAM Policies to Your existing EKS\u2019s Node Role", "3. Install Kubernetes Components", "4. Register the Anyscale Cloud", "5. Install the Anyscale Operator", "6. Verify the Installation", "7. Test", "8. Troubleshooting", "9. Clean up", "10. Conclusion", "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster", "Introduction: Deploy Anyscale Ray on A New Google Kubernetes Engine (GKE) Cluster", "Prerequisites", "1. Installation", "2. Create Anyscale Resources with Terraform", "3. Troubleshooting GPU Availability", "4. kubectl Configuration", "5. Install NGINX Ingress Controller", "6. (Optional) Upgrade Anyscale Dependencies", "7. Register the Anyscale Cloud", "8. Install the Anyscale Operator", "8. Test", "9. Cleanup", "Welcome to Anyscale Administration", "101 \u2014 Introduction to Anyscale Workspaces", "101 \u2014 Introduction to Anyscale Workspaces", "101 \u2013 Developing Application with Anyscale", "101 \u2013 Developing Application with Anyscale", "101 \u2013 Compute Configs and Execution Environments in Anyscale", "101 \u2013 Compute Configs and Execution Environments in Anyscale", "101 \u2013 Storage Options in the Anyscale Platform", "101 \u2013 Storage Options in the Anyscale Platform", "101 \u2013 Debug and Monitor Your Anyscale Application", "101 \u2013 Debug and Monitor Your Anyscale Application", "101 \u2013 Introduction to Anyscale Jobs", "101 \u2013 Introduction to Anyscale Jobs", "101 \u2013  Introduction to Anyscale Services", "101 \u2013  Introduction to Anyscale Services", "101 \u2013 Collaboration on Anyscale", "101 \u2013 Collaboration on Anyscale", "101 - Anyscale Organization and Cloud Setup", "101 - Anyscale Organization and Cloud Setup", "\ud83d\udccc Overview of Structure", "\ud83e\udde0 Summary", "Last Updated 6/19", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "Introduction to Ray Serve LLM: Foundations of Large Language Model Serving", "What is LLM Serving?", "Key Concepts and Optimizations", "Challenges in LLM Serving", "Ray Serve LLM + Anyscale Architecture", "Getting Started with Ray Serve LLM", "Key Takeaways", "Deploy a Medium-Sized LLM with Ray Serve LLM", "Deploy a Medium-Sized LLM with Ray Serve LLM", "Overview: Why Medium-Sized Models?", "Setting up Ray Serve LLM", "Local Deployment &amp; Inference", "Deploying to Anyscale Services", "Advanced Topics: Monitoring &amp; Optimization", "Summary &amp; Outlook", "Advanced LLM Features with Ray Serve LLM", "Advanced LLM Features with Ray Serve LLM", "Overview: Advanced Features Preview", "Example: Deploying LoRA Adapters", "Example: Getting Structured JSON Output", "Example: Setting up Tool Calling", "How to Choose an LLM?", "Conclusion: Next Steps", "Multi-modal AI pipeline", "Multi-modal AI pipeline", "Batch inference", "Batch inference", "Data ingestion", "Batch embeddings", "Ray Data", "Data storage", "Monitoring and Debugging", "Production jobs", "Similar images", "Distributed training", "Distributed training", "Preprocess", "Model", "Batching", "Model registry", "Training", "Ray Train", "Production Job", "Evaluation", "Online serving", "Online serving", "Deployments", "Application", "Ray Serve", "Observability", "Production services", "CI/CD", "Observability Introduction", "Observability Introduction", "Observability Overview", "Setting Up Local Ray Observability", "Ray and Anyscale Observability Introduction", "Ray and Anyscale Observability Introduction", "Ray Observability", "Anyscale Observability", "Example", "Ray and Anyscale Observability in Detail", "Ray and Anyscale Observability in Detail", "Data Pipeline Observability (Ray Data)", "Web Application Observability (Ray Serve)", "Multi-Actor Ray Serve Tracing Example", "Introduction to Ray: Developer", "Introduction to Ray: Developer", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model", "1. Overview of the Ray AI Libraries", "2. Quick end-to-end example", "Introduction to Ray Core: Getting Started", "Introduction to Ray Core: Getting Started", "0. Overview", "1. Creating Remote Functions", "2. Executing Remote Functions", "4. Putting It All Together", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "Introduction to Ray Core (Advancement): Object store, Tasks, Actors", "1. Object store", "2. Chaining Tasks and Passing Data", "3. Task retries", "4. Task Runtime Environments", "5. Resource allocation and management", "6. Nested Tasks", "7. Pattern: Pipeline data processing and waiting for results", "8. Ray Actors", "Introduction to Ray Data: Industry Landscape", "Introduction to Ray Data: Industry Landscape", "The Compute Layer", "The Orchestration Layer", "Distributed Computing Frameworks", "Data Processing with Ray Data", "Ray Serve", "Introduction to Ray Data: Ray Data + Structured Data", "Introduction to Ray Data: Ray Data + Structured Data", "0. What is Ray Data?", "2. Loading Data", "3. Transforming Data", "4. Writing Data", "5. Data Operations: Shuffling, Grouping and Aggregation", "6. When to use Ray Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "Intro to Ray Data:  Ray Data + Unstructured Data", "1. When to Consider Ray Data", "2. How to work with Ray Data", "3. Loading data", "3. Lazy execution mode", "4. Transforming data", "5. Stateful transformations with Ray Actors", "6. Materializing data", "7. Data Operations: grouping, aggregation, and shuffling", "8. Persisting data", "9. Ray Data in production", "Introduction to Ray Serve with PyTorch", "Introduction to Ray Serve with PyTorch", "1. When to Consider Ray Serve", "2. Overview of Ray Serve", "3. Implement an image classification service", "4. Development workflow", "\ud83d\udcda 01 \u00b7 Introduction to Ray Train", "\ud83d\udcda 01 \u00b7 Introduction to Ray Train", "01 \u00b7 Imports", "04 \u00b7 Define ResNet-18 Model for MNIST", "05 \u00b7 Define the Ray Train Loop (DDP per-worker)", "06 \u00b7 Define <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop_config</span></code>", "07 \u00b7 Configure Scaling with <code class=\"docutils literal notranslate\"><span class=\"pre\">ScalingConfig</span></code>", "08 \u00b7 Wrap the Model with <code class=\"docutils literal notranslate\"><span class=\"pre\">prepare_model()</span></code>", "09 \u00b7 Build the DataLoader with <code class=\"docutils literal notranslate\"><span class=\"pre\">prepare_data_loader()</span></code>", "10 \u00b7 Report Training Metrics", "11 \u00b7 Save Checkpoints and Report Metrics", "14 \u00b7 Create the <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code>", "16 \u00b7 Inspect the Training Results", "18 \u00b7 Load a Checkpoint for Inference", "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data", "\ud83d\udd04 02 \u00b7 Integrating Ray Train with Ray Data", "01 \u00b7 Define Training Loop with Ray Data", "02 \u00b7 Build DataLoader from Ray Data", "03 \u00b7 Prepare Dataset for Ray Data", "05 \u00b7 Define Image Transformation", "07 \u00b7 Configure <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code> with Ray Data", "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train", "\ud83d\udee1\ufe0f 03 \u00b7 Fault Tolerance in Ray Train", "01 \u00b7 Modify Training Loop to Enable Checkpoint Loading", "02 \u00b7 Save Full Checkpoint with Extra State", "04 \u00b7 Launch Fault-Tolerant Training", "05 \u00b7 Manual Restoration from Checkpoints", "07 \u00b7 Clean Up Cluster Storage", "\ud83c\udf89 Wrapping Up &amp; Next Steps", "Introduction to Ray Tune", "Introduction to Ray Tune", "1. Loading the data", "2. Starting out with vanilla PyTorch", "3. Hyperparameter tuning with Ray Tune", "4. Ray Tune in Production", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "Introduction to Ray Train: Ray Train + PyTorch Lightning", "1. When to use Ray Train", "2. Single GPU Training with PyTorch Lightning", "3. Distributed Training with Ray Train and PyTorch Lightning", "4. Ray Train in Production", "Batch Inference with Ray Data\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb <strong>Launch Locally</strong>: You can run this notebook locally.\ud83d\ude80 <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)This example shows how to do batch inference with Ray Data.Batch inference with Ray Data enables you to efficiently generate predictions from machine learning models on large datasets by processing multiple data points at once. Instead of running inference on one row at a time, which can be slow and resource-inefficient, batch inference leverages vectorized computation and parallelism to maximize throughput. This is especially useful when working with modern deep learning models, which are optimized for batch processing on CPUs, GPUs, or Apple Silicon devices.The typical workflow begins by loading your dataset\u2014such as a public dataset from Hugging Face\u2014into a Ray Dataset. Ray Data can automatically partition the data for parallel processing, or you can repartition it explicitly to control the number of data blocks. Once the data is loaded, you define a callable class (such as a text embedding model) that loads the machine learning model in its constructor and implements a <code class=\"docutils literal notranslate\"><span class=\"pre\">__call__</span></code> method to process each batch. Ray Data\u2019s <code class=\"docutils literal notranslate\"><span class=\"pre\">map_batches</span></code> API is then used to apply this callable to each batch of data, with options to control concurrency and resource allocation (e.g., number of GPUs).This approach allows you to spin up multiple concurrent model instances, each processing different batches of data in parallel. The result is a significant speedup in inference time, especially for large datasets. After inference, you can materialize the results, inspect the output, and shut down the Ray cluster to free up resources. Batch inference with Ray Data is scalable, flexible, and integrates seamlessly with modern ML workflows, making it a powerful tool for production and research environments alike.", "Batch Inference with Ray Data\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb <strong>Launch Locally</strong>: You can run this notebook locally.\ud83d\ude80 <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)This example shows how to do batch inference with Ray Data.Batch inference with Ray Data enables you to efficiently generate predictions from machine learning models on large datasets by processing multiple data points at once. Instead of running inference on one row at a time, which can be slow and resource-inefficient, batch inference leverages vectorized computation and parallelism to maximize throughput. This is especially useful when working with modern deep learning models, which are optimized for batch processing on CPUs, GPUs, or Apple Silicon devices.The typical workflow begins by loading your dataset\u2014such as a public dataset from Hugging Face\u2014into a Ray Dataset. Ray Data can automatically partition the data for parallel processing, or you can repartition it explicitly to control the number of data blocks. Once the data is loaded, you define a callable class (such as a text embedding model) that loads the machine learning model in its constructor and implements a <code class=\"docutils literal notranslate\"><span class=\"pre\">__call__</span></code> method to process each batch. Ray Data\u2019s <code class=\"docutils literal notranslate\"><span class=\"pre\">map_batches</span></code> API is then used to apply this callable to each batch of data, with options to control concurrency and resource allocation (e.g., number of GPUs).This approach allows you to spin up multiple concurrent model instances, each processing different batches of data in parallel. The result is a significant speedup in inference time, especially for large datasets. After inference, you can materialize the results, inspect the output, and shut down the Ray cluster to free up resources. Batch inference with Ray Data is scalable, flexible, and integrates seamlessly with modern ML workflows, making it a powerful tool for production and research environments alike.", "Batch Inference with Ray Data", "Batch Inference with Ray Data", "Architecture", "Architecture", "Architecture Overview", "Load a datasetLoad a dataset from hugging face or local and convert into Ray Dataset. A Ray cluster automatically initialized on local or on Anyscale platform. You can also use <strong>ray.init()</strong> To explicitly create or connect to an existing Ray cluster.https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html#ray.init# load a Hugging Face datasethf_dataset = load_dataset(\u201ccardiffnlp/tweet_eval\u201d, \u201csentiment\u201d, split=\u201dtrain\u201d)# Convert the Hugging Face dataset to a Ray Datasetds = ray.data.from_huggingface(hf_dataset).repartition(2) # repartition to 2 blocks for parallel processing. Not necessary if already partitioned due to the size of the dataset.", "Load a datasetLoad a dataset from hugging face or local and convert into Ray Dataset. A Ray cluster automatically initialized on local or on Anyscale platform. You can also use <strong>ray.init()</strong> To explicitly create or connect to an existing Ray cluster.https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html#ray.init# load a Hugging Face datasethf_dataset = load_dataset(\u201ccardiffnlp/tweet_eval\u201d, \u201csentiment\u201d, split=\u201dtrain\u201d)# Convert the Hugging Face dataset to a Ray Datasetds = ray.data.from_huggingface(hf_dataset).repartition(2) # repartition to 2 blocks for parallel processing. Not necessary if already partitioned due to the size of the dataset.", "Loading a Dataset", "Batch Inference ClassMany machine learning models are optimized for processing a batch of inputs at once. When working with a large dataset, there could be many batches of data. Instead of loading machine learning models repeatedly to run each batch of data, you want to spin up a number of actor processes that are <strong>initialized once</strong> with your model <strong>and reused</strong> to process multiple batches. To implement this, you can use the <code class=\"docutils literal notranslate\"><span class=\"pre\">map_batches</span></code> API with a \u201cCallable\u201d class method that implements:- <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code>: Initialize any expensive state.- <code class=\"docutils literal notranslate\"><span class=\"pre\">__call__</span></code>: Perform the stateful transformation.In this example, a lightweight sentence transformer model, <strong>all-MiniLM-L6-v2</strong> is used to generate embeddings of text data.", "Batch Inference ClassMany machine learning models are optimized for processing a batch of inputs at once. When working with a large dataset, there could be many batches of data. Instead of loading machine learning models repeatedly to run each batch of data, you want to spin up a number of actor processes that are <strong>initialized once</strong> with your model <strong>and reused</strong> to process multiple batches. To implement this, you can use the <code class=\"docutils literal notranslate\"><span class=\"pre\">map_batches</span></code> API with a \u201cCallable\u201d class method that implements:- <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code>: Initialize any expensive state.- <code class=\"docutils literal notranslate\"><span class=\"pre\">__call__</span></code>: Perform the stateful transformation.In this example, a lightweight sentence transformer model, <strong>all-MiniLM-L6-v2</strong> is used to generate embeddings of text data.", "Defining the Batch Inference Class", "Create a batch data and call the modelDefine a Ray Data map_batches function to embed text using the SentenceTransformer model. This function will be applied to each batch of data in the Ray Data dataset. It will take a batch of sentences, encode them into embeddings, and return the batch with the embeddings added.Showcasing two options of to do batch inference based on if the ray cluster has have GPU nodes or if it has just CPU nodes. The second option also works on a local ray cluster on an Apple Silicon Mac with MPS.# setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPSworker_device = \u201ccpu\u201d # or \u201ccuda\u201d if you have a nvidia gpu on worker nodes# batch_size should be set based on VRAM if worker_device == \u201ccuda\u201d: # if you have a nvidia gpu on worker nodes    # adjust batch_size based on the VRAM available on the GPU    ds = ds.map_batches(TextEmbedder, num_gpus=1, concurrency=2, batch_size=64) # 2 nodes with 1 GPU eachelse:    ds = ds.map_batches(TextEmbedder, concurrency=2, batch_size=64) # either cpu or mps (on a mac)", "Create a batch data and call the modelDefine a Ray Data map_batches function to embed text using the SentenceTransformer model. This function will be applied to each batch of data in the Ray Data dataset. It will take a batch of sentences, encode them into embeddings, and return the batch with the embeddings added.Showcasing two options of to do batch inference based on if the ray cluster has have GPU nodes or if it has just CPU nodes. The second option also works on a local ray cluster on an Apple Silicon Mac with MPS.# setting manually so that code works on ray clusters with both CPU or GPU workers, or on a local mac with MPSworker_device = \u201ccpu\u201d # or \u201ccuda\u201d if you have a nvidia gpu on worker nodes# batch_size should be set based on VRAM if worker_device == \u201ccuda\u201d: # if you have a nvidia gpu on worker nodes    # adjust batch_size based on the VRAM available on the GPU    ds = ds.map_batches(TextEmbedder, num_gpus=1, concurrency=2, batch_size=64) # 2 nodes with 1 GPU eachelse:    ds = ds.map_batches(TextEmbedder, concurrency=2, batch_size=64) # either cpu or mps (on a mac)", "Creating a Data Batch and Calling the Model", "Run inference on the entire datasetExecute and materialize this dataset into object store memory. This operation will trigger execution of the lazy transformations performed on this dataset. The embedding model \u2018TextEmbedder\u2019 in map_batches() is called on the entire dataset.# Run inference on the entire dataset# Note that this does not mutate the original Dataset.materialized_ds = ds.materialize()# metadata after inferenceprint(\u2018** Original dataset:\u2019, ds)print(\u2018\\n** Materialized dataset:\u2019, materialized_ds)# Show a few rows of the materialized dataset with embeddingsmaterialized_ds.show(3)", "Run inference on the entire datasetExecute and materialize this dataset into object store memory. This operation will trigger execution of the lazy transformations performed on this dataset. The embedding model \u2018TextEmbedder\u2019 in map_batches() is called on the entire dataset.# Run inference on the entire dataset# Note that this does not mutate the original Dataset.materialized_ds = ds.materialize()# metadata after inferenceprint(\u2018** Original dataset:\u2019, ds)print(\u2018\\n** Materialized dataset:\u2019, materialized_ds)# Show a few rows of the materialized dataset with embeddingsmaterialized_ds.show(3)", "Running inference on the entire dataset", "Data Processing and ML examples with Ray", "Batch Inference with Ray Data", "Architecture", "Load a dataset", "Batch Inference Class", "Create a batch data and call the model", "Run inference on the entire dataset", "Data Processing with Ray Data", "Data Processing and ML examples with Ray", "Data Processing with Ray Data", "Library Imports", "Convert to Ray Dataset", "Filter Ray Dataset", "Join Two Ray Datasets", "Preprocessing with a Tokenizer", "Distributed training with Ray Train, PyTorch and Hugging Face", "Data Processing and ML examples with Ray", "Distributed training with Ray Train, PyTorch and Hugging Face", "1. Architecture", "3. Metrics Setup", "4. Training function per worker", "5. Main Training Function", "6. Start Training", "Online Model Serving with Ray Serve\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb <strong>Launch Locally</strong>: You can run this notebook locally.\ud83d\ude80 <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)Model serving is the process of deploying machine learning models to production so that they can be accessed and used by applications or users. It involves creating an API or interface that allows users to send requests to the modeland receive predictions in response. There are several libraries and frameworks available for model serving, each with its own features and capabilities. In this notebook, we showcase Ray Serve and FastAPI to deploy a sentiment analysismachine learning (ML) model.", "Online Model Serving with Ray Serve\u00a9 2025, Anyscale. All Rights Reserved\ud83d\udcbb <strong>Launch Locally</strong>: You can run this notebook locally.\ud83d\ude80 <strong>Launch on Cloud</strong>: Think about running this notebook on a Ray Cluster (Click here to easily start a Ray cluster on Anyscale)Model serving is the process of deploying machine learning models to production so that they can be accessed and used by applications or users. It involves creating an API or interface that allows users to send requests to the modeland receive predictions in response. There are several libraries and frameworks available for model serving, each with its own features and capabilities. In this notebook, we showcase Ray Serve and FastAPI to deploy a sentiment analysismachine learning (ML) model.", "Online Model Serving with Ray Serve", "Architecture### Import librariesIn addition to ray and serve, we also import FastAPI to create webservice and Hugging Face transformers to download ML models.# Import ray serve and FastAPI librariesimport rayfrom ray import servefrom fastapi import FastAPI# library for pre-trained modelsfrom transformers import pipeline", "Architecture### Import librariesIn addition to ray and serve, we also import FastAPI to create webservice and Hugging Face transformers to download ML models.# Import ray serve and FastAPI librariesimport rayfrom ray import servefrom fastapi import FastAPI# library for pre-trained modelsfrom transformers import pipeline", "Architecture Overview", "FastAPI webservice and deploy a modelFastAPI is used to create a webservice \u2018app\u2019 to accept HTTP requests.MySentimentModel class loads the ML model and defines <em>predict</em> function for online inference. &#64;serve.deployment decorator defines the Ray Serve deployment.<em>&#64;app.get()</em> is used to create a GET \u2018/predict\u2019 route. Similarly, &#64;app.post() can be used POST requests. See https://docs.ray.io/en/latest/serve/http-guide.html for more details.In this example, <em>application_logic()</em> function is used to define a sample transformation or business logic that can be applied before sending the input to the ML model for inference. See inline comments for further explanation.### Scaling deployment<em>num_replicas</em> parameter sets the number of instances of the deployment. FastAPI and RayServe automatically load balances to send requests to each instance. There are more options to set the <em>accelerator_type</em> to GPU and even use fractional GPUs. See configuration options here: https://docs.ray.io/en/latest/serve/configure-serve-deployment.html .", "FastAPI webservice and deploy a modelFastAPI is used to create a webservice \u2018app\u2019 to accept HTTP requests.MySentimentModel class loads the ML model and defines <em>predict</em> function for online inference. &#64;serve.deployment decorator defines the Ray Serve deployment.<em>&#64;app.get()</em> is used to create a GET \u2018/predict\u2019 route. Similarly, &#64;app.post() can be used POST requests. See https://docs.ray.io/en/latest/serve/http-guide.html for more details.In this example, <em>application_logic()</em> function is used to define a sample transformation or business logic that can be applied before sending the input to the ML model for inference. See inline comments for further explanation.### Scaling deployment<em>num_replicas</em> parameter sets the number of instances of the deployment. FastAPI and RayServe automatically load balances to send requests to each instance. There are more options to set the <em>accelerator_type</em> to GPU and even use fractional GPUs. See configuration options here: https://docs.ray.io/en/latest/serve/configure-serve-deployment.html .", "Building a FastAPI Web Service and Deploying a Model", "Online Model Serving with Ray Serve", "Deploy the modelserve.run(MySentimentModel.bind()) # Bind the deployment to the Ray Serve runtimeDeploymentHandle(deployment=\u2019MySentimentModel\u2019)", "Deploy the modelserve.run(MySentimentModel.bind()) # Bind the deployment to the Ray Serve runtimeDeploymentHandle(deployment=\u2019MySentimentModel\u2019)", "Simulate Client: Send test requestsWe use <em>requests</em> library to send HTTP requests to the deployed model.Note: if you encounter any errors with serve not able to start, most likely it is due to previous instance of serve not being shutdown properly. Restart the notebook or see towards the end of notebook to see how to gracefully shutdown ray serve and the ray cluster.import requests # used to send HTTP requests to the deployed model# Query the deployed modelresponse = requests.get(\u201dhttp://localhost:8000/predict\u201d, params={\u201ctext\u201d: \u201cI love Ray Serve!\u201d})print(response.json())  # Should print the sentiment analysis result{\u2018text\u2019: \u2018i love ray serve!\u2019, \u2018sentiment\u2019: [{\u2018label\u2019: \u2018POSITIVE\u2019, \u2018score\u2019: 0.9998507499694824}]}", "Deploying Our Model and Testing it", "Shutdown the Ray Serve instances and Ray Cluster# stop ray serveserve.shutdown()  # Shutdown Ray Serve when done, ray cluster will still be runningray.shutdown()  # Shutdown Ray cluster", "Shutdown the Ray Serve instances and Ray Cluster# stop ray serveserve.shutdown()  # Shutdown Ray Serve when done, ray cluster will still be runningray.shutdown()  # Shutdown Ray cluster", "Shutdown and Summary", "Data Processing and ML examples with Ray", "Online Model Serving with Ray Serve", "Architecture", "FastAPI webservice and deploy a model", "Simulate Client: Send test requests", "04-d1 Generative computer-vision pattern with Ray Train", "04-d1 Generative computer-vision pattern with Ray Train", "1. Imports and setup", "8. Pixel diffusion LightningModule", "9. Ray Train <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop</span></code> (Lightning + Ray integration)", "12. Resume from latest checkpoint", "13. Reverse diffusion sampler", "04-d2 Diffusion-Policy Pattern with Ray Train", "04-d2 Diffusion-Policy Pattern with Ray Train", "1. Imports and setup", "4. DiffusionPolicy LightningModule", "5. Distributed Train loop with checkpointing", "8. Reverse diffusion helper", "04e Recommendation system pattern with Ray Train", "04e Recommendation system pattern with Ray Train", "1. Imports", "7. Define matrix factorization model", "8. Define Ray Train loop (with validation, checkpointing, and Ray-managed metrics)", "11. Resume training from checkpoint", "12. Inference: recommend top-N items for a user", "04b Tabular workload pattern with Ray Train", "04b Tabular workload pattern with Ray Train", "1. Imports", "8. Define the Ray Train worker loop (Arrow-based, memory-efficient)", "12. Confusion matrix visualization", "15. Continue training from the latest checkpoint", "04c Time-Series workload pattern with Ray Train", "04c Time-Series workload pattern with Ray Train", "1. Imports", "9. PositionalEncoding and Transformer model", "10. Ray Train training loop (with teacher forcing)", "13. Resume training from checkpoint", "14. Inference helper \u2014 Ray Data batch predictor on GPU", "04a Computer-vision pattern with Ray Train", "04a Computer-vision pattern with Ray Train", "1. Imports", "6. Custom <code class=\"docutils literal notranslate\"><span class=\"pre\">Food101Dataset</span></code> for Parquet", "10. Helper: Ray-prepared DataLoaders", "11. <code class=\"docutils literal notranslate\"><span class=\"pre\">train_loop_per_worker</span></code>", "12. Launch distributed training with <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code>", "13. Plot training and validation loss curves", "14. Demonstrate fault-tolerant resumption", "15. Batch inference with Ray Data", "Ray Enablement Content"], "titleterms": {"": [53, 57, 84, 85, 155, 158, 265, 266, 278, 279, 281, 282, 336, 337, 349, 350, 355, 356], "0": [2, 9, 17, 18, 175, 177, 198, 200, 224, 234, 317, 319], "01": [224, 225, 226, 238, 240, 245, 247, 252], "02": [224, 226, 238, 239, 241, 245, 248, 252], "03": [224, 226, 238, 242, 245, 246, 248, 252], "04": [224, 227, 238, 242, 245, 249, 329, 330, 336, 337], "04a": [362, 363], "04b": [349, 350], "04c": [355, 356], "04e": [342, 343], "05": [224, 228, 238, 243, 245, 250], "06": [224, 229, 238, 243, 245, 250], "07": [224, 230, 238, 244, 245, 251], "08": [224, 231, 238, 244], "09": [224, 232], "1": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 22, 24, 25, 26, 28, 30, 35, 38, 39, 43, 45, 46, 53, 56, 58, 66, 69, 70, 86, 87, 90, 91, 92, 93, 100, 101, 102, 105, 106, 107, 108, 110, 112, 116, 118, 124, 155, 158, 168, 169, 170, 171, 173, 174, 175, 178, 180, 181, 183, 186, 187, 189, 198, 200, 206, 208, 212, 213, 215, 218, 220, 253, 255, 259, 261, 262, 263, 278, 279, 299, 302, 329, 331, 336, 337, 338, 342, 344, 349, 351, 355, 357, 362, 364], "10": [5, 53, 65, 224, 233, 329, 331, 333, 336, 341, 342, 346, 349, 352, 355, 359, 362, 364, 366], "100k": [342, 344], "101": [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 100, 329, 331, 362, 363, 364], "11": [224, 234, 329, 333, 342, 347, 349, 352, 355, 359, 362, 367], "12": [224, 234, 329, 334, 342, 348, 349, 353, 355, 359, 362, 368], "128": [278, 279, 280], "13": [224, 234, 329, 335, 342, 348, 349, 353, 355, 360, 362, 369], "14": [224, 235, 329, 335, 342, 348, 349, 353, 355, 361, 362, 370], "15": [224, 235, 329, 335, 349, 354, 355, 361, 362, 371], "16": [224, 236, 349, 354, 355, 361, 362, 371], "17": [224, 236, 349, 354, 362, 371], "18": [224, 227, 237], "19": [100, 224, 237], "2": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 22, 24, 26, 28, 31, 35, 38, 39, 43, 46, 53, 57, 58, 66, 69, 70, 86, 87, 90, 91, 100, 101, 102, 105, 106, 107, 108, 110, 116, 118, 124, 155, 158, 168, 169, 170, 171, 174, 175, 179, 180, 181, 184, 187, 189, 198, 201, 206, 209, 210, 212, 213, 215, 218, 221, 253, 256, 259, 262, 263, 272, 273, 278, 279, 299, 302, 329, 331, 336, 337, 338, 342, 344, 349, 351, 355, 357, 362, 364], "20": [224, 237], "2025": [265, 266, 307, 308], "3": [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 24, 26, 27, 28, 32, 35, 40, 43, 46, 47, 53, 58, 66, 71, 86, 87, 100, 101, 102, 105, 106, 107, 108, 110, 112, 116, 118, 124, 155, 158, 169, 170, 171, 174, 175, 179, 181, 185, 187, 198, 202, 206, 210, 211, 215, 218, 222, 253, 257, 259, 262, 263, 281, 282, 299, 303, 329, 331, 336, 337, 338, 342, 344, 349, 351, 355, 357, 362, 364], "30": [355, 357], "4": [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 22, 23, 24, 26, 27, 28, 33, 35, 41, 43, 46, 48, 53, 58, 59, 66, 72, 86, 87, 101, 102, 105, 106, 108, 110, 116, 118, 124, 155, 158, 169, 170, 171, 174, 175, 180, 181, 186, 198, 203, 206, 212, 218, 223, 253, 258, 259, 262, 263, 264, 299, 304, 329, 331, 336, 339, 342, 344, 349, 351, 355, 357, 362, 364], "5": [1, 3, 4, 5, 6, 9, 10, 13, 14, 15, 17, 22, 28, 34, 35, 42, 43, 49, 53, 60, 66, 73, 110, 116, 169, 170, 171, 174, 181, 187, 198, 204, 206, 213, 259, 263, 299, 305, 329, 331, 336, 340, 342, 344, 349, 351, 355, 357, 362, 364], "6": [1, 3, 4, 5, 6, 9, 10, 13, 17, 22, 43, 50, 53, 61, 66, 74, 100, 169, 170, 171, 174, 181, 188, 198, 205, 206, 214, 259, 263, 299, 306, 329, 331, 336, 340, 342, 344, 349, 351, 355, 357, 362, 365], "64": [278, 279], "7": [1, 3, 5, 6, 9, 10, 13, 17, 22, 43, 51, 53, 62, 66, 75, 169, 170, 181, 189, 198, 205, 206, 215, 259, 263, 299, 306, 329, 331, 336, 340, 342, 345, 349, 351, 355, 357, 362, 365], "70b": [110, 112], "8": [1, 3, 5, 9, 10, 13, 17, 22, 43, 52, 53, 63, 66, 76, 77, 169, 170, 181, 190, 198, 205, 206, 216, 299, 306, 329, 332, 336, 341, 342, 346, 349, 352, 355, 357, 359, 362, 365], "8000": [317, 319], "9": [5, 10, 53, 64, 66, 78, 206, 217, 329, 333, 336, 341, 342, 346, 349, 352, 355, 358, 362, 365], "9998507499694824": [317, 319], "A": [17, 21, 43, 44, 66, 67, 272, 273], "For": 372, "If": [278, 279, 281, 282], "In": [1, 8, 169, 170, 191, 192, 265, 266, 275, 276, 281, 282, 307, 308, 313, 314], "It": [1, 2, 169, 170, 175, 180, 278, 279, 281, 282, 307, 308], "No": [126, 127], "Not": [272, 273], "On": [8, 10, 191, 196, 206, 212], "The": [8, 90, 91, 101, 102, 104, 191, 192, 193, 194, 265, 266, 278, 279, 281, 282], "There": [307, 308, 313, 314, 321, 322], "These": [118, 120], "To": [24, 26, 272, 273, 275, 276], "With": [307, 308], "__call__": [265, 266, 275, 276], "__init__": [275, 276], "abl": [317, 319], "about": [2, 3, 15, 175, 180, 181, 186, 265, 266, 307, 308], "accelerator_typ": [313, 314], "accept": [307, 308, 313, 314], "access": [5, 6, 13, 24, 26, 259, 263, 307, 308], "accomplish": [110, 117, 118, 125], "across": [307, 308], "action": [336, 337, 341], "activ": [1, 5, 6, 13, 169, 170, 259, 263], "actor": [3, 10, 15, 168, 181, 182, 190, 206, 213, 224, 237, 265, 266, 275, 276], "ad": [0, 278, 279, 329, 330], "adapt": [118, 121], "add": [1, 169, 170], "addit": [126, 127, 155, 158, 307, 308, 310, 311], "adjust": [278, 279], "admin": 372, "administr": [17, 18, 79], "advanc": [3, 16, 110, 116, 118, 119, 120, 125, 181, 182], "after": [265, 266, 281, 282, 283], "aggreg": [9, 10, 15, 198, 204, 206, 215], "ai": [4, 8, 12, 126, 127, 171, 172, 173, 191, 193, 372], "alert": [164, 167], "align": [118, 124], "alik": [265, 266], "all": [2, 9, 10, 175, 180, 198, 204, 206, 215, 265, 266, 275, 276, 307, 308, 355, 361], "alloc": [3, 181, 187, 265, 266], "allow": [265, 266, 307, 308], "alreadi": [272, 273], "also": [272, 273, 278, 279, 281, 282, 307, 308, 310, 311], "altern": [101, 102, 105], "an": [3, 4, 11, 12, 16, 17, 19, 53, 54, 84, 85, 110, 115, 118, 124, 171, 172, 174, 181, 183, 218, 222, 272, 273, 278, 279, 307, 308, 336, 341], "analysi": [317, 319, 321, 322], "analysismachin": [307, 308], "ani": [275, 276, 317, 319], "annot": 14, "anti": [2, 175, 180], "anyscal": [17, 18, 19, 22, 23, 24, 25, 26, 28, 29, 30, 31, 35, 36, 39, 40, 43, 44, 45, 47, 48, 53, 54, 56, 59, 60, 66, 67, 70, 74, 75, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 100, 101, 102, 107, 110, 115, 159, 160, 162, 164, 165, 167, 265, 266, 272, 273, 307, 308, 329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363, 372], "apach": [8, 191, 192], "api": [35, 38, 66, 69, 168, 265, 266, 272, 273, 275, 276, 281, 282, 307, 308], "app": [307, 308, 313, 314], "appl": [265, 266, 278, 279], "appli": [238, 243, 265, 266, 278, 279, 281, 282, 313, 314], "applic": [8, 11, 82, 83, 88, 89, 100, 147, 150, 164, 167, 191, 195, 218, 221, 307, 308], "application_log": [313, 314], "approach": [118, 120, 265, 266], "ar": [17, 20, 265, 266, 275, 276, 278, 279, 307, 308, 313, 314, 321, 322, 329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363], "architectur": [17, 22, 101, 102, 107, 168, 265, 266, 268, 271, 286, 299, 302, 307, 308, 310, 311, 312, 316, 326], "architecturearchitectur": [269, 270], "area": 13, "argument": [3, 181, 183], "arm": [1, 169, 170], "arrow": [8, 191, 192, 349, 352], "artifact": [355, 361], "assist": [118, 121, 123], "assumpt": [155, 158], "attach": [53, 57], "auroc": 13, "authent": [35, 38, 66, 69], "autom": [90, 91], "automat": [245, 248, 265, 266, 272, 273, 313, 314], "autosc": [10, 16, 206, 213, 307, 308], "autoscal": [43, 46, 53, 58, 321, 322], "avail": [3, 66, 71, 168, 181, 187, 278, 279, 281, 282, 307, 308], "avoid": [281, 282, 283], "aw": [17, 21, 28, 29, 43, 44, 46, 53, 54, 58], "awai": [224, 225, 238, 239, 245, 246, 329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363], "b": [265, 266], "backend": [24, 26], "balanc": [43, 46, 53, 58, 307, 308, 313, 314, 349, 351], "base": [9, 10, 15, 198, 204, 206, 215, 278, 279, 280, 342, 343, 349, 352], "batch": [3, 4, 8, 9, 10, 15, 101, 102, 105, 128, 129, 131, 137, 141, 171, 174, 181, 189, 191, 195, 198, 204, 206, 215, 265, 266, 267, 268, 275, 276, 277, 278, 279, 280, 281, 282, 285, 288, 289, 349, 351, 353, 355, 357, 361, 362, 365, 371, 372], "batch_siz": [278, 279, 280, 281, 282], "befor": [313, 314], "begin": [265, 266], "being": [317, 319], "benchmark": [118, 124], "benefit": [118, 121, 122, 123], "best": [281, 282, 329, 335], "better": [307, 308], "bind": [317, 318], "block": [9, 10, 15, 198, 204, 206, 210, 215, 265, 266, 272, 273], "book": 0, "both": [278, 279, 280], "bound": [3, 181, 187], "breakdown": [110, 113], "build": [0, 5, 13, 224, 232, 238, 241, 315, 349, 352], "built": [307, 308], "busi": [313, 314], "cach": [101, 102, 105], "california": [349, 351], "call": [2, 118, 123, 175, 180, 268, 278, 279, 280, 281, 282, 289], "callabl": [265, 266, 275, 276, 281, 282], "caller": [278, 279], "can": [265, 266, 272, 273, 275, 276, 278, 279, 281, 282, 307, 308, 313, 314, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "capabl": [307, 308], "car": [118, 122], "cardiffnlp": [272, 273], "case": [1, 118, 124, 169, 170, 281, 282], "caus": [3, 181, 189], "cd": [147, 154], "chain": [3, 181, 184], "challeng": [8, 101, 102, 106, 191, 195], "characterist": [92, 93], "check": [329, 331, 355, 357, 362, 364], "checkpoint": [5, 6, 13, 224, 234, 237, 245, 247, 248, 250, 259, 263, 329, 334, 335, 336, 340, 342, 346, 347, 349, 354, 355, 360], "choic": [307, 308], "choos": [110, 112, 118, 124], "ci": [147, 154], "class": [265, 266, 268, 275, 276, 277, 281, 282, 288, 313, 314, 349, 351], "classif": [11, 218, 222, 349, 350, 362, 363], "classifi": 16, "classmani": [275, 276], "clean": [1, 4, 16, 43, 51, 53, 64, 88, 89, 90, 91, 92, 93, 169, 170, 171, 174, 224, 237, 245, 251, 329, 335, 336, 341, 342, 348, 349, 354, 362, 371], "cleanup": [28, 33, 35, 42, 66, 78, 355, 361], "cli": 100, "click": [265, 266, 307, 308], "client": [307, 308, 316, 317, 319, 320, 328], "clone": [94, 95, 100], "cloud": [17, 19, 20, 23, 28, 31, 35, 38, 40, 43, 47, 53, 59, 66, 69, 75, 86, 87, 96, 97, 98, 265, 266, 307, 308], "cluster": [3, 43, 44, 46, 53, 54, 58, 66, 67, 86, 87, 155, 158, 181, 187, 245, 251, 265, 266, 268, 272, 273, 278, 279, 280, 281, 282, 283, 290, 299, 306, 307, 308, 316, 317, 319, 321, 322, 323, 328], "code": [4, 118, 121, 171, 174, 278, 279, 280, 307, 308], "collabor": [94, 95, 100], "collison": [281, 282, 283], "command": [35, 39], "comment": [313, 314], "comparison": [110, 112], "compon": [24, 27, 43, 46, 53, 58, 110, 113], "compos": 16, "comput": [1, 8, 13, 35, 36, 84, 85, 169, 170, 191, 193, 195, 265, 266, 307, 308, 329, 330, 362, 363], "concept": [5, 6, 7, 14, 101, 102, 105, 253, 257, 259, 263], "conclus": [28, 34, 43, 52, 53, 65, 118, 125], "concurr": [10, 110, 116, 206, 212, 265, 266, 278, 279], "conda": [1, 169, 170], "config": [84, 85], "configur": [3, 5, 6, 13, 35, 38, 66, 69, 72, 84, 85, 101, 102, 108, 110, 113, 115, 118, 121, 164, 167, 168, 181, 187, 224, 230, 234, 238, 244, 245, 248, 259, 263, 313, 314, 349, 352], "confus": [349, 353], "connect": [272, 273], "consid": [10, 11, 206, 208, 218, 220], "consider": [101, 102, 105, 118, 124], "constraint": [281, 282], "constructor": [265, 266], "contain": [84, 85], "content": [0, 372], "context": [101, 102, 105, 118, 124], "continu": [101, 102, 105, 349, 354], "control": [17, 22, 24, 26, 43, 46, 53, 58, 66, 73, 265, 266, 278, 279], "convert": [272, 273, 274, 281, 282, 291, 295, 298], "core": [2, 3, 8, 175, 176, 181, 182, 191, 196, 272, 273, 372], "correct": [278, 279], "cost": [101, 102, 106, 118, 124], "could": [275, 276, 307, 308], "count": [355, 357], "cours": [0, 1, 169, 170, 245, 252], "cover": [90, 91, 92, 93, 118, 120, 281, 282, 349, 350, 351], "cpu": [265, 266, 278, 279, 280, 281, 282, 349, 353], "creat": [1, 2, 5, 6, 17, 22, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 84, 85, 90, 91, 169, 170, 175, 178, 224, 235, 259, 262, 263, 265, 266, 268, 272, 273, 278, 279, 280, 289, 291, 295, 307, 308, 310, 311, 313, 314, 342, 344], "creation": [17, 21], "cuda": [278, 279], "cursor": [82, 83], "curv": [13, 329, 333, 342, 346, 362, 369], "custom": [0, 9, 10, 15, 16, 17, 22, 198, 204, 206, 215, 362, 365], "cv": 372, "d": [278, 279, 281, 282], "d1": [329, 330], "d2": [336, 337], "dashboard": [88, 89, 164, 166], "data": [3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 24, 26, 128, 130, 132, 133, 164, 166, 171, 172, 174, 181, 184, 189, 191, 192, 193, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 214, 215, 216, 217, 224, 225, 234, 238, 239, 240, 241, 242, 243, 244, 245, 252, 253, 255, 259, 263, 265, 266, 267, 268, 272, 273, 275, 276, 278, 279, 280, 281, 282, 284, 285, 289, 291, 292, 293, 300, 324, 329, 331, 342, 344, 349, 353, 355, 361, 362, 371, 372], "databas": [8, 191, 192], "datafram": [224, 236, 291, 298], "dataload": [5, 6, 224, 232, 238, 241, 259, 262, 299, 304, 355, 357, 362, 365, 366], "dataset": [5, 9, 12, 13, 15, 198, 201, 224, 226, 238, 242, 265, 266, 268, 269, 270, 272, 273, 274, 275, 276, 278, 279, 281, 282, 283, 287, 290, 291, 294, 295, 296, 297, 336, 337, 338, 342, 344, 349, 351, 355, 357], "datasetd": [272, 273], "datasetexecut": [281, 282], "datasethf_dataset": [272, 273], "datasetload": [272, 273], "ddp": [224, 225, 228], "de": [329, 330], "debug": [88, 89, 128, 134], "decod": [101, 102, 104, 329, 331], "decor": [313, 314], "deep": [265, 266], "deeper": [7, 14, 253, 257], "default": 14, "defin": [6, 13, 17, 20, 224, 227, 228, 229, 238, 240, 243, 259, 262, 265, 266, 277, 281, 282, 313, 314, 342, 345, 346, 349, 352], "definit": [17, 22], "demand": [101, 102, 106, 355, 356], "demonstr": [17, 21, 281, 282, 362, 370], "depend": [1, 3, 24, 26, 66, 74, 126, 127, 169, 170, 181, 186], "deploi": [24, 25, 27, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 70, 100, 110, 111, 115, 118, 121, 265, 266, 268, 278, 279, 280, 289, 307, 308, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 327], "deploy": [11, 16, 17, 20, 24, 25, 27, 79, 101, 102, 108, 110, 114, 147, 149, 168, 218, 221, 307, 308, 313, 314, 315, 316, 317, 318, 327], "deploymentnum_replica": [313, 314], "descript": [118, 122], "design": [307, 308], "detail": [164, 165, 313, 314], "develop": [1, 11, 82, 83, 100, 126, 127, 168, 169, 170, 218, 223], "devic": [43, 46, 53, 58, 265, 266, 278, 279], "diagnost": [349, 353], "diagram": [269, 270], "dictimport": [269, 270], "differ": [110, 117, 265, 266], "diffus": [6, 259, 262, 329, 330, 332, 335, 336, 337, 341], "diffusionpolici": [336, 339], "digit": [224, 226], "directori": 13, "disabl": 0, "displai": [329, 335], "distribut": [4, 5, 6, 8, 13, 137, 138, 171, 174, 191, 195, 224, 225, 234, 259, 263, 281, 282, 284, 292, 299, 300, 301, 307, 308, 324, 329, 330, 333, 336, 340, 342, 343, 346, 349, 350, 352, 355, 356, 361, 362, 363, 368, 372], "div": [265, 266, 307, 308], "dive": [7, 14, 253, 257], "do": [265, 266, 278, 279], "doc": [272, 273, 307, 308, 313, 314], "doe": [281, 282, 283, 362, 363], "domain": [118, 124], "done": [321, 322], "down": [1, 110, 114, 115, 169, 170, 265, 266], "download": [224, 226, 310, 311], "downscal": [321, 322], "dual": [17, 22], "due": [272, 273, 317, 319], "duplic": [94, 95], "dure": [281, 282], "e": [265, 266], "each": [24, 26, 265, 266, 275, 276, 278, 279, 307, 308, 313, 314], "eachels": [278, 279], "easi": [307, 308], "easili": [265, 266, 307, 308], "easilydeploi": [307, 308], "ec2": [17, 21, 28, 29], "ef": [17, 22], "effici": [265, 266, 281, 282, 349, 352], "either": [278, 279], "ek": [43, 44, 53, 54, 57], "emb": [278, 279], "embed": [128, 131, 265, 266, 275, 276, 278, 279, 281, 282, 283, 342, 343], "embeddingsmaterialized_d": [281, 282], "en": [272, 273, 307, 308, 313, 314], "enabl": [0, 35, 38, 66, 69, 110, 116, 245, 247, 265, 266, 281, 282, 372], "encod": [278, 279, 329, 331, 342, 344, 362, 364], "encount": [317, 319], "end": [4, 12, 171, 174, 317, 319], "endpoint": 168, "engin": [8, 35, 36, 66, 67, 101, 102, 107, 191, 193], "ensembl": [4, 171, 174], "ensur": [278, 279], "entir": [265, 266, 268, 281, 282, 283, 290], "environ": [1, 3, 84, 85, 169, 170, 181, 186, 265, 266, 307, 308, 336, 337], "error": [265, 266, 268, 281, 282, 283, 290, 317, 319], "errorsgpu": [281, 282], "especi": [265, 266], "evalu": [137, 146, 349, 352], "even": [313, 314], "exampl": [0, 1, 4, 12, 13, 17, 21, 24, 27, 79, 110, 112, 117, 118, 121, 122, 123, 159, 163, 168, 169, 170, 171, 172, 174, 265, 266, 275, 276, 278, 279, 284, 292, 300, 313, 314, 324], "execut": [0, 2, 9, 10, 15, 24, 26, 84, 85, 175, 179, 198, 202, 206, 211, 278, 279, 281, 282], "exercis": [7, 14, 253, 257], "exist": [17, 22, 53, 54, 57, 272, 273, 307, 308], "expect": [118, 122], "expens": [275, 276], "experi": [12, 14], "explan": [313, 314], "explicitli": [265, 266, 272, 273], "explor": [88, 89], "extern": [24, 26], "extra": [245, 248], "face": [265, 266, 272, 273, 274, 281, 282, 299, 301, 310, 311, 321, 322], "factor": [342, 343, 345], "failur": [3, 181, 189], "failureconfig": [245, 248], "fastapi": [16, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 321, 322, 325, 327], "fault": [245, 246, 249, 252, 362, 370], "featur": [0, 9, 12, 16, 118, 119, 120, 198, 205, 307, 308, 349, 353], "fetch": [3, 181, 189], "few": [281, 282, 283], "file": [9, 10, 15, 86, 87, 110, 115, 198, 204, 206, 215, 349, 351], "filter": [291, 296], "first": [90, 91, 92, 93], "fit": [6, 224, 235, 259, 263], "flask": [307, 308, 309, 316, 325], "flexibl": [265, 266], "flow": [8, 168, 191, 197], "follow": [90, 91, 92, 93], "food": [329, 331, 362, 363, 364], "food101dataset": [362, 365], "forc": [355, 359], "forecast": [355, 356], "forest": [349, 350], "format": [8, 191, 192], "forward": [329, 330], "foundat": [101, 102, 103, 372], "fraction": [3, 16, 181, 187, 313, 314], "framework": [8, 118, 124, 191, 195, 307, 308], "frameworkthat": [307, 308], "free": [265, 266], "from": [238, 241, 245, 250, 265, 266, 272, 273, 281, 282, 321, 322, 329, 334, 335, 336, 341, 342, 344, 347, 349, 354, 355, 360], "from_huggingfac": [272, 273], "full": [245, 248], "function": [2, 8, 17, 19, 175, 178, 179, 191, 193, 278, 279, 299, 304, 305, 313, 314], "further": [313, 314], "g": [265, 266], "gce": [35, 36], "gcp": [35, 36], "gener": [0, 5, 6, 8, 101, 102, 104, 191, 195, 259, 263, 265, 266, 275, 276, 329, 330, 335, 336, 338, 372], "get": [2, 3, 7, 14, 100, 101, 102, 108, 118, 122, 168, 175, 176, 179, 180, 181, 189, 253, 257, 313, 314, 317, 319, 372], "gke": [66, 67], "global": [9, 10, 15, 198, 204, 206, 215], "go": [245, 252, 265, 266], "googl": [35, 38, 66, 67, 69], "gpu": [5, 6, 13, 16, 66, 71, 259, 262, 263, 265, 266, 278, 279, 280, 281, 282, 313, 314, 355, 359, 361], "gracefulli": [317, 319], "grafana": [155, 158], "group": [9, 10, 15, 17, 22, 198, 204, 206, 215], "groupbi": [9, 10, 15, 198, 204, 206, 215], "guid": [313, 314], "ha": [278, 279, 281, 282], "handl": [281, 282, 307, 308], "hardwar": [110, 116, 118, 124], "harm": [2, 175, 180], "have": [278, 279, 307, 308], "head": [155, 158], "headach": [126, 127], "helper": [336, 341, 355, 361, 362, 366], "here": [265, 266, 307, 308, 313, 314], "hf_dataset": [272, 273], "high": [281, 282, 307, 308, 321, 322], "hourli": [355, 357], "how": [0, 9, 10, 12, 17, 20, 24, 26, 110, 117, 118, 124, 198, 200, 206, 209, 224, 225, 265, 266, 278, 279, 281, 282, 317, 319, 329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363], "html": [272, 273, 307, 308, 313, 314], "http": [272, 273, 307, 308, 313, 314, 317, 319], "hug": [265, 266, 272, 273, 274, 281, 282, 299, 301, 310, 311, 321, 322], "huggingfac": [269, 270, 284, 292, 300, 324], "hyperparamet": [4, 7, 14, 171, 174, 253, 257], "i": [8, 9, 17, 19, 101, 102, 104, 110, 115, 191, 196, 197, 198, 200, 265, 266, 275, 276, 278, 279, 281, 282, 307, 308, 309, 313, 314, 316, 317, 319, 321, 322, 325], "iam": [17, 22, 53, 57], "id": [2, 17, 22, 82, 83, 175, 180, 342, 344, 348], "imag": [11, 84, 85, 128, 136, 218, 222, 238, 243, 329, 330, 331, 362, 363, 364, 365], "implement": [4, 11, 16, 171, 172, 218, 222, 265, 266, 275, 276], "import": [224, 226, 265, 266, 268, 269, 270, 271, 286, 291, 294, 299, 302, 307, 308, 310, 311, 312, 316, 317, 319, 326, 329, 331, 336, 338, 342, 344, 349, 351, 353, 355, 357, 362, 364], "improv": [110, 116], "includ": [307, 308], "increas": [321, 322], "index": [307, 308], "industri": [8, 191, 192], "ineffici": [265, 266], "infer": [4, 101, 102, 104, 107, 110, 114, 115, 128, 129, 171, 174, 224, 237, 265, 266, 267, 268, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 288, 290, 313, 314, 342, 343, 348, 349, 353, 354, 355, 361, 362, 371, 372], "inferenceprint": [281, 282], "inform": [307, 308], "infrastructur": [17, 20, 24, 27, 66, 70, 101, 102, 107, 126, 127], "ingest": [128, 130], "ingress": [43, 46, 53, 58, 66, 73], "init": [272, 273], "initi": [272, 273, 275, 276, 291, 294], "inlin": [313, 314], "input": [275, 276, 313, 314, 329, 330, 342, 343, 362, 363], "inspect": [12, 224, 236, 265, 266, 349, 351, 355, 357, 362, 365], "instal": [0, 1, 35, 38, 43, 46, 48, 49, 53, 58, 60, 61, 66, 69, 73, 76, 100, 155, 158, 168, 169, 170, 284, 292, 300, 324], "instanc": [17, 22, 28, 29, 35, 36, 265, 266, 313, 314, 316, 317, 319, 321, 322, 323, 328], "instead": [265, 266, 275, 276], "instruct": [90, 91], "integr": [16, 238, 239, 245, 252, 265, 266, 307, 308, 329, 333], "interfac": [307, 308], "intro": [7, 10, 14, 15, 16, 206, 207, 253, 257], "introduct": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 100, 101, 102, 103, 155, 156, 159, 160, 169, 170, 171, 172, 175, 176, 181, 182, 191, 192, 198, 199, 218, 219, 224, 225, 245, 252, 253, 254, 259, 260], "introductori": 13, "invert": [336, 337], "involv": [307, 308], "io": [3, 181, 187, 272, 273, 307, 308, 313, 314], "irvin": [349, 351], "item": [342, 343, 344, 348], "its": [265, 266, 307, 308], "job": [5, 13, 90, 91, 100, 128, 135, 137, 145, 281, 282, 283], "join": [291, 297, 342, 348], "json": [118, 122, 317, 319], "jupyt": [0, 1, 169, 170], "just": [278, 279, 307, 308, 309, 316, 325], "jvm": [8, 191, 195], "k8": [24, 26], "keep": [281, 282], "kei": [5, 6, 13, 16, 17, 19, 92, 93, 101, 102, 105, 109, 110, 113, 117, 118, 121, 122, 123, 125, 259, 263], "kubectl": [66, 72], "kubernet": [24, 25, 26, 27, 43, 46, 53, 58, 66, 67, 101, 102, 106], "kv": [101, 102, 105], "l6": [275, 276], "label": [317, 319, 362, 363], "lake": [8, 191, 192], "lakehous": [8, 191, 192], "landscap": [8, 191, 192], "languag": [101, 102, 103], "larg": [3, 101, 102, 103, 181, 187, 265, 266, 275, 276, 281, 282, 307, 308], "last": [100, 245, 250], "latenc": [101, 102, 106], "latest": [272, 273, 307, 308, 313, 314, 329, 334, 349, 354], "launch": [1, 5, 13, 80, 81, 110, 114, 115, 155, 158, 164, 167, 169, 170, 224, 235, 238, 244, 245, 249, 265, 266, 307, 308, 329, 333, 336, 340, 342, 346, 355, 359, 362, 368], "layer": [8, 24, 26, 191, 192, 193, 194], "lazi": [10, 206, 211, 278, 279, 281, 282], "learn": [8, 79, 100, 118, 120, 121, 122, 123, 191, 193, 224, 225, 238, 239, 245, 246, 265, 266, 275, 276, 281, 282, 307, 308, 329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363, 372], "legend": [1, 169, 170], "level": [3, 181, 183], "leverag": [265, 266, 281, 282], "lib": 372, "librari": [4, 12, 171, 172, 173, 265, 266, 268, 271, 286, 291, 294, 299, 302, 307, 308, 310, 311, 312, 316, 317, 319, 326], "librariesimport": [269, 270, 310, 311], "librariesin": [310, 311], "lifecycl": [5, 224, 234], "lightn": [6, 259, 260, 262, 263, 329, 333, 372], "lightningmodul": [329, 332, 336, 339], "lightweight": [275, 276], "like": [307, 308, 317, 319], "limit": [10, 206, 212], "lite": [362, 363], "ll": [79, 118, 120, 224, 225, 238, 239, 245, 246], "llama": [110, 112], "llm": [101, 102, 103, 104, 106, 107, 108, 110, 111, 113, 116, 118, 119, 121, 124, 372], "load": [5, 6, 7, 9, 10, 13, 14, 15, 43, 46, 53, 58, 198, 201, 206, 210, 224, 237, 238, 242, 245, 247, 253, 255, 259, 263, 265, 266, 268, 272, 273, 274, 275, 276, 281, 282, 287, 291, 294, 307, 308, 313, 314, 329, 331, 342, 344, 349, 351, 355, 357, 362, 364], "load_dataset": [269, 270, 272, 273], "loader": 13, "local": [0, 1, 13, 82, 83, 86, 87, 110, 114, 155, 158, 168, 169, 170, 265, 266, 272, 273, 278, 279, 280, 307, 308], "localhost": [317, 319], "log": [88, 89, 164, 166, 167], "logic": [313, 314], "loop": [2, 5, 6, 13, 175, 180, 224, 228, 238, 240, 245, 247, 259, 262, 336, 340, 342, 346, 349, 352, 355, 359], "lora": [118, 121], "loss": [329, 333, 336, 340, 342, 346, 355, 359, 362, 369], "love": [317, 319], "mac": [1, 169, 170, 278, 279, 280], "machin": [8, 24, 25, 26, 191, 193, 265, 266, 275, 276, 278, 279, 281, 282, 283, 307, 308], "machinerai": [281, 282], "mai": [307, 308], "main": [299, 305], "make": [265, 266, 307, 308], "manag": [1, 3, 24, 26, 101, 102, 106, 169, 170, 181, 187, 307, 308, 342, 346], "mani": [3, 181, 189, 275, 276, 278, 279, 321, 322], "manual": [245, 250, 278, 279, 280], "map_batch": [265, 266, 275, 276, 278, 279, 281, 282], "materi": [10, 15, 206, 214, 265, 266, 281, 282, 283], "materialized_d": [281, 282], "matrix": [342, 343, 345, 349, 353], "matter": [118, 120, 122, 123], "max_model_len": [110, 116], "maxim": [265, 266], "medium": [110, 111, 112, 113], "memori": [8, 101, 102, 106, 191, 192, 265, 266, 268, 278, 279, 281, 282, 283, 290, 349, 352], "memorydb": [17, 22], "metadata": [281, 282, 283], "method": [265, 266, 275, 276], "metric": [5, 13, 88, 89, 164, 166, 167, 224, 233, 234, 236, 299, 303, 342, 346], "migrat": [5, 6, 13, 259, 263, 329, 330, 342, 343, 349, 350, 355, 356, 362, 363], "min": [355, 357], "mini": [349, 351], "miniforg": [1, 169, 170], "minilm": [275, 276], "ml": [265, 266, 281, 282, 284, 292, 300, 307, 308, 310, 311, 313, 314, 324], "mnist": [13, 224, 226, 227], "modal": [126, 127], "mode": [9, 10, 15, 198, 202, 206, 211], "model": [4, 5, 6, 7, 13, 14, 101, 102, 103, 105, 110, 112, 113, 116, 118, 124, 137, 140, 142, 171, 172, 174, 224, 227, 231, 253, 257, 259, 262, 263, 265, 266, 268, 275, 276, 278, 279, 280, 281, 282, 289, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 325, 327, 342, 343, 345, 349, 352, 355, 358, 362, 363], "modeland": [307, 308], "modeldefin": [278, 279], "modelfastapi": [313, 314], "modelrespons": [317, 319], "modelserv": [317, 318], "modelsfrom": [310, 311], "modern": [265, 266, 281, 282], "modifi": [245, 247], "modul": [245, 252], "monitor": [88, 89, 110, 116, 128, 134], "more": [5, 6, 15, 24, 27, 110, 116, 118, 121, 122, 123, 125, 259, 263, 307, 308, 313, 314, 321, 322], "most": [317, 319], "move": [265, 266], "movi": [342, 348], "movielen": [342, 344], "mp": [278, 279, 280, 281, 282], "mpsworker_devic": [278, 279], "multi": [126, 127, 168, 355, 356], "multimod": 372, "multipl": [13, 265, 266, 275, 276, 307, 308], "mutat": [281, 282, 283], "mysentimentmodel": [313, 314, 317, 318], "n": [281, 282, 342, 348], "navig": 0, "necessari": [272, 273], "need": [24, 26, 278, 279, 307, 308], "nest": [3, 181, 188], "new": [0, 1, 12, 17, 22, 43, 44, 66, 67, 168, 169, 170, 329, 330], "next": [101, 102, 109, 110, 117, 118, 125, 155, 158, 245, 252, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "nginx": [43, 46, 53, 58, 66, 73], "node": [53, 57, 84, 85, 155, 158, 278, 279, 355, 356], "nois": [329, 330], "normal": [336, 338, 355, 357], "note": [2, 3, 5, 6, 10, 175, 180, 181, 186, 187, 189, 206, 210, 213, 224, 234, 259, 263, 281, 282, 283, 317, 319], "notebook": [0, 1, 12, 82, 83, 90, 91, 92, 93, 169, 170, 265, 266, 281, 282, 283, 291, 293, 307, 308, 317, 319, 321, 322], "now": [155, 158], "npfrom": [269, 270], "num_gpu": [278, 279], "num_replica": [321, 322], "number": [265, 266, 275, 276, 313, 314, 321, 322], "numpi": [269, 270], "nvidia": [43, 46, 53, 58, 278, 279], "nyc": [12, 355, 356, 357], "o": [1, 169, 170], "object": [3, 24, 26, 86, 87, 100, 181, 182, 183, 189, 265, 266, 281, 282, 329, 330, 336, 337, 342, 343], "observ": [147, 152, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 372], "onc": [3, 181, 189, 265, 266, 275, 276], "one": [265, 266, 355, 357], "ones": [307, 308], "onli": [164, 167, 224, 234], "onlin": [147, 148, 307, 308, 309, 313, 314, 316, 325, 372], "oper": [9, 10, 15, 24, 25, 43, 48, 53, 60, 66, 76, 198, 204, 206, 215, 281, 282], "optim": [101, 102, 105, 106, 110, 116, 265, 266, 275, 276], "option": [1, 17, 22, 24, 25, 27, 43, 46, 53, 58, 66, 74, 86, 87, 100, 155, 158, 169, 170, 265, 266, 278, 279, 313, 314, 321, 322, 349, 351], "orchestr": [8, 101, 102, 107, 191, 194], "order": [9, 10, 15, 198, 204, 206, 215], "organ": [96, 97, 98], "origin": [281, 282, 283], "other": [110, 117, 281, 282, 283, 307, 308], "our": [13, 110, 112, 320], "out": [7, 253, 256, 265, 266, 268, 281, 282, 283, 290], "outlin": [265, 266, 267, 268, 285, 291, 293, 299, 301, 307, 308, 309, 316, 325], "outlook": [15, 17, 18, 24, 27, 110, 117], "output": [0, 118, 122, 265, 266], "over": [8, 191, 196, 355, 357], "overview": [1, 2, 4, 5, 6, 11, 12, 13, 16, 17, 18, 96, 98, 100, 110, 112, 118, 120, 155, 157, 168, 169, 170, 171, 173, 175, 177, 218, 221, 259, 262, 271, 312], "own": [307, 308], "packag": [155, 158], "panda": [291, 298], "parallel": [2, 5, 6, 13, 101, 102, 105, 110, 116, 175, 180, 224, 225, 234, 259, 263, 265, 266, 272, 273, 278, 279, 281, 282, 307, 308], "param": [317, 319], "paramet": [278, 279, 313, 314], "parquet": [329, 331, 342, 344, 349, 351, 355, 357, 362, 364, 365], "part": [90, 91, 92, 93], "partit": [265, 266, 272, 273], "pass": [3, 181, 183, 184], "passeng": [355, 357], "path": 100, "pattern": [2, 3, 175, 180, 181, 183, 189, 329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363, 372], "pendulum": [336, 337, 338], "per": [224, 228, 299, 304, 342, 343], "perform": [275, 276, 278, 279, 281, 282], "persist": [10, 13, 15, 206, 216, 224, 234, 329, 331, 362, 364], "phase": [101, 102, 104], "pip": [3, 181, 186], "pipelin": [3, 100, 110, 116, 126, 127, 164, 166, 181, 189, 310, 311], "pixel": [329, 332], "plane": [17, 22], "platform": [86, 87, 272, 273], "plot": [329, 333, 336, 340, 342, 346, 355, 359, 362, 369], "plugin": [43, 46, 53, 58], "point": [13, 265, 266, 342, 344], "polici": [53, 57, 329, 330, 336, 337, 341, 372], "posit": [317, 319], "positionalencod": [355, 358], "post": [313, 314, 349, 354], "power": [265, 266], "practic": [118, 124, 281, 282], "pre": [310, 311, 312], "predict": [5, 6, 12, 224, 237, 259, 263, 265, 266, 307, 308, 313, 314, 317, 319], "predictor": [355, 361], "prefil": [101, 102, 104], "prepar": [238, 242, 355, 357, 362, 366], "prepare_data_load": [224, 232], "prepare_model": [224, 231], "preprocess": [137, 139, 291, 298], "prerequisit": [1, 28, 29, 35, 37, 43, 44, 53, 55, 66, 68, 79, 110, 114, 155, 158, 164, 165, 168, 169, 170], "preview": [118, 120], "previou": [317, 319], "print": [281, 282, 317, 319], "problem": [329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363], "process": [3, 8, 101, 102, 104, 118, 124, 181, 189, 191, 195, 196, 265, 266, 272, 273, 275, 276, 278, 279, 281, 282, 284, 291, 292, 293, 300, 307, 308, 324, 329, 330, 372], "product": [5, 6, 7, 9, 10, 13, 15, 16, 126, 127, 128, 135, 137, 145, 147, 153, 198, 205, 206, 217, 253, 258, 259, 264, 265, 266, 307, 308], "profil": 168, "project": [94, 95, 96, 98], "prometheu": [155, 158], "properli": [317, 319], "provid": [17, 23, 43, 44, 307, 308], "public": [265, 266, 281, 282], "publish": 0, "purpos": [8, 17, 19, 191, 195], "put": [2, 175, 180], "python": [307, 308], "pytorch": [5, 6, 7, 11, 13, 14, 218, 219, 253, 256, 257, 259, 260, 262, 263, 284, 292, 299, 300, 301, 307, 308, 324, 355, 357, 372], "qualiti": [118, 124], "quantiz": [110, 116], "queri": [101, 102, 108, 317, 319, 320], "quick": [4, 171, 174, 355, 357], "rai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 24, 26, 28, 29, 35, 36, 43, 44, 53, 54, 66, 67, 88, 89, 100, 101, 102, 103, 107, 108, 110, 111, 113, 114, 118, 119, 121, 128, 132, 137, 144, 147, 151, 155, 158, 159, 160, 161, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 180, 181, 182, 189, 190, 191, 192, 196, 197, 198, 199, 200, 205, 206, 207, 208, 209, 213, 217, 218, 219, 220, 221, 224, 225, 228, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 252, 253, 254, 257, 258, 259, 260, 261, 263, 264, 265, 266, 267, 268, 272, 273, 274, 278, 279, 280, 281, 282, 283, 284, 285, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 306, 307, 308, 309, 310, 311, 312, 313, 314, 316, 317, 318, 319, 321, 322, 323, 324, 325, 328, 329, 330, 331, 333, 336, 337, 340, 342, 343, 344, 346, 349, 350, 351, 352, 353, 355, 356, 357, 359, 361, 362, 363, 366, 371, 372], "random": [355, 357], "rang": [307, 308], "rank": [224, 234, 342, 343], "rate": [342, 343, 344], "rayfrom": [310, 311], "rayimport": [269, 270], "rayserv": [313, 314], "read": [9, 10, 15, 198, 204, 206, 215], "real": [336, 338], "rec": 372, "recap": 12, "receiv": [307, 308], "recommend": [1, 100, 118, 124, 169, 170, 342, 343, 348], "reduc": [110, 116, 281, 282], "regist": [17, 23, 28, 31, 35, 40, 43, 47, 53, 59, 66, 75, 168], "registr": 168, "registri": [137, 142], "regress": [4, 171, 172], "relat": [110, 112, 117], "remot": [2, 5, 175, 178, 179], "remov": [355, 361], "repartit": [265, 266, 272, 273], "repeatedli": [275, 276], "replica": [11, 110, 116, 218, 221, 321, 322], "report": [5, 13, 224, 233, 234], "repositori": 100, "request": [3, 110, 114, 168, 181, 187, 307, 308, 313, 314, 316, 317, 319, 320, 328], "requestsw": [317, 319], "requir": [1, 35, 38, 43, 44, 53, 57, 66, 69, 101, 102, 106, 118, 124, 155, 158, 169, 170, 278, 279], "resampl": [355, 357], "research": [265, 266], "reserv": [265, 266, 307, 308], "resiz": [329, 331, 362, 364], "resnet": [224, 227], "resourc": [3, 10, 17, 20, 21, 28, 30, 35, 39, 43, 45, 53, 56, 66, 70, 94, 95, 101, 102, 109, 110, 117, 118, 125, 181, 187, 206, 212, 213, 265, 266], "respons": [307, 308, 317, 319], "rest": [307, 308], "restart": [317, 319], "restor": [245, 250], "result": [2, 3, 5, 6, 13, 175, 179, 181, 189, 224, 236, 259, 263, 265, 266, 317, 319, 355, 361], "resum": [245, 250, 329, 334, 342, 347, 355, 360], "resumpt": [362, 370], "retri": [3, 181, 185, 245, 248], "retriev": 168, "return": [278, 279], "reus": [275, 276], "revers": [329, 330, 335, 336, 337, 341], "right": [265, 266, 307, 308], "roc": 13, "role": [17, 22, 53, 57, 96, 98], "rout": [313, 314], "row": [9, 10, 15, 198, 204, 206, 215, 265, 266, 278, 279, 280, 281, 282, 283], "run": [5, 6, 12, 24, 26, 35, 39, 90, 91, 110, 115, 164, 166, 224, 237, 259, 263, 265, 266, 268, 275, 276, 278, 279, 280, 281, 282, 283, 290, 307, 308, 317, 318, 355, 361, 362, 371], "runconfig": [224, 234], "runningrai": [321, 322], "runtim": [3, 181, 186], "runtimedeploymenthandl": [317, 318], "s3": [17, 22], "same": [281, 282, 283], "sampl": [168, 224, 226, 313, 314, 329, 330, 335, 336, 337, 341], "sampler": [329, 335], "saniti": [329, 331, 355, 357, 362, 364], "save": [13, 224, 234, 245, 248], "scalabl": [101, 102, 106, 265, 266, 281, 282, 307, 308], "scale": [5, 6, 13, 110, 116, 224, 230, 259, 263, 265, 266, 268, 278, 279, 280, 281, 282, 289, 307, 308, 313, 314, 315, 316, 321, 322, 327, 336, 337], "scalingconfig": [224, 230], "schedul": [13, 90, 91], "scikit": [307, 308], "score": [317, 319], "seamlessli": [265, 266], "second": [278, 279, 291, 295], "secur": [17, 22], "see": [313, 314, 317, 319], "select": [118, 124], "send": [110, 114, 307, 308, 313, 314, 316, 317, 319, 320, 328], "sentenc": [265, 266, 269, 270, 275, 276, 278, 279], "sentence_transform": [269, 270], "sentencetransform": [265, 266, 269, 270, 278, 279, 281, 282], "sentiment": [272, 273, 307, 308, 317, 319, 321, 322], "sequenc": [355, 356], "seri": [355, 356, 372], "serv": [0, 4, 8, 11, 16, 101, 102, 103, 104, 106, 107, 108, 110, 111, 113, 114, 118, 119, 121, 147, 148, 151, 164, 167, 168, 171, 172, 174, 191, 197, 218, 219, 220, 221, 307, 308, 309, 310, 311, 312, 313, 314, 316, 317, 318, 319, 321, 322, 323, 325, 328, 372], "servefrom": [310, 311], "server": [1, 169, 170], "serveserv": [321, 322], "servic": [11, 16, 92, 93, 100, 110, 115, 147, 153, 218, 222, 307, 308, 315], "set": [1, 14, 110, 113, 115, 118, 123, 155, 158, 169, 170, 278, 279, 280, 313, 314], "setup": [96, 97, 155, 158, 168, 299, 303, 329, 330, 331, 336, 338, 342, 343, 349, 350, 355, 356, 362, 363], "sever": [307, 308], "share": [86, 87, 329, 335, 342, 348], "should": [278, 279, 280, 317, 319], "show": [265, 266, 278, 279, 281, 282, 283], "showcas": [278, 279, 307, 308], "shuffl": [9, 10, 15, 198, 204, 206, 215, 329, 331], "shut": [1, 110, 114, 115, 169, 170, 265, 266], "shutdown": [102, 108, 268, 281, 282, 283, 290, 291, 298, 299, 306, 307, 308, 316, 317, 319, 321, 322, 323, 328], "sign": 100, "signific": [265, 266], "silicon": [265, 266, 278, 279], "similar": [128, 136], "similarli": [313, 314], "simpl": [1, 164, 166, 169, 170, 307, 308], "simpli": [307, 308], "simul": [307, 308, 316, 317, 319, 320, 328], "singl": [5, 6, 13, 259, 262], "size": [110, 111, 112, 113, 117, 272, 273, 278, 279, 349, 351], "slide": [355, 357], "slow": [265, 266], "so": [278, 279, 280, 307, 308], "solv": [329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363], "spark": [8, 191, 196], "specif": [2, 10, 175, 180, 206, 212, 213], "speedup": [265, 266], "spin": [265, 266, 275, 276], "split": [0, 272, 273, 329, 331, 336, 338, 342, 344, 349, 351, 362, 365], "stabl": [6, 259, 262], "start": [2, 7, 14, 92, 93, 100, 101, 102, 108, 155, 158, 175, 176, 253, 256, 257, 265, 266, 299, 306, 307, 308, 317, 319, 349, 352, 372], "state": [10, 15, 206, 213, 245, 248, 275, 276, 336, 337], "step": [12, 101, 102, 108, 109, 110, 117, 118, 125, 155, 158, 245, 252, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "still": [321, 322], "stop": [321, 322, 323], "storag": [5, 13, 24, 26, 86, 87, 128, 133, 224, 234, 245, 251, 329, 335, 342, 348], "store": [3, 24, 26, 86, 87, 181, 182, 183, 265, 266, 281, 282], "strategi": [110, 116], "stream": [8, 191, 195], "structur": [8, 9, 79, 96, 98, 118, 122, 168, 191, 192, 198, 199], "style": [336, 337], "submit": [90, 91], "subnet": [17, 22], "summar": [281, 282], "summari": [17, 22, 96, 99, 110, 117, 265, 266, 268, 283, 290, 291, 298, 299, 306, 316, 323, 328], "summaryin": [321, 322], "summarythi": [281, 282], "support": [17, 20, 307, 308], "sy": 372, "system": [342, 343], "tab": [88, 89], "tabl": 14, "tabular": [349, 350, 372], "take": [224, 225, 238, 239, 245, 246, 278, 279, 329, 330, 335, 336, 337, 341, 342, 343, 348, 349, 350, 354, 355, 356, 361, 362, 363, 371], "take_batch": [278, 279], "takeawai": [101, 102, 109, 110, 117, 118, 125], "task": [3, 12, 118, 124, 181, 182, 184, 185, 186, 187, 188], "taxi": [12, 355, 356, 357], "teacher": [355, 359], "team": 100, "templat": [110, 117], "tensor": [329, 330], "tensorflow": [307, 308], "termin": [155, 158], "terraform": [28, 30, 35, 39, 43, 45, 53, 56, 66, 70], "test": [0, 28, 32, 35, 41, 43, 50, 53, 62, 66, 77, 84, 85, 278, 279, 280, 307, 308, 316, 317, 319, 320, 328], "text": [101, 102, 104, 265, 266, 275, 276, 278, 279, 281, 282, 317, 319], "textembedd": [278, 279, 281, 282], "tfvar": [35, 39, 66, 70], "thatcan": [307, 308], "thei": [307, 308], "them": [278, 279], "thi": [1, 12, 90, 91, 92, 93, 169, 170, 265, 266, 275, 276, 278, 279, 281, 282, 283, 307, 308, 313, 314, 321, 322, 329, 330, 335, 336, 337, 341, 342, 343, 348, 349, 350, 354, 355, 356, 361, 362, 363, 371], "think": [265, 266, 307, 308], "through": [90, 91, 265, 266, 281, 282], "throughput": [265, 266, 281, 282], "throw": [281, 282], "time": [265, 266, 355, 356, 372], "tip": 12, "titl": [342, 348], "togeth": [2, 175, 180], "token": [291, 298, 299, 304], "toler": [245, 246, 249, 252, 362, 370], "too": [3, 181, 189, 281, 282], "tool": [118, 123, 265, 266], "top": [3, 181, 183, 307, 308, 342, 348], "topic": [110, 116, 118, 125], "torch": [6, 259, 262], "torchfrom": [269, 270], "torchtrain": [6, 13, 224, 235, 238, 244, 259, 263, 329, 333, 336, 340, 362, 368], "toward": [317, 319], "trace": [164, 167, 168], "traffic": [307, 308, 321, 322], "train": [4, 5, 6, 13, 137, 138, 143, 144, 171, 172, 174, 224, 225, 228, 233, 234, 235, 236, 238, 239, 240, 244, 245, 246, 247, 249, 250, 252, 259, 260, 261, 262, 263, 264, 272, 273, 284, 292, 299, 300, 301, 304, 305, 306, 310, 311, 312, 324, 329, 330, 331, 333, 336, 337, 340, 341, 342, 343, 344, 346, 347, 349, 350, 351, 352, 354, 355, 356, 359, 360, 361, 362, 363, 365, 368, 369, 372], "train_loop": [329, 333], "train_loop_config": [224, 229], "train_loop_per_work": [362, 367], "trainer": [224, 235, 349, 352], "transform": [9, 10, 15, 198, 202, 206, 212, 213, 238, 243, 275, 276, 281, 282, 310, 311, 313, 314, 355, 356, 358, 362, 365], "transformersfrom": [269, 270], "trigger": [281, 282], "tripl": [342, 343], "troubleshoot": [53, 63, 66, 71], "tune": [4, 7, 14, 171, 172, 174, 253, 254, 257, 258, 372], "tupl": [336, 337], "tutori": [245, 252], "tweet_ev": [272, 273], "two": [101, 102, 104, 278, 279, 291, 297], "type": [17, 20, 118, 122, 269, 270, 349, 351], "typic": [265, 266], "uci": [349, 351], "ul": [265, 266, 307, 308], "under": 13, "univers": [349, 351], "unstructur": [10, 206, 207], "until": [278, 279], "up": [1, 4, 14, 16, 43, 51, 53, 64, 88, 89, 90, 91, 92, 93, 100, 110, 113, 115, 118, 123, 155, 158, 169, 170, 171, 174, 224, 237, 245, 251, 252, 265, 266, 275, 276, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "upcom": [9, 198, 205], "updat": [13, 100, 307, 308], "upgrad": [66, 74, 110, 116], "uri": [342, 344], "us": [1, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 22, 24, 26, 110, 116, 118, 121, 122, 123, 124, 164, 167, 169, 170, 171, 172, 191, 196, 198, 200, 204, 205, 206, 215, 224, 225, 253, 257, 259, 261, 265, 266, 272, 273, 275, 276, 278, 279, 281, 282, 307, 308, 309, 313, 314, 316, 317, 319, 321, 322, 325, 329, 330, 336, 337, 342, 343, 344, 349, 350, 355, 356, 362, 363], "usag": 0, "user": [96, 98, 168, 307, 308, 342, 343, 344, 348], "uv": [1, 169, 170], "v": [8, 24, 25, 26, 191, 196, 197], "v2": [275, 276], "val": [329, 331, 336, 340], "valid": [342, 344, 346, 349, 351, 355, 359, 362, 365, 369], "valu": [101, 102, 105], "vanilla": [4, 7, 171, 174, 253, 256], "vector": [265, 266], "verifi": [1, 43, 49, 53, 61, 169, 170, 349, 354], "version": [307, 308], "view": [224, 236], "viewer": [88, 89], "virtual": [24, 25, 26], "vision": [329, 330, 362, 363, 372], "visual": [13, 14, 224, 226, 237, 329, 331, 342, 344, 349, 351, 353, 355, 357, 361, 362, 364, 371], "vllm": [101, 102, 107], "vm": [24, 26], "vpc": [17, 22], "vram": [278, 279, 280], "vscode": [82, 83], "wai": [307, 308], "wait": [3, 181, 189], "walk": [90, 91, 281, 282], "want": [275, 276], "warehous": [8, 191, 192], "we": [1, 12, 110, 117, 118, 120, 125, 169, 170, 265, 266, 307, 308, 310, 311, 321, 322], "weather": [118, 123], "web": [164, 167, 307, 308, 315], "webservic": [310, 311, 313, 314, 315, 316, 327], "welcom": [1, 79, 169, 170], "well": [307, 308], "what": [8, 9, 17, 19, 24, 26, 79, 101, 102, 104, 110, 115, 117, 118, 120, 125, 155, 158, 191, 196, 197, 198, 200, 224, 225, 238, 239, 245, 246, 307, 308, 309, 316, 325, 329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363], "when": [5, 6, 8, 9, 10, 11, 15, 16, 24, 26, 191, 196, 198, 205, 206, 208, 218, 220, 224, 225, 259, 261, 265, 266, 275, 276, 321, 322], "where": [245, 252, 307, 308, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "which": [24, 26, 265, 266, 307, 308], "why": [1, 8, 100, 101, 102, 106, 110, 112, 118, 120, 121, 122, 123, 169, 170, 191, 196, 197, 307, 308, 309, 316, 325, 329, 330], "wide": [307, 308], "window": [101, 102, 105, 118, 124, 355, 357], "work": [0, 10, 206, 209, 224, 225, 265, 266, 275, 276, 278, 279, 280, 329, 330], "worker": [5, 6, 84, 85, 155, 158, 224, 228, 259, 263, 278, 279, 280, 299, 304, 349, 352], "worker_devic": [278, 279], "workflow": [0, 11, 84, 85, 218, 223, 265, 266, 281, 282, 307, 308], "workload": [24, 26, 96, 98, 164, 166, 307, 308, 329, 330, 336, 337, 342, 343, 349, 350, 355, 356, 362, 363, 372], "workspac": [80, 81, 100], "wrap": [224, 231, 245, 252, 329, 335, 336, 341, 342, 348, 349, 354, 355, 361, 362, 371], "write": [9, 198, 203, 349, 351], "xgboost": [4, 171, 172, 174, 349, 350, 352], "york": 12, "you": [79, 90, 91, 224, 225, 238, 239, 245, 246, 265, 266, 272, 273, 275, 276, 278, 279, 307, 308, 317, 319, 329, 330, 335, 336, 337, 341, 342, 343, 348, 349, 350, 354, 355, 356, 361, 362, 363, 371], "your": [1, 17, 23, 24, 27, 53, 57, 88, 89, 90, 91, 92, 93, 100, 169, 170, 265, 266, 275, 276], "zero": [321, 322]}})